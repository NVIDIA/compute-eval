{"task_id": "CUDA/0", "date": "2025-07-30", "prompt": "Implement a function called `launch` that launches a kernel function named `kernel` with the provided grid and block dimensions using triple chevrons. The x,y,z grid sizes and block sizes will be provided as parameters\nto the `launch` function. Assume that the `kernel` function is already defined. \n\nThe signature of the `kernel` function is\n```cuda\n__global__ void kernel(int *output, const int *input) \n```\n\nThe function signature is \n```cuda\nvoid launch(int gridSizeX, int blockSizeX, int gridSizeY = 1, int blockSizeY = 1, int gridSizeZ = 1, int blockSizeZ = 1)\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n#include <iostream>\n\nusing namespace std;\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n\n__global__ void kernel(int *output, const int *input)\n{\n    int id     = threadIdx.x + blockIdx.x * blockDim.x;\n    output[id] = input[id];\n}\n", "test": "int main() {\nlaunch(4, 1024);\ncudaCheckErrors(\"kernel launch failed\");\nlaunch(4, 32, 4, 32);\ncudaCheckErrors(\"kernel launch failed\");\nlaunch(4, 16, 4, 16, 4, 4);\ncudaCheckErrors(\"kernel launch failed\");\n\n}\n", "example_test": "", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/1", "date": "2025-07-30", "prompt": "Implement a function called `launch` that launches a kernel function named `kernel` with the provided grid and block dimensions using triple chevrons and also allocates dynamic shared memory. The x,y,z grid sizes and block sizes will be provided as parameters\nto the `launch` function. Assume that the `kernel` function is already defined. \n\nThe signature of the `kernel` function is\n```cuda\n__global__ void kernel(int *output, const int *input) \n```\n\nThe function signature is \n```cuda\nvoid launch(int gridSizeX, int blockSizeX, int gridSizeY = 1, int blockSizeY = 1, int gridSizeZ = 1, int blockSizeZ = 1)\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n#include <iostream>\n\nusing namespace std;\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n\n__global__ void kernel(int *output, const int *input)\n{\n    int id     = threadIdx.x + blockIdx.x * blockDim.x;\n    output[id] = input[id];\n}\n\n", "test": "int main() {\nlaunch(4, 256);\ncudaCheckErrors(\"kernel launch failed\");\nlaunch(4, 16, 4, 16);\ncudaCheckErrors(\"kernel launch failed\");\nlaunch(4, 16, 4, 16, 4, 1);\ncudaCheckErrors(\"kernel launch failed\");\n\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/2", "date": "2025-07-30", "prompt": "Implement a function called `launch` that launches a kernel function named `kernel` with the provided grid and block dimensions using triple chevrons, allocates dynamic shared memory and also uses cuda streams. The x,y,z grid sizes and block sizes will be provided as parameters\nto the `launch` function. Assume that the `kernel` function is already defined. \n\nThe signature of the `kernel` function is\n```cuda\n__global__ void kernel(int *output, const int *input) \n```\n\nThe function signature is \n```cuda\nvoid launch(int gridSizeX, int blockSizeX, int gridSizeY = 1, int blockSizeY = 1, int gridSizeZ = 1, int blockSizeZ = 1)\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n#include <iostream>\n\nusing namespace std;\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n__global__ void kernel(int *output, const int *input)\n{\n}\n\n", "test": "int main() {\nlaunch(4, 256);\ncudaCheckErrors(\"kernel launch failed\");\nlaunch(4, 16, 4, 16);\ncudaCheckErrors(\"kernel launch failed\");\nlaunch(4, 16, 4, 16, 4, 1);\ncudaCheckErrors(\"kernel launch failed\");\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/3", "date": "2025-07-30", "prompt": "Implement a function called `launch` that launches a kernel function named `kernel` without using triple chevrons. The x,y,z grid and dimensions will be provided as parameters\nto the `launch` function. Assume that the `kernel` function is already defined. \n\nThe signature of the `kernel` function is\n```cuda\n__global__ void kernel(int *output, const int *input) \n```\n\nThe function signature is \n```cuda\nvoid launch(int gridSizeX, int blockSizeX, int gridSizeY = 1, int blockSizeY = 1, int gridSizeZ = 1, int blockSizeZ = 1)\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n#include <fstream>\n#include <iostream>\n\nusing namespace std;\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n__global__ void kernel(int *output, const int *input)\n{\n    int id     = threadIdx.x + blockIdx.x * blockDim.x;\n    output[id] = input[id];\n}\n\nstd::string trim(const std::string& str) {\n    size_t first = str.find_first_not_of(' ');\n    if (std::string::npos == first) {\n        return str;\n    }\n    size_t last = str.find_last_not_of(' ');\n    return str.substr(first, last - first + 1);\n}\n", "test": "int main() {\n    auto static_test = [] () {\n        const char* path = std::getenv(\"COMPUTE_EVAL_SRC_FILE\");\n        if (path == nullptr) {\n            std::cerr << \"Environment variable not found!\" << std::endl;\n            std::exit(1);\n        }\n\n        std::ifstream file(path);\n        if (!file.is_open()) {\n            std::cerr << \"File not found!\" << std::endl;\n            std::exit(1);\n        }\n\n        std::string line;\n\n        // Skip until the beginning of the completion block\n        while (std::getline(file, line)) {\n\n            if (line.find(\"completion-begin\") != std::string::npos && \n                line.find(\"std::string::npos\") == std::string::npos) {\n                break;\n            }\n        }\n\n        // Search for the CUDA kernel launch API call\n        bool found = false;\n        while (std::getline(file, line)) {\n\n            std::string trimmedLine = trim(line);\n\n            // If the line contains the completion-end marker, stop searching\n            if (trimmedLine.find(\"completion-end\") != std::string::npos) {\n                break;\n            }\n\n            // ignore commented lines\n            if (trimmedLine.find(\"//\") == 0) continue;\n            \n            if (trimmedLine.find(\"cudaLaunchKernelEx\") != std::string::npos) {\n                found = true;\n                break;\n            }\n\n        }\n\n        if (!found) {\n            std::cerr << \"Test failed because the generated code doesn't use CUDA kernel launch API!\" << std::endl;\n            std::exit(1);\n        }\n    };\n\n    auto dynamic_test = [] () {\n        int *output, *input;\n        launch(4, 256);\n        cudaCheckErrors(\"kernel launch failed\");\n        launch(4, 16, 4, 16);\n        cudaCheckErrors(\"kernel launch failed\");\n        launch(4, 16, 4, 16, 4, 1);\n        cudaCheckErrors(\"kernel launch failed\");\n    };\n\n    static_test();\n    dynamic_test();\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/4", "date": "2025-07-30", "prompt": "Implement a function called `launch` that launches a kernel function named `kernel` with thread block clusters and wihout using triple chevrons. The x,y,z grid and dimensions will be provided as parameters\nto the `launch` function. Assume that the `kernel` function is already defined. \n\nThe signature of the `kernel` function is\n```cuda\n__global__ void kernel(int *output, const int *input) \n```\n\nThe function signature is \n```cuda\nvoid launch(int gridSizeX, int blockSizeX, int gridSizeY = 1, int blockSizeY = 1, int gridSizeZ = 1, int blockSizeZ = 1)\n", "cc_flags": "-arch=sm_90a", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n#include <iostream>\n#include <fstream>\n\nusing namespace std;\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                      \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err == cudaErrorInvalidKernelImage || __err == cudaErrorNoKernelImageForDevice) \\\n        {                                                                                     \\\n            fprintf(stderr, \"Invalid GPU architecture: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(200);                                                                       \\\n        }                                                                                       \\\n        else if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n__global__ void kernel(int *output, const int *input)\n{\n\n}\n\nstd::string trim(const std::string& str) {\n    size_t first = str.find_first_not_of(' ');\n    if (std::string::npos == first) {\n        return str;\n    }\n    size_t last = str.find_last_not_of(' ');\n    return str.substr(first, last - first + 1);\n}\n", "test": "int main() {\nauto static_test = [] () {\n        const char* path = std::getenv(\"COMPUTE_EVAL_SRC_FILE\");\n        if (path == nullptr) {\n            std::cerr << \"Environment variable not found!\" << std::endl;\n            std::exit(1);\n        }\n\n        std::ifstream file(path);\n        if (!file.is_open()) {\n            std::cerr << \"File not found!\" << std::endl;\n            std::exit(1);\n        }\n\n        std::string line;\n\n        // Skip until the beginning of the completion block\n        while (std::getline(file, line)) {\n\n            if (line.find(\"completion-begin\") != std::string::npos && \n                line.find(\"std::string::npos\") == std::string::npos) {\n                break;\n            }\n        }\n\n        // Search for the CUDA kernel launch API call\n        bool foundKernelLaunch = false, foundClusterDim = false;\n        while (std::getline(file, line)) {\n\n            std::string trimmedLine = trim(line);\n\n            // If the line contains the completion-end marker, stop searching\n            if (trimmedLine.find(\"completion-end\") != std::string::npos) {\n                break;\n            }\n\n            // ignore commented lines\n            if (trimmedLine.find(\"//\") == 0) continue;\n            \n            if (trimmedLine.find(\"cudaLaunchKernelEx\") != std::string::npos) foundKernelLaunch = true;\n            if (trimmedLine.find(\"cudaLaunchAttributeClusterDimension\") != std::string::npos) foundClusterDim = true;\n\n            if (foundKernelLaunch && foundClusterDim) break;\n\n        }\n\n        if (!foundKernelLaunch) {\n            std::cerr << \"Test failed because the generated code doesn't use CUDA kernel launch API!\" << std::endl;\n            std::exit(1);\n        }\n\n        if (!foundClusterDim) {\n            std::cerr << \"Test failed because the generated code doesn't use cluster dimension attribute!\" << std::endl;\n            std::exit(1);\n        }\n    };\n\n    auto dynamic_test = [] () {\n        int *output, *input;\n        launch(4, 256);\n        cudaCheckErrors(\"kernel launch failed\");\n        launch(4, 16, 4, 16);\n        cudaCheckErrors(\"kernel launch failed\");\n        launch(4, 16, 4, 16, 4, 1);\n        cudaCheckErrors(\"kernel launch failed\");\n    };\n\n    static_test();\n    dynamic_test();\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/5", "date": "2025-07-30", "prompt": "Implement a kernel called `kernel` that has launch bounds set to ensure optimal execution efficiency by limiting the maximum number of threads per block to MAX_THREADS_PER_BLOCK and the minimum number of blocks per multiprocessor to MIN_BLOCKS_PER_MULTIPROCESSOR. \nThis ensures that the kernel runs efficiently on the GPU by maximizing occupancy while maintaining sufficient resources per block.\n\nAssume that the following constants are defined:\n- `MAX_THREADS_PER_BLOCK`: the maximum number of threads that can be assigned to a block\n- `MIN_BLOCKS_PER_MULTIPROCESSOR`: the minimum number of blocks that can be to multiprocessor core.\n\nImplement the kernel function that takes an int* and const int* as parameters.\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n#include <iostream>\n\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n#define cudaCheckSuccess(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err == cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\nstatic const int MAX_THREADS_PER_BLOCK         = 128;\nstatic const int MIN_BLOCKS_PER_MULTIPROCESSOR = 1;\n\n__global__ void kernel(int*, const int*);\n\nvoid launch()\n{\n    int *output, *input;\n    kernel<<<dim3(1, 1, 1), dim3(128, 1, 1)>>>(output, input);\n    cudaCheckErrors(\"kernel launch failed\");\n\n    kernel<<<dim3(1, 1, 1), dim3(2 * MAX_THREADS_PER_BLOCK, 1, 1)>>>(output, input);\n    cudaCheckSuccess(\"kernel launch has should have failed, because the number of threads exceeds the maximum allowed\");\n\n}\n", "test": "int main() {\nlaunch();\n\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/6", "date": "2025-07-30", "prompt": "Implement a kernel called `kernel` that has launch bounds set to ensure optimal execution efficiency by limiting the maximum number of threads per block to MAX_THREADS_PER_BLOCK, the minimum number of blocks per multiprocessor to MIN_BLOCKS_PER_MULTIPROCESSOR and the maximum number of thread blocks in a cluster to MAX_BLOCKS_PER_CLUSTER. \n\nAssume that the following constants are defined:\n- `MAX_THREADS_PER_BLOCK`: the maximum number of threads that can be assigned to a block\n- `MIN_BLOCKS_PER_MULTIPROCESSOR`: the minimum number of blocks that can be to multiprocessor core.\n- `MAX_BLOCKS_PER_CLUSTER`:  the maximum number of thread blocks per cluster\n\nImplement the kernel function that takes an int* and const int* as parameters.\n", "cc_flags": "-arch=sm_90a", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n#include <iostream>\n\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err == cudaErrorInvalidKernelImage || __err == cudaErrorNoKernelImageForDevice) \\\n        {                                                                                     \\\n            fprintf(stderr, \"Invalid GPU architecture: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(200);                                                                       \\\n        }                                              \\\n        else if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n#define cudaCheckSuccess(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err == cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"This should not have succeded: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\nstatic const int MAX_THREADS_PER_BLOCK         = 128;\nstatic const int MIN_BLOCKS_PER_MULTIPROCESSOR = 1;\nstatic const int MAX_BLOCKS_PER_CLUSTER        = 32;\n\n__global__ void kernel(int*, const int*);\n\nvoid launch()\n{\n    int *output, *input;\n    kernel<<<dim3(1, 1, 1), dim3(128, 1, 1)>>>(output, input);\n    cudaCheckErrors(\"kernel launch failed\");\n\n    kernel<<<dim3(1, 1, 1), dim3(2 * MAX_THREADS_PER_BLOCK, 1, 1)>>>(output, input);\n    cudaCheckSuccess(\"kernel launch has should have failed, because the number of threads exceeds the maximum allowed\");\n\n    kernel<<<dim3(MAX_BLOCKS_PER_CLUSTER + 1, 1, 1), dim3(MAX_THREADS_PER_BLOCK, 1, 1)>>>(output, input);\n    cudaCheckSuccess(\"kernel launch has should have failed, because the number of thread blocks per cluster exceeds the maximum allowed\");\n\n}\n", "test": "int main() {\nlaunch();\n\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/7", "date": "2025-07-30", "prompt": "Implement a CUDA kernel called conv2d_kernel that performs a 2-dimensional convolution on the input\nfloat matrix and populates the output matrix with these values. Assume that the input matrix is in\nrow-major order and ensure that the output matrix is populated in row-major order as well. You can\nassume that there is no padding in this convolution and the stride is 1.\n\nThe signature of the function is\n```cuda\n__global__ void conv2d_kernel(float *input, float *output, unsigned int W, unsigned int H, unsigned\nint oW, unsigned int oH)\n```\nwhere W, H are the input width and heights and oW, oH are the output width and height\n\nAssume that the following constants are defined:\n- `MASK_RADIUS`:  the mask radius is the distance from the center of the convolution mask or kernel\nto its outermost elements.\n- `MASK_DIM`: The mask dimension (or kernel dimension) refers to the spatial dimensions or the shape\nof the convolution mask. Defined as 2 * MASK_RADIUS + 1\n\nAdditionally, assume that the mask is stored in constant memory as a 2d array named `mask_c`.\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <chrono>\n#include <iostream>\n#include <random>\nusing namespace std;\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n#define MASK_RADIUS 2\n#define MASK_DIM ((MASK_RADIUS)*2 + 1)\n\n__global__ void conv2d_kernel(float *input, float *output, unsigned int W, unsigned int H,\n                              unsigned int oW, unsigned int oH);\n\n__constant__ float mask_c[MASK_DIM][MASK_DIM];\n\nbool validate_convolution(float mask[][MASK_DIM], float *input, float *output, unsigned int W,\n                          unsigned int H)\n{\n    unsigned int out_W = W - MASK_DIM + 1;   // here we assume there is no padding, and stride is 1\n    unsigned int out_H = H - MASK_DIM + 1;   // here we assume there is no padding, and stride is 1\n    for (unsigned int orow = 0; orow < out_H; orow++)\n    {\n        for (unsigned int ocol = 0; ocol < out_W; ocol++)\n        {\n            float sum = 0.0f;\n\n            for (int i = 0; i < MASK_DIM; i++)\n            {\n                for (int j = 0; j < MASK_DIM; j++)\n                {\n                    int irow = orow + (i - MASK_RADIUS);\n                    int icol = ocol + (j - MASK_RADIUS);\n                    if (irow >= 0 && irow < H && icol >= 0 && icol < W)\n                    {\n                        sum += mask[i][j] * input[irow * W + icol];\n                    }\n                }\n            }\n            if (fabs(output[orow * out_W + ocol] - sum) > 1e-5)\n            {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n\nvoid conv2d(float mask[][MASK_DIM], float *input, float *output, unsigned int W, unsigned int H)\n{\n    // Allocate the memory on the GPU\n    unsigned int out_W = W - MASK_DIM + 1;   // here we assume there is no padding, and stride is 1\n    unsigned int out_H = H - MASK_DIM + 1;   // here we assume there is no padding, and stride is 1\n    float *input_d, *output_d;\n    cudaMalloc(&input_d, W * H * sizeof(float));\n    cudaMalloc(&output_d, out_W * out_H * sizeof(float));\n    cudaCheckErrors(\"cudaMalloc failed\");\n\n    // Copy the memory from the host to the GPU\n    cudaMemcpy(input_d, input, W * H * sizeof(float), cudaMemcpyHostToDevice);\n    cudaCheckErrors(\"cudaMemcpy H2D failed\");\n\n    // Allocate the mask on constant memory\n    cudaMemcpyToSymbol(mask_c, mask, MASK_DIM * MASK_DIM * sizeof(float));\n    cudaCheckErrors(\"cudaMemcpy Constant Memory failed\");\n\n    // Perform the convolution operation\n    dim3 numberOfThreadsPerBlock(32, 32);\n    dim3 numberOfBlocks((W + numberOfThreadsPerBlock.x - 1) / numberOfThreadsPerBlock.x,\n                        (H + numberOfThreadsPerBlock.y - 1) / numberOfThreadsPerBlock.y);\n    conv2d_kernel<<<numberOfBlocks, numberOfThreadsPerBlock>>>(input_d, output_d, W, H, out_W,\n                                                               out_H);\n    cudaCheckErrors(\"kernel launch failed\");\n\n    // Copy the result back to the host\n    cudaMemcpy(output, output_d, out_W * out_H * sizeof(float), cudaMemcpyDeviceToHost);\n    cudaCheckErrors(\"cudaMemcpy D2H failed\");\n\n    // Free the GPU Memory\n    cudaFree(input_d);\n    cudaFree(output_d);\n}\n\nvoid test(unsigned int W, unsigned int H)\n{\n    unsigned int oW = W - MASK_DIM + 1;\n    unsigned int oH = H - MASK_DIM + 1;\n    float mask[MASK_DIM][MASK_DIM];\n\n    // Allocate host memory\n    float *img = (float *)malloc(H * W * sizeof(float));\n    float *out = (float *)malloc(oH * oW * sizeof(float));\n\n    // Populate the arrays with random values\n    for (int i = 0; i < H * W; i++)\n    {\n        img[i] = static_cast<float>(rand()) / RAND_MAX;\n    }\n    for (int i = 0; i < MASK_DIM; i++)\n    {\n        for (int j = 0; j < MASK_DIM; j++)\n        {\n            mask[i][j] = static_cast<float>(rand()) / RAND_MAX;\n        }\n    }\n\n    // Perform the GPU operation\n    conv2d(mask, img, out, W, H);\n\n    // Validate the convolution operation\n    assert(validate_convolution(mask, img, out, W, H));\n\n    free(img);\n    free(out);\n}\n\nvoid launch()\n{\n    cudaDeviceSynchronize();\n\n    // Seed the random number generator\n    srand(static_cast<unsigned int>(time(nullptr)));\n\n    const int NUM_TESTS = 5;\n    // the lenght of the Ws and Hs must be TESTS\n    unsigned int Ws[] = {1 << 5, 1 << 12, 1983, 1 << 13, 10};\n    unsigned int Hs[] = {1 << 5, 1 << 12, 1285, 10, 1 << 13};\n    for (int i = 0; i < NUM_TESTS; i++)\n    {\n        test(Ws[i], Hs[i]);\n    }\n}\n\n// This function takes the input array and applies the mask that is present in constant memory and\n// stores the resultant value in the output variable. The input matrix is stored in row-major order\n// and the output matrix matrix should also be stored in row-major order.\n__global__ void conv2d_kernel(float *input, float *output, unsigned int W, unsigned int H,\n                              unsigned int oW, unsigned int oH)\n{\n", "test": "int main() {\n    launch();\n\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/8", "date": "2025-07-30", "prompt": "Implement a CUDA function called `run_cuda_graph` that sets up and executes a CUDA graph for\nprocessing an image represented as a 1D array. The function signature is `void run_cuda_graph(float*\nd_img, float* h_result, int width, int height)`. In this function, you will create a CUDA graph to\napply several image processing steps: edge detection, normalization, blurring, combining results,\nand a final transformation. You will use the following kernels in your graph:\n\n- `apply_edge_detection(float* img, float* result, int width, int height)`\n- `normalize_image(float* img, int width, int height)`\n- `apply_blur_filter(float* img, float* result, int width, int height)`\n- `combine_filtered_results(float* edge_result, float* blur_result, float* combined_result, int\nwidth, int height)`\n- `final_transformation(float* img, int width, int height)`\n\nAllocate the necessary device memory and configure the graph nodes with appropriate parameters.\nFirst, apply the edge detection filter to the input image and normalize the resulting image values.\nThen, apply a blur filter to the normalized image. Combine the results of the edge detection and the\nblur filters. Perform a final transformation on the combined result.\n\nUse a block size of 256 threads and ensure the graph executes 100 times. After executing the graph,\ncopy the final result back to the host.\n\nThe signature of the function is:\n```cuda\nvoid run_cuda_graph(float* d_img, float* h_result, int width, int height)\n```\n\nYou should assume the kernels are already defined.\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cassert>\n#include <cstdlib>\n#include <ctime>\n#include <iostream>\n\n// CUDA kernel to apply edge detection\n__global__ void apply_edge_detection(float* img, float* result, int width, int height)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < width * height)\n    {\n        result[idx] = img[idx] * 1.5f;   // Simplified edge detection operation\n    }\n}\n\n// CUDA kernel to normalize image values\n__global__ void normalize_image(float* img, int width, int height)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < width * height)\n    {\n        img[idx] /= 255.0f;   // Normalize to range [0, 1]\n    }\n}\n\n// CUDA kernel to apply blur filter\n__global__ void apply_blur_filter(float* img, float* result, int width, int height)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < width * height)\n    {\n        result[idx] = img[idx] * 0.8f;   // Simplified blur operation\n    }\n}\n\n// CUDA kernel to combine filtered results\n__global__ void combine_filtered_results(float* edge_result, float* blur_result,\n                                         float* combined_result, int width, int height)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < width * height)\n    {\n        combined_result[idx] = edge_result[idx] + blur_result[idx];\n    }\n}\n\n// CUDA kernel to apply final transformation\n__global__ void final_transformation(float* img, int width, int height)\n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < width * height)\n    {\n        img[idx] *= 2.0f;\n    }\n}\n\nvoid run_cuda_graph(float* d_img, float* h_result, int width, int height);\n\nint launch()\n{\n    const int width  = 256;\n    const int height = 256;\n    const int size   = width * height;\n    float* h_img     = (float*)malloc(size * sizeof(float));\n    float* h_result  = (float*)malloc(size * sizeof(float));\n\n    // Initialize the image with random values for testing\n    std::srand(std::time(0));\n    for (int i = 0; i < size; ++i)\n    {\n        h_img[i] = static_cast<float>(std::rand() % 256);\n    }\n\n    float* d_img;\n    cudaMalloc(&d_img, size * sizeof(float));\n    cudaMemcpy(d_img, h_img, size * sizeof(float), cudaMemcpyHostToDevice);\n\n    // Run the CUDA graph\n    run_cuda_graph(d_img, h_result, width, height);\n\n    // Check the results using assertions\n    for (int i = 0; i < size; ++i)\n    {\n        float expected = 2 * ((1.5f * h_img[i] / 255.0f) * 0.8f + (1.5f * h_img[i] / 255.0f));\n        assert(h_result[i] == expected && \"Assertion failed!\");\n    }\n\n    free(h_img);\n    free(h_result);\n    cudaFree(d_img);\n    return 0;\n}\n\n// Function to set up and execute the CUDA graph\nvoid run_cuda_graph(float* d_img, float* h_result, int width, int height)\n{\n", "test": "int main() {\n    launch();\n\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/9", "date": "2025-07-30", "prompt": "Write a CUDA function called `dot_product` that calculates the dot product of two vectors using\ndynamic shared memory. The function should efficiently handle cases where the size of the vectors\nmay be larger than the total number of available threads.\n\nThe signature of the function is:\n```cuda\n__global__ void dot_product(const float *A, const float *B, float *result, int ds)\n```\n\nImplement the body of the function. Use dynamic shared memory to store the partial sums within each\nblock.\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <assert.h>\n#include <stdio.h>\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n__global__ void dot_product(const float *A, const float *B, float *result, int ds);\n\nvoid test_dot_product(int ds, const float *h_A, const float *h_B, float expected_result)\n{\n    float *d_A, *d_B, *d_result;\n    float h_result = 0.0f;\n\n    cudaMalloc(&d_A, ds * sizeof(float));\n    cudaMalloc(&d_B, ds * sizeof(float));\n    cudaMalloc(&d_result, sizeof(float));\n    cudaCheckErrors(\"cudaMalloc failure\");\n\n    cudaMemcpy(d_A, h_A, ds * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, h_B, ds * sizeof(float), cudaMemcpyHostToDevice);\n    cudaCheckErrors(\"cudaMemcpy H2D failure\");\n\n    int block_size      = 256;\n    int grid_size       = (ds + block_size - 1) / block_size;\n    int shared_mem_size = block_size * sizeof(float);\n    dot_product<<<grid_size, block_size, shared_mem_size>>>(d_A, d_B, d_result, ds);\n    cudaCheckErrors(\"kernel launch failure\");\n\n    cudaMemcpy(&h_result, d_result, sizeof(float), cudaMemcpyDeviceToHost);\n    cudaCheckErrors(\"cudaMemcpy D2H failure\");\n\n    assert(fabs(h_result - expected_result) < 1e-5);\n\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_result);\n    cudaCheckErrors(\"cudaFree failure\");\n}\n\nint launch()\n{\n    // Test case 1: Simple vectors with all ones\n    int ds1     = 1 << 24;   // 16M elements\n    float *h_A1 = new float[ds1];\n    float *h_B1 = new float[ds1];\n    for (int i = 0; i < ds1; ++i)\n    {\n        h_A1[i] = 1.0f;\n        h_B1[i] = 1.0f;\n    }\n    float expected_result1 = ds1;\n    test_dot_product(ds1, h_A1, h_B1, expected_result1);\n\n    delete[] h_A1;\n    delete[] h_B1;\n\n    // Test case 2: Vectors with increasing sequence\n    int ds2     = 1 << 10;   // 1024 elements to avoid overflow of integer\n    float *h_A2 = new float[ds2];\n    float *h_B2 = new float[ds2];\n    for (int i = 0; i < ds2; ++i)\n    {\n        h_A2[i] = static_cast<float>(i);\n        h_B2[i] = static_cast<float>(i);\n    }\n    float expected_result2 = (ds2 - 1) * (ds2) * (2 * ds2 - 1) / 6.0f;   // Sum of squares formula\n    test_dot_product(ds2, h_A2, h_B2, expected_result2);\n\n    delete[] h_A2;\n    delete[] h_B2;\n\n    // Test case 3: Vectors with alternating pattern\n    int ds3     = 1 << 18;   // 256K elements\n    float *h_A3 = new float[ds3];\n    float *h_B3 = new float[ds3];\n    for (int i = 0; i < ds3; ++i)\n    {\n        h_A3[i] = (i % 2 == 0) ? 1.0f : -1.0f;\n        h_B3[i] = (i % 2 == 0) ? -1.0f : 1.0f;\n    }\n    float expected_result3 = -ds3;   // Each pair contributes -1 to the dot product\n    test_dot_product(ds3, h_A3, h_B3, expected_result3);\n\n    delete[] h_A3;\n    delete[] h_B3;\n\n    return 0;\n}\n\n// This CUDA function calculates the dot product of two vectors using dynamic shared memory.\n// It efficiently handles cases where the size of the vectors may be larger than the total number of\n// available threads.\n__global__ void dot_product(const float *A, const float *B, float *result, int ds)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/10", "date": "2025-07-30", "prompt": "Write a CUDA function called `gpuRecursiveReduce` that performs recursive reduction on an input\narray using dynamic parallelism and dynamic shared memory.\n\nThe signature of the function is:\n```cuda\n__global__ void gpuRecursiveReduce(int *g_idata, int *g_odata, unsigned int isize)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "-rdc=true -lcudadevrt", "declaration": "#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n\n#define cudaCheckErrors(msg)                                                                   \\\n    do                                                                                         \\\n    {                                                                                          \\\n        cudaError_t __err = cudaGetLastError();                                                \\\n        if (__err != cudaSuccess)                                                              \\\n        {                                                                                      \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                       \\\n            fprintf(stderr, \"*** FAILED - ABORTING\\n\");                                        \\\n            exit(1);                                                                           \\\n        }                                                                                      \\\n    }                                                                                          \\\n    while (0)\n\n__global__ void gpuRecursiveReduce(int *g_idata, int *g_odata, unsigned int isize);\n\nvoid initializeArray(int *data, int size)\n{\n    for (int i = 0; i < size; i++)\n    {\n        // set random seed\n        srand(time(NULL));\n\n        data[i] = rand() % 100;\n    }\n}\n\nint cpuReduce(int *data, int size)\n{\n    int sum = 0;\n    for (int i = 0; i < size; i++)\n    {\n        sum += data[i];\n    }\n    return sum;\n}\n\nint launch(void)\n{\n    int isize    = 1 << 20;   // 2^20 elements\n    int *h_idata = (int *)malloc(isize * sizeof(int));\n    int *h_odata = (int *)malloc(isize * sizeof(int));\n    int *d_idata, *d_odata;\n\n    initializeArray(h_idata, isize);\n\n    int cpu_sum = cpuReduce(h_idata, isize);\n\n    cudaMalloc(&d_idata, isize * sizeof(int));\n    cudaMalloc(&d_odata, isize * sizeof(int));\n\n    cudaMemcpy(d_idata, h_idata, isize * sizeof(int), cudaMemcpyHostToDevice);\n\n    int threads = 256;\n    int blocks  = (isize + threads - 1) / threads;\n    gpuRecursiveReduce<<<blocks, threads, threads * sizeof(int)>>>(d_idata, d_odata, isize);\n    cudaCheckErrors(\"Kernel launch failure\");\n\n    cudaMemcpy(h_odata, d_odata, blocks * sizeof(int), cudaMemcpyDeviceToHost);\n    cudaCheckErrors(\"Memcpy failure\");\n\n    int gpu_sum = cpuReduce(h_odata, blocks);\n\n    assert(cpu_sum == gpu_sum);\n\n    free(h_idata);\n    free(h_odata);\n    cudaFree(d_idata);\n    cudaFree(d_odata);\n\n    return 0;\n}\n\n__global__ void gpuRecursiveReduce(int *g_idata, int *g_odata, unsigned int isize)\n{\n", "test": "int main() {\n    launch();\n\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/11", "date": "2025-07-30", "prompt": "Implement a CUDA kernel function called `histogram` that efficiently calculates the histogram of a large input array of non-negative integers. The number of histogram bins is determined by the `num_bins` parameter. Use dynamic shared memory for storing the block-level histogram bins. The function should be optimized for performance and handle input arrays that are larger than the number of threads in a block.\n\nThe signature of the function is:\n```cuda\n__global__ void histogram(int *input, int *bins, int size, int num_bins)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <cassert>\n#include <cstdlib>\n\n#define cudaCheckErrors(msg) \\\n    do { \\\n        cudaError_t __err = cudaGetLastError(); \\\n        if (__err != cudaSuccess) { \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", \\\n                msg, cudaGetErrorString(__err), \\\n                __FILE__, __LINE__); \\\n            fprintf(stderr, \"*** FAILED - ABORTING\"); \\\n            exit(1); \\\n        } \\\n    } while (0)\n\n__global__ void histogram(int *input, int *bins, int size, int num_bins);\n\n\nbool validate(const int *input, const int *bins, int size, int num_bins) {\n    int *reference_bins = new int[num_bins]{0};\n\n    for (int i = 0; i < size; ++i) {\n        int value = input[i];\n        if (value >= 0 && value < num_bins) {\n            reference_bins[value]++;\n        }\n    }\n\n    for (int i = 0; i < num_bins; ++i) {\n        if (bins[i] != reference_bins[i]) {\n            delete[] reference_bins;\n            return false;\n        }\n    }\n\n    delete[] reference_bins;\n    return true;\n}\n\nint launch() {\n    int size = (1 << 24) + 1;  // 16M + 1 elements\n    int num_bins = 128;\n    const int BLOCK_SIZE = 256;\n\n    int *h_input = new int[size];\n    int *h_bins = new int[num_bins]{0};\n\n    for (int i = 0; i < size; ++i) {\n        h_input[i] = rand() % num_bins;\n    }\n\n    int *d_input, *d_bins;\n    cudaMalloc(&d_input, size * sizeof(int));\n    cudaMalloc(&d_bins, num_bins * sizeof(int));\n    cudaCheckErrors(\"cudaMalloc failure\");\n\n    cudaMemcpy(d_input, h_input, size * sizeof(int), cudaMemcpyHostToDevice);\n    cudaCheckErrors(\"cudaMemcpy H2D failure\");\n\n    int gridSize = (size + BLOCK_SIZE - 1) / BLOCK_SIZE;\n    int shared_memory_size = num_bins * sizeof(int);\n    histogram<<<gridSize, BLOCK_SIZE, shared_memory_size>>>(d_input, d_bins, size, num_bins);\n    cudaCheckErrors(\"kernel launch failure\");\n\n    cudaMemcpy(h_bins, d_bins, num_bins * sizeof(int), cudaMemcpyDeviceToHost);\n    cudaCheckErrors(\"cudaMemcpy D2H failure\");\n\n    assert(validate(h_input, h_bins, size, num_bins));\n\n    delete[] h_input;\n    delete[] h_bins;\n    cudaFree(d_input);\n    cudaFree(d_bins);\n\n    return 0;\n}\n\n// This CUDA kernel efficiently calculates the histogram of a large input array of non-negative integers.\n// It is optimized for performance and handles input arrays larger than the number of threads in a block.\n// The histogram bins are stored in global memory.\n__global__ void histogram(int *input, int *bins, int size, int num_bins)\n{\n", "test": "int main() {\n    launch();\n\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/12", "date": "2025-07-30", "prompt": "Implement a CUDA kernel function called `histogram_2d` to calculate the 2D histogram of an input\narray of non-negative integer pairs. Each pair represents coordinates in a 2D space. The histogram\nbins, determined by `num_bins_x` and `num_bins_y`, should be stored in global memory as a 1D array\nin row-major order.\n\nThe input array `input` is a 1D array of size `2 * size`, where each pair of consecutive elements\nrepresents the x and y coordinates of a point. Use dynamic shared memory for the histogram bins\nwithin each block.\n\nThe function signature is:\n```cuda\n__global__ void histogram_2d(int *input, int *bins, int size, int num_bins_x, int num_bins_y)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <cstdio>\n#include <cstdlib>\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n__global__ void histogram_2d(int *input, int *bins, int size, int num_bins_x, int num_bins_y);\n\nint launch()\n{\n    const int size       = 1024 * 1024 * 16;\n    const int num_bins_x = 100;\n    const int num_bins_y = 100;\n\n    int *input = new int[2 * size];\n    int *bins  = new int[num_bins_x * num_bins_y];\n\n    // Initialize input array with random values\n    for (int i = 0; i < size; i++)\n    {\n        input[2 * i]     = random() % num_bins_x;\n        input[2 * i + 1] = random() % num_bins_y;\n    }\n\n    int *d_input;\n    int *d_bins;\n    cudaMalloc(&d_input, 2 * size * sizeof(int));\n    cudaMalloc(&d_bins, num_bins_x * num_bins_y * sizeof(int));\n    cudaCheckErrors(\"cudaMalloc\");\n\n    cudaMemcpy(d_input, input, 2 * size * sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemset(d_bins, 0, num_bins_x * num_bins_y * sizeof(int));\n    cudaCheckErrors(\"cudaMemcpy/cudaMemset\");\n\n    int block_size      = 256;\n    int num_blocks      = (size + block_size - 1) / block_size;\n    int shared_mem_size = num_bins_x * num_bins_y * sizeof(int);\n\n    histogram_2d<<<num_blocks, block_size, shared_mem_size>>>(d_input, d_bins, size, num_bins_x,\n                                                              num_bins_y);\n    cudaCheckErrors(\"histogram_2d kernel\");\n\n    cudaMemcpy(bins, d_bins, num_bins_x * num_bins_y * sizeof(int), cudaMemcpyDeviceToHost);\n    cudaCheckErrors(\"cudaMemcpy\");\n\n    // Compute histogram on CPU for comparison\n    int *cpu_bins = new int[num_bins_x * num_bins_y];\n    memset(cpu_bins, 0, num_bins_x * num_bins_y * sizeof(int));\n\n    for (int i = 0; i < size; i++)\n    {\n        int x = input[2 * i];\n        int y = input[2 * i + 1];\n        cpu_bins[y * num_bins_x + x]++;\n    }\n\n    // Verify the histogram\n    for (int i = 0; i < num_bins_x * num_bins_y; i++)\n    {\n        assert(bins[i] == cpu_bins[i]);\n    }\n\n    // Clean up\n    delete[] input;\n    delete[] bins;\n    delete[] cpu_bins;\n    cudaFree(d_input);\n    cudaFree(d_bins);\n\n    return 0;\n}\n\n// This CUDA kernel efficiently calculates the 2D histogram of a large input array of non-negative\n// integer pairs. It is optimized for performance and handles input arrays larger than the number of\n// threads in a block. The histogram bins are stored in global memory as a 1D array in row-major\n// order.\n__global__ void histogram_2d(int *input, int *bins, int size, int num_bins_x, int num_bins_y)\n{\n", "test": "int main() {\n    launch();\n\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/13", "date": "2025-07-30", "prompt": "Write a CUDA function called `inc` that increments a large array on the GPU. The function should be\nable to handle arrays that are larger than the total number of threads launched in the grid.\n\nThe signature of the function is:\n```cuda\n__global__ void inc(int *array, size_t n)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#include <ctime>\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n__global__ void inc(int *array, size_t n);\n\nconst size_t ds = 32ULL * 1024ULL * 1024ULL;\n\ntemplate <typename T>\nvoid alloc_bytes(T &ptr, size_t num_bytes)\n{\n    cudaMallocManaged(&ptr, num_bytes);\n}\n\nint launch()\n{\n    int *h_array;\n    int *h_array_original;   // To store the original values for validation\n    alloc_bytes(h_array, ds * sizeof(h_array[0]));\n    alloc_bytes(h_array_original, ds * sizeof(h_array_original[0]));\n    cudaCheckErrors(\"cudaMallocManaged Error\");\n\n    // Initialize random seed\n    srand(time(NULL));\n\n    // Initialize the array with random numbers and save original values\n    for (size_t i = 0; i < ds; i++)\n    {\n        h_array[i]          = rand() % 100;   // Random numbers between 0 and 99\n        h_array_original[i] = h_array[i];\n    }\n\n    cudaMemPrefetchAsync(h_array, ds * sizeof(h_array[0]), 0);\n\n    inc<<<256, 256>>>(h_array, ds);\n    cudaCheckErrors(\"kernel launch error\");\n\n    cudaMemPrefetchAsync(h_array, ds * sizeof(h_array[0]), cudaCpuDeviceId);\n    cudaDeviceSynchronize();\n    cudaCheckErrors(\"kernel execution error\");\n\n    // Validate the results inside the main function\n    for (size_t i = 0; i < ds; i++)\n    {\n        assert(h_array[i] == h_array_original[i] + 1);\n    }\n\n    cudaFree(h_array);\n    cudaFree(h_array_original);\n    return 0;\n}\n\n// This CUDA kernel increments each element of the input array by 1.\n// It can handle arrays of any size, even those larger than the total number of threads in the grid.\n__global__ void inc(int *array, size_t n)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/14", "date": "2025-07-30", "prompt": "Write a CUDA function called `mmul` that performs matrix multiplication of two square matrices using\nstatically allocated shared memory. Assume that the block size is already defined as a constant\n`block_size`. Use it appropriately in your function.\n\nThe signature of the function is:\n```cuda\n__global__ void mmul(const float *A, const float *B, float *C, int ds)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <assert.h>\n#include <stdio.h>\n#include <time.h>\n#include <cstdlib>\n\n#define cudaCheckErrors(msg)                                                                   \\\n    do                                                                                         \\\n    {                                                                                          \\\n        cudaError_t __err = cudaGetLastError();                                                \\\n        if (__err != cudaSuccess)                                                              \\\n        {                                                                                      \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                       \\\n            fprintf(stderr, \"*** FAILED - ABORTING\\n\");                                        \\\n            exit(1);                                                                           \\\n        }                                                                                      \\\n    }                                                                                          \\\n    while (0)\n\nconst int DSIZE      = 4096;\nconst int block_size = 32;\n\n__global__ void mmul(const float *A, const float *B, float *C, int ds);\n\nint launch()\n{\n    float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n\n    h_A = new float[DSIZE * DSIZE];\n    h_B = new float[DSIZE * DSIZE];\n    h_C = new float[DSIZE * DSIZE];\n\n    // Initialize random seed\n    srand(time(NULL));\n\n    // Initialize the matrices with random numbers from 1 to 10\n    for (int i = 0; i < DSIZE * DSIZE; i++)\n    {\n        h_A[i] = static_cast<float>(rand() % 10 + 1);\n        h_B[i] = static_cast<float>(rand() % 10 + 1);\n        h_C[i] = 0;\n    }\n\n    cudaMalloc(&d_A, DSIZE * DSIZE * sizeof(float));\n    cudaMalloc(&d_B, DSIZE * DSIZE * sizeof(float));\n    cudaMalloc(&d_C, DSIZE * DSIZE * sizeof(float));\n    cudaCheckErrors(\"cudaMalloc failure\");\n\n    cudaMemcpy(d_A, h_A, DSIZE * DSIZE * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, h_B, DSIZE * DSIZE * sizeof(float), cudaMemcpyHostToDevice);\n    cudaCheckErrors(\"cudaMemcpy H2D failure\");\n\n    dim3 block(block_size, block_size);\n    dim3 grid((DSIZE + block.x - 1) / block.x, (DSIZE + block.y - 1) / block.y);\n\n    mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n    cudaCheckErrors(\"kernel launch failure\");\n\n    cudaMemcpy(h_C, d_C, DSIZE * DSIZE * sizeof(float), cudaMemcpyDeviceToHost);\n    cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n\n    // Validate the results using a subset of the elements\n    // as the original matrices are too large to compare all elements within a reasonable time\n    const int num_checks = 2048;\n    for (int i = 0; i < num_checks; i++)\n    {\n        int row              = rand() % DSIZE;\n        int col              = rand() % DSIZE;\n        float expected_value = 0;\n        for (int k = 0; k < DSIZE; k++)\n        {\n            expected_value += h_A[row * DSIZE + k] * h_B[k * DSIZE + col];\n        }\n        assert(fabs(h_C[row * DSIZE + col] - expected_value) < 1e-3);\n    }\n\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n    delete[] h_A;\n    delete[] h_B;\n    delete[] h_C;\n\n    return 0;\n}\n\n// This CUDA kernel multiplies two matrices A and B using shared memory\n// and stores the result in matrix C.\n__global__ void mmul(const float *A, const float *B, float *C, int ds)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/15", "date": "2025-07-30", "prompt": "Write CUDA functions to perform parallel reductions using specific partitioning strategies:\n\n1. A device function called `reduce`, which uses a cooperative group to perform a reduction on\nintegers stored in dynamic shared memory. This function should return the reduced result from shared\nmemory.\n2. Multiple kernel functions demonstrating different partitioning strategies:\n   - `block_reduce_kernel` uses the entire block as a single group.\n   - `tile32_reduce_kernel` uses tiled partitioning with a tile size of 32.\n\nThe following headers are already defined and should not be included in the response:\n```\n#include <cooperative_groups.h>\nusing namespace cooperative_groups;\n```\n\nImplement the functions in the following order using the provided signatures:\n```cuda\n__device__ int reduce(thread_group g, int *x, int val);\n__global__ void block_reduce_kernel(int *data, int *result, int size);\n__global__ void tile32_reduce_kernel(int *data, int *result, int size);\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <assert.h>\n#include <cooperative_groups.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n\nusing namespace cooperative_groups;\n\n#define cudaCheckErrors(msg)                                                                   \\\n    do                                                                                         \\\n    {                                                                                          \\\n        cudaError_t __err = cudaGetLastError();                                                \\\n        if (__err != cudaSuccess)                                                              \\\n        {                                                                                      \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                       \\\n            fprintf(stderr, \"*** FAILED - ABORTING\\n\");                                        \\\n            exit(1);                                                                           \\\n        }                                                                                      \\\n    }                                                                                          \\\n    while (0)\n\n__global__ void tile32_reduce_kernel(int *data, int *result, int size);\n__global__ void block_reduce_kernel(int *data, int *result, int size);\n\nvoid test_kernel(void (*kernel)(int *, int *, int), int size, const int *h_data,\n                 int expected_result, int block_size, int grid_size)\n{\n    int *d_data, *d_result;\n    int h_result = 0;\n\n    cudaMalloc(&d_data, size * sizeof(int));\n    cudaMalloc(&d_result, sizeof(int));\n    cudaCheckErrors(\"cudaMalloc failure\");\n\n    cudaMemcpy(d_data, h_data, size * sizeof(int), cudaMemcpyHostToDevice);\n    cudaCheckErrors(\"cudaMemcpy H2D failure\");\n\n    cudaMemset(d_result, 0, sizeof(int));\n    cudaCheckErrors(\"cudaMemset failure\");\n\n    int shared_mem_size = block_size * sizeof(int);\n    kernel<<<grid_size, block_size, shared_mem_size>>>(d_data, d_result, size);\n    cudaCheckErrors(\"Kernel launch failure\");\n\n    cudaMemcpy(&h_result, d_result, sizeof(int), cudaMemcpyDeviceToHost);\n    cudaCheckErrors(\"cudaMemcpy D2H failure\");\n\n    assert(fabs(h_result - expected_result) < 1e-5);\n    cudaFree(d_data);\n    cudaFree(d_result);\n    cudaCheckErrors(\"cudaFree failure\");\n}\n\nvoid launch()\n{\n    srand(time(NULL));\n\n    const int size   = 2 << 20;   // 2M elements\n    int *h_data      = new int[size];\n    int expected_sum = 0;\n    for (int i = 0; i < size; ++i)\n    {\n        h_data[i] = rand() % 100;   // Random values between 0 and 99\n        expected_sum += h_data[i];\n    }\n\n    int block_size = 256;\n    int normal_grid_size =\n        (size + block_size - 1) / block_size;   // this will be (2M + 256 - 1) / 256 = 8192\n    int limited_grid_size =\n        min(normal_grid_size, 512);   // Force a smaller grid size to test striding\n\n    // Test with normal grid size, where size >= block_size * grid_size\n    test_kernel(block_reduce_kernel, size, h_data, expected_sum, block_size, normal_grid_size);\n    test_kernel(tile32_reduce_kernel, size, h_data, expected_sum, block_size, normal_grid_size);\n\n    // Test with limited grid size, where size < block_size * grid_size\n    test_kernel(block_reduce_kernel, size, h_data, expected_sum, block_size, limited_grid_size);\n    test_kernel(tile32_reduce_kernel, size, h_data, expected_sum, block_size, limited_grid_size);\n\n    delete[] h_data;\n}\n\n", "test": "int main() {\nlaunch();\n\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/16", "date": "2025-07-30", "prompt": "Write a CUDA function called `performComplexConcurrentOperations` that takes three input arrays and\nan output array, performs a series of operations using CUDA kernels, and returns the processed data\nback to the host. The CUDA kernel functions you need to apply are defined as follows:\n\n```cuda\n__global__ void kernelSquare(int *data, int size) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size) {\n        data[i] = data[i] * data[i];\n    }\n}\n\n__global__ void kernelIncrement(int *data, int size) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size) {\n        data[i] += 1;\n    }\n}\n\n__global__ void kernelDouble(int *data, int size) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size) {\n        data[i] *= 2;\n    }\n}\n\n__global__ void kernelAdd(int *result, const int *a, const int *b, int size) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size) {\n        result[i] = a[i] + b[i];\n    }\n}\n```\n\nIn the `performComplexConcurrentOperations` function, use CUDA streams to perform the following\noperations efficiently: compute the intermediate arrays \\( X' = X^2 \\), \\( Y' = Y + 1 \\), and \\( Z'\n= 2Z \\); then calculate \\( A = X' + Y' \\) and \\( B = Y + Z' \\); and finally compute the result as \\(\n\\text{result} = (A + B)^2 \\). Ensure the use of appropriate concurrency mechanisms to optimize\nperformance.\n\nThe signature of the `performComplexConcurrentOperations` function is:\n```cuda\nvoid performComplexConcurrentOperations(int *result, const int *arrayX, const int *arrayY, const int\n*arrayZ, int size)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <cstdlib>\n\n__global__ void kernelSquare(int *data, int size)\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size)\n    {\n        data[i] = data[i] * data[i];\n    }\n}\n\n__global__ void kernelIncrement(int *data, int size)\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size)\n    {\n        data[i] += 1;\n    }\n}\n\n__global__ void kernelDouble(int *data, int size)\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size)\n    {\n        data[i] *= 2;\n    }\n}\n\n__global__ void kernelAdd(int *result, const int *a, const int *b, int size)\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size)\n    {\n        result[i] = a[i] + b[i];\n    }\n}\nvoid performComplexConcurrentOperations(int *result, const int *arrayX, const int *arrayY,\n                                        const int *arrayZ, int size);\n\n// CPU function to calculate the expected result\nvoid calculateOnCPU(int *result, const int *arrayX, const int *arrayY, const int *arrayZ, int size)\n{\n    for (int i = 0; i < size; ++i)\n    {\n        int X_prime = arrayX[i] * arrayX[i];\n        int Y_prime = arrayY[i] + 1;\n        int Z_prime = arrayZ[i] * 2;\n        int A       = X_prime + Y_prime;\n        int B       = arrayY[i] + Z_prime;\n        result[i]   = (A + B) * (A + B);\n    }\n}\n\nint launch()\n{\n    const int size = 1 << 20;   // 1M elements\n    int *arrayX    = new int[size];\n    int *arrayY    = new int[size];\n    int *arrayZ    = new int[size];\n    int *result    = new int[size];\n    int *expected  = new int[size];\n\n    // Generate random test data\n    for (int i = 0; i < size; ++i)\n    {\n        arrayX[i] = rand() % 100;\n        arrayY[i] = rand() % 10;\n        arrayZ[i] = rand() % 50;\n    }\n    // Perform the operations on the GPU\n    performComplexConcurrentOperations(result, arrayX, arrayY, arrayZ, size);\n\n    // Calculate the expected results on the CPU\n    calculateOnCPU(expected, arrayX, arrayY, arrayZ, size);\n\n    // Verify the results\n    for (int i = 0; i < size; ++i)\n    {\n        assert(result[i] == expected[i]);\n    }\n\n    // Clean up\n    delete[] arrayX;\n    delete[] arrayY;\n    delete[] arrayZ;\n    delete[] result;\n    delete[] expected;\n\n    return 0;\n}\n\n// CUDA function prototypes\nvoid performComplexConcurrentOperations(int *result, const int *arrayX, const int *arrayY,\n                                        const int *arrayZ, int size)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/17", "date": "2025-07-30", "prompt": "Write a CUDA function called `reduce` that performs a max-finding reduction to find the maximum\nvalue in an array using a parallel-sweep-reduction technique. Assume that `BLOCK_SIZE` is defined as\na constant. Use `BLOCK_SIZE` appropriately in your function.\n\nThe signature of the function is:\n```cuda\n__global__ void reduce(float *gdata, float *out, size_t n)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <assert.h>\n#include <float.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\nconst int BLOCK_SIZE = 256;\n\n__global__ void reduce(float *gdata, float *out, size_t n);\n\nbool validate(float *h_A, float *h_sum, size_t n)\n{\n    float max_val = -FLT_MAX;\n    for (size_t i = 0; i < n; i++)\n    {\n        if (h_A[i] > max_val)\n        {\n            max_val = h_A[i];\n        }\n    }\n    return fabs(*h_sum - max_val) < 1e-5;\n}\n\n// This CUDA kernel finds the maximum value in the 'gdata' array\n// using parallel reduction and stores the result in the 'out' array.\n__global__ void reduce(float *gdata, float *out, size_t n)\n{\n", "test": "int main() {\n\n    auto launch = []() -> void {\n\n        float *h_A, *h_sum, *d_A, *d_sums;\n        const int blocks = 640;\n        const size_t N   = 8ULL * 1024ULL * 1024ULL;\n        h_A              = new float[N];\n        h_sum            = new float;\n\n        // Initialize random seed\n        srand(time(NULL));\n\n        // Initialize the array with random numbers\n        float max_val = -FLT_MAX;\n        for (size_t i = 0; i < N; i++)\n        {\n            h_A[i] = static_cast<float>(rand()) / RAND_MAX;\n            if (h_A[i] > max_val)\n            {\n                max_val = h_A[i];\n            }\n        }\n\n        cudaMalloc(&d_A, N * sizeof(float));\n        cudaMalloc(&d_sums, blocks * sizeof(float));\n        cudaCheckErrors(\"cudaMalloc failure\");\n\n        cudaMemcpy(d_A, h_A, N * sizeof(float), cudaMemcpyHostToDevice);\n        cudaCheckErrors(\"cudaMemcpy H2D failure\");\n\n        reduce<<<blocks, BLOCK_SIZE>>>(d_A, d_sums, N);\n        cudaCheckErrors(\"reduction kernel launch failure\");\n\n        reduce<<<1, BLOCK_SIZE>>>(d_sums, d_A, blocks);\n        cudaCheckErrors(\"reduction kernel launch failure\");\n\n        cudaMemcpy(h_sum, d_A, sizeof(float), cudaMemcpyDeviceToHost);\n        cudaCheckErrors(\"reduction w/atomic kernel execution failure or cudaMemcpy D2H failure\");\n\n        assert(validate(h_A, h_sum, N));\n\n\n        // Put the maximum value at the end of the array to check if the program correctly handles arrays longer than the no of threads\n        max_val = 1.1;\n        h_A[N - 1] = max_val;\n\n        cudaMemcpy(d_A, h_A, N * sizeof(float), cudaMemcpyHostToDevice);\n        cudaCheckErrors(\"cudaMemcpy H2D failure\");\n\n        reduce<<<blocks, BLOCK_SIZE>>>(d_A, d_sums, N);\n        cudaCheckErrors(\"reduction kernel launch failure\");\n\n        reduce<<<1, BLOCK_SIZE>>>(d_sums, d_A, blocks);\n        cudaCheckErrors(\"reduction kernel launch failure\");\n\n        cudaMemcpy(h_sum, d_A, sizeof(float), cudaMemcpyDeviceToHost);\n        cudaCheckErrors(\"reduction w/atomic kernel execution failure or cudaMemcpy D2H failure\");\n\n        cudaMemcpy(h_sum, d_A, sizeof(float), cudaMemcpyDeviceToHost);\n        cudaCheckErrors(\"reduction w/atomic kernel execution failure or cudaMemcpy D2H failure\");\n\n        assert(validate(h_A, h_sum, N));\n\n        delete[] h_A;\n        delete h_sum;\n        cudaFree(d_A);\n        cudaFree(d_sums);\n    };\n\n    launch();\n\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/18", "date": "2025-07-30", "prompt": "Write a CUDA function called `row_sums` that performs a simple matrix row sum.\n\nThe signature of the function is:\n```cuda\n__global__ void row_sums(const float *A, float *sums, size_t ds)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <cmath>\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n__global__ void row_sums(const float *A, float *sums, size_t ds);\n\nint launch()\n{\n    const int block_size = 256;\n\n    // Initialize random seed\n    srand(time(NULL));\n\n    for (int i = 1; i <= 32; i *= 4)\n    {\n        int DSIZE = 256 * i;\n\n        float *h_A, *h_sums, *d_A, *d_sums;\n        h_A    = new float[DSIZE * DSIZE];\n        h_sums = new float[DSIZE]();\n\n        // Initialize host arrays with random numbers\n        for (int i = 0; i < DSIZE * DSIZE; i++)\n        {\n            h_A[i] = static_cast<float>(rand()) / RAND_MAX * 100;\n        }\n\n        cudaMalloc(&d_A, DSIZE * DSIZE * sizeof(float));\n        cudaMalloc(&d_sums, DSIZE * sizeof(float));\n        cudaCheckErrors(\"cudaMalloc failure\");\n\n        cudaMemcpy(d_A, h_A, DSIZE * DSIZE * sizeof(float), cudaMemcpyHostToDevice);\n        cudaCheckErrors(\"cudaMemcpy H2D failure\");\n\n        row_sums<<<(DSIZE + block_size - 1) / block_size, block_size>>>(d_A, d_sums, DSIZE);\n        cudaCheckErrors(\"kernel launch failure\");\n\n        cudaMemcpy(h_sums, d_sums, DSIZE * sizeof(float), cudaMemcpyDeviceToHost);\n        cudaCheckErrors(\"kernel execution failure or cudaMemcpy D2H failure\");\n\n        // Validate the results inside the main function\n        for (size_t j = 0; j < DSIZE; j++)\n        {\n            float expected_sum = 0.0f;\n            for (size_t k = 0; k < DSIZE; k++)\n            {\n                expected_sum += h_A[j * DSIZE + k];\n            }\n            assert(fabs(h_sums[j] - expected_sum) < 1e-5);\n        }\n\n        cudaFree(d_A);\n        cudaFree(d_sums);\n        delete[] h_A;\n        delete[] h_sums;\n    }\n\n    return 0;\n}\n\n__global__ void row_sums(const float *A, float *sums, size_t ds)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/19", "date": "2025-07-30", "prompt": "Write a CUDA function called `process_data_on_gpu` that takes in a large dataset of floating-point\nvalues, applies a special operation to each element using a separate CUDA kernel, and returns the\nprocessed data back to the host. The CUDA kernel function you need to apply is defined as follows:\n\n```\n__global__ void apply_special_operation(float *data, size_t n) {\nsize_t idx = threadIdx.x + blockDim.x * blockIdx.x;\nif (idx < n) {\n    if (data[idx] > 0 && data[idx] < 10) {\n        data[idx] *= 2;\n    } else if (data[idx] >= 10) {\n        data[idx] -= 10;\n    } else {\n        data[idx] = 0;\n    }\n}\n```\n\nIn the `process_data_on_gpu` function, allocate memory for the input and output data on the GPU,\ntransfer the input data from the host to the GPU, launch the kernel aove with an appropriate number\nof blocks and threads to process the entire dataset, transfer the output data back to the host, and\nfree the allocated memory on the GPU.\n\nThe signature of the `process_data_on_gpu` function is:\n```cuda\nvoid process_data_on_gpu(float *h_input, float *h_output, size_t n)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <assert.h>\n#include <cuda_runtime.h>\n#include <math.h>\n#include <stdio.h>\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n__global__ void apply_special_operation(float *data, size_t n)\n{\n    size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < n)\n    {\n        if (data[idx] > 0 && data[idx] < 10)\n        {\n            data[idx] *= 2;\n        }\n        else if (data[idx] >= 10)\n        {\n            data[idx] -= 10;\n        }\n        else\n        {\n            data[idx] = 0;\n        }\n    }\n}\n\nvoid process_data_on_gpu(float *h_input, float *h_output, size_t n);\n\nint launch()\n{\n    const int N     = 10000;\n    float *h_input  = new float[N];\n    float *h_output = new float[N];\n\n    // Initialize input data\n    for (int i = 0; i < N; i++)\n    {\n        h_input[i] = static_cast<float>(rand()) / RAND_MAX * 20.0f;\n    }\n\n    // Process data on GPU\n    process_data_on_gpu(h_input, h_output, N);\n\n    // Validate results\n    for (int i = 0; i < N; i++)\n    {\n        float expected;\n        if (h_input[i] > 0 && h_input[i] < 10)\n        {\n            expected = h_input[i] * 2;\n        }\n        else if (h_input[i] >= 10)\n        {\n            expected = h_input[i] - 10;\n        }\n        else\n        {\n            expected = 0;\n        }\n        assert(fabs(h_output[i] - expected) < 1e-5);\n    }\n\n    // Free memory\n    delete[] h_input;\n    delete[] h_output;\n\n    return 0;\n}\n\n\nvoid process_data_on_gpu(float *h_input, float *h_output, size_t n)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/20", "date": "2025-07-30", "prompt": "Write a CUDA kernel function called `stencil_1d` that performs a 1D stencil operation on a large 1D\narray. The stencil operation should calculate the sum of each element and its neighboring elements\nwithin a specified radius. The function should handle array sizes larger than the number of threads\nin a block and utilize statical shared memory for optimization.\n\nAssume that the following constants are defined:\n- `BLOCK_SIZE`: The number of threads per block\n- `RADIUS`: The radius of the stencil\n\nThe signature of the function is:\n```cuda\n__global__ void stencil_1d(int *in, int *out)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <cstdio>\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\nconst int BLOCK_SIZE = 256;\nconst int RADIUS     = 3;\n\n__global__ void stencil_1d(int *in, int *out);\n\nbool validate(const int *input, const int *output, int size)\n{\n    for (int i = 0; i < size; ++i)\n    {\n        int expected = 0;\n        for (int j = -RADIUS; j <= RADIUS; ++j)\n        {\n            int index = i + j;\n            if (index >= 0 && index < size)\n            {\n                expected += input[index];\n            }\n        }\n        if (output[i] != expected)\n        {\n            return false;\n        }\n    }\n    return true;\n}\n\nint launch()\n{\n    int size = 1 << 24;   // 16M elements\n\n    int *h_input  = new int[size];\n    int *h_output = new int[size];\n\n    for (int i = 0; i < size; ++i)\n    {\n        h_input[i] = rand() % 100;\n    }\n\n    int *d_input, *d_output;\n    cudaMalloc(&d_input, size * sizeof(int));\n    cudaMalloc(&d_output, size * sizeof(int));\n    cudaCheckErrors(\"cudaMalloc failure\");\n\n    cudaMemcpy(d_input, h_input, size * sizeof(int), cudaMemcpyHostToDevice);\n    cudaCheckErrors(\"cudaMemcpy H2D failure\");\n\n    int gridSize = (size + BLOCK_SIZE - 1) / BLOCK_SIZE;\n    stencil_1d<<<gridSize, BLOCK_SIZE>>>(d_input, d_output);\n    cudaCheckErrors(\"kernel launch failure\");\n\n    cudaMemcpy(h_output, d_output, size * sizeof(int), cudaMemcpyDeviceToHost);\n    cudaCheckErrors(\"cudaMemcpy D2H failure\");\n\n    assert(validate(h_input, h_output, size));\n\n    delete[] h_input;\n    delete[] h_output;\n    cudaFree(d_input);\n    cudaFree(d_output);\n\n    return 0;\n}\n\n// This CUDA kernel performs a 1D stencil operation on a large 1D array.\n// The stencil operation calculates the sum of each element and its neighboring elements within a\n// specified radius. The function handles array sizes larger than the number of threads in a block\n// and utilizes shared memory for optimization.\n__global__ void stencil_1d(int *in, int *out)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/21", "date": "2025-07-30", "prompt": "Implement a CUDA kernel called stencil3d_kernel that performs a 3d stencil operation on the input\nfloat matrix and populates the output matrix with these values. The stencil operation should\ncalculate the sum of each element and its neighboring elements within a specified radius weighted by\na two values (C0) and (C1). C0 indicates how much a The function should handle array sizes larger\nthan the number of threads in a block and utilize statical shared memory and thread coarsening for\noptimization.\n\nAssume that the following constants are defined:\n- `BLOCK_DIM`: The number of threads per block\n- `C0`: the weight of the element itself\n- `C1`: the weight of the neighbouring elements\n- `IN_TILE_DIM`: the tile size on the input array\n- `OUT_TILE_DIM`: the tile size on the output array\n\n\nThe signature of the function is:\n```cuda\n__global__ void stencil3d_kernel(float *input, float *output, unsigned int N)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <cstdlib>\n#include <ctime>\n#include <iostream>\n#include <random>\nusing namespace std;\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n#define BLOCK_DIM 8\n#define IN_TILE_DIM BLOCK_DIM\n#define OUT_TILE_DIM ((IN_TILE_DIM)-2)\n\n#define C0 0.95\n#define C1 0.05\n\n__global__ void stencil3d_kernel(float *input, float *output, unsigned int N);\n\nvoid stencil3d(float *input, float *output, unsigned int N)\n{\n    float *input_d, *output_d;\n    cudaMalloc(&input_d, N * N * N * sizeof(float));\n    cudaMalloc(&output_d, N * N * N * sizeof(float));\n    cudaCheckErrors(\"cudaMalloc failed\");\n\n    // Copy the memory from the host to the GPU\n    cudaMemcpy(input_d, input, N * N * N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaCheckErrors(\"cudaMemcpu H2D failed\");\n\n    // Perform the 3d stencil operation\n    dim3 numberOfThreadsPerBlock(BLOCK_DIM, BLOCK_DIM, BLOCK_DIM);\n    dim3 numberOfBlocks((N + BLOCK_DIM - 1) / BLOCK_DIM, (N + BLOCK_DIM - 1) / BLOCK_DIM,\n                        (N + BLOCK_DIM - 1) / BLOCK_DIM);\n    stencil3d_kernel<<<numberOfBlocks, numberOfThreadsPerBlock>>>(input_d, output_d, N);\n    cudaCheckErrors(\"kernel execution failed\");\n\n    // Copy the result back to the host\n    cudaMemcpy(output, output_d, N * N * N * sizeof(float), cudaMemcpyDeviceToHost);\n    cudaCheckErrors(\"cudaMemcpy D2H failed\");\n\n    // Free the GPU Memory\n    cudaFree(input_d);\n    cudaFree(output_d);\n}\n\nvoid test(unsigned int N)\n{\n    // Allocate host memory\n    float *img = (float *)malloc(N * N * N * sizeof(float));\n    float *out = (float *)malloc(N * N * N * sizeof(float));\n\n    // Populate the arrays\n    for (int i = 0; i < N * N * N; i++)\n    {\n        img[i] = static_cast<float>(rand()) / RAND_MAX;\n    }\n\n    // Time the GPU operation\n    stencil3d(img, out, N);\n\n    // Free the allocated memory\n    free(img);\n    free(out);\n}\n\nvoid launch()\n{\n    cudaDeviceSynchronize();\n\n    // Seed the random number generator\n    srand(static_cast<unsigned int>(time(nullptr)));\n\n    const unsigned int TESTS = 2;\n    unsigned int Ns[]        = {1 << 6, 4096};\n    for (int i = 0; i < TESTS; i++)\n    {\n        test(Ns[i]);\n    }\n}\n\n// This CUDA kernel performs a 3D stencil operation on a large 3D array.\n// The stencil operation calculates the sum of each element and its neighboring elements within a\n// specified radius. The function handles array sizes larger than the number of threads in a block\n// and utilizes shared memory, and thread coarsening for optimization.\n__global__ void stencil3d_kernel(float *input, float *output, unsigned int N)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/22", "date": "2025-07-30", "prompt": "Write a CUDA function called `transpose` that performs matrix transpose on a non-square matrix using\nstatically allocated shared memory. The input matrix is stored in row-major order, and the output\nmatrix should also be stored in row-major order.\n\nAssume that the following constant is defined:\n- `BLOCK_SIZE`: The size of the square thread block\n\nThe matrix dimensions, `width` and `height`, can be larger than `BLOCK_SIZE`. The function should\nhandle non-square matrices efficiently.\n\nThe signature of the function is:\n```cuda\n__global__ void transpose(const float *input, float *output, int width, int height)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <assert.h>\n#include <stdio.h>\n\n#define BLOCK_SIZE 16\n\n#define cudaCheckErrors(msg)                                                                 \\\n    do                                                                                       \\\n    {                                                                                        \\\n        cudaError_t __err = cudaGetLastError();                                              \\\n        if (__err != cudaSuccess)                                                            \\\n        {                                                                                    \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                     \\\n            fprintf(stderr, \"*** FAILED - ABORTING\");                                        \\\n            exit(1);                                                                         \\\n        }                                                                                    \\\n    }                                                                                        \\\n    while (0)\n\n__global__ void transpose(const float *input, float *output, int width, int height);\n\nbool validate_transpose(const float *input, const float *output, int width, int height)\n{\n    for (int i = 0; i < height; ++i)\n    {\n        for (int j = 0; j < width; ++j)\n        {\n            if (input[i * width + j] != output[j * height + i])\n            {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n\nint launch()\n{\n    int width       = 1024;\n    int height      = 768;\n    int size        = width * height;\n    float *h_input  = new float[size];\n    float *h_output = new float[size];\n\n    for (int i = 0; i < size; ++i)\n    {\n        h_input[i] = static_cast<float>(i);\n    }\n\n    float *d_input, *d_output;\n    cudaMalloc(&d_input, size * sizeof(float));\n    cudaMalloc(&d_output, size * sizeof(float));\n    cudaCheckErrors(\"cudaMalloc failure\");\n\n    cudaMemcpy(d_input, h_input, size * sizeof(float), cudaMemcpyHostToDevice);\n    cudaCheckErrors(\"cudaMemcpy H2D failure\");\n\n    dim3 block_size(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 grid_size((width + BLOCK_SIZE - 1) / BLOCK_SIZE, (height + BLOCK_SIZE - 1) / BLOCK_SIZE);\n    transpose<<<grid_size, block_size>>>(d_input, d_output, width, height);\n    cudaCheckErrors(\"kernel launch failure\");\n\n    cudaMemcpy(h_output, d_output, size * sizeof(float), cudaMemcpyDeviceToHost);\n    cudaCheckErrors(\"cudaMemcpy D2H failure\");\n\n    assert(validate_transpose(h_input, h_output, width, height));\n\n    delete[] h_input;\n    delete[] h_output;\n    cudaFree(d_input);\n    cudaFree(d_output);\n\n    return 0;\n}\n\n// This CUDA function performs matrix transpose on a non-square matrix using statically allocated\n// shared memory. The input matrix is stored in row-major order, and the output matrix is also\n// stored in row-major order.\n__global__ void transpose(const float *input, float *output, int width, int height)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/23", "date": "2025-07-30", "prompt": "Implement a function called `transposeSquareMatrixManaged` that uses CUDA's Unified Memory to\ntranspose a square matrix. Below is the CUDA kernel `transposeSquareMatrixKernel` that performs the\nmatrix transposition which you can use in your function.\n\n```cuda\n__global__ void transposeSquareMatrixKernel(float* out, const float* in, int N) {\n    extern __shared__ float tile[];\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n    int width = blockDim.x;\n\n    if (x < N && y < N) {\n        tile[threadIdx.y * width + threadIdx.x] = in[y * N + x];\n    }\n    __syncthreads();\n\n    x = blockIdx.y * blockDim.y + threadIdx.x; // transpose block offset\n    y = blockIdx.x * blockDim.x + threadIdx.y;\n\n    if (x < N && y < N) {\n        out[y * N + x] = tile[threadIdx.x * width + threadIdx.y];\n    }\n}\n```\n\nUse a block size of \\( 16 \\times 16 \\) for the CUDA kernel launch.\n\nThe signature of the function is:\n```cuda\nvoid transposeSquareMatrixManaged(float* h_in, float* h_out, int N)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cassert>\n#include <cstdlib>\n#include <ctime>\n\n__global__ void transposeSquareMatrixKernel(float* out, const float* in, int N)\n{\n    extern __shared__ float tile[];\n    int x     = blockIdx.x * blockDim.x + threadIdx.x;\n    int y     = blockIdx.y * blockDim.y + threadIdx.y;\n    int width = blockDim.x;\n\n    if (x < N && y < N)\n    {\n        tile[threadIdx.y * width + threadIdx.x] = in[y * N + x];\n    }\n    __syncthreads();\n\n    x = blockIdx.y * blockDim.y + threadIdx.x;   // transpose block offset\n    y = blockIdx.x * blockDim.x + threadIdx.y;\n\n    if (x < N && y < N)\n    {\n        out[y * N + x] = tile[threadIdx.x * width + threadIdx.y];\n    }\n}\n\nvoid transposeSquareMatrixManaged(float* h_in, float* h_out, int N);\n\nint launch()\n{\n    const int N = 256;   // Size of the square matrix\n    float h_in[N * N];\n    float h_out[N * N];\n\n    // Initialize random seed\n    std::srand(std::time(nullptr));\n\n    // Fill the input matrix with random values\n    for (int i = 0; i < N * N; ++i)\n    {\n        h_in[i] = static_cast<float>(std::rand()) / RAND_MAX;\n    }\n\n    // Call the function\n    transposeSquareMatrixManaged(h_in, h_out, N);\n\n    // Verify the transposed matrix\n    for (int i = 0; i < N; ++i)\n    {\n        for (int j = 0; j < N; ++j)\n        {\n            assert(h_out[i * N + j] == h_in[j * N + i]);\n        }\n    }\n\n    return 0;\n}\n\nvoid transposeSquareMatrixManaged(float* h_in, float* h_out, int N)\n{\n", "test": "int main() {\n    launch();\n\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/24", "date": "2025-07-30", "prompt": "Write a CUDA function called `vadd` that adds two vectors, the parameters are A and B.\n\nThe signature of the function is:\n```cuda\n__global__ void vadd(const float *A, const float *B, float *C, int ds)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <assert.h>\n#include <cuda.h>\n#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n\n#define cudaCheckErrors(msg)                                                                   \\\n    do                                                                                         \\\n    {                                                                                          \\\n        cudaError_t __err = cudaGetLastError();                                                \\\n        if (__err != cudaSuccess)                                                              \\\n        {                                                                                      \\\n            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", msg, cudaGetErrorString(__err), \\\n                    __FILE__, __LINE__);                                                       \\\n            fprintf(stderr, \"*** FAILED - ABORTING \");                                         \\\n            exit(1);                                                                           \\\n        }                                                                                      \\\n    }                                                                                          \\\n    while (0)\n\n__global__ void vadd(const float *A, const float *B, float *C, int ds);\n\nvoid launch()\n{\n    const int block_size = 256;\n\n    // Initialize random seed\n    srand(time(NULL));\n\n    // Loop through different vector sizes\n    for (int DSIZE = 3; DSIZE < 150; DSIZE *= 2)\n    {\n        // Allocate and initialize host and device memory\n        float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n        h_A = new float[DSIZE];\n        h_B = new float[DSIZE];\n        h_C = new float[DSIZE];\n\n        // Initialize host arrays with random numbers from 0 to 100\n        for (int i = 0; i < DSIZE; i++)\n        {\n            h_A[i] = static_cast<float>(rand()) / RAND_MAX * 100;\n            h_B[i] = static_cast<float>(rand()) / RAND_MAX * 100;\n            h_C[i] = 0;\n        }\n\n        cudaMalloc(&d_A, DSIZE * sizeof(float));\n        cudaMalloc(&d_B, DSIZE * sizeof(float));\n        cudaMalloc(&d_C, DSIZE * sizeof(float));\n        cudaCheckErrors(\"cudaMalloc failure\");\n\n        cudaMemcpy(d_A, h_A, DSIZE * sizeof(float), cudaMemcpyHostToDevice);\n        cudaMemcpy(d_B, h_B, DSIZE * sizeof(float), cudaMemcpyHostToDevice);\n        cudaCheckErrors(\"cudaMemcpy H2D failure\");\n\n        // Launch the vadd kernel with the current size\n        vadd<<<(DSIZE + block_size - 1) / block_size, block_size>>>(d_A, d_B, d_C, DSIZE);\n        cudaCheckErrors(\"kernel launch failure\");\n\n        // Copy the results back to the host\n        cudaMemcpy(h_C, d_C, DSIZE * sizeof(float), cudaMemcpyDeviceToHost);\n        cudaCheckErrors(\"cudaMemcpy D2H failure\");\n\n        // Validate the results inside the main function\n        for (int i = 0; i < DSIZE; ++i)\n        {\n            assert(fabs(h_C[i] - (h_A[i] + h_B[i])) < 1e-5);\n        }\n\n        // Free the allocated memory\n        cudaFree(d_A);\n        cudaFree(d_B);\n        cudaFree(d_C);\n        cudaCheckErrors(\"cudaFree failure\");\n\n        delete[] h_A;\n        delete[] h_B;\n        delete[] h_C;\n    }\n}\n\n__global__ void vadd(const float *A, const float *B, float *C, int ds)\n{\n", "test": "int main() {\n    launch();\n\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/25", "date": "2025-07-30", "prompt": "Write 2 device level functions called `reduce_warp` and `reduce_threadblock` to reduce data spread across threads in a thread block.\nThis includes performing a reduction operation within a warp and then across the entire thread block using shared memory. Use warp-level primitives to achieve this task.\nThe `reduce_warp` function should support the following warp sizes: 2, 4, 8, 16, and 32.\n\nA CUDA kernel will call these functions. The definition for the kernel `kernel` is \n```cuda\n__global__ void kernel(float *pOut, const float *pIn) {\n    /// this kernel assumes the threadblock size is 1024\n    extern __shared__ char smem_[];\n    float *smem = reinterpret_cast<float *>(smem_);\n    int tx = threadIdx.x;\n    float v = pIn[tx];\n    v = reduce_warp(v);\n    v = reduce_threadblock(v, smem);\n    if (threadIdx.x == 0) {\n        pOut[0] = v;\n    }\n```\n\nThe functions signatures are \n```cuda\ntemplate <int WARP_SIZE> __device__ float reduce_warp(float v)\ntemplate <int WARP_SIZE> __device__ float reduce_threadblock(float v, float *smem)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n\ntemplate <int WARP_SIZE = 32>\n__device__ float reduce_warp(float);\ntemplate <int WARP_SIZE = 32>\n__device__ float reduce_threadblock(float, float *);\n\n__global__ void kernel(float *pOut, const float *pIn)\n{\n    /// this kernel assumes the threadblock size is 1024\n    extern __shared__ char smem_[];\n    float *smem = reinterpret_cast<float *>(smem_);\n    int tx      = threadIdx.x;\n    float v     = pIn[tx];\n    v           = reduce_warp(v);\n    v           = reduce_threadblock(v, smem);\n    if (threadIdx.x == 0)\n    {\n        pOut[0] = v;\n    }\n}\n\nvoid launch(int blockSize)\n{\n    float *output, *input;\n    dim3 threadsPerBlock(blockSize);\n    dim3 numBlocks(1);\n\n    cudaLaunchConfig_t config = {0};\n    config.gridDim            = numBlocks;\n    config.blockDim           = threadsPerBlock;\n    config.dynamicSmemBytes   = (config.blockDim.x / 32) * 4;\n    cudaLaunchKernelEx(&config, kernel, output, input);\n}\n\n", "test": "int main() {\nlaunch(1024);\n\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/26", "date": "2025-07-30", "prompt": "Write 2 device level functions called `reduce_warp` and `reduce_threadblock` to minimize data spread across threads within a warp to enhance data locality and performance. \nUse warp-level primitives to achieve this task.  \nA CUDA kernel will call these functions. The definition of the kernel is \n\n```cuda\n__global__ void kernel(float *pOut, const float *pIn) {\n    extern __shared__ char smem_[];\n    float *smem = reinterpret_cast<float *>(smem_);\n    int tx = threadIdx.x;\n    float v = pIn[tx];\n    v = reduce_warp(v);\n    v = reduce_threadblock(v, smem);\n    if (threadIdx.x == 0) {\n        pOut[0] = v;\n    }\n}\n```\n\nThe signature of the functions must be \n```cuda\n__device__ float reduce_warp(float v)\n__device__ float reduce_threadblock(float v, float *smem)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n\n__device__ float reduce_warp(float);\n__device__ float reduce_threadblock(float, float *);\n\n__global__ void kernel(float *pOut, const float *pIn)\n{\n    extern __shared__ char smem_[];\n    float *smem = reinterpret_cast<float *>(smem_);\n    int tx      = threadIdx.x;\n    float v     = pIn[tx];\n    v           = reduce_warp(v);\n    v           = reduce_threadblock(v, smem);\n    if (threadIdx.x == 0)\n    {\n        pOut[0] = v;\n    }\n}\n\nvoid launch(int blockSize)\n{\n    float *output, *input;\n    dim3 threadsPerBlock(blockSize);\n    dim3 numBlocks(1);\n\n    cudaLaunchConfig_t config = {0};\n    config.gridDim            = numBlocks;\n    config.blockDim           = threadsPerBlock;\n    config.dynamicSmemBytes   = (config.blockDim.x / 32) * 4;\n    cudaLaunchKernelEx(&config, kernel, output, input);\n}\n\n", "test": "int main() {\nlaunch(1024);\n\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/27", "date": "2025-07-30", "prompt": "Implement 2 functions `reduce_warp` and `reduce_threadblock` that perform partial sum reduction across a thread block. \nThe reduction should minimize data spread across threads using warp-level and thread block-level reduction techniques\nWrite 2 device-level functions to handle the warp-wide reduction and thread block-wide reduction, ensuring that the final sum is correctly computed across all thread blocks.\n\nThe definition for the kernel is \n```cuda\n__global__ void kernel_partialsum(float *pOut, const float *pIn, int N) {\n    extern __shared__ char smem_[];\n    float *smem = reinterpret_cast<float *>(smem_);\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n    float v = 0.0f;\n\n    if (index < N) {\n        v = pIn[index];\n        v = reduce_warp(v);\n        v = reduce_threadblock(v, smem);\n        if (threadIdx.x == 0) {\n            pOut[blockIdx.x] = v;\n        }\n    }\n}\n```\n\nThe functions signatures are \n```cuda\n__device__ float reduce_warp(float v)\n__device__ float reduce_threadblock(float v, float *smem)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n\n__device__ float reduce_warp(float);\n__device__ float reduce_threadblock(float, float *);\n\n__global__ void kernel_partialsum(float *pOut, const float *pIn, int N)\n{\n    extern __shared__ char smem_[];\n    float *smem = reinterpret_cast<float *>(smem_);\n\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n    float v = 0.0f;\n\n    if (index < N)\n    {\n        v = pIn[index];\n        v = reduce_warp(v);\n        v = reduce_threadblock(v, smem);\n        if (threadIdx.x == 0)\n        {\n            pOut[blockIdx.x] = v;\n        }\n    }\n}\n\nvoid launch(int length)\n{\n    float *output, *input;\n    dim3 threadsPerBlock(1024);\n    dim3 numBlocks(length / 1024);\n\n    cudaLaunchConfig_t config = {0};\n    config.gridDim            = numBlocks;\n    config.blockDim           = threadsPerBlock;\n    config.dynamicSmemBytes   = (config.blockDim.x / 32) * 4;\n    // launch first kernel to reduce data inside a single threadblock\n    cudaLaunchKernelEx(&config, kernel_partialsum, output, input, length);\n    // when the first kernel is done, all the partial sums will be\n    // in contiguous location. we launch the kernel again to do\n    // final sum on partial sums\n    config.gridDim = dim3((numBlocks.x - 1024) / 1024 + 1);\n    cudaLaunchKernelEx(&config, kernel_partialsum, output, output, length);\n    // output[0] should have the final sum\n}\n\n", "test": "int main() {\nlaunch(1024 * 1024);\n\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/28", "date": "2025-07-30", "prompt": "Write a device level function called `reduce_warp` to minimize data spread across threads in a warp.\nImplement a summation of floating point data across threads in a warp using shuffle instructions in CUDA.\nThe function should support the following warp sizes: 2, 4, 8, 16, and 32. \n\nA CUDA kernel will call this function `reduce_warp`. The definition of the kernel is \n```cuda\n__global__ void kernel(float *pOut, const float *pIn) {\n    int tx  = threadIdx.x;\n    float v = pIn[tx];\n    v = reduce_warp(v);\n    if (threadIdx.x == 0) {\n        pOut[0] = v;\n    }\n}\n```\n\nThe function signature is \n```cuda\ntemplate <int WARP_SIZE> __device__ float reduce_warp(float);\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n\ntemplate <int WARP_SIZE = 32>\n__device__ float reduce_warp(float);\n\n__global__ void kernel(float *pOut, const float *pIn)\n{\n    int tx  = threadIdx.x;\n    float v = pIn[tx];\n    v       = reduce_warp(v);\n    if (threadIdx.x == 0)\n    {\n        pOut[0] = v;\n    }\n}\n\nvoid launch(int blockSize)\n{\n    float *output, *input;\n    dim3 threadsPerBlock(blockSize);\n    dim3 numBlocks(1);\n\n    cudaLaunchConfig_t config = {0};\n    config.gridDim            = numBlocks;\n    config.blockDim           = threadsPerBlock;\n\n    cudaLaunchKernelEx(&config, kernel, output, input);\n}\n\n", "test": "int main() {\nlaunch(32);\n\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/29", "date": "2025-07-30", "prompt": "Write a device level function called `reduce_warp` to minimize data spread across threads in a warp.\nImplement a summation of floating point data across threads in a warp using shuffle instructions in CUDA. \n\nA CUDA kernel will call this function `reduce_warp`. The definition of the kernel is \n```cuda\n__global__ void kernel(float *pOut, const float *pIn) {\n    int tx  = threadIdx.x;\n    float v = pIn[tx];\n    v = reduce_warp(v);\n    if (threadIdx.x == 0) {\n        pOut[0] = v;\n    }\n}\n```\n\nThe function signature is \n```cuda\n__device__ float reduce_warp(float v)\n```\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda.h>\n#include \"cuda_runtime.h\"\n\n__device__ float reduce_warp(float);\n\n__global__ void kernel(float *pOut, const float *pIn)\n{\n    int tx  = threadIdx.x;\n    float v = pIn[tx];\n    v       = reduce_warp(v);\n    if (threadIdx.x == 0)\n    {\n        pOut[0] = v;\n    }\n}\n\nvoid launch(int blockSize)\n{\n    float *output, *input;\n    dim3 threadsPerBlock(blockSize);\n    dim3 numBlocks(1);\n\n    cudaLaunchConfig_t config = {0};\n    config.gridDim            = numBlocks;\n    config.blockDim           = threadsPerBlock;\n\n    cudaLaunchKernelEx(&config, kernel, output, input);\n}\n\n", "test": "int main() {\nlaunch(32);\n\n}\n", "example_test": "@example_test\n", "cuda_toolkit": "12.0"}
{"task_id": "CUDA/80", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute the dot product of two vectors using shared memory.\n \nThe signature of the function is __global__ void k_dotProduct(const int *inputVectorA, const int *inputVectorB, int *resultDotProduct, const int n).\n\n>>> k_dotProduct({1, 2, 3, 4}, {1, 0, 0, 1}, resultDotProduct, 4) -> resultDotProduct: 5\n>>> k_dotProduct({1, 2, 3, 4, 5, 6, 7, 8, 9}, {1, 0, 0, 1, 0, 0, 1, 0, 0}, resultDotProduct, 9) -> resultDotProduct: 12 \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <algorithm>\n#include <cstdio>\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n#undef NDEBUG\n#include <assert.h>\n\n__global__ void k_dotProduct(const int *inputVectorA, const int *inputVectorB, int *resultDotProduct, const int n);\n\nvoid launch() {\n    const int testCaseCount = 9; // Number of test cases\n    int vectorSize[testCaseCount] = {4, 4, 4, 4, 4, 4, 10, 4, 9}; // Sizes of the vectors in each test case\n    const int expectedDotProduct[testCaseCount] = {20, 10,  70, 100, 30, 140, 10, 5, 12}; // Expected results for each test\n    const int maxVectorSize = *std::max_element(vectorSize, vectorSize + testCaseCount);\n    const int BLOCK_SIZE = 512; // number of threads per block\n\n    // Input vectors for the tests\n    int inputVectorA_h[testCaseCount][maxVectorSize] =  {\n        {2, 4, 6, 8},                   // test case 1\n        {1, 3, 5, 7},                   // test case 2\n        {9, 8, 7, 6},                   // test case 3\n        {5, 10, 15, 20},                // test case 4\n        {3, 6, 9, 12},                  // test case 5\n        {7, 14, 21, 28},                // test case 6\n        {1, 1, 1, 1, 1, 1, 1, 1, 1, 1}, // test case 7\n        {1, 2, 3, 4},                   // test case 8\n        {1, 2, 3, 4, 5, 6, 7, 8, 9}};   // test case 9\n\n    int inputVectorB_h[testCaseCount][maxVectorSize] =  {\n        {1, 1, 1, 1},                   // test case 1\n        {0, 1, 0, 1},                   // test case 2\n        {1, 2, 3, 4},                   // test case 3\n        {2, 2, 2, 2},                   // test case 4\n        {3, 2, 1, 0},                   // test case 5\n        {4, 3, 2, 1},                   // test case 6\n        {1, 1, 1, 1, 1, 1, 1, 1, 1, 1}, // test case 7\n        {1, 0, 0, 1},                   // test case 8\n        {1, 0, 0, 1, 0, 0, 1, 0, 0}};   // test case 9\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    int dotProduct_h = 0; // result on the host\n    int *inputVectorA_d, *inputVectorB_d, *dotProduct_d;  // Pointers for device memory (GPU)\n\n    // Allocate the memory on the device\n    CUDA_CHECK(cudaMallocAsync(&inputVectorA_d, maxVectorSize * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&inputVectorB_d, maxVectorSize * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&dotProduct_d, sizeof(int), stream));\n\n    // Loop to execute each test case\n    for (int i = 0; i < testCaseCount; ++i) {\n        // Copy input data to the device\n        CUDA_CHECK(cudaMemcpyAsync(inputVectorA_d, inputVectorA_h[i], vectorSize[i] * sizeof(int), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(inputVectorB_d, inputVectorB_h[i], vectorSize[i] * sizeof(int), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemsetAsync(dotProduct_d, 0, sizeof(int), stream));  // Initialize the result on the device\n\n        // Determine the number of threads and blocks\n        int blocks  = (vectorSize[i] + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n        // Execute the kernel\n        // Grid:  (ceil(N/ 512), 1, 1) \n        // Block: (512, 1, 1)\n        void *args[] = {&inputVectorA_d, &inputVectorB_d, &dotProduct_d, &vectorSize[i]};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_dotProduct, blocks, BLOCK_SIZE, args, BLOCK_SIZE * sizeof(int), stream));\n\n        // Copy the result back to the host (CPU)\n        CUDA_CHECK(cudaMemcpyAsync(&dotProduct_h, dotProduct_d, sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify if the calculated dot product matches the expected result\n        assert(dotProduct_h == expectedDotProduct[i]);\n    }\n\n    // Clean up\n    CUDA_CHECK(cudaFreeAsync(inputVectorA_d, stream));\n    CUDA_CHECK(cudaFreeAsync(inputVectorB_d, stream));\n    CUDA_CHECK(cudaFreeAsync(dotProduct_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));    \n}\n\n__global__ void k_dotProduct(const int *inputVectorA, const int *inputVectorB, int *resultDotProduct, const int n) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/81", "date": "2025-07-30", "prompt": "Write a CUDA kernel to transpose a NxN square matrix in-place using statically allocated shared memory. Assume that the block size is pre-defined as a constant BLOCK_SIZE. The signature of the function is __global__ void k_transposeMatrix(int* matrix, const int height, const int width).\n\nThe matrix transpose needs to be computed in-place without creating a copy of the original matrix.\n\n>>> k_transposeMatrix({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}, 3, 3) -> {{1, 4, 7}, {2, 5, 8}, {3, 6, 9}}\n>>> k_transposeMatrix({{1, 2}, {3, 4}}, 2, 2) -> {{1, 3}, {2, 4}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cstdio>\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\nconst int BLOCK_SIZE = 16;\nconst int TILE_DIM = 4;\n\n#undef NDEBUG\n#include <assert.h>\n\n__global__ void k_transposeMatrix(int* matrix, const int height, const int width);\n\nvoid launch() {\n    int *matrix_d;\n    int *d_transpose_matrix;\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    CUDA_CHECK(cudaMallocAsync(&matrix_d, TILE_DIM * TILE_DIM * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&d_transpose_matrix, TILE_DIM * TILE_DIM * sizeof(int), stream));\n\n    // Test 1 - 3x3 matrix\n    {\n        int height = 3;\n        int width = 3;\n        int matrix_h[height * width] = {1, 2, 3, 4, 5, 6, 7, 8, 9};\n        int transposeMatrix_h[height * width] = {0};\n        int expectedOutput[height * width] = {1, 4, 7, 2, 5, 8, 3, 6, 9};\n\n        CUDA_CHECK(cudaMemcpyAsync(matrix_d, matrix_h, height * width * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 dimGrid((height + BLOCK_SIZE - 1)/BLOCK_SIZE, (width + BLOCK_SIZE - 1)/BLOCK_SIZE);\n\n        void *args[] = {&matrix_d, &height, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_transposeMatrix, dimGrid, dimBlock, args, TILE_DIM * TILE_DIM * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(transposeMatrix_h, matrix_d, height * width * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i=0; i < height * width; i++) {\n            assert(transposeMatrix_h[i] == expectedOutput[i]);\n        }\n    }\n\n    // Test 2 - 4x4 matrix\n    {\n        int height = 4;\n        int width = 4;\n        int matrix_h[height * width] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};\n        int transposeMatrix_h[height * width] = {0};\n        int expectedOutput[height * width] = {1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15, 4, 8, 12, 16};\n\n        CUDA_CHECK(cudaMemcpyAsync(matrix_d, matrix_h, height * width * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 dimGrid((height + BLOCK_SIZE - 1)/BLOCK_SIZE, (width + BLOCK_SIZE - 1)/BLOCK_SIZE);\n\n        void *args[] = {&matrix_d, &height, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_transposeMatrix, dimGrid, dimBlock, args, TILE_DIM * TILE_DIM * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(transposeMatrix_h, matrix_d, height * width * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i=0; i < height * width; i++) {\n            assert(transposeMatrix_h[i] == expectedOutput[i]);\n        }\n    }\n\n    // Test 3 - 3x3 matrix\n    {\n        int height = 3;\n        int width = 3;\n        int matrix_h[height * width] = {1, 2, 3, 4, 5, 6, 7, 8, 9};\n        int transposeMatrix_h[height * width] = {0};\n        int expectedOutput[height * width] = {1, 4, 7, 2, 5, 8, 3, 6, 9};\n\n        CUDA_CHECK(cudaMemcpyAsync(matrix_d, matrix_h, height * width * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 dimGrid((height + BLOCK_SIZE - 1)/BLOCK_SIZE, (width + BLOCK_SIZE - 1)/BLOCK_SIZE);\n\n        void *args[] = {&matrix_d, &height, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_transposeMatrix, dimGrid, dimBlock, args, TILE_DIM * TILE_DIM * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(transposeMatrix_h, matrix_d, height * width * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i=0; i < height * width; i++) {\n            assert(transposeMatrix_h[i] == expectedOutput[i]);\n        }\n    }\n\n    // Test 4 - 2x2 matrix\n    {\n        int height = 2;\n        int width = 2;\n        int matrix_h[height * width] = {1, 2, 3, 4};\n        int transposeMatrix_h[height * width] = {0};\n        int expectedOutput[height * width] = {1, 3, 2, 4};\n\n        CUDA_CHECK(cudaMemcpyAsync(matrix_d, matrix_h, height * width * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 dimGrid((height + BLOCK_SIZE - 1)/BLOCK_SIZE, (width + BLOCK_SIZE - 1)/BLOCK_SIZE);\n\n        void *args[] = {&matrix_d, &height, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_transposeMatrix, dimGrid, dimBlock, args, TILE_DIM * TILE_DIM * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(transposeMatrix_h, matrix_d, height * width * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i=0; i < height * width; i++) {\n            assert(transposeMatrix_h[i] == expectedOutput[i]);\n        }\n    }\n\n    // Test 5 - 1x1 matrix\n    {\n        int height = 1;\n        int width = 1;\n        int matrix_h[height * width] = {1};\n        int transposeMatrix_h[height * width] = {0};\n        int expectedOutput[height * width] = {1};\n\n        CUDA_CHECK(cudaMemcpyAsync(matrix_d, matrix_h, height * width * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 dimGrid((height + BLOCK_SIZE - 1)/BLOCK_SIZE, (width + BLOCK_SIZE - 1)/BLOCK_SIZE);\n\n        void *args[] = {&matrix_d, &height, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_transposeMatrix, dimGrid, dimBlock, args, TILE_DIM * TILE_DIM * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(transposeMatrix_h, matrix_d, height * width * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i=0; i < height * width; i++) {\n            assert(transposeMatrix_h[i] == expectedOutput[i]);\n        }\n    }\n\n    // Test 6 - 9x9 matrix\n    {\n        int height = 3;\n        int width = 3;\n        int matrix_h[height * width] = {0, 0, 0, 0, 0, 0, 0, 0, 0};\n        int transposeMatrix_h[height * width] = {0};\n        int expectedOutput[height * width] = {0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n        CUDA_CHECK(cudaMemcpyAsync(matrix_d, matrix_h, height * width * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 dimGrid((height + BLOCK_SIZE - 1)/BLOCK_SIZE, (width + BLOCK_SIZE - 1)/BLOCK_SIZE);\n\n        void *args[] = {&matrix_d, &height, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_transposeMatrix, dimGrid, dimBlock, args, TILE_DIM * TILE_DIM * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(transposeMatrix_h, matrix_d, height * width * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i=0; i < height * width; i++) {\n            assert(transposeMatrix_h[i] == expectedOutput[i]);\n        }\n    }\n\n    // Test 7 - 16x16 matrix\n    {\n        int height = 2;\n        int width = 2;\n        int matrix_h[height * width] = {1, 1, 1, 1};\n        int transposeMatrix_h[height * width] = {0};\n        int expectedOutput[height * width] = {1, 1, 1, 1};\n\n        CUDA_CHECK(cudaMemcpyAsync(matrix_d, matrix_h, height * width * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 dimGrid((height + BLOCK_SIZE - 1)/BLOCK_SIZE, (width + BLOCK_SIZE - 1)/BLOCK_SIZE);\n\n        void *args[] = {&matrix_d, &height, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_transposeMatrix, dimGrid, dimBlock, args, TILE_DIM * TILE_DIM * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(transposeMatrix_h, matrix_d, height * width * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i=0; i < height * width; i++) {\n            assert(transposeMatrix_h[i] == expectedOutput[i]);\n        }\n    }\n\n    // Clean up\n    CUDA_CHECK(cudaFreeAsync(matrix_d, stream));\n    CUDA_CHECK(cudaFreeAsync(d_transpose_matrix, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_transposeMatrix(int* matrix, const int height, const int width) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/82", "date": "2025-07-30", "prompt": "Write a CUDA kernel to find minimum and maximum values in a vector. The kernel needs to use warp shuffles, reductions and atomics in finding minimum and maximum values in the vector.\n\nThe signature of the kernel is __global__ void k_findMinMax(int* data, int* minResult, int* maxResult, int n).\n\n>>> k_findMinMax({2, 1, 5, 4}) -> min: 1 and max: 5\n>>> k_findMinMax({9, 3, 3, 5, 2, 8, 3, 10, 21}) -> min: 2 and max: 21 \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <iostream>\n#include <limits.h>\n#include <cuda_runtime.h>\n#include <algorithm>\n#define CUDA_CHECK(call)                                                 \\\ndo {                                                                     \\\n    cudaError_t error = call;                                           \\\n    if (error != cudaSuccess) {                                         \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\", __FILE__, __LINE__, cudaGetErrorString(error)); \\\n        exit(EXIT_FAILURE);                                             \\\n    }                                                                   \\\n} while (0)\n// Kernel to find both the minimum and maximum values in a vector\n#undef NDEBUG\n#include <assert.h>\n\n__global__ void k_findMinMax(int* data, int* minResult, int* maxResult, int n);\n\nint launch() {\n    const int testCaseCount = 7; // Number of test cases\n    const int vectorSize[testCaseCount] = {4, 4, 8, 16, 32, 64, 100}; // Sizes of the vectors in each test case\n    const int expectedMin[testCaseCount] = {1, 3, 0, 1, 1, 1, 1}; // Expected results for each test\n    const int expectedMax[testCaseCount] = {8, 9, 11, 20, 99, 99, 100}; // Expected results for each test\n    int maxVectorSize = *std::max_element(vectorSize, vectorSize + testCaseCount);\n\n    // Input vectors for the tests\n    int inputVector[testCaseCount][maxVectorSize] =  {   \n        {2, 5, 1, 8},                                                     // test case 1\n        {3, 9, 5, 7},                                                     // test case 2\n        {9, 11, 6, 9, 0, 8, 7, 6},                                        // test case 3\n        {2, 5, 1, 8, 5, 10, 15, 20, 3, 9, 5, 7, 3, 9, 10, 12},            // test case 4\n        {32, 4, 98, 7, 18, 15, 3, 8, 99, 12, 11, 1, 24, 97, 6, 13, \n         30, 9, 14, 20, 2, 5, 27, 10, 21, 22, 17, 26, 16, 28, 29, 31},    // test case 5\n        {32, 4, 98, 7, 18, 15, 3, 8, 99, 12, 11, 1, 24, 97, 6, 13, \n         30, 9, 14, 20, 2, 5, 27, 10, 21, 22, 17, 26, 16, 28, 29, 31,\n         33, 38, 93, 40, 95, 41, 34, 36, 94, 39, 43, 35, 44, 48, 37, 42,\n         49, 54, 77, 56, 79, 57, 50, 52, 78, 55, 59, 51, 60, 64, 53, 58}, // test case 6\n        {32, 4, 98, 7, 18, 15, 3, 8, 99, 12, 11, 1, 24, 97, 6, 13,\n         30, 9, 14, 20, 2, 5, 27, 10, 21, 22, 17, 26, 16, 28, 29, 31,\n         33, 38, 93, 40, 95, 41, 34, 36, 94, 39, 43, 35, 44, 48, 37, 42,\n         49, 54, 77, 56, 79, 57, 50, 52, 78, 55, 59, 51, 60, 64, 53, 58,\n         65, 70, 61, 72, 63, 73, 66, 68, 62, 71, 75, 67, 76, 80, 69, 74,\n         81, 86, 45, 88, 47, 89, 82, 84, 46, 87, 91, 83, 92, 96, 85, 90,\n         25, 23, 19, 100}                                                 // test case 7\n         };\n\n    // Loop to execute each test case\n    for (int i = 0; i < testCaseCount; ++i) {\n        // Allocate memory on the device\n        int* d_data;\n        int* d_minResult;\n        int* d_maxResult;\n        CUDA_CHECK(cudaMalloc(&d_data, vectorSize[i] * sizeof(int)));\n        CUDA_CHECK(cudaMalloc(&d_minResult, sizeof(int)));\n        CUDA_CHECK(cudaMalloc(&d_maxResult, sizeof(int)));\n        \n        // Initialize result values on the host\n        int minResult = INT_MAX;\n        int maxResult = INT_MIN;        \n        \n        // Copy the data to the device\n        CUDA_CHECK(cudaMemcpy(d_data, inputVector[i], vectorSize[i] * sizeof(int), cudaMemcpyHostToDevice));\n        CUDA_CHECK(cudaMemcpy(d_minResult, &minResult, sizeof(int), cudaMemcpyHostToDevice));\n        CUDA_CHECK(cudaMemcpy(d_maxResult, &maxResult, sizeof(int), cudaMemcpyHostToDevice));\n        \n        // Launch the kernel with enough threads\n        int blockSize = 512;\n        int numBlocks = (vectorSize[i] + blockSize - 1) / blockSize;\n        //Grid: (ceil(vectorSize / 512), 1, 1)\n        //Block: (512, 1, 1)\n        k_findMinMax<<<numBlocks, blockSize>>>(d_data, d_minResult, d_maxResult, vectorSize[i]);\n        \n        // Synchronize the device to ensure the kernel has completed\n        CUDA_CHECK(cudaDeviceSynchronize());\n        \n        // Copy the results back to the CPU\n        CUDA_CHECK(cudaMemcpy(&minResult, d_minResult, sizeof(int), cudaMemcpyDeviceToHost));\n        CUDA_CHECK(cudaMemcpy(&maxResult, d_maxResult, sizeof(int), cudaMemcpyDeviceToHost));\n\n        assert(minResult == expectedMin[i]);\n        assert(maxResult == expectedMax[i]);\n\n        // Free GPU memory\n        cudaFree(d_data);\n        cudaFree(d_minResult);\n        cudaFree(d_maxResult);\n    }\n    return 0;\n}\n\n__global__ void k_findMinMax(int* data, int* minResult, int* maxResult, int n) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/83", "date": "2025-07-30", "prompt": "Write a function that uses the cuBLAS library to compute the Euclidean distances between two vectors.\n\nThe signature of the function is float calculateEuclideanDistance(float* inputVectorA_d, float* inputVectorB_d, int n, cudaStream_t stream, cublasHandle_t handle), where inputVectorA_d and inputVectorB_d are device pointers to the input vectors, n is the number of elements in the vectors, the stream is the CUDA stream for asynchronous execution, and the handle is the cuBLAS handle for managing cuBLAS operations.\n\n>>> calculateEuclideanDistance({41.48, 79.25, 93.81, 75.60, 72.91, 39.09, 34.96, 88.32, 32.21, 95.47}, {7.21, 1.02, 95.87, 61.54, 99.16, 25.89, 85.60, 87.90, 12.21, 22.57}, 10) -> 128.991\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "-lcublas", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n#undef NDEBUG\n#include <assert.h>\n\n#define CUDA_CHECK(call) \\\ndo { \\\n       cudaError_t error = call; \\\n       if (error != cudaSuccess) { \\\n           fprintf(stderr, \"CUDA error at %s:%d %s\\n\", \\\n                   __FILE__, __LINE__, \\\n                   cudaGetErrorString(error)); \\\n           exit(EXIT_FAILURE); \\\n       } \\\n} while(0)\n\nfloat calculateEuclideanDistance(float* inputVectorA_d, float* inputVectorB_d, int n, cudaStream_t stream, cublasHandle_t handle);\n\nvoid launch() {\n    const int testCaseCount = 9; // Number of test cases\n    const int vectorSize[testCaseCount] = {10, 10, 10, 10, 10, 10, 10, 10, 2}; // Sizes of the vectors in each test case\n    const float expectedEuclideanDistance[testCaseCount] = {89.5842f,91.4034f,95.1649f,134.572f,130.933f,104.339f,142.695f, 128.991f, 7.99826f}; // Expected results for each test\n    int maxVectorSize = *std::max_element(vectorSize, vectorSize + testCaseCount);\n    float precisionTolerance=1e-3;\n    float *inputVectorA_d, *inputVectorB_d;\n\n    float inputVectorA_h[testCaseCount][maxVectorSize] = {\n        {10.50f, 82.88f, 77.71f, 54.94f, 88.26f, 57.97f, 33.54f, 40.78f, 21.73f, 42.08f}, // Test case 1\n        {21.01f, 46.84f, 47.99f, 18.44f, 12.15f, 17.98f, 10.74f, 37.09f, 56.35f, 35.77f}, // Test case 2\n        {59.50f, 35.18f, 59.55f, 94.73f, 64.39f, 82.98f, 43.86f, 60.24f, 71.31f, 45.59f}, // Test case 3\n        {13.12f, 66.54f, 13.45f, 91.62f, 45.72f, 86.90f, 69.65f, 4.61f, 90.95f, 56.26f}, // Test case 4\n        {42.22f, 66.05f, 49.51f, 84.67f, 85.78f, 49.76f, 10.12f, 91.17f, 8.28f, 33.39f}, // Test case 5\n        {92.24f, 83.72f, 31.96f, 31.48f, 43.90f, 82.75f, 19.45f, 74.00f, 72.62f, 46.73f}, // Test case 6\n        {26.32f, 80.19f, 61.36f, 76.85f, 91.31f, 82.48f, 32.80f, 65.68f, 22.03f, 63.05f}, // Test case 7\n        {41.48f, 79.25f, 93.81f, 75.60f, 72.91f, 39.09f, 34.96f, 88.32f, 32.21f, 95.47f}, // Test case 8\n        {1.45, 7.56} // Test case 9\n    };\n\n    float inputVectorB_h[testCaseCount][maxVectorSize] = {\n        {18.43f, 47.28f, 60.18f, 30.84f, 95.26f, 35.39f, 42.01f, 57.82f, 83.87f, 10.07f}, // Test case 1\n        {27.45f, 89.76f, 93.14f, 20.94f, 2.15f, 51.48f, 56.54f, 4.93f, 49.58f, 40.89f}, // Test case 2\n        {50.50f, 21.56f, 72.17f, 95.49f, 25.66f, 22.71f, 99.55f, 61.44f, 59.20f, 29.84f}, // Test case 3\n        {43.07f, 66.37f, 23.35f, 27.40f, 10.15f, 10.16f, 87.45f, 32.45f, 27.87f, 83.92f}, // Test case 4\n        {48.77f, 58.12f, 97.22f, 73.13f, 96.90f, 97.11f, 67.14f, 19.07f, 68.33f, 19.01f}, // Test case 5\n        {7.78f, 85.31f, 64.76f, 24.15f, 17.03f, 40.79f, 16.89f, 64.71f, 67.48f, 42.35f}, // Test case 6\n        {97.74f, 61.81f, 7.85f, 44.32f, 96.69f, 9.15f, 64.12f, 30.51f, 34.74f, 6.34f}, // Test case 7\n        {7.21f, 1.02f, 95.87f, 61.54f, 99.16f, 25.89f, 85.60f, 87.90f, 12.21f, 22.57f}, // Test case 8\n        {7.81f, 12.41f} // Test case 9\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Initialize cuBLAS\n    cublasHandle_t handle;\n    cublasCreate(&handle);\n    cublasSetStream(handle, stream);\n\n    // Allocate device memory on the GPU for the vectors\n    CUDA_CHECK(cudaMallocAsync(&inputVectorA_d, maxVectorSize * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&inputVectorB_d, maxVectorSize * sizeof(float), stream));\n\n    // Loop to execute each test case\n    for (int i = 0; i < testCaseCount; ++i) {\n        // Copy data from the host (CPU) to the device (GPU)\n        CUDA_CHECK(cudaMemcpyAsync(inputVectorA_d, inputVectorA_h[i], vectorSize[i] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(inputVectorB_d, inputVectorB_h[i], vectorSize[i] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        float euclideanDistance = calculateEuclideanDistance(inputVectorA_d, inputVectorB_d, vectorSize[i], stream, handle);\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        assert(fabs(euclideanDistance - expectedEuclideanDistance[i]) < precisionTolerance);\n    }\n\n    // Clean up\n    cublasDestroy(handle);\n    CUDA_CHECK(cudaFreeAsync(inputVectorA_d, stream));\n    CUDA_CHECK(cudaFreeAsync(inputVectorB_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\nfloat calculateEuclideanDistance(float* inputVectorA_d, float* inputVectorB_d, int n, cudaStream_t stream, cublasHandle_t handle) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/84", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform a Linear Translation using unified memory, where each thread updates the coordinates (x, y, z) by applying translation offsets.\n\nThe signature of function is __global__ void k_translatePoints(float* x_coords, float* y_coords, float* z_coords, float tx, float ty, float tz, int size).\n\n>>> k_translatePoints({0,1,2,3,4}, {0,1,2,3,4}, {0,1,2,3,4}, 1, 1, 1, 5) -> x_coords: {1,2,3,4,5}, y_coords: {1,2,3,4,5}, z_coords: {1,2,3,4,5})\n>>> k_translatePoints({0,1,2,3,4,5,6,7,8,9}, {0,1,2,3,4,5,6,7,8,9}, {0,1,2,3,4,5,6,7,8,9}, 0.1, 0.2, 0.3, 10) -> x_coords: {0.1,1.1,2.1,3.1,4.1,5.1,6.1,7.1,8.1,9.1}, y_coords: {0.2,1.2,2.2,3.2,4.2,5.2,6.2,7.2,8.2,9.2}, z_coords: {0.3,1.3,2.3,3.3,4.3,5.3,6.3,7.3,8.3,9.3} \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#undef NDEBUG\n#include <assert.h>\n\n__global__ void k_translatePoints(float* x_coords, float* y_coords, float* z_coords, float tx, float ty, float tz, int size);\n\nvoid launch() {\n    // Define 7 test cases\n    const int num_tests = 7;\n\n    // Arrays to hold input sizes, translation vectors, and tolerance for each test case\n    int sizes[num_tests] = { 5, 1000, 10, 5, 1000, 1, 10 };\n    float translations[num_tests][3] = {\n        {1.0f, 1.0f, 1.0f},  // Translation vector for Test Case 1\n        {2.0f, 2.0f, 2.0f},  // Test Case 2\n        {0.1f, 0.2f, 0.3f},  // Test Case 3\n        {-1.0f, -2.0f, -3.0f},  // Test Case 4\n        {0.0f, 0.0f, 0.0f},  // Test Case 5 (no translation)\n        {5.0f, 5.0f, 5.0f},  // Test Case 6\n        {0.5f, 0.5f, 0.5f}   // Test Case 7\n    };\n    float tolerances[num_tests] = { 0.1f, 0.1f, 0.01f, 0.2f, 0.01f, 0.1f, 0.05f }; // Tolerances for each test case\n\n    for (int t = 0; t < num_tests; t++) {\n        int size = sizes[t];\n        float tx = translations[t][0];\n        float ty = translations[t][1];\n        float tz = translations[t][2];\n        float tolerance = tolerances[t];\n\n        // Allocate memory for the x, y, and z coordinate arrays using managed memory\n        float* x_coords, * y_coords, * z_coords;\n        cudaMallocManaged(&x_coords, size * sizeof(float));\n        cudaMallocManaged(&y_coords, size * sizeof(float));\n        cudaMallocManaged(&z_coords, size * sizeof(float));\n\n        // Allocate memory for the expected results\n        float* expected_x = new float[size];\n        float* expected_y = new float[size];\n        float* expected_z = new float[size];\n\n        // Initialize points and expected results\n        for (int i = 0; i < size; i++) {\n            x_coords[i] = float(i); // Initial x-coordinate\n            y_coords[i] = float(i); // Initial y-coordinate\n            z_coords[i] = float(i); // Initial z-coordinate\n            expected_x[i] = float(i) + tx;  // Expected x-coordinate\n            expected_y[i] = float(i) + ty;  // Expected y-coordinate\n            expected_z[i] = float(i) + tz;  // Expected z-coordinate\n        }\n\n        // Launch translation kernel\n        k_translatePoints<<<(size + 255) / 256, 256>>>(x_coords, y_coords, z_coords, tx, ty, tz, size);\n        cudaDeviceSynchronize();\n\n        // Validate the results with assertions and a tolerance\n        for (int i = 0; i < size; i++) {\n            // Check if the result is within the expected tolerance for x, y, and z coordinates\n            assert(std::abs(x_coords[i] - expected_x[i]) <= tolerance);\n            assert(std::abs(y_coords[i] - expected_y[i]) <= tolerance);\n            assert(std::abs(z_coords[i] - expected_z[i]) <= tolerance);\n        }\n\n        // Free managed memory\n        cudaFree(x_coords);\n        cudaFree(y_coords);\n        cudaFree(z_coords);\n        delete[] expected_x;\n        delete[] expected_y;\n        delete[] expected_z;\n    }\n}\n\n__global__ void k_translatePoints(float* x_coords, float* y_coords, float* z_coords, float tx, float ty, float tz, int size) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/85", "date": "2025-07-30", "prompt": "Create a CUDA kernel for RGB to grayscale conversion, where each thread processes a single pixel using a weighted sum of RGB channels.\n\nThe signature of the functions is __global__ void k_rgbToGrayscaleKernel(unsigned char* rgbInput_d, float* weights_d, unsigned char* grayscaleOutput_d, int width, int height), where rgbInput_d is input pixels of RGB, weights_d is the weights for RGB conversion, grayscaleOutput_d is converted grayscale output, width & height are the dimension of the image.\n\n>>> k_rgbToGrayscaleKernel({{255, 0, 0}, {0, 255, 0}, {0, 0, 255}, {255, 255, 0}, {255, 255, 255}}, {0.299, 0.587, 0.114}, grayscaleOutput_d, 5, 1)-> grayscaleOutput_d: ({76, 149, 29, 225, 255})\n>>> k_rgbToGrayscaleKernel({{123, 231, 12}, {45, 67, 89}, {190, 12, 220}, {12, 180, 45}, {255, 123, 89}}, {0.299, 0.587, 0.114}, grayscaleOutput_d, 5, 1)-> grayscaleOutput_d: ({173, 62, 88, 114, 158}) \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cstdio>\n#undef NDEBUG\n#include <assert.h>\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n\n__global__ void k_rgbToGrayscaleKernel(unsigned char* rgbInput_d, float* weights_d, unsigned char* grayscaleOutput_d, int width, int height);\n\nvoid launch() {\n    const int BLOCK_SIZE = 16;\n    // Input RGB values for 7 test cases\n    unsigned char testInputRGB[7][8][3] = {\n        { {255, 0, 0}, {0, 255, 0}, {0, 0, 255}, {255, 255, 0}, {255, 255, 255} },        // Test case 1\n        { {123, 231, 12}, {45, 67, 89}, {190, 12, 220}, {12, 180, 45}, {255, 123, 89} },  // Test case 2\n        { {0, 0, 0}, {128, 128, 128}, {64, 64, 64}, {255, 255, 255}, {1, 2, 3} },         // Test case 3\n        { {12, 34, 56}, {78, 90, 123}, {200, 150, 100}, {255, 0, 255}, {150, 75, 0} },    // Test case 4\n        { {255, 128, 0}, {0, 128, 255}, {128, 255, 128}, {64, 128, 192}, {192, 128, 64} },// Test case 5\n        { {50, 150, 200}, {200, 50, 150}, {150, 200, 50}, {100, 100, 100}, {0, 0, 0} },   // Test case 6\n        { {100, 200, 50}, {50, 100, 200}, {200, 50, 100}, {123, 45, 67}, {12, 180, 45} }  // Test case 7\n    };\n\n    // Expected grayscale output for each test case\n    unsigned char expectedOutputGray[7][8] = {\n        {76, 149, 29, 225, 255},   // Test case 1\n        {173, 62, 88, 114, 158},   // Test case 2\n        {0, 128, 64, 255, 1},      // Test case 3\n        {29, 90, 159, 105, 88},    // Test case 4\n        {151, 104, 202, 116, 139}, // Test case 5\n        {125, 106, 167, 100, 0},   // Test case 6\n        {153, 96, 100, 70, 114}    // Test case 7\n    };\n\n    float grayscaleInput[3] = {0.299, 0.587, 0.114};\n    int width = 5;  // 5 RGB values in each test case\n    int height = 1; // Single row of RGB values in each test case\n    int channels = 3; // RGB has 3 channels\n    float* weights_d;\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    CUDA_CHECK(cudaMallocAsync(&weights_d, 3 * sizeof(float), stream));\n    CUDA_CHECK(cudaMemcpyAsync(weights_d, grayscaleInput, 3 * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n    // Allocate device memory for input and output\n    unsigned char* rgbInput_d;\n    unsigned char* grayscaleOutput_d;\n    CUDA_CHECK(cudaMallocAsync(&rgbInput_d, width * height * channels * sizeof(unsigned char), stream));\n    CUDA_CHECK(cudaMallocAsync(&grayscaleOutput_d, width * height * sizeof(unsigned char), stream));\n    // Allocate host memory for output\n    unsigned char grayscaleOutput_h[5];\n\n    // Iterate over test cases\n    for (int i = 0; i < 7; ++i) {\n        // Copy input RGB values to device\n        CUDA_CHECK(cudaMemcpyAsync(rgbInput_d, testInputRGB[i], width * height * channels * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n\n        // Define block and grid sizes\n        dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 grid((width + block.x - 1) / block.x, (height + block.y - 1) / block.y);\n\n        // Grid: (ceil(width / 16), ceil(width / 16), 1)\n        // Block: (16, 16, 1)\n        void *args[] = {&rgbInput_d, &weights_d, &grayscaleOutput_d, &width, &height};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_rgbToGrayscaleKernel, grid, block, args, BLOCK_SIZE * BLOCK_SIZE * sizeof(int), stream));\n\n        // Copy result back to host\n        CUDA_CHECK(cudaMemcpyAsync(grayscaleOutput_h, grayscaleOutput_d, width * height * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Compare the result with the expected output\n        for (int j = 0; j < 5; ++j) {\n            assert(grayscaleOutput_h[j] == expectedOutputGray[i][j]);\n        }\n    }\n\n    // Free device memory\n    CUDA_CHECK(cudaFreeAsync(rgbInput_d, stream));\n    CUDA_CHECK(cudaFreeAsync(grayscaleOutput_d, stream));\n    CUDA_CHECK(cudaFreeAsync(weights_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream)); \n}\n\n__global__ void k_rgbToGrayscaleKernel(unsigned char* rgbInput_d, float* weights_d, unsigned char* grayscaleOutput_d, int width, int height) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/86", "date": "2025-07-30", "prompt": "Write a CUDA kernel k_sumSpectralBands using managed memory to perform summation across spectral bands for each pixel in a 3D hyperspectral data cube. The kernel should reduce the 3D data cube (height  width  bands) to a 2D output (height  width) by summing along the spectral bands, optimizing for real-time processing of large datasets.\n\nThe signature of the function is __global__ void k_sumSpectralBands(float* inputData_d, float* outputData_d, int x_dim, int y_dim, int z_dim).\n\n>>> k_sumSpectralBands({10, 20, 30, 15, 25, 35, 11, 21, 31, 9, 19, 29}) -> ({60, 75, 63, 57})\n>>> k_sumSpectralBands({5, 15, 25, 10, 20, 30, 7, 17, 27, 12, 22, 32}) -> ({45, 60, 51, 66}) \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n\n#define X_DIM 4   // Number of rows (spatial)\n#define Y_DIM 4   // Number of columns (spatial)\n#define Z_DIM 3   // Number of spectral bands (depth)\n\n#undef NDEBUG\n#include <assert.h>\n\n__global__ void k_sumSpectralBands(float* inputData_d, float* outputData_d, int x_dim, int y_dim, int z_dim);\n\nvoid launch() {\n    // Test Case 1\n    {\n        // Define the dimensions of the hyperspectral data cube\n        int x_dim = X_DIM;\n        int y_dim = Y_DIM;\n        int z_dim = Z_DIM;\n\n        // Predefined input data for testing (non-sequential values)\n        float inputData_h[X_DIM * Y_DIM * Z_DIM] = {\n            10, 20, 30,    15, 25, 35,   11, 21, 31,   9, 19, 29,     // First row of Z bands\n            12, 22, 32,    17, 27, 37,   13, 23, 33,   8, 18, 28,     // Second row of Z bands\n            14, 24, 34,    19, 29, 39,   16, 26, 36,   7, 17, 27,     // Third row of Z bands\n            18, 28, 38,    10, 30, 40,   20, 30, 50,   6, 16, 26      // Fourth row of Z bands\n        };\n\n        float expectedOutput_h[X_DIM * Y_DIM] = {\n            60, 75, 63, 57,\n            66, 81, 69, 54,\n            72, 87, 78, 51,\n            84, 80, 100, 48\n        };\n\n        // Allocate managed memory for the 2D output array on the device\n        float* outputData_d;\n        cudaMallocManaged(&outputData_d, x_dim * y_dim * sizeof(float));\n\n        // Allocate managed memory for the input data on the device and copy from predefined inputData_h\n        float* inputData_d;\n        cudaMallocManaged(&inputData_d, x_dim * y_dim * z_dim * sizeof(float));\n        cudaMemcpy(inputData_d, inputData_h, x_dim * y_dim * z_dim * sizeof(float), cudaMemcpyHostToDevice);\n\n        // Define block size and grid size\n        dim3 blockSize(4, 4); // Each block has 4x4 threads\n        dim3 gridSize((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y);\n\n        // Block: (4, 4, 1)\n        // Grid: ((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y, 1)\n        k_sumSpectralBands << <gridSize, blockSize >> > (inputData_d, outputData_d, x_dim, y_dim, z_dim);\n\n        // Wait for the GPU to finish before accessing the results\n        cudaDeviceSynchronize();\n\n        for (int i = 0; i < x_dim; ++i) {\n            for (int j = 0; j < y_dim; ++j) {\n                int idx = i * y_dim + j;\n                assert(outputData_d[idx] == expectedOutput_h[idx]);\n            }\n        }\n\n        // Free the allocated memory\n        cudaFree(inputData_d);\n        cudaFree(outputData_d);\n    }\n\n    // Test Case 2\n    {\n        // Define the dimensions of the hyperspectral data cube\n        int x_dim = X_DIM;\n        int y_dim = Y_DIM;\n        int z_dim = Z_DIM;\n\n        // Predefined input data for testing (non-sequential values)\n        float inputData_h[X_DIM * Y_DIM * Z_DIM] = {\n            5, 15, 25,    10, 20, 30,   7, 17, 27,    12, 22, 32,\n            9, 19, 29,    14, 24, 34,   6, 16, 26,    8, 18, 28,\n            11, 21, 31,   13, 23, 33,   15, 25, 35,   9, 19, 29,\n            10, 20, 30,   18, 28, 38,   12, 22, 32,   7, 17, 27\n        };\n\n        float expectedOutput_h[X_DIM * Y_DIM] = {\n            45, 60, 51, 66,\n            57, 72, 48, 54,\n            63, 69, 75, 57,\n            60, 84, 66, 51\n        };\n\n        // Allocate managed memory for the 2D output array on the device\n        float* outputData_d;\n        cudaMallocManaged(&outputData_d, x_dim * y_dim * sizeof(float));\n\n        // Allocate managed memory for the input data on the device and copy from predefined inputData_h\n        float* inputData_d;\n        cudaMallocManaged(&inputData_d, x_dim * y_dim * z_dim * sizeof(float));\n        cudaMemcpy(inputData_d, inputData_h, x_dim * y_dim * z_dim * sizeof(float), cudaMemcpyHostToDevice);\n\n        // Define block size and grid size\n        dim3 blockSize(4, 4); // Each block has 4x4 threads\n        dim3 gridSize((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y);\n\n        // Block: (4, 4, 1)\n        // Grid: ((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y, 1)\n        k_sumSpectralBands << <gridSize, blockSize >> > (inputData_d, outputData_d, x_dim, y_dim, z_dim);\n\n        // Wait for the GPU to finish before accessing the results\n        cudaDeviceSynchronize();\n\n        for (int i = 0; i < x_dim; ++i) {\n            for (int j = 0; j < y_dim; ++j) {\n                int idx = i * y_dim + j;\n                assert(outputData_d[idx] == expectedOutput_h[idx]);\n            }\n        }\n\n        // Free the allocated memory\n        cudaFree(inputData_d);\n        cudaFree(outputData_d);\n    }\n\n    // Test Case 3\n    {\n        // Define the dimensions of the hyperspectral data cube\n        int x_dim = X_DIM;\n        int y_dim = Y_DIM;\n        int z_dim = Z_DIM;\n\n        // Predefined input data for testing (non-sequential values)\n        float inputData_h[X_DIM * Y_DIM * Z_DIM] = {\n            13, 23, 33,   19, 29, 39,   14, 24, 34,   16, 26, 36,\n            8, 18, 28,    10, 20, 30,   5, 15, 25,    6, 16, 26,\n            17, 27, 37,   21, 31, 41,   11, 21, 31,   15, 25, 35,\n            12, 22, 32,   9, 19, 29,    14, 24, 34,   18, 28, 38\n        };\n\n        float expectedOutput_h[X_DIM * Y_DIM] = {\n            69, 87, 72, 78,\n            54, 60, 45, 48,\n            81, 93, 63, 75,\n            66, 57, 72, 84\n        };\n\n        // Allocate managed memory for the 2D output array on the device\n        float* outputData_d;\n        cudaMallocManaged(&outputData_d, x_dim * y_dim * sizeof(float));\n\n        // Allocate managed memory for the input data on the device and copy from predefined inputData_h\n        float* inputData_d;\n        cudaMallocManaged(&inputData_d, x_dim * y_dim * z_dim * sizeof(float));\n        cudaMemcpy(inputData_d, inputData_h, x_dim * y_dim * z_dim * sizeof(float), cudaMemcpyHostToDevice);\n\n        // Define block size and grid size\n        dim3 blockSize(4, 4); // Each block has 4x4 threads\n        dim3 gridSize((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y);\n\n        // Block: (4, 4, 1)\n        // Grid: ((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y, 1)\n        k_sumSpectralBands << <gridSize, blockSize >> > (inputData_d, outputData_d, x_dim, y_dim, z_dim);\n\n        // Wait for the GPU to finish before accessing the results\n        cudaDeviceSynchronize();\n\n        for (int i = 0; i < x_dim; ++i) {\n            for (int j = 0; j < y_dim; ++j) {\n                int idx = i * y_dim + j;\n                assert(outputData_d[idx] == expectedOutput_h[idx]);\n            }\n        }\n\n        // Free the allocated memory\n        cudaFree(inputData_d);\n        cudaFree(outputData_d);\n    }\n\n    // Test Case 4\n    {\n        // Define the dimensions of the hyperspectral data cube\n        int x_dim = X_DIM;\n        int y_dim = Y_DIM;\n        int z_dim = Z_DIM;\n\n        // Predefined input data for testing (non-sequential values)\n        float inputData_h[X_DIM * Y_DIM * Z_DIM] = {\n            20, 30, 40,   22, 32, 42,   16, 26, 36,   18, 28, 38,\n            11, 21, 31,   13, 23, 33,   9, 19, 29,    10, 20, 30,\n            7, 17, 27,    12, 22, 32,   5, 15, 25,    8, 18, 28,\n            17, 27, 37,   15, 25, 35,   14, 24, 34,   19, 29, 39\n        };\n\n        float expectedOutput_h[X_DIM * Y_DIM] = {\n            90, 96, 78, 84,\n            63, 69, 57, 60,\n            51, 66, 45, 54,\n            81, 75, 72, 87\n        };\n\n        // Allocate managed memory for the 2D output array on the device\n        float* outputData_d;\n        cudaMallocManaged(&outputData_d, x_dim * y_dim * sizeof(float));\n\n        // Allocate managed memory for the input data on the device and copy from predefined inputData_h\n        float* inputData_d;\n        cudaMallocManaged(&inputData_d, x_dim * y_dim * z_dim * sizeof(float));\n        cudaMemcpy(inputData_d, inputData_h, x_dim * y_dim * z_dim * sizeof(float), cudaMemcpyHostToDevice);\n\n        // Define block size and grid size\n        dim3 blockSize(4, 4); // Each block has 4x4 threads\n        dim3 gridSize((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y);\n\n        // Block: (4, 4, 1)\n        // Grid: ((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y, 1)\n        k_sumSpectralBands << <gridSize, blockSize >> > (inputData_d, outputData_d, x_dim, y_dim, z_dim);\n\n        // Wait for the GPU to finish before accessing the results\n        cudaDeviceSynchronize();\n\n        for (int i = 0; i < x_dim; ++i) {\n            for (int j = 0; j < y_dim; ++j) {\n                int idx = i * y_dim + j;\n                assert(outputData_d[idx] == expectedOutput_h[idx]);\n            }\n        }\n\n        // Free the allocated memory\n        cudaFree(inputData_d);\n        cudaFree(outputData_d);\n    }\n\n    // Test Case 5\n    {\n        // Define the dimensions of the hyperspectral data cube\n        int x_dim = X_DIM;\n        int y_dim = Y_DIM;\n        int z_dim = Z_DIM;\n\n        // Predefined input data for testing (non-sequential values)\n        float inputData_h[X_DIM * Y_DIM * Z_DIM] = {\n            18, 28, 38,   16, 26, 36,   14, 24, 34,   12, 22, 32,\n            10, 20, 30,   13, 23, 33,   7, 17, 27,    9, 19, 29,\n            11, 21, 31,   8, 18, 28,    15, 25, 35,   6, 16, 26,\n            20, 30, 40,   5, 15, 25,    19, 29, 39,   17, 27, 37\n        };\n\n        float expectedOutput_h[X_DIM * Y_DIM] = {\n            84, 78, 72, 66,\n            60, 69, 51, 57,\n            63, 54, 75, 48,\n            90, 45, 87, 81\n        };\n\n        // Allocate managed memory for the 2D output array on the device\n        float* outputData_d;\n        cudaMallocManaged(&outputData_d, x_dim * y_dim * sizeof(float));\n\n        // Allocate managed memory for the input data on the device and copy from predefined inputData_h\n        float* inputData_d;\n        cudaMallocManaged(&inputData_d, x_dim * y_dim * z_dim * sizeof(float));\n        cudaMemcpy(inputData_d, inputData_h, x_dim * y_dim * z_dim * sizeof(float), cudaMemcpyHostToDevice);\n\n        // Define block size and grid size\n        dim3 blockSize(4, 4); // Each block has 4x4 threads\n        dim3 gridSize((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y);\n\n        // Block: (4, 4, 1)\n        // Grid: ((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y, 1)\n        k_sumSpectralBands << <gridSize, blockSize >> > (inputData_d, outputData_d, x_dim, y_dim, z_dim);\n\n        // Wait for the GPU to finish before accessing the results\n        cudaDeviceSynchronize();\n\n        for (int i = 0; i < x_dim; ++i) {\n            for (int j = 0; j < y_dim; ++j) {\n                int idx = i * y_dim + j;\n                assert(outputData_d[idx] == expectedOutput_h[idx]);\n            }\n        }\n\n        // Free the allocated memory\n        cudaFree(inputData_d);\n        cudaFree(outputData_d);\n    }\n\n    // Test Case 6\n    {\n        // Define the dimensions of the hyperspectral data cube\n        int x_dim = X_DIM;\n        int y_dim = Y_DIM;\n        int z_dim = Z_DIM;\n\n        // Predefined input data for testing (non-sequential values)\n        float inputData_h[X_DIM * Y_DIM * Z_DIM] = {\n            12, 22, 32,   14, 24, 34,   19, 29, 39,   15, 25, 35,\n            7, 17, 27,    8, 18, 28,    13, 23, 33,   10, 20, 30,\n            16, 26, 36,   11, 21, 31,   14, 24, 34,   9, 19, 29,\n            5, 15, 25,    20, 30, 40,   18, 28, 38,   6, 16, 26\n        };\n\n        float expectedOutput_h[X_DIM * Y_DIM] = {\n            66, 72, 87, 75,\n            51, 54, 69, 60,\n            78, 63, 72, 57,\n            45, 90, 84, 48\n        };\n\n        // Allocate managed memory for the 2D output array on the device\n        float* outputData_d;\n        cudaMallocManaged(&outputData_d, x_dim * y_dim * sizeof(float));\n\n        // Allocate managed memory for the input data on the device and copy from predefined inputData_h\n        float* inputData_d;\n        cudaMallocManaged(&inputData_d, x_dim * y_dim * z_dim * sizeof(float));\n        cudaMemcpy(inputData_d, inputData_h, x_dim * y_dim * z_dim * sizeof(float), cudaMemcpyHostToDevice);\n\n        // Define block size and grid size\n        dim3 blockSize(4, 4); // Each block has 4x4 threads\n        dim3 gridSize((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y);\n\n        // Block: (4, 4, 1)\n        // Grid: ((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y, 1)\n        k_sumSpectralBands << <gridSize, blockSize >> > (inputData_d, outputData_d, x_dim, y_dim, z_dim);\n\n        // Wait for the GPU to finish before accessing the results\n        cudaDeviceSynchronize();\n\n        for (int i = 0; i < x_dim; ++i) {\n            for (int j = 0; j < y_dim; ++j) {\n                int idx = i * y_dim + j;\n                assert(outputData_d[idx] == expectedOutput_h[idx]);\n            }\n        }\n\n        // Free the allocated memory\n        cudaFree(inputData_d);\n        cudaFree(outputData_d);\n    }\n\n    // Test Case 7\n    {\n        // Define the dimensions of the hyperspectral data cube\n        int x_dim = X_DIM;\n        int y_dim = Y_DIM;\n        int z_dim = Z_DIM;\n\n        // Predefined input data for testing (non-sequential values)\n        float inputData_h[X_DIM * Y_DIM * Z_DIM] = {\n            9, 19, 29,    8, 18, 28,    14, 24, 34,   10, 20, 30,\n            12, 22, 32,   15, 25, 35,   11, 21, 31,   5, 15, 25,\n            17, 27, 37,   16, 26, 36,   18, 28, 38,   19, 29, 39,\n            13, 23, 33,   7, 17, 27,    6, 16, 26,    20, 30, 40\n        };\n\n        float expectedOutput_h[X_DIM * Y_DIM] = {\n            57, 54, 72, 60,\n            66, 75, 63, 45,\n            81, 78, 84, 87,\n            69, 51, 48, 90\n        };\n\n        // Allocate managed memory for the 2D output array on the device\n        float* outputData_d;\n        cudaMallocManaged(&outputData_d, x_dim * y_dim * sizeof(float));\n\n        // Allocate managed memory for the input data on the device and copy from predefined inputData_h\n        float* inputData_d;\n        cudaMallocManaged(&inputData_d, x_dim * y_dim * z_dim * sizeof(float));\n        cudaMemcpy(inputData_d, inputData_h, x_dim * y_dim * z_dim * sizeof(float), cudaMemcpyHostToDevice);\n\n        // Define block size and grid size\n        dim3 blockSize(4, 4); // Each block has 4x4 threads\n        dim3 gridSize((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y);\n\n        // Block: (4, 4, 1)\n        // Grid: ((x_dim + blockSize.x - 1) / blockSize.x, (y_dim + blockSize.y - 1) / blockSize.y, 1)\n        k_sumSpectralBands << <gridSize, blockSize >> > (inputData_d, outputData_d, x_dim, y_dim, z_dim);\n\n        // Wait for the GPU to finish before accessing the results\n        cudaDeviceSynchronize();\n\n        for (int i = 0; i < x_dim; ++i) {\n            for (int j = 0; j < y_dim; ++j) {\n                int idx = i * y_dim + j;\n                assert(outputData_d[idx] == expectedOutput_h[idx]);\n            }\n        }\n\n        // Free the allocated memory\n        cudaFree(inputData_d);\n        cudaFree(outputData_d);\n    }\n}\n\n__global__ void k_sumSpectralBands(float* inputData_d, float* outputData_d, int x_dim, int y_dim, int z_dim) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/87", "date": "2025-07-30", "prompt": "Write a CUDA kernel to apply a 3x3 sharpening filter. Each thread computes one pixel for efficient access to neighboring pixels and handles zero-padding at image boundaries.\n\nSharpening Kernel = [0  -1  0\n                     -1  5  -1\n                     0  -1  0]  \n\nThe signature of the function is __global__ void k_sharpenImage(const unsigned char* inputImage, unsigned char* outputImage, int width, int height).\n\n>>> k_sharpenImage({10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250}, outputImage, 5, 5) -> outputImage: {0,0,10,30,110,110,70,80,90,210,210,120,130,140,255,255,170,180,190,255,255,255,255,255,255}\n>>> k_sharpenImage({255,254,253,252,251,250,249,248,247,246,245,244,243,242,241,240,239,238,237,236,235,234,233,232,2310},outputImage, 5, 5) -> outputImage: {255,255,255,255,255,255,249,248,247,255,255,244,243,242,255,255,239,238,237,255,255,255,255,255,255} \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cstdio>\n\n#define CUDA_CHECK(call)                                                          \\\n    {                                                                             \\\n        cudaError_t err = call;                                                   \\\n        if (err != cudaSuccess) {                                                 \\\n            fprintf(stderr, \"CUDA error in file '%s' in line %d: %s.\\n\",          \\\n                    __FILE__, __LINE__, cudaGetErrorString(err));                 \\\n            exit(EXIT_FAILURE);                                                   \\\n        }                                                                         \\\n    }\n\n#undef NDEBUG\n#include <assert.h>\n\n__global__ void k_sharpenImage(const unsigned char* inputImage, unsigned char* outputImage, int width, int height);\n\nvoid launch() {\n    const int BLOCK_SIZE = 16;\n    int W = 5;\n    int H = 5;\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate host memory for output image\n    unsigned char outputImage_h [W * H];\n\n    // Allocate device memory\n    unsigned char* inputImage_d, * outputImage_d;\n    CUDA_CHECK(cudaMallocAsync(&inputImage_d, W * H * sizeof(unsigned char), stream));\n    CUDA_CHECK(cudaMallocAsync(&outputImage_d, W * H * sizeof(unsigned char), stream));\n\n    //Test Case 1\n    {\n        unsigned char inputImage_h [W * H] = {\n            10, 20, 30, 40, 50,\n            60, 70, 80, 90, 100,\n            110, 120, 130, 140, 150,\n            160, 170, 180, 190, 200,\n            210, 220, 230, 240, 250\n        };\n\n        unsigned char expectedOutput_h [W * H] = {\n            0, 0, 10, 30, 110,\n            110, 70, 80, 90, 210,\n            210, 120, 130, 140, 255,\n            255, 170, 180, 190, 255,\n            255, 255, 255, 255, 255\n        };\n\n        CUDA_CHECK(cudaMemcpyAsync(inputImage_d, inputImage_h, W * H * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n\n        // Define block and grid sizes\n        dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 gridDim((W + blockDim.x - 1) / blockDim.x, (H + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(W / 16), ceil(H / 16), 1)        \n        // Block: (16, 16, 1)\n        void *args[] = {&inputImage_d, &outputImage_d, &W, &H};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_sharpenImage, gridDim, blockDim, args, BLOCK_SIZE * BLOCK_SIZE * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(outputImage_h, outputImage_d, W * H * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < H; y++) {\n            for (int x = 0; x < W; x++) {\n                assert(outputImage_h[y * W + x] == expectedOutput_h[y * W + x]);\n            }\n        }\n    }\n\n    //Test Case 2\n    {\n        unsigned char inputImage_h [W * H] = {\n            255, 254, 253, 252, 251,\n            250, 249, 248, 247, 246,\n            245, 244, 243, 242, 241,\n            240, 239, 238, 237, 236,\n            235, 234, 233, 232, 231\n        };\n\n        unsigned char expectedOutput_h [W * H] = {\n            255, 255, 255, 255, 255,\n            255, 249, 248, 247, 255,\n            255, 244, 243, 242, 255,\n            255, 239, 238, 237, 255,\n            255, 255, 255, 255, 255\n        };\n\n        CUDA_CHECK(cudaMemcpyAsync(inputImage_d, inputImage_h, W * H * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n\n        // Define block and grid sizes\n        dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 gridDim((W + blockDim.x - 1) / blockDim.x, (H + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(W / 16), ceil(H / 16), 1)        \n        // Block: (16, 16, 1)\n        void *args[] = {&inputImage_d, &outputImage_d, &W, &H};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_sharpenImage, gridDim, blockDim, args, BLOCK_SIZE * BLOCK_SIZE * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(outputImage_h, outputImage_d, W * H * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < H; y++) {\n            for (int x = 0; x < W; x++) {\n                assert(outputImage_h[y * W + x] == expectedOutput_h[y * W + x]);\n            }\n        }\n    }\n\n    //Test Case 3\n    {\n        unsigned char inputImage_h [W * H] = {\n            0, 10, 20, 30, 40,\n            50, 60, 70, 80, 90,\n            100, 110, 120, 130, 140,\n            150, 160, 170, 180, 190,\n            200, 210, 220, 230, 240\n        };\n\n        unsigned char expectedOutput_h [W * H] = {\n            0, 0, 0, 10, 80,\n            90, 60, 70, 80, 190,\n            190, 110, 120, 130, 255,\n            255, 160, 170, 180, 255,\n            255, 255, 255, 255, 255\n        };\n\n        CUDA_CHECK(cudaMemcpyAsync(inputImage_d, inputImage_h, W * H * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n\n        // Define block and grid sizes\n        dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 gridDim((W + blockDim.x - 1) / blockDim.x, (H + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(W / 16), ceil(H / 16), 1)        \n        // Block: (16, 16, 1)\n        void *args[] = {&inputImage_d, &outputImage_d, &W, &H};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_sharpenImage, gridDim, blockDim, args, BLOCK_SIZE * BLOCK_SIZE * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(outputImage_h, outputImage_d, W * H * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < H; y++) {\n            for (int x = 0; x < W; x++) {\n                assert(outputImage_h[y * W + x] == expectedOutput_h[y * W + x]);\n            }\n        }\n    }\n\n    //Test Case 4\n    {\n        unsigned char inputImage_h [W * H] = {\n            100, 90, 80, 70, 60,\n            50, 40, 30, 20, 10,\n            0, 10, 20, 30, 40,\n            50, 60, 70, 80, 90,\n            100, 110, 120, 130, 140\n        };\n\n        unsigned char expectedOutput_h [W * H] = {\n            255, 230, 210, 190, 220,\n            110, 20, 0, 0, 0,\n            0, 0, 0, 0, 70,\n            90, 60, 70, 80, 190,\n            255, 255, 255, 255, 255\n        };\n\n        CUDA_CHECK(cudaMemcpyAsync(inputImage_d, inputImage_h, W * H * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n\n        // Define block and grid sizes\n        dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 gridDim((W + blockDim.x - 1) / blockDim.x, (H + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(W / 16), ceil(H / 16), 1)        \n        // Block: (16, 16, 1)\n        void *args[] = {&inputImage_d, &outputImage_d, &W, &H};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_sharpenImage, gridDim, blockDim, args, BLOCK_SIZE * BLOCK_SIZE * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(outputImage_h, outputImage_d, W * H * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < H; y++) {\n            for (int x = 0; x < W; x++) {\n                assert(outputImage_h[y * W + x] == expectedOutput_h[y * W + x]);\n            }\n        }\n    }\n\n    //Test Case 5\n    {\n        unsigned char inputImage_h [W * H] = {\n            5, 10, 15, 20, 25,\n            30, 35, 40, 45, 50,\n            55, 60, 65, 70, 75,\n            80, 85, 90, 95, 100,\n            105, 110, 115, 120, 125\n        };\n\n        unsigned char expectedOutput_h [W * H] = {\n            0, 0, 5, 15, 55,\n            55, 35, 40, 45, 105,\n            105, 60, 65, 70, 155,\n            155, 85, 90, 95, 205,\n            255, 245, 255, 255, 255\n        };\n\n        CUDA_CHECK(cudaMemcpyAsync(inputImage_d, inputImage_h, W * H * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n\n        // Define block and grid sizes\n        dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 gridDim((W + blockDim.x - 1) / blockDim.x, (H + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(W / 16), ceil(H / 16), 1)        \n        // Block: (16, 16, 1)\n        void *args[] = {&inputImage_d, &outputImage_d, &W, &H};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_sharpenImage, gridDim, blockDim, args, BLOCK_SIZE * BLOCK_SIZE * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(outputImage_h, outputImage_d, W * H * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < H; y++) {\n            for (int x = 0; x < W; x++) {\n                assert(outputImage_h[y * W + x] == expectedOutput_h[y * W + x]);\n            }\n        }\n    }\n\n    //Test Case 6\n    {\n        unsigned char inputImage_h [W * H] = {\n            0, 0, 0, 0, 0,\n            0, 255, 255, 255, 0,\n            0, 255, 0, 255, 0,\n            0, 255, 255, 255, 0,\n            0, 0, 0, 0, 0\n        };\n\n        unsigned char expectedOutput_h [W * H] = {\n            0, 0, 0, 0, 0,\n            0, 255, 255, 255, 0,\n            0, 255, 0, 255, 0,\n            0, 255, 255, 255, 0,\n            0, 0, 0, 0, 0\n        };\n\n        CUDA_CHECK(cudaMemcpyAsync(inputImage_d, inputImage_h, W * H * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n\n        // Define block and grid sizes\n        dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 gridDim((W + blockDim.x - 1) / blockDim.x, (H + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(W / 16), ceil(H / 16), 1)        \n        // Block: (16, 16, 1)\n        void *args[] = {&inputImage_d, &outputImage_d, &W, &H};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_sharpenImage, gridDim, blockDim, args, BLOCK_SIZE * BLOCK_SIZE * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(outputImage_h, outputImage_d, W * H * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < H; y++) {\n            for (int x = 0; x < W; x++) {\n                assert(outputImage_h[y * W + x] == expectedOutput_h[y * W + x]);\n            }\n        }\n    }\n\n    //Test Case 7\n    {\n        unsigned char inputImage_h [W * H] = {\n            255, 0, 255, 0, 255,\n            0, 255, 0, 255, 0,\n            255, 0, 255, 0, 255,\n            0, 255, 0, 255, 0,\n            255, 0, 255, 0, 255\n        };\n\n        unsigned char expectedOutput_h [W * H] = {\n            255, 0, 255, 0, 255,\n            0, 255, 0, 255, 0,\n            255, 0, 255, 0, 255,\n            0, 255, 0, 255, 0,\n            255, 0, 255, 0, 255\n        };\n\n        CUDA_CHECK(cudaMemcpyAsync(inputImage_d, inputImage_h, W * H * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n\n        // Define block and grid sizes\n        dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 gridDim((W + blockDim.x - 1) / blockDim.x, (H + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(W / 16), ceil(H / 16), 1)        \n        // Block: (16, 16, 1)\n        void *args[] = {&inputImage_d, &outputImage_d, &W, &H};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_sharpenImage, gridDim, blockDim, args, BLOCK_SIZE * BLOCK_SIZE * sizeof(int), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(outputImage_h, outputImage_d, W * H * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < H; y++) {\n            for (int x = 0; x < W; x++) {\n                assert(outputImage_h[y * W + x] == expectedOutput_h[y * W + x]);\n            }\n        }\n    }\n\n    // Cleanup\n    CUDA_CHECK(cudaFreeAsync(inputImage_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputImage_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream)); \n}\n\n__global__ void k_sharpenImage(const unsigned char* inputImage, unsigned char* outputImage, int width, int height) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/88", "date": "2025-07-30", "prompt": "Write a CUDA kernel for 1D signal convolution filter with smoothing and boundary condition handling.\n\nThe signature of the function is __global__ void k_convolve1D(const float* signal, const float* filter, float* output, int signalLen, int filterLen).\n\n>>> k_convolve1D({3.0f, 1.0f, 4.0f, 1.0f, 5.0f, 9.0f, 2.0f, 6.0f, 5.0f, 3.0f}, {0.25, 0.5, 0.25}, output, 10, 3)-> output: ({0.75, 1.75, 2.25, 2.50, 2.75, 5.00, 6.25, 4.75, 4.75, 4.75})\n>>> k_convolve1D({1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {0.25, 0.5, 0.25}, output, 10, 3)-> output: ({0.25, 1.00, 2.00, 3.00, 4.00, 5.00, 6.00, 7.00, 8.00, 9.00}) \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cstdio>\n\n#define CUDA_CHECK(call) {                                                        \\\n        cudaError_t err = call;                                                   \\\n        if (err != cudaSuccess) {                                                 \\\n            fprintf(stderr, \"CUDA error in file '%s' in line %d: %s.\\n\",          \\\n                    __FILE__, __LINE__, cudaGetErrorString(err));                 \\\n            exit(EXIT_FAILURE);                                                   \\\n        }                                                                         \\\n}\n\n#undef NDEBUG\n#include <assert.h>\n\n#define TOLERANCE 1E-2\n\n__global__ void k_convolve1D(const float* signal, const float* filter, float* output, int signalLen, int filterLen);\n\nvoid launch() {\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    int signalLen = 10;\n    int filterLen = 3;\n    // Allocate device memory\n    float* signal_d, * filter_d, * output_d;\n    // Allocate and check device memory\n    CUDA_CHECK(cudaMallocAsync(&signal_d, signalLen * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&filter_d, filterLen * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, signalLen * sizeof(float), stream));\n\n    //Test Case 1\n    {\n        // Non-sequential input signal and kernel\n        float signal_h[10] = {3.0f, 1.0f, 4.0f, 1.0f, 5.0f, 9.0f, 2.0f, 6.0f, 5.0f, 3.0f};\n        float filter_h[3] = { 0.25, 0.5, 0.25 };\n        float output_h[10] = { 0 };\n\n        float expectedOutput_h[10] = { 0.75, 1.75, 2.25, 2.50, 2.75, 5.00, 6.25, 4.75, 4.75, 4.75 };\n    \n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(signal_d, signal_h, signalLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(filter_d, filter_h, filterLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Define block size and grid size\n        int blockSize = 16;\n        int gridSize = (signalLen + blockSize - 1) / blockSize;\n\n        // Block: (16, 1, 1)\n        // Grid: (ceil(signalLen / 16), 1, 1)\n        void *args[] = {&signal_d, &filter_d, &output_d, &signalLen, &filterLen};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_convolve1D, gridSize, blockSize, args, blockSize * sizeof(int), stream));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, signalLen * sizeof(float), cudaMemcpyDeviceToHost, stream));\n    \n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validate output\n        for (int i = 0; i < signalLen; ++i) {\n            assert(abs(output_h[i] - expectedOutput_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 2\n    {\n        // Non-sequential input signal and kernel\n        float signal_h[10] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f};\n        float filter_h[3] = { 0.25, 0.5, 0.25 };\n        float output_h[10] = { 0 };\n        float expectedOutput_h[10] = { 0.25, 1.00, 2.00, 3.00, 4.00, 5.00, 6.00, 7.00, 8.00, 9.00 };\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(signal_d, signal_h, signalLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(filter_d, filter_h, filterLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Define block size and grid size\n        int blockSize = 16;\n        int gridSize = (signalLen + blockSize - 1) / blockSize;\n\n        // Block: (16, 1, 1)\n        // Grid: (ceil(signalLen / 16), 1, 1)\n        void *args[] = {&signal_d, &filter_d, &output_d, &signalLen, &filterLen};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_convolve1D, gridSize, blockSize, args, blockSize * sizeof(int), stream));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, signalLen * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validate output\n        for (int i = 0; i < signalLen; ++i) {\n            assert(abs(output_h[i] - expectedOutput_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 3\n    {\n        // Non-sequential input signal and kernel\n        float signal_h[10] = {10.0f, 9.0f, 8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f};\n        float filter_h[3] = { 0.25, 0.5, 0.25 };\n        float output_h[10] = { 0 };\n        float expectedOutput_h[10] = { 2.50, 7.25, 9.00, 8.00, 7.00, 6.00, 5.00, 4.00, 3.00, 2.00 };\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(signal_d, signal_h, signalLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(filter_d, filter_h, filterLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Define block size and grid size\n        int blockSize = 16;\n        int gridSize = (signalLen + blockSize - 1) / blockSize;\n\n        // Block: (16, 1, 1)\n        // Grid: (ceil(signalLen / 16), 1, 1)\n        void *args[] = {&signal_d, &filter_d, &output_d, &signalLen, &filterLen};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_convolve1D, gridSize, blockSize, args, blockSize * sizeof(int), stream));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, signalLen * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validate output\n        for (int i = 0; i < signalLen; ++i) {\n            assert(abs(output_h[i] - expectedOutput_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 4\n    {\n        // Non-sequential input signal and kernel\n        float signal_h[10] = {5.0f, 1.0f, 5.0f, 1.0f, 5.0f, 1.0f, 5.0f, 1.0f, 5.0f, 1.0f};\n        float filter_h[3] = { 0.25, 0.5, 0.25 };\n        float output_h[10] = { 0 };\n        float expectedOutput_h[10] = { 1.25, 2.75, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00 };\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(signal_d, signal_h, signalLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(filter_d, filter_h, filterLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Define block size and grid size\n        int blockSize = 16;\n        int gridSize = (signalLen + blockSize - 1) / blockSize;\n\n        // Block: (16, 1, 1)\n        // Grid: (ceil(signalLen / 16), 1, 1)\n        void *args[] = {&signal_d, &filter_d, &output_d, &signalLen, &filterLen};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_convolve1D, gridSize, blockSize, args, blockSize * sizeof(int), stream));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, signalLen * sizeof(float), cudaMemcpyDeviceToHost, stream));\n    \n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validate output\n        for (int i = 0; i < signalLen; ++i) {\n            assert(abs(output_h[i] - expectedOutput_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 5\n    {\n        // Non-sequential input signal and kernel\n        float signal_h[10] = {7.0f, 3.0f, 9.0f, 2.0f, 8.0f, 1.0f, 6.0f, 4.0f, 10.0f, 5.0f};\n        float filter_h[3] = { 0.25, 0.5, 0.25 };\n        float output_h[10] = { 0 };\n\n        float expectedOutput_h[10] = { 1.75, 4.25, 5.50, 5.75, 5.25, 4.75, 4.00, 4.25, 6.00, 7.25 };\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(signal_d, signal_h, signalLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(filter_d, filter_h, filterLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Define block size and grid size\n        int blockSize = 16;\n        int gridSize = (signalLen + blockSize - 1) / blockSize;\n\n        // Block: (16, 1, 1)\n        // Grid: (ceil(signalLen / 16), 1, 1)\n        void *args[] = {&signal_d, &filter_d, &output_d, &signalLen, &filterLen};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_convolve1D, gridSize, blockSize, args, blockSize * sizeof(int), stream));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, signalLen * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validate output\n        for (int i = 0; i < signalLen; ++i) {\n            assert(abs(output_h[i] - expectedOutput_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 6\n    {\n        // Non-sequential input signal and kernel\n        float signal_h[10] = {2.0f, 2.0f, 2.0f, 2.0f, 2.0f, 2.0f, 2.0f, 2.0f, 2.0f, 2.0f};\n        float filter_h[3] = { 0.25, 0.5, 0.25 };\n        float output_h[10] = { 0 };\n\n        float expectedOutput_h[10] = { 0.50, 1.50, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00 };\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(signal_d, signal_h, signalLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(filter_d, filter_h, filterLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Define block size and grid size\n        int blockSize = 16;\n        int gridSize = (signalLen + blockSize - 1) / blockSize;\n\n        // Block: (16, 1, 1)\n        // Grid: (ceil(signalLen / 16), 1, 1)\n        void *args[] = {&signal_d, &filter_d, &output_d, &signalLen, &filterLen};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_convolve1D, gridSize, blockSize, args, blockSize * sizeof(int), stream));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, signalLen * sizeof(float), cudaMemcpyDeviceToHost, stream));\n    \n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validate output\n        for (int i = 0; i < signalLen; ++i) {\n            assert(abs(output_h[i] - expectedOutput_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 7\n    {\n        // Non-sequential input signal and kernel\n        float signal_h[10] = {-3.0f, 1.0f, -4.0f, 1.0f, -5.0f, 9.0f, -2.0f, 6.0f, -5.0f, 3.0f};\n        float filter_h[3] = { 0.25, 0.5, 0.25 };\n        float output_h[10] = { 0 };\n\n        float expectedOutput_h[10] = { -0.75, -1.25, -1.25, -1.50, -1.75, 0.00, 2.75, 2.75, 1.25, -0.25 };\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(signal_d, signal_h, signalLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(filter_d, filter_h, filterLen * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Define block size and grid size\n        int blockSize = 16;\n        int gridSize = (signalLen + blockSize - 1) / blockSize;\n\n        // Block: (16, 1, 1)\n        // Grid: (ceil(signalLen / 16), 1, 1)\n        void *args[] = {&signal_d, &filter_d, &output_d, &signalLen, &filterLen};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_convolve1D, gridSize, blockSize, args, blockSize * sizeof(int), stream));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, signalLen * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validate output\n        for (int i = 0; i < signalLen; ++i) {\n            assert(abs(output_h[i] - expectedOutput_h[i]) < TOLERANCE);\n        }\n    }\n\n    // Free device memory\n    CUDA_CHECK(cudaFreeAsync(signal_d, stream));\n    CUDA_CHECK(cudaFreeAsync(filter_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_convolve1D(const float* signal, const float* filter, float* output, int signalLen, int filterLen) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/89", "date": "2025-07-30", "prompt": "Write a CUDA kernel to find the L2 norm of a vector that utilizes shared memory, atomic operations, and reduction.\n\nThe signature of the function is __global__ void k_l2Norm(float *input, float *result, int n, bool square), where input is a vector for which L2 norm need to be calculated, result is a pointer where the output is stored, n is the total number of elements in the vector, and square controls whether the elements are squared before summing or summed directly.\n\n\n>>> k_l2Norm({2, 5, 1, 8}, result, 4, true) -> result: 9.695360\n>>> k_l2Norm({3, 9, 5, 7}, result, 4, true) -> result: 12.806249\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <algorithm>\n#include <cstdio>\n#include <cmath>\n\n#define EPSILON 1e-5  // Tolerance for floating-point comparison\n#define CUDA_CHECK(call)                                                                               \\\ndo {                                                                                                  \\\n    cudaError_t error = call;                                                                         \\\n    if (error != cudaSuccess) {                                                                       \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\", __FILE__, __LINE__, cudaGetErrorString(error)); \\\n        exit(EXIT_FAILURE);                                                                           \\\n    }                                                                                                 \\\n} while (0)\n\n#undef NDEBUG\n#include <assert.h>\n\n//Kernel to find the L2 norm of a vector\n__global__ void k_l2Norm(float *input, float *result, int n, bool square);\n\nvoid launch() {\n    const int testCaseCount = 8; // Number of test cases\n    int vectorSize[testCaseCount] = {4, 4, 8, 16, 32, 64, 100, 100000}; // Sizes of the vectors in each test case\n    const float expectedOutput[testCaseCount] = {9.695360, 12.806249, 21.633308, 36.633320, 196.822250, 378.970978, 581.678589, 316.22776601683796}; // Expected results for each test\n    int maxVectorSize = *std::max_element(vectorSize, vectorSize + testCaseCount);    \n\n    // Input vectors for the tests\n    float inputVector[testCaseCount][maxVectorSize] =  {\n        {2, 5, 1, 8},                                                     // test case 1\n        {3, 9, 5, 7},                                                     // test case 2\n        {9, 11, 6, 9, 0, 8, 7, 6},                                        // test case 3\n        {2, 5, 1, 8, 5, 10, 15, 20, 3, 9, 5, 7, 3, 9, 10, 12},            // test case 4\n        {32, 4, 98, 7, 18, 15, 3, 8, 99, 12, 11, 1, 24, 97, 6, 13,\n         30, 9, 14, 20, 2, 5, 27, 10, 21, 22, 17, 26, 16, 28, 29, 31},    // test case 5\n        {32, 4, 98, 7, 18, 15, 3, 8, 99, 12, 11, 1, 24, 97, 6, 13,\n         30, 9, 14, 20, 2, 5, 27, 10, 21, 22, 17, 26, 16, 28, 29, 31,\n         33, 38, 93, 40, 95, 41, 34, 36, 94, 39, 43, 35, 44, 48, 37, 42,\n         49, 54, 77, 56, 79, 57, 50, 52, 78, 55, 59, 51, 60, 64, 53, 58}, // test case 6\n        {32, 4, 98, 7, 18, 15, 3, 8, 99, 12, 11, 1, 24, 97, 6, 13,\n         30, 9, 14, 20, 2, 5, 27, 10, 21, 22, 17, 26, 16, 28, 29, 31,\n         33, 38, 93, 40, 95, 41, 34, 36, 94, 39, 43, 35, 44, 48, 37, 42,\n         49, 54, 77, 56, 79, 57, 50, 52, 78, 55, 59, 51, 60, 64, 53, 58,\n         65, 70, 61, 72, 63, 73, 66, 68, 62, 71, 75, 67, 76, 80, 69, 74,\n         81, 86, 45, 88, 47, 89, 82, 84, 46, 87, 91, 83, 92, 96, 85, 90,\n         25, 23, 19, 100},                                                 // test case 7\n         {1}\n         };\n\n    // set all elements of the test case 8 to 1\n    for (int j = 0; j < 100000; ++j) {\n        inputVector[7][j]=1;\n    }\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    int threadsPerBlock = 256, blocksPerGrid=0;\n    bool square;\n    float *input_d, *result_d, l2norm=0;\n\n    // Allocate memory on device\n    CUDA_CHECK(cudaMallocAsync(&input_d, maxVectorSize * sizeof(float), stream));\n    int maxBlocksPerGrid = (maxVectorSize + threadsPerBlock - 1) / threadsPerBlock;\n    CUDA_CHECK(cudaMallocAsync(&result_d, maxBlocksPerGrid * sizeof(float), stream));\n\n    // Loop to execute each test case\n    for (int i = 0; i < testCaseCount; ++i) {\n        blocksPerGrid = (vectorSize[i] + threadsPerBlock - 1) / threadsPerBlock;\n        float result_h[blocksPerGrid] = {0};\n\n        // Copy input data to device\n        CUDA_CHECK(cudaMemcpyAsync(input_d, inputVector[i], vectorSize[i] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemsetAsync(result_d, 0.0f, blocksPerGrid * sizeof(float), stream));\n        \n        square=true; // this is to calculate the square of each element\n        void *argsL2Norm[] = {&input_d, &result_d, &vectorSize[i], &square};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_l2Norm, blocksPerGrid, threadsPerBlock, argsL2Norm, threadsPerBlock * sizeof(float), stream));\n\n        square=false; // this is to sum the elements of the vector\n        while(blocksPerGrid>1) {\n          void *argsL2Norm[] = {&result_d, &result_d, &blocksPerGrid, &square};\n          CUDA_CHECK(cudaLaunchKernel((void*)k_l2Norm, blocksPerGrid, threadsPerBlock, argsL2Norm, threadsPerBlock * sizeof(float), stream));\n\n          blocksPerGrid = (blocksPerGrid + threadsPerBlock - 1) / threadsPerBlock;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(result_h, result_d, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        l2norm = sqrt(result_h[0]);\n\n        assert(fabs(l2norm - expectedOutput[i]) <= EPSILON);\n    }\n\n    // Free device memory\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(result_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_l2Norm(float *input, float *result, int n, bool square) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/90", "date": "2025-07-30", "prompt": "By leveraging CUDA device memory, write a CUDA kernel to find the distance of two sets of geographical coordinates, A and B. Here, $A=((lat_{a0}, long_{a0}), (lat_{a1}, long_{a1}), (lat_{a2}, long_{a2}), \\cdots (lat_a{n-1}, long_a{n-1}))$ and $B=((lat_{b0}, long_{b0}), (lat_{b1}, long_{b1}), (lat_{b2}, long_{b2}), \\cdots , (lat_b{n-1}, long_b{n-1}))$.\nThe distance between each pair of A and B should be in kilometers, such as $Output = (dist(a_0, b_0), dist(a_1,b_1), dist(a_2,b_2) \\cdots )$ in km, here each $a_i = (lat_{ai}, long_{ai})$, and each $b_i = (lat_{bi}, long_{bi})$.\n\nThe signature of the kernel is __global__ void k_GeoDistance(double *lat1, double *lon1, double *lat2, double *lon2, double *result, int n) where lat1 and lon1  are arrays containing the latitudes and longitudes of the first set of coordinates, lat1 and lon1  are arrays containing the latitudes and longitudes of the second set of coordinates, result is an array where the calculated distances will be stored, n is the number of coordinate pairs to process.\n\n>>> k_MatrixMul(A{1,2,3}{4,5,6}, B{7,8}{9,10}{11,12}) --> Output{58,64}{139,154}\n>>> k_MatrixMul(A{1,0,2}{-1,3,1}{2,-2,0}, B{3,1,2}{2,1,1}{1,0,1}) --> Output{5,1,4}{4,2,4}{4,0,4} \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <stdio.h>\n#include <math.h>\n#define EPSILON (1e-2)  // Tolerance for floating-point comparison\n#define R 6371 // Earth's radius in kilometers\n#undef NDEBUG\n#include <assert.h>\n#define FACTOR (M_PI / 180.0)\n\n#define CUDA_CHECK(call)                                                 \\\ndo {                                                                     \\\n    cudaError_t error = call;                                           \\\n    if (error != cudaSuccess) {                                         \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\", __FILE__, __LINE__, cudaGetErrorString(error)); \\\n        exit(EXIT_FAILURE);                                             \\\n    }                                                                   \\\n} while (0)\n\n\n__global__ void k_GeoDistance(double *lat1, double *lon1, double *lat2, double *lon2, double *result, int n);\n\nvoid launch() {\n\n    int MAX_NUM_POINTS = 10;\n    double *d_lat1, *d_lon1, *d_lat2, *d_lon2, *d_result;\n    \n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate memory on the device\n    CUDA_CHECK(cudaMallocAsync(&d_lat1, MAX_NUM_POINTS * sizeof(double), stream));\n    CUDA_CHECK(cudaMallocAsync(&d_lon1, MAX_NUM_POINTS * sizeof(double), stream));\n    CUDA_CHECK(cudaMallocAsync(&d_lat2, MAX_NUM_POINTS * sizeof(double), stream));\n    CUDA_CHECK(cudaMallocAsync(&d_lon2, MAX_NUM_POINTS * sizeof(double), stream));\n    CUDA_CHECK(cudaMallocAsync(&d_result, MAX_NUM_POINTS * sizeof(double), stream));\n    \n    //Test case: 1\n    {\n        int NUM_POINTS = 2;\n        double lat1_h[NUM_POINTS] = {34.052235, 48.856613};\n        double lon1_h[NUM_POINTS] = {-118.243683, 2.352222};\n        double lat2_h[NUM_POINTS] = {35.689487, 19.076090};\n        double lon2_h[NUM_POINTS] = {139.691711, 72.877426};\n        double result_h[NUM_POINTS];\n        double expectedOutput[NUM_POINTS] = {8815.47, 7009.30};\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_lat1, lat1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon1, lon1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lat2, lat2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon2, lon2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n\n        // Launch the kernel\n        int threadsPerBlock = 256;\n        int blocksPerGrid = (NUM_POINTS + threadsPerBlock - 1) / threadsPerBlock;\n        // Grid: (ceil(NUM_POINTS / 256), 1, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&d_lat1, &d_lon1, &d_lat2, &d_lon2, &d_result, &NUM_POINTS};\n        cudaLaunchKernel((void*)k_GeoDistance, blocksPerGrid, threadsPerBlock, args, 0, stream);\n\n        // Copy result back to host\n        CUDA_CHECK(cudaMemcpyAsync(result_h, d_result, NUM_POINTS * sizeof(double), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; ++i) {\n            assert(fabs(result_h[i] - expectedOutput[i]) <= EPSILON);\n        }\n    }\n\n    //Test case: 2\n    {\n        int NUM_POINTS = 3;\n        double lat1_h[NUM_POINTS] = {51.507351, 40.712776, 35.689487};\n        double lon1_h[NUM_POINTS] = {-0.127758, -74.006058, 139.691711};\n        double lat2_h[NUM_POINTS] = {40.712776, 51.507351, 34.052235};\n        double lon2_h[NUM_POINTS] = {-74.006058, -0.127758, -118.243683};\n        double result_h[NUM_POINTS];\n        double expectedOutput[NUM_POINTS] = {5570.23, 5570.23, 8815.47};\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_lat1, lat1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon1, lon1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lat2, lat2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon2, lon2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n\n        // Launch the kernel\n        int threadsPerBlock = 256;\n        int blocksPerGrid = (NUM_POINTS + threadsPerBlock - 1) / threadsPerBlock;\n        // Grid: (ceil(NUM_POINTS / 256), 1, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&d_lat1, &d_lon1, &d_lat2, &d_lon2, &d_result, &NUM_POINTS};\n        cudaLaunchKernel((void*)k_GeoDistance, blocksPerGrid, threadsPerBlock, args, 0, stream);\n\n        // Copy result back to host\n        CUDA_CHECK(cudaMemcpyAsync(result_h, d_result, NUM_POINTS * sizeof(double), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; ++i) {\n            assert(fabs(result_h[i] - expectedOutput[i]) <= EPSILON);\n        }\n    }\n\n    //Test case: 3\n    {\n        int NUM_POINTS = 5;\n        double lat1_h[NUM_POINTS] = {34.052235, 48.856613, 51.507351, 40.712776, 35.689487};\n        double lon1_h[NUM_POINTS] = {-118.243683, 2.352222, -0.127758, -74.006058, 139.691711};\n        double lat2_h[NUM_POINTS] = {35.689487, 19.076090, 40.712776, 51.507351, 34.052235};\n        double lon2_h[NUM_POINTS] = {139.691711, 72.877426, -74.006058, -0.127758, -118.243683};\n        double result_h[NUM_POINTS];\n        double expectedOutput[NUM_POINTS] = {8815.47, 7009.30, 5570.23, 5570.23, 8815.47};\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_lat1, lat1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon1, lon1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lat2, lat2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon2, lon2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n\n        // Launch the kernel\n        int threadsPerBlock = 256;\n        int blocksPerGrid = (NUM_POINTS + threadsPerBlock - 1) / threadsPerBlock;\n        // Grid: (ceil(NUM_POINTS / 256), 1, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&d_lat1, &d_lon1, &d_lat2, &d_lon2, &d_result, &NUM_POINTS};\n        cudaLaunchKernel((void*)k_GeoDistance, blocksPerGrid, threadsPerBlock, args, 0, stream);\n\n        // Copy result back to host\n        CUDA_CHECK(cudaMemcpyAsync(result_h, d_result, NUM_POINTS * sizeof(double), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; ++i) {\n            assert(fabs(result_h[i] - expectedOutput[i]) <= EPSILON);\n        }\n    }\n\n    //Test case: 4\n    {\n        int NUM_POINTS = 5;\n        double lat1_h[NUM_POINTS] = {33.052235, 48.856613, 50.507351, 39.712776, 35.689487};\n        double lon1_h[NUM_POINTS] = {-118.243683, 2.352222, -0.127758, -74.006058, 138.691711};\n        double lat2_h[NUM_POINTS] = {35.689487, 18.076090, 40.712776, 51.507351, 34.052235};\n        double lon2_h[NUM_POINTS] = {139.691711, 72.877426, -74.006058, -0.127758, -118.243683};\n        double result_h[NUM_POINTS];\n        double expectedOutput[NUM_POINTS] = {8881.05, 7089.39, 5605.93, 5640.37, 8889.72};\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_lat1, lat1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon1, lon1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lat2, lat2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon2, lon2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n\n        // Launch the kernel\n        int threadsPerBlock = 256;\n        int blocksPerGrid = (NUM_POINTS + threadsPerBlock - 1) / threadsPerBlock;\n        // Grid: (ceil(NUM_POINTS / 256), 1, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&d_lat1, &d_lon1, &d_lat2, &d_lon2, &d_result, &NUM_POINTS};\n        cudaLaunchKernel((void*)k_GeoDistance, blocksPerGrid, threadsPerBlock, args, 0, stream);\n\n        // Copy result back to host\n        CUDA_CHECK(cudaMemcpyAsync(result_h, d_result, NUM_POINTS * sizeof(double), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; ++i) {\n            assert(fabs(result_h[i] - expectedOutput[i]) <= EPSILON);\n        }\n    }\n\n    //Test case: 5\n    {\n        int NUM_POINTS = 6;\n        double lat1_h[NUM_POINTS] = {-33.868820, 37.774929, 25.276987, 55.755826, 19.076090, 12.971619};\n        double lon1_h[NUM_POINTS] = {151.209290, -122.419418, 55.296249, 37.617300, 72.877426, 77.594566};\n        double lat2_h[NUM_POINTS] = {55.755826, 48.856613, 25.276987, 37.774929, -33.868820, 28.704059};\n        double lon2_h[NUM_POINTS] = {37.617300, 2.352222, 55.296249, -122.419418, 151.209290, 77.102491};\n        double result_h[NUM_POINTS];\n        double expectedOutput[NUM_POINTS] = {14496.16, 8953.39, 0.00, 9444.18, 10156.85, 1750.11};\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_lat1, lat1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon1, lon1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lat2, lat2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon2, lon2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n\n        // Launch the kernel\n        int threadsPerBlock = 256;\n        int blocksPerGrid = (NUM_POINTS + threadsPerBlock - 1) / threadsPerBlock;\n        // Grid: (ceil(NUM_POINTS / 256), 1, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&d_lat1, &d_lon1, &d_lat2, &d_lon2, &d_result, &NUM_POINTS};\n        cudaLaunchKernel((void*)k_GeoDistance, blocksPerGrid, threadsPerBlock, args, 0, stream);\n\n        // Copy result back to host\n        CUDA_CHECK(cudaMemcpyAsync(result_h, d_result, NUM_POINTS * sizeof(double), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; ++i) {\n            assert(fabs(result_h[i] - expectedOutput[i]) <= EPSILON);\n        }\n    }\n\n    //Test case: 6\n    {\n        int NUM_POINTS = 6;\n        double lat1_h[NUM_POINTS] = {-34.868820, 38.774929, 26.276987, 54.755826, 18.076090, 11.971619};\n        double lon1_h[NUM_POINTS] = {150.209290, -121.419418, 54.296249, 37.617300, 72.877426, 77.594566};\n        double lat2_h[NUM_POINTS] = {54.755826, 48.856613, 25.276987, 37.774929, -33.868820, 28.704059};\n        double lon2_h[NUM_POINTS] = {37.617300, 2.352222, 55.296249, -122.419418, 151.209290, 77.102491};\n        double result_h[NUM_POINTS];\n        double expectedOutput[NUM_POINTS] = {14505.46, 8812.48, 149.63, 9551.22, 10092.15, 1861.26};\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_lat1, lat1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon1, lon1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lat2, lat2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon2, lon2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n\n        // Launch the kernel\n        int threadsPerBlock = 256;\n        int blocksPerGrid = (NUM_POINTS + threadsPerBlock - 1) / threadsPerBlock;\n        // Grid: (ceil(NUM_POINTS / 256), 1, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&d_lat1, &d_lon1, &d_lat2, &d_lon2, &d_result, &NUM_POINTS};\n        cudaLaunchKernel((void*)k_GeoDistance, blocksPerGrid, threadsPerBlock, args, 0, stream);\n\n        // Copy result back to host\n        CUDA_CHECK(cudaMemcpyAsync(result_h, d_result, NUM_POINTS * sizeof(double), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; ++i) {\n            assert(fabs(result_h[i] - expectedOutput[i]) <= EPSILON);\n        }\n    }\n\n    //Test case: 7\n    {\n        int NUM_POINTS = 10;\n        double lat1_h[NUM_POINTS] = {34.052235, 48.856613, 51.507351, 40.712776, 35.689487, -33.868820, 37.774929, 25.276987, 55.755826, 19.076090};\n        double lon1_h[NUM_POINTS] = {-118.243683, 2.352222, -0.127758, -74.006058, 139.691711, 151.209290, -122.419418, 55.296249, 37.617300, 72.877426};\n        double lat2_h[NUM_POINTS] = {35.689487, 19.076090, 40.712776, 51.507351, 34.052235, 55.755826, 48.856613, 25.276987, 37.774929, -33.868820};\n        double lon2_h[NUM_POINTS] = {139.691711, 72.877426, -74.006058, -0.127758, -118.243683, 37.617300, 2.352222, 55.296249, -122.419418, 151.209290};\n        double result_h[NUM_POINTS];\n        double expectedOutput[NUM_POINTS] = {8815.47, 7009.30, 5570.23, 5570.23, 8815.47, 14496.16, 8953.39, 0.00, 9444.18, 10156.85};\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_lat1, lat1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon1, lon1_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lat2, lat2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_lon2, lon2_h, NUM_POINTS * sizeof(double), cudaMemcpyHostToDevice, stream));\n\n        // Launch the kernel\n        int threadsPerBlock = 256;\n        int blocksPerGrid = (NUM_POINTS + threadsPerBlock - 1) / threadsPerBlock;\n        // Grid: (ceil(NUM_POINTS / 256), 1, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&d_lat1, &d_lon1, &d_lat2, &d_lon2, &d_result, &NUM_POINTS};\n        cudaLaunchKernel((void*)k_GeoDistance, blocksPerGrid, threadsPerBlock, args, 0, stream);\n\n        // Copy result back to host\n        CUDA_CHECK(cudaMemcpyAsync(result_h, d_result, NUM_POINTS * sizeof(double), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; ++i) {\n            assert(fabs(result_h[i] - expectedOutput[i]) <= EPSILON);\n        }\n    }\n\n    // Clean up\n    CUDA_CHECK(cudaFreeAsync(d_lat1, stream));\n    CUDA_CHECK(cudaFreeAsync(d_lon1, stream));\n    CUDA_CHECK(cudaFreeAsync(d_lat2, stream));\n    CUDA_CHECK(cudaFreeAsync(d_lon2, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));    \n}\n\n__global__ void k_GeoDistance(double *lat1, double *lon1, double *lat2, double *lon2, double *result, int n) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/91", "date": "2025-07-30", "prompt": "Write a CUDA kernel to calculate the signal power, where each thread computes the power of an element in an input signal by squaring its amplitude.\n\nThe signature of the function is __global__ void k_computeSignalPower(float* outputSignal_d, const float* inputSignal_d, int size), where outputSignal_d is the array storing the computed power values, inputSignal_d is the array containing the original amplitude values, and size represents the length of both arrays.\n\n>>> k_computeSignalPower(outputSignal_d, {0.0f, 1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f}, 10) -> outputSignal_d: ({0.0f, 1.0f, 4.0f, 9.0f, 16.0f, 25.0f, 36.0f, 49.0f, 64.0f, 81.0f})\n>>> k_computeSignalPower(outputSignal_d, {2.3f, 6.9f, 1.5f, 8.1f, 3.6f, 4.8f, 7.4f, 5.0f, 9.3f, 0.4f}, 10) -> outputSignal_d: ({5.29f, 47.61f, 2.25f, 65.61f, 12.96f, 23.04f, 54.76f, 25.0f, 86.49f, 0.16f}) \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cstdio>\n#undef NDEBUG\n#include <cassert>\n\n#define ARRAY_SIZE 10\n#define TOLERANCE  1e-2\n\n// Macro for CUDA error checking\n#define CUDA_CHECK(call) {                                                        \\\n        cudaError_t err = call;                                                   \\\n        if (err != cudaSuccess) {                                                 \\\n            fprintf(stderr, \"CUDA error in file '%s' in line %d: %s.\\n\",          \\\n                    __FILE__, __LINE__, cudaGetErrorString(err));                 \\\n            exit(EXIT_FAILURE);                                                   \\\n        }                                                                         \\\n}\n\n// Struct to define test cases\nstruct TestCase {\n    float inputSignal_h[ARRAY_SIZE];\n    float expectedOutput_h[ARRAY_SIZE];\n};\n\n__global__ void k_computeSignalPower(float* outputSignal_d, const float* inputSignal_d, int size);\n\nvoid launch() {\n    // Create a CUDA stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate device memory once for the largest dataset\n    float* inputSignal_d;\n    float* outputSignal_d;\n    size_t arrayBytes = ARRAY_SIZE * sizeof(float);\n\n    CUDA_CHECK(cudaMallocAsync((void**)&inputSignal_d, arrayBytes, stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&outputSignal_d, arrayBytes, stream));\n\n    // Define execution configuration\n    int threadsPerBlock = 16;\n    int blocksPerGrid = (ARRAY_SIZE + threadsPerBlock - 1) / threadsPerBlock;\n    int size = ARRAY_SIZE;\n\n    // Define all test cases\n    TestCase testCases[] = {\n        // Test Case 1\n        {\n            { 0.0f, 1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f },\n            { 0.0f, 1.0f, 4.0f, 9.0f, 16.0f, 25.0f, 36.0f, 49.0f, 64.0f, 81.0f }\n        },\n        // Test Case 2\n        {\n            { 2.3f, 6.9f, 1.5f, 8.1f, 3.6f, 4.8f, 7.4f, 5.0f, 9.3f, 0.4f },\n            { 5.29f, 47.61f, 2.25f, 65.61f, 12.96f, 23.04f, 54.76f, 25.0f, 86.49f, 0.16f }\n        },\n        // Test Case 3\n        {\n            { 3.4f, 7.6f, 1.2f, 8.9f, 5.3f, 0.2f, 6.1f, 9.8f, 2.7f, 4.5f },\n            { 11.56f, 57.76f, 1.44f, 79.21f, 28.09f, 0.04f, 37.21f, 96.04f, 7.29f, 20.25f }\n        },\n        // Test Case 4\n        {\n            { 4.1f, 5.7f, 3.9f, 2.4f, 7.5f, 1.1f, 8.2f, 9.0f, 6.3f, 0.8f },\n            { 16.81f, 32.49f, 15.21f, 5.76f, 56.25f, 1.21f, 67.24f, 81.0f, 39.69f, 0.64f }\n        },\n        // Test Case 5\n        {\n            { 7.2f, 3.5f, 4.7f, 9.1f, 2.8f, 5.6f, 1.3f, 8.4f, 0.9f, 6.0f },\n            { 51.84f, 12.25f, 22.09f, 82.81f, 7.84f, 31.36f, 1.69f, 70.56f, 0.81f, 36.0f }\n        },\n        // Test Case 6\n        {\n            { 1.9f, 8.5f, 6.7f, 4.2f, 0.5f, 7.3f, 5.8f, 2.1f, 9.6f, 3.0f },\n            { 3.61f, 72.25f, 44.89f, 17.64f, 0.25f, 53.29f, 33.64f, 4.41f, 92.16f, 9.0f }\n        },\n        // Test Case 7\n        {\n            { 0.3f, 5.9f, 7.1f, 6.4f, 1.0f, 4.6f, 3.2f, 2.5f, 9.7f, 8.8f },\n            { 0.09f, 34.81f, 50.41f, 40.96f, 1.0f, 21.16f, 10.24f, 6.25f, 94.09f, 77.44f }\n        }\n    };\n\n    const int numTestCases = sizeof(testCases) / sizeof(TestCase);\n\n    // Loop through all test cases\n    for (int tc = 0; tc < numTestCases; tc++) {\n        void* kernelArgs[] = { &outputSignal_d, &inputSignal_d, &size };\n\n        // Copy input data to device asynchronously\n        CUDA_CHECK(cudaMemcpyAsync(inputSignal_d, testCases[tc].inputSignal_h, arrayBytes, cudaMemcpyHostToDevice, stream));\n\n        // Launch the kernel asynchronously using cudaLaunchKernel\n        // Grid: (ceil(ARRAY_SIZE / 16), 1, 1)\n        // Block: (16, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((const void*)k_computeSignalPower, dim3(blocksPerGrid), dim3(threadsPerBlock), kernelArgs, 0, stream));\n\n        // Copy output data back to host asynchronously\n        float outputSignal_h[ARRAY_SIZE];\n        CUDA_CHECK(cudaMemcpyAsync(outputSignal_h, outputSignal_d, arrayBytes, cudaMemcpyDeviceToHost, stream));\n\n        // Synchronize to ensure all operations for the current test case are complete\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify the results\n        for (int i = 0; i < ARRAY_SIZE; i++) {\n            assert(abs(outputSignal_h[i] - testCases[tc].expectedOutput_h[i]) < TOLERANCE);\n        }\n    }\n\n    // Free device memory asynchronously\n    CUDA_CHECK(cudaFreeAsync(inputSignal_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputSignal_d, stream));\n\n    // Destroy the CUDA stream\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n// Kernel definition\n__global__ void k_computeSignalPower(float* outputSignal_d, const float* inputSignal_d, int size) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/92", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute the difference between current and background images using managed memory.\n\nThe signature of the function is __global__ void k_backgroundSubtraction(float* currentImage, float* backgroundImage, float* output, int width), where currentImage is a pointer to the current image data array, backgroundImage is a pointer to the  back ground image data array, the output is an array with the subtraction results, and width is the number of elements of the arrays being processed.\n\n>>> k_backgroundSubtraction({100, 101, 102, 103, 104, 105, 106, 107, 108, 109}, {100, 102, 101, 105, 103, 110, 106, 105, 110, 108}, output, 10)-> output: ({0, 1, -1, 2, -1, 5, 0, -2, 2, -1})\n>>> k_backgroundSubtraction({50, 55, 60, 65, 70, 75, 80, 85, 90, 95}, {52, 54, 61, 67, 69, 78, 81, 83, 91, 97}, output, 10)-> output: ({2, -1, 1, 2, -1, 3, 1, -2, 1, 2}) \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cstdio>\n#undef NDEBUG\n#include <cassert>\n\nconst int BLOCK_SIZE = 16;\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n    cudaError_t error = call;                                                      \\\n    if (error != cudaSuccess) {                                                    \\\n        fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),    \\\n                __FILE__, __LINE__);                                               \\\n        exit(error);                                                               \\\n    }                                                                              \\\n} while (0)\n\n__global__ void k_backgroundSubtraction(float* currentImage, float* backgroundImage, float* output, int width);\n\nvoid launch() {\n    int width = 10;\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    float *background_d, *currentScan_d, *output_d;\n    CUDA_CHECK(cudaMallocManaged(&background_d, width * sizeof(float)));\n    CUDA_CHECK(cudaMallocManaged(&currentScan_d, width * sizeof(float)));\n    CUDA_CHECK(cudaMallocManaged(&output_d, width * sizeof(float)));\n    \n    //Test Case 1\n    {\n        // Predefined input for background and current scan\n        float backgroundData[] = { 100, 101, 102, 103, 104, 105, 106, 107, 108, 109 };\n        float currentScanData[] = { 100, 102, 101, 105, 103, 110, 106, 105, 110, 108 };\n\n        float expectedOutput[] = { 0, 1, -1, 2, -1, 5, 0, -2, 2, -1 };\n\n        // Copy predefined values to managed memory\n        for (int i = 0; i < width; ++i) {\n            background_d[i] = backgroundData[i];\n            currentScan_d[i] = currentScanData[i];\n        }\n\n        // Launch background subtraction kernel\n        \n        int grid = (width + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n        // Block: (16,1,1)\n        // Grid: (ceil(width / 16),1,1)\n        void *args[] = {&currentScan_d, &background_d, &output_d, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_backgroundSubtraction, grid, BLOCK_SIZE, args, BLOCK_SIZE * sizeof(int), stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < width; ++i) {\n            assert(output_d[i] == expectedOutput[i]);\n        }\n    }\n\n    //Test Case 2\n    {\n        // Predefined input for background and current scan\n        float backgroundData[] = { 50, 55, 60, 65, 70, 75, 80, 85, 90, 95 };\n        float currentScanData[] = { 52, 54, 61, 67, 69, 78, 81, 83, 91, 97 };\n\n        float expectedOutput[] = { 2, -1, 1, 2, -1, 3, 1, -2, 1, 2 };\n\n        float *background_d, *currentScan_d, *output_d;\n        CUDA_CHECK(cudaMallocManaged(&background_d, width * sizeof(float)));\n        CUDA_CHECK(cudaMallocManaged(&currentScan_d, width * sizeof(float)));\n        CUDA_CHECK(cudaMallocManaged(&output_d, width * sizeof(float)));\n\n        // Copy predefined values to managed memory\n        for (int i = 0; i < width; ++i) {\n            background_d[i] = backgroundData[i];\n            currentScan_d[i] = currentScanData[i];\n        }\n\n        // Launch background subtraction kernel\n        \n        int grid = (width + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n        // Block: (16,1,1)\n        // Grid: (ceil(width / 16),1,1)\n        void *args[] = {&currentScan_d, &background_d, &output_d, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_backgroundSubtraction, grid, BLOCK_SIZE, args, BLOCK_SIZE * sizeof(int), stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < width; ++i) {\n            assert(output_d[i] == expectedOutput[i]);\n        }\n    }\n\n    //Test Case 3\n    {\n        float backgroundData[] = { 200, 201, 202, 203, 204, 205, 206, 207, 208, 209 };\n        float currentScanData[] = { 205, 200, 202, 200, 203, 210, 206, 208, 210, 212 };\n\n        float expectedOutput[] = { 5, -1, 0, -3, -1, 5, 0, 1, 2, 3 };\n\n        // Copy predefined values to managed memory\n        for (int i = 0; i < width; ++i) {\n            background_d[i] = backgroundData[i];\n            currentScan_d[i] = currentScanData[i];\n        }\n\n        // Launch background subtraction kernel\n        \n        int grid = (width + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n        // Block: (16,1,1)\n        // Grid: (ceil(width / 16),1,1)\n        void *args[] = {&currentScan_d, &background_d, &output_d, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_backgroundSubtraction, grid, BLOCK_SIZE, args, BLOCK_SIZE * sizeof(int), stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < width; ++i) {\n            assert(output_d[i] == expectedOutput[i]);\n        }\n    }\n\n    //Test Case 4\n    {\n        float backgroundData[] = { 10, 20, 30, 40, 50, 60, 70, 80, 90, 100 };\n        float currentScanData[] = { 15, 18, 33, 42, 48, 65, 68, 85, 95, 105 };\n\n        float expectedOutput[] = { 5, -2, 3, 2, -2, 5, -2, 5, 5, 5 };\n\n        // Copy predefined values to managed memory\n        for (int i = 0; i < width; ++i) {\n            background_d[i] = backgroundData[i];\n            currentScan_d[i] = currentScanData[i];\n        }\n\n        // Launch background subtraction kernel\n        \n        int grid = (width + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n        // Block: (16,1,1)\n        // Grid: (ceil(width / 16),1,1)\n        void *args[] = {&currentScan_d, &background_d, &output_d, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_backgroundSubtraction, grid, BLOCK_SIZE, args, BLOCK_SIZE * sizeof(int), stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < width; ++i) {\n            assert(output_d[i] == expectedOutput[i]);\n        }\n    }\n\n    //Test Case 5\n    {\n        float backgroundData[] = { 150, 155, 160, 165, 170, 175, 180, 185, 190, 195 };\n        float currentScanData[] = { 155, 160, 165, 170, 175, 180, 185, 190, 195, 200 };\n\n        float expectedOutput[] = { 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 };\n\n        // Copy predefined values to managed memory\n        for (int i = 0; i < width; ++i) {\n            background_d[i] = backgroundData[i];\n            currentScan_d[i] = currentScanData[i];\n        }\n\n        // Launch background subtraction kernel\n        \n        int grid = (width + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n        // Block: (16,1,1)\n        // Grid: (ceil(width / 16),1,1)\n        void *args[] = {&currentScan_d, &background_d, &output_d, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_backgroundSubtraction, grid, BLOCK_SIZE, args, BLOCK_SIZE * sizeof(int), stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < width; ++i) {\n            assert(output_d[i] == expectedOutput[i]);\n        }\n    }\n\n    //Test Case 6\n    {\n        float backgroundData[] = { 300, 305, 310, 315, 320, 325, 330, 335, 340, 345 };\n        float currentScanData[] = { 302, 303, 312, 317, 318, 330, 332, 337, 338, 350 };\n\n        float expectedOutput[] = { 2, -2, 2, 2, -2, 5, 2, 2, -2, 5 };\n\n        // Copy predefined values to managed memory\n        for (int i = 0; i < width; ++i) {\n            background_d[i] = backgroundData[i];\n            currentScan_d[i] = currentScanData[i];\n        }\n\n        // Launch background subtraction kernel\n        \n        int grid = (width + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n        // Block: (16,1,1)\n        // Grid: (ceil(width / 16),1,1)\n        void *args[] = {&currentScan_d, &background_d, &output_d, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_backgroundSubtraction, grid, BLOCK_SIZE, args, BLOCK_SIZE * sizeof(int), stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < width; ++i) {\n            assert(output_d[i] == expectedOutput[i]);\n        }\n    }\n\n    //Test Case 7\n    {\n        int width = 10;\n\n        float backgroundData[] = { 500, 510, 520, 530, 540, 550, 560, 570, 580, 590 };\n        float currentScanData[] = { 495, 505, 515, 535, 545, 555, 565, 575, 585, 595 };\n\n        float expectedOutput[] = { -5, -5, -5, 5, 5, 5, 5, 5, 5, 5 };\n\n        // Copy predefined values to managed memory\n        for (int i = 0; i < width; ++i) {\n            background_d[i] = backgroundData[i];\n            currentScan_d[i] = currentScanData[i];\n        }\n\n        // Launch background subtraction kernel\n        \n        int grid = (width + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n        // Block: (16,1,1)\n        // Grid: (ceil(width / 16),1,1)\n        void *args[] = {&currentScan_d, &background_d, &output_d, &width};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_backgroundSubtraction, grid, BLOCK_SIZE, args, BLOCK_SIZE * sizeof(int), stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < width; ++i) {\n            assert(output_d[i] == expectedOutput[i]);\n        }\n    }\n    // Free managed memory\n    CUDA_CHECK(cudaFree(background_d));\n    CUDA_CHECK(cudaFree(currentScan_d));\n    CUDA_CHECK(cudaFree(output_d));\n    CUDA_CHECK(cudaStreamDestroy(stream)); \n}\n\n__global__ void k_backgroundSubtraction(float* currentImage, float* backgroundImage, float* output, int width) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/93", "date": "2025-07-30", "prompt": "Write a CUDA kernel to convert polar coordinates (radius, angle) into Cartesian coordinates (x, y) using data parallelism to efficiently convert multiple polar coordinates. \nThe Cartesian coordinates are computed as \\( x = r \\cos(\\text{angle}) \\) and \\( y = r \\sin(\\text{angle}) \\).\n\nThe signature of the kernel is __global__ void k_polarToCartesian(float* radius, float* angle, float* x, float* y, int n), where the radius is an input array of radial distances, angle is an input array of angles in radians, x is an output array for Cartesian x-coordinates, y is an output array for Cartesian y-coordinates, and n is the total number of coordinates to convert.\n\n>>> k_polarToCartesian({2.5, 3.0, 4.0}, {0.2, 0.6, 1.0}, x, y, 3) -> {x: {2.45017, 2.47601, 2.16121} y:{0.49667, 1.69393, 3.36588}}\n>>> k_polarToCartesian({1.5, 2.0, 3.5, 4.5, 5.5}, {0.3, 0.7, 1.1, 1.5, 1.9}, x, y, 5) -> {x: {1.433, 1.52968, 1.58759, 0.318317, -1.77809},  y:{0.44328, 1.28844, 3.11923, 4.48873, 5.20465}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <algorithm>\n#include <cmath>\n#include <cstdio>\n#undef NDEBUG\n#include <cassert>\n\n#define EPSILON 1e-4  // Tolerance for floating-point comparison\n\n#define CUDA_CHECK(call)                                                                               \\\ndo {                                                                                                  \\\n    cudaError_t error = call;                                                                         \\\n    if (error != cudaSuccess) {                                                                       \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\", __FILE__, __LINE__, cudaGetErrorString(error)); \\\n        exit(EXIT_FAILURE);                                                                           \\\n    }                                                                                                 \\\n} while (0)\n\n__global__ void k_polarToCartesian(float* radius, float* angle, float* x, float* y, int n);\n\nvoid launch() {\n    // Device arrays\n    float *d_Radius, *d_Angle, *d_x, *d_y;\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    int maxN =16;\n\n    // Allocate device memory\n    CUDA_CHECK(cudaMallocAsync(&d_Radius, maxN * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&d_Angle, maxN * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&d_x, maxN * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&d_y, maxN * sizeof(float), stream));\n\n    //Test case 1\n    {\n        int n = 3;  // Number of points\n        // Host arrays\n        float inputRadius[n] = {2.5, 3.0, 4.0};         // Example radial distances\n        float inputAngle[n] = {0.2, 0.6, 1.0};      // Example angles in radians\n        float outputX[n], outputY[n];  // Output arrays for Cartesian coordinates\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_Radius, inputRadius, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_Angle, inputAngle, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Block: (256, 1, 1)\n        // Grid (ceil(numBlocks/256), 1, 1)\n        int blockSize = 256;\n        int numBlocks = (n + blockSize - 1) / blockSize;\n\n        void *argsPolarToCartesian[] = {&d_Radius, &d_Angle, &d_x, &d_y, &n};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_polarToCartesian, numBlocks, blockSize, argsPolarToCartesian, 0, stream));\n\n        // Copy the results back to the host\n        CUDA_CHECK(cudaMemcpyAsync(outputX, d_x, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(outputY, d_y, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        \n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedOutputX[n] = {2.45017, 2.47601, 2.16121};\n        float expectedOutputY[n] = {0.49667, 1.69393, 3.36588};\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputX[j] - expectedOutputX[j]) <= EPSILON);\n        }\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputY[j] - expectedOutputY[j]) <= EPSILON);\n        }\n    }\n\n    //Test case 2\n    {\n        int n = 10;  // Number of points\n        // Host arrays\n        float inputRadius[n] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0};         // Example radial distances\n        float inputAngle[n] = {0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5};      // Example angles in radians\n        float outputX[n], outputY[n];  // Output arrays for Cartesian coordinates\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_Radius, inputRadius, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_Angle, inputAngle, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Block: (256, 1, 1)\n        // Grid (ceil(numBlocks/256), 1, 1)\n        int blockSize = 256;\n        int numBlocks = (n + blockSize - 1) / blockSize;\n\n        // Launch the kernel with enough threads to cover all points\n        void *argsPolarToCartesian[] = {&d_Radius, &d_Angle, &d_x, &d_y, &n};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_polarToCartesian, numBlocks, blockSize, argsPolarToCartesian, 0, stream));\n\n        // Copy the results back to the host\n        CUDA_CHECK(cudaMemcpyAsync(outputX, d_x, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(outputY, d_y, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        \n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedOutputX[n] = {1, 1.75517, 1.62091, 0.282949, -2.08073, -4.80686,\n                                  -6.92995, -7.49165, -5.88279, -2.10796\n                                };\n        float expectedOutputY[n] = {0, 0.958851, 2.52441, 3.98998, 4.54649, 3.59083,\n                                  0.98784, -2.80627, -6.81122, -9.7753\n                                };\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputX[j] - expectedOutputX[j]) <= EPSILON);\n        }\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputY[j] - expectedOutputY[j]) <= EPSILON);\n        }\n    }\n\n    //Test case 3\n    {\n        int n = 5;  // Number of points\n        // Host arrays\n        float inputRadius[n] = {1.5, 2.0, 3.5, 4.5, 5.5};         // Example radial distances\n        float inputAngle[n]  = {0.3, 0.7, 1.1, 1.5, 1.9};      // Example angles in radians\n        float outputX[n], outputY[n];  // Output arrays for Cartesian coordinates\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_Radius, inputRadius, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_Angle, inputAngle, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Block: (256, 1, 1)\n        // Grid (ceil(numBlocks/256), 1, 1)\n        int blockSize = 256;\n        int numBlocks = (n + blockSize - 1) / blockSize;\n\n        // Launch the kernel with enough threads to cover all points\n        void *argsPolarToCartesian[] = {&d_Radius, &d_Angle, &d_x, &d_y, &n};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_polarToCartesian, numBlocks, blockSize, argsPolarToCartesian, 0, stream));\n\n        // Copy the results back to the host\n        CUDA_CHECK(cudaMemcpyAsync(outputX, d_x, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(outputY, d_y, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        \n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedOutputX[n] = {1.433, 1.52968, 1.58759, 0.318317, -1.77809};\n        float expectedOutputY[n] = {0.44328, 1.28844, 3.11923, 4.48873, 5.20465};\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputX[j] - expectedOutputX[j]) <= EPSILON);\n        }\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputY[j] - expectedOutputY[j]) <= EPSILON);\n        }\n    }\n\n    //Test case 4\n    {\n        int n = 10;  // Number of points\n        // Host arrays\n        float inputRadius[n] = {3.1, 4.4, 5.6, 6.2, 8.1, 9.3};         // Example radial distances\n        float inputAngle[n]  = {0.4, 0.8, 1.2, 1.6, 2.0, 2.4};      // Example angles in radians\n        float outputX[n], outputY[n];  // Output arrays for Cartesian coordinates\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_Radius, inputRadius, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_Angle, inputAngle, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Block: (256, 1, 1)\n        // Grid (ceil(numBlocks/256), 1, 1)\n        int blockSize = 256;\n        int numBlocks = (n + blockSize - 1) / blockSize;\n\n        // Launch the kernel with enough threads to cover all points\n        void *argsPolarToCartesian[] = {&d_Radius, &d_Angle, &d_x, &d_y, &n};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_polarToCartesian, numBlocks, blockSize, argsPolarToCartesian, 0, stream));\n\n        // Copy the results back to the host\n        CUDA_CHECK(cudaMemcpyAsync(outputX, d_x, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(outputY, d_y, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        \n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedOutputX[n] = {2.85529, 3.06551, 2.0292, -0.181037, -3.37079, -6.85776};\n        float expectedOutputY[n] = {1.2072, 3.15637, 5.21942, 6.19736, 7.36531, 6.28181};\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputX[j] - expectedOutputX[j]) <= EPSILON);\n        }\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputY[j] - expectedOutputY[j]) <= EPSILON);\n        }\n    }\n\n    //Test case 5\n    {\n        int n = 7;  // Number of points\n        // Host arrays\n        float inputRadius[n] = {1.9, 3.0, 4.2, 5.5, 7.3, 8.6, 9.9};         // Example radial distances\n        float inputAngle[n]  = {0.3, 0.7, 1.1, 1.5, 1.9, 2.3, 2.7};      // Example angles in radians\n        float outputX[n], outputY[n];  // Output arrays for Cartesian coordinates\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_Radius, inputRadius, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_Angle, inputAngle, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Block: (256, 1, 1)\n        // Grid (ceil(numBlocks/256), 1, 1)\n        int blockSize = 256;\n        int numBlocks = (n + blockSize - 1) / blockSize;\n\n        // Launch the kernel with enough threads to cover all points\n        void *argsPolarToCartesian[] = {&d_Radius, &d_Angle, &d_x, &d_y, &n};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_polarToCartesian, numBlocks, blockSize, argsPolarToCartesian, 0, stream));\n\n        // Copy the results back to the host\n        CUDA_CHECK(cudaMemcpyAsync(outputX, d_x, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(outputY, d_y, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedOutputX[n] = {1.81514, 2.29453, 1.9051, 0.389055, -2.36001, -5.72997, -8.95031};\n        float expectedOutputY[n] = {0.561488, 1.93265, 3.74307, 5.48622, 6.90799, 6.41307, 4.23106};\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputX[j] - expectedOutputX[j]) <= EPSILON);\n        }\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputY[j] - expectedOutputY[j]) <= EPSILON);\n        }\n    }\n\n    //Test case 6\n    {\n        int n = 10;  // Number of points\n        // Host arrays\n        float inputRadius[n] = {1.0, 2.1, 3.2, 4.3, 5.4, 6.5, 7.6, 8.7, 9.8, 10.9};         // Example radial distances\n        float inputAngle[n]  = {0.1, 0.5, 0.9, 1.3, 1.7, 2.1, 2.5, 2.9, 3.3, 3.7};      // Example angles in radians\n        float outputX[n], outputY[n];  // Output arrays for Cartesian coordinates\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_Radius, inputRadius, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_Angle, inputAngle, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Block: (256, 1, 1)\n        // Grid (ceil(numBlocks/256), 1, 1)\n        int blockSize = 256;\n        int numBlocks = (n + blockSize - 1) / blockSize;\n\n        // Launch the kernel with enough threads to cover all points\n        void *argsPolarToCartesian[] = {&d_Radius, &d_Angle, &d_x, &d_y, &n};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_polarToCartesian, numBlocks, blockSize, argsPolarToCartesian, 0, stream));\n\n        // Copy the results back to the host\n        CUDA_CHECK(cudaMemcpyAsync(outputX, d_x, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(outputY, d_y, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        \n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedOutputX[n] = {0.995004, 1.84292, 1.98915, 1.15025, -0.695761,\n                                    -3.2815, -6.08869, -8.44734, -9.6773, -9.24429\n                                   };\n        float expectedOutputY[n] = {0.0998334, 1.00679, 2.50665, 4.1433, 5.35499,\n                                    5.61086, 4.54839, 2.08147, -1.54591, -5.77521};\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputX[j] - expectedOutputX[j]) <= EPSILON);\n        }\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputY[j] - expectedOutputY[j]) <= EPSILON);\n        }\n    }\n\n    //Test case 7\n    {\n        int n = 16;  // Number of points\n        // Host arrays\n        float inputRadius[n] = {0.5, 1.2, 2.0, 2.8, 3.6, 4.4, 5.2, 6.0, 6.8, 7.6, 8.4, 9.2, 10.0, 10.8, 11.6, 12.4};         // Example radial distances\n        float inputAngle[n]  = {0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2    };      // Example angles in radians\n        float outputX[n], outputY[n];  // Output arrays for Cartesian coordinates\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(d_Radius, inputRadius, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(d_Angle, inputAngle, n * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Block: (256, 1, 1)\n        // Grid (ceil(numBlocks/256), 1, 1)\n        int blockSize = 256;\n        int numBlocks = (n + blockSize - 1) / blockSize;\n\n        // Launch the kernel with enough threads to cover all points\n        void *argsPolarToCartesian[] = {&d_Radius, &d_Angle, &d_x, &d_y, &n};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_polarToCartesian, numBlocks, blockSize, argsPolarToCartesian, 0, stream));\n\n        // Copy the results back to the host\n        CUDA_CHECK(cudaMemcpyAsync(outputX, d_x, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(outputY, d_y, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedOutputX[n] = {0.490033, 1.10527, 1.65067, 1.95078,\n                                    1.94509, 1.59437, 0.883829, -0.175197,\n                                    -1.54497, -3.16272, -4.94341, -6.78402,\n                                    -8.56889, -10.176, -11.4839, -12.3789\n                                    };\n        float expectedOutputY[n] = {0.0993347, 0.467302, 1.12928, 2.0086,\n                                    3.0293, 4.10097, 5.12434, 5.99744,\n                                    6.62216, 6.91066, 6.79137, 6.21426,\n                                    5.15501, 3.61787, 1.63699, -0.72384\n                                   };\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputX[j] - expectedOutputX[j]) <= EPSILON);\n        }\n        for (int j = 0; j < n; j++) {\n            assert (fabs(outputY[j] - expectedOutputY[j]) <= EPSILON);\n        }\n    }\n\n    // Free the allocated device memory\n    CUDA_CHECK(cudaFreeAsync(d_Radius, stream));\n    CUDA_CHECK(cudaFreeAsync(d_Angle, stream));\n    CUDA_CHECK(cudaFreeAsync(d_x, stream));\n    CUDA_CHECK(cudaFreeAsync(d_y, stream));\n\n    CUDA_CHECK(cudaStreamDestroy(stream)); \n}\n\n__global__ void k_polarToCartesian(float* radius, float* angle, float* x, float* y, int n) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/94", "date": "2025-07-30", "prompt": "Write a CUDA kernel to join two large images side-by-side, processing each pixel in parallel and designed to handle large datasets.\n\nThe signature of the function is __global__ void k_joinImages(const unsigned char* image1_d, const unsigned char* image2_d, unsigned char* outputImage_d, int width, int height), where image1_d is the array of first image, image2_d is the array of second image, outputImage_d is the array of joined images, and width and height specify the dimensions of both input images.\n\n>>> k_joinImages({{1,2,3}, {4,5,6}, {7,8,9}, {10,11,12}}, {{5,6,7}, {8,9,10}, {11,12,13}, {14,15,16}}, outputImage_d, 3, 4) -> outputImage_d: ({{1,2,3,5,6,7},{4,5,6,8,9,10},{7,8,9,11,12,13},{10,11,12,14,15,16}})\n>>> k_joinImages({{1,2,3,4,5}, {6,7,8,9,10}, {11,12,13,14,15}, {16,17,18,19,20}}, {{5,6,7,8,9}, {10,11,12,13,14}, {15,16,17,18,19}, {20,21,22,23,24}},outputImage_d, 5, 4) -> outputImage_d: ({{1,2,3,4,5,5,6,7,8,9}, {6,7,8,9,10,10,11,12,13,14}, {11,12,13,14,15,15,16,17,18,19}, {16,17,18,19,20,20,21,22,23,24}})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <stdio.h>\n#include <stdlib.h>\n\n#define BLOCK_SIZE 16\n#define TEST_CASES 10\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n\ntypedef unsigned char uchar;\n\n__global__ void k_joinImages(const uchar* image1_d, \n                             const uchar* image2_d, \n                                   uchar* outputImage_d, \n                                   int    width,\n                                   int    height);\n\n#undef NDEBUG\n#include <assert.h>\nvoid launch() {\n\n    int testCaseRows[TEST_CASES] = { 4,5,8,10,12,15,17,20,22,25 };\n    int testCaseCols[TEST_CASES] = { 3,4,11,13,16,18,20,21,23,26 };\n\n    for (int testcase = 0; testcase < TEST_CASES; testcase++) {\n\n        // Initialization\n        int numRows = testCaseRows[testcase];\n        int numCols = testCaseCols[testcase];\n        uchar* image1_h = (uchar*)malloc(numRows * numCols * sizeof(uchar));\n        uchar* image2_h = (uchar*)malloc(numRows * numCols * sizeof(uchar));\n        uchar* outputImage = (uchar*)malloc((2 * numCols) * numRows * sizeof(uchar));\n\n        for (int i = 0; i < numRows * numCols; i++) {\n            image1_h[i] = i + 1;\n            image2_h[i] = i + 5;\n        }\n\n        // Running the code on CPU\n        for (int row = 0; row < numRows; row++) {\n            for (int col = 0; col < numCols; col++) {\n                outputImage[row * (2 * numCols) + col] = image1_h[row * numCols + col];\n                outputImage[row * (2 * numCols) + col + numCols] = image2_h[row * numCols + col];\n            }\n        }\n\n        // CUDA Initialization and memcpy\n        cudaStream_t stream;\n        CUDA_CHECK(cudaStreamCreate(&stream));\n        uchar* image1_d = nullptr; \n        uchar* image2_d = nullptr; \n        uchar* outputImage_d = nullptr;\n        uchar* outputImage_h = (uchar*)malloc((2 * numCols) * numRows * sizeof(uchar));\n        \n        CUDA_CHECK(cudaMallocAsync(&image1_d, numRows * numCols * sizeof(uchar), stream));\n        CUDA_CHECK(cudaMallocAsync(&image2_d, numRows * numCols * sizeof(uchar), stream));\n        CUDA_CHECK(cudaMallocAsync(&outputImage_d, (2 * numCols) * numRows * sizeof(uchar), stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(image1_d, \n                                   image1_h, \n                                   numRows * numCols * sizeof(uchar), \n                                   cudaMemcpyHostToDevice, \n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(image2_d, \n                                   image2_h, \n                                   numRows * numCols * sizeof(uchar), \n                                   cudaMemcpyHostToDevice, \n                                   stream));\n\n        // Running the code on GPU\n        dim3 blockSize(2, 2);  // Each block will handle a 2x2 tile\n        dim3 gridSize((numCols + blockSize.x - 1) / blockSize.x, (numRows + blockSize.y - 1) / blockSize.y);\n        void* args[] = { &image1_d, &image2_d, &outputImage_d, (void*)&numCols, (void*)&numRows };\n        CUDA_CHECK(cudaLaunchKernel((void*)k_joinImages, gridSize, blockSize, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(outputImage_h, \n                                   outputImage_d, \n                                   (2 * numCols) * numRows * sizeof(uchar), \n                                   cudaMemcpyDeviceToHost, \n                                   stream));\n\n        // Verification\n        for (int row = 0; row < numRows; row++) {\n            for (int col = 0; col < 2 * numCols; col++) {\n                assert(outputImage[row * (2 * numCols) + col] == outputImage_h[row * (2 * numCols) + col]);\n            }\n        }\n\n        free(image1_h);\n        free(image2_h);\n        free(outputImage);\n        free(outputImage_h);\n        CUDA_CHECK(cudaFreeAsync(image1_d, stream));\n        CUDA_CHECK(cudaFreeAsync(image2_d, stream));\n        CUDA_CHECK(cudaFreeAsync(outputImage_d, stream));\n        CUDA_CHECK(cudaStreamDestroy(stream));\n    }\n}\n\n__global__ void k_joinImages(const uchar* image1_d, \n                             const uchar* image2_d, \n                             uchar*       outputImage_d, \n                             int          width, \n                             int          height) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/95", "date": "2025-07-30", "prompt": "Write a CUDA kernel to estimate the value of $\\pi$ using the Monte Carlo simulation method. Utilize shared memory for local reduction of points that fall inside the unit circle.\n\nThe signature of the function is __global__ void k_monteCarlo(int *blockCount_d, int num_points), where blockCount_d is an array that holds the number of points within the circle for each corresponding block, and num_points represents the total number of points used in the simulation.\n\n>>> k_monteCarlo(40, 10000) -> 3.13744\n>>> k_monteCarlo(1563, 400000) -> 3.1466\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <limits>\n#include <cstdio>\n\n#include <cuda.h>\n#include <curand_kernel.h>\n#include <cuda_runtime.h>\n\n#include <algorithm> // For std::max_element\n#include <cassert>\n\n#define CUDA_CHECK(call)                                  \\\ndo {                                                      \\\n        cudaError_t error = call;                         \\\n        if (error != cudaSuccess) {                       \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\", \\\n                    __FILE__, __LINE__,                   \\\n                    cudaGetErrorString(error));           \\\n                exit(EXIT_FAILURE);                       \\\n        }                                                 \\\n} while (0)\n\nconst int THREADS_PER_BLOCK = 256;\n\nconst float pi_actual_value = 3.14159f;\n\n__global__ void k_monteCarlo(int *blockCount_d, int num_points); \n\n#undef NDEBUG\n#include <assert.h>\n\nvoid launch() {\n    const int testCaseCount = 7; // Number of test cases\n\n    long int number_of_samples[testCaseCount] = {10, 100, 1000, 10000, 50000, 100000, 400000};\n    int maxVectorSize = *std::max_element(number_of_samples, number_of_samples + testCaseCount);\n\n    // Define number of blocks per grid\n    int blocksPerGrid = ceil(float(maxVectorSize) / THREADS_PER_BLOCK);\n\n    // Declare host and device pointers\n    int *blockCount_d = 0;\n    int *blockCount_h = 0; \n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate memory on host and device\n    blockCount_h = (int *)malloc(sizeof(int) * blocksPerGrid);\n    CUDA_CHECK(cudaMallocAsync(&blockCount_d, sizeof(int) * blocksPerGrid, stream));\n\n    // Loop to execute each test case\n    for (int i = 0; i < testCaseCount; ++i) {\n        long int num_of_points = number_of_samples[i];\n        int count_h = 0;     \n\n        // Launch the kernel\n        // Grid: (ceil(num_of_points / THREADS_PER_BLOCK), 1, 1)\n        // Blocks: (THREADS_PER_BLOCK, 1, 1)\n        void *args[] = {&blockCount_d, &num_of_points};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_monteCarlo, blocksPerGrid, THREADS_PER_BLOCK, args, THREADS_PER_BLOCK * sizeof(int), stream));\n        \n        // Copy the result back to host\n        CUDA_CHECK(cudaMemcpyAsync(blockCount_h, blockCount_d, sizeof(int) * blocksPerGrid, cudaMemcpyDeviceToHost));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Calculate the value of Pi : Pi = 4 * (number of points inside cirle / Total number of points)\n        for (int i = 0; i < blocksPerGrid; i++) {\n            count_h += blockCount_h[i];\n        }\n\n        float pi_estimated_value = 4.0f * (float(count_h) / num_of_points);\n\n        float tolerance = 0.1f + (10000.0f / num_of_points); // Lower tolerance for larger inputs\n\n        assert(fabs(pi_estimated_value - pi_actual_value) <= tolerance);\n    }\n    // Free device memory\n    CUDA_CHECK(cudaFreeAsync(blockCount_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_monteCarlo(int *blockCount_d, \n                            int num_points) \n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/96", "date": "2025-07-30", "prompt": "Write a CUDA kernel that applies a rotation transformation to a 2D image using a specified angle theta, where each thread calculates the rotated position of a pixel and maps it to the corresponding input pixel, ensuring proper boundary handling.\n\nThe signature of the function is __global__ void k_rotateImage(float* image_d, float* rotatedImage_d, float angle, int width, int height), where image_d is the input image in row-major order, rotatedImage_d is the output image in row-major order, theta is the rotation angle in radians, width and height specify the dimensions of the image.\n\n>>> k_rotateImage({1,2,3,4,5,6,7,8,9}, rotatedImage_d, 0.707, 3, 3) -> rotatedImage_d: ({4,1,2,7,5,3,8,9,6})\n>>> k_rotateImage({1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16}, rotatedImage_d, 0.707, 4, 4)-> rotatedImage_d: ({0,5,0,2,13,9,6,3,0,14,11,8,0,15,16,12}) \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cstdio>\n#undef NDEBUG\n#include <assert.h>\n\n#define TOLERANCE 1E-1\n\nconstexpr int BLOCK_SIZE_X = 16;\nconstexpr int BLOCK_SIZE_Y = 16;\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n\n\n__global__ void k_rotateImage( float* image_d, \n                               float* rotatedImage_d, \n                               float angle, \n                               int width, \n                               int height);\n\nvoid launch() {\n    constexpr float PI_VALUE = 3.141592;\n\n    // Set rotation angle (in radians)\n    constexpr float angle = PI_VALUE / 4;  // 45 degrees rotation\n\n    constexpr int MAX_IMG_WIDTH = 10;\n    constexpr int MAX_IMG_HEIGHT = 10;\n    constexpr int MAX_IMG_SIZE = (MAX_IMG_WIDTH * MAX_IMG_HEIGHT);\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    float* image_d;\n    float* rotatedImage_d;\n\n    // Allocate device memory\n    CUDA_CHECK(cudaMallocAsync(&image_d, MAX_IMG_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&rotatedImage_d, MAX_IMG_SIZE * sizeof(float), stream));\n\n    //Test Case 1\n    {\n        constexpr int width = 3;\n        constexpr int height = 3;\n\n        float image_h[width * height] = {\n            1, 2, 3,\n            4, 5, 6,\n            7, 8, 9\n        };\n\n        float expectedOutput_h[width * height] = {\n            4, 1, 2,\n            7, 5, 3,\n            8, 9, 6\n        };\n\n        // Host and device variables\n        float rotatedImage_h[width * height] = { 0 };  // Output matrix (host)\n    \n        // Copy predefined image to device\n        CUDA_CHECK(cudaMemcpyAsync(image_d, image_h, width * height * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Launch kernel with block size 16x16\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(width/16),ceil(height/16),1)\n        // Block: (16,16,1)\n        void *args[] = {&image_d, &rotatedImage_d, (void*)&angle, (void*)&width, (void*)&height};        \n        CUDA_CHECK(cudaLaunchKernel((void*)k_rotateImage, gridDim, blockDim, args, 0, stream));\n\n        // Copy rotated image data back to host\n        CUDA_CHECK(cudaMemcpyAsync(rotatedImage_h, rotatedImage_d, width * height * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                assert(fabs(rotatedImage_h[y * width + x] - expectedOutput_h[y * width + x]) < TOLERANCE);\n            }\n        }\n    }\n\n    //Test Case 2\n    {\n        constexpr int width = 4;\n        constexpr int height = 4;\n\n        float image_h[width * height] = {\n            1,  2,  3,  4,\n            5,  6,  7,  8,\n            9, 10, 11, 12,\n            13, 14, 15, 16\n        };\n\n        float expectedOutput_h[width * height] = {\n            4, 5, 2, 2,\n            13, 9, 6, 3,\n            6, 14, 11, 8,\n            0, 15, 16, 12\n        };\n\n        // Host and device variables\n        float rotatedImage_h[width * height] = { 0 };  // Output matrix (host)\n        // Copy predefined image to device\n        CUDA_CHECK(cudaMemcpyAsync(image_d, image_h, width * height * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Launch kernel with block size 16x16\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(width/16),ceil(height/16),1)\n        // Block: (16,16,1)\n        void *args[] = {&image_d, &rotatedImage_d, (void*)&angle, (void*)&width, (void*)&height};        \n        CUDA_CHECK(cudaLaunchKernel((void*)k_rotateImage, gridDim, blockDim, args, 0, stream));\n\n        // Copy rotated image data back to host\n        CUDA_CHECK(cudaMemcpyAsync(rotatedImage_h, rotatedImage_d, width * height * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                assert(fabs(rotatedImage_h[y * width + x] - expectedOutput_h[y * width + x]) < TOLERANCE);\n            }\n        }\n    }\n\n    //Test Case 3\n    {\n        constexpr int width = 5;\n        constexpr int height = 5;\n\n        float image_h[width * height] = {\n            10,  9,  8,  7,  6,\n            5,   4,  3,  2,  1,\n            20, 19, 18, 17, 16,\n            15, 14, 13, 12, 11,\n            25, 24, 23, 22, 21\n        };\n\n        float expectedOutput_h[width * height] = {\n            4, 5, 2, 9, 13,\n            15, 20, 4, 8, 7,\n            11, 14, 18, 2, 16,\n            24, 23, 12, 17, 1,\n            0, 22, 0, 11, 0\n        };\n\n        // Host and device variables\n        float rotatedImage_h[width * height] = { 0 };  // Output matrix (host)\n\n        // Copy predefined image to device\n        CUDA_CHECK(cudaMemcpyAsync(image_d, image_h, width * height * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Launch kernel with block size 16x16\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(width/16),ceil(height/16),1)\n        // Block: (16,16,1)\n        void *args[] = {&image_d, &rotatedImage_d, (void*)&angle, (void*)&width, (void*)&height};        \n        CUDA_CHECK(cudaLaunchKernel((void*)k_rotateImage, gridDim, blockDim, args, 0, stream));\n\n        // Copy rotated image data back to host\n        CUDA_CHECK(cudaMemcpyAsync(rotatedImage_h, rotatedImage_d, width * height * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                assert(fabs(rotatedImage_h[y * width + x] - expectedOutput_h[y * width + x]) < TOLERANCE);\n            }\n        }\n    }\n\n    //Test Case 4\n    {\n        constexpr int width = 5;\n        constexpr int height = 5;\n\n        float image_h[width * height] = {\n            1,  2,  3,  4,  5,\n            6,  7,  8,  9, 10,\n            11, 12, 13, 14, 15,\n            16, 17, 18, 19, 20,\n            21, 22, 23, 24, 25\n        };\n\n        float expectedOutput_h[width * height] = {\n            4, 6, 2, 2, 13,\n            16, 11, 7, 3, 4,\n            11, 17, 13, 9, 16,\n            22, 23, 19, 14, 10,\n            0, 24, 0, 20, 0\n        };\n\n        // Host and device variables\n        float rotatedImage_h[width * height] = { 0 };  // Output matrix (host)\n\n        // Copy predefined image to device\n        CUDA_CHECK(cudaMemcpyAsync(image_d, image_h, width * height * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Launch kernel with block size 16x16\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(width/16),ceil(height/16),1)\n        // Block: (16,16,1)\n        void *args[] = {&image_d, &rotatedImage_d, (void*)&angle, (void*)&width, (void*)&height};        \n        CUDA_CHECK(cudaLaunchKernel((void*)k_rotateImage, gridDim, blockDim, args, 0, stream));\n\n        // Copy rotated image data back to host\n        CUDA_CHECK(cudaMemcpyAsync(rotatedImage_h, rotatedImage_d, width * height * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                assert(fabs(rotatedImage_h[y * width + x] - expectedOutput_h[y * width + x]) < TOLERANCE);\n            }\n        }\n    }\n\n    //Test Case 5\n    {\n        constexpr int width = 6;\n        constexpr int height = 6;\n\n        float image_h[width * height] = {\n            1,  2,  3,  4,  5,  6,\n            7,  8,  9, 10, 11, 12,\n            13, 14, 15, 16, 17, 18,\n            19, 20, 21, 22, 23, 24,\n            25, 26, 27, 28, 29, 30,\n            31, 32, 33, 34, 35, 36\n        };\n\n        float expectedOutput_h[width * height] = {\n            4, 6, 13, 8, 3, 16,\n            11, 19, 14, 4, 9, 4,\n            25, 26, 20, 15, 10, 11,\n            32, 10, 27, 22, 17, 20,\n            0, 33, 28, 29, 23, 18,\n            0, 0, 35, 0, 30, 0\n        };\n\n        // Host and device variables\n        float rotatedImage_h[width * height] = { 0 };  // Output matrix (host)\n\n        // Copy predefined image to device\n        CUDA_CHECK(cudaMemcpyAsync(image_d, image_h, width * height * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Launch kernel with block size 16x16\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(width/16),ceil(height/16),1)\n        // Block: (16,16,1)\n        void *args[] = {&image_d, &rotatedImage_d, (void*)&angle, (void*)&width, (void*)&height};        \n        CUDA_CHECK(cudaLaunchKernel((void*)k_rotateImage, gridDim, blockDim, args, 0, stream));\n\n        // Copy rotated image data back to host\n        CUDA_CHECK(cudaMemcpyAsync(rotatedImage_h, rotatedImage_d, width * height * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                assert(fabs(rotatedImage_h[y * width + x] - expectedOutput_h[y * width + x]) < TOLERANCE);\n            }\n        }\n    }\n\n    //Test Case 6\n    {\n        constexpr int width = 8;\n        constexpr int height = 8;\n\n        float image_h[width * height] = {\n            1,  2,  3,  4,  5,  6,  7,  8,\n            9, 10, 11, 12, 13, 14, 15, 16,\n            17, 18, 19, 20, 21, 22, 23, 24,\n            25, 26, 27, 28, 29, 30, 31, 32,\n            33, 34, 35, 36, 37, 38, 39, 40,\n            41, 42, 43, 44, 45, 46, 47, 48,\n            49, 50, 51, 52, 53, 54, 55, 56,\n            57, 58, 59, 60, 61, 62, 63, 64\n        }; \n\n        float expectedOutput_h[width * height] = {\n            4, 6, 25, 17, 10, 3, 4, 19,\n            14, 33, 9, 26, 19, 12, 20, 5,\n            41, 11, 34, 27, 27, 20, 13, 20,\n            49, 42, 43, 35, 28, 21, 22, 14,\n            58, 51, 30, 44, 37, 30, 0, 23,\n            59, 60, 52, 53, 46, 38, 31, 32,\n            0, 0, 61, 54, 0, 47, 40, 0,\n            0, 0, 0, 62, 55, 48, 0, 0\n        };\n\n        // Host and device variables\n        float rotatedImage_h[width * height] = { 0 };  // Output matrix (host)\n\n        // Copy predefined image to device\n        CUDA_CHECK(cudaMemcpyAsync(image_d, image_h, width * height * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Launch kernel with block size 16x16\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(width/16),ceil(height/16),1)\n        // Block: (16,16,1)\n        void *args[] = {&image_d, &rotatedImage_d, (void*)&angle, (void*)&width, (void*)&height};        \n        CUDA_CHECK(cudaLaunchKernel((void*)k_rotateImage, gridDim, blockDim, args, 0, stream));\n\n        // Copy rotated image data back to host\n        CUDA_CHECK(cudaMemcpyAsync(rotatedImage_h, rotatedImage_d, width * height * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                assert(fabs(rotatedImage_h[y * width + x] - expectedOutput_h[y * width + x]) < TOLERANCE);\n            }\n        }\n    }\n\n    //Test Case 7\n    {\n        constexpr int width = 10;\n        constexpr int height = 10;\n\n        float image_h[width * height] = {\n            1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n            11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n            21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n            31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n            41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n            51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n            61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n            71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n            81, 82, 83, 84, 85, 86, 87, 88, 89, 90,\n            91, 92, 93, 94, 95, 96, 97, 98, 99, 100\n        };\n\n        float expectedOutput_h[width * height] = {\n            4, 6, 25, 31, 22, 3, 13, 4, 14, 33,\n            9, 51, 41, 42, 32, 23, 14, 15, 5, 6,\n            27, 61, 52, 20, 43, 34, 25, 35, 16, 7,\n            71, 62, 58, 53, 44, 44, 35, 26, 0, 17,\n            82, 72, 63, 64, 54, 45, 36, 37, 27, 18,\n            61, 83, 74, 47, 65, 56, 47, 0, 38, 29,\n            93, 84, 85, 75, 66, 67, 57, 48, 49, 39,\n            0, 95, 0, 86, 77, 0, 68, 59, 0, 50,\n            0, 0, 96, 0, 87, 78, 69, 0, 60, 0,\n            0, 0, 0, 97, 88, 89, 79, 70, 0, 0\n        };\n\n        // Host and device variables\n        float rotatedImage_h[width * height] = { 0 };  // Output matrix (host)\n\n        // Copy predefined image to device\n        CUDA_CHECK(cudaMemcpyAsync(image_d, image_h, width * height * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Launch kernel with block size 16x16\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n\n        // Grid: (ceil(width/16),ceil(height/16),1)\n        // Block: (16,16,1)\n        void *args[] = {&image_d, &rotatedImage_d, (void*)&angle, (void*)&width, (void*)&height};        \n        CUDA_CHECK(cudaLaunchKernel((void*)k_rotateImage, gridDim, blockDim, args, 0, stream));\n\n        // Copy rotated image data back to host\n        CUDA_CHECK(cudaMemcpyAsync(rotatedImage_h, rotatedImage_d, width * height * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                assert(fabs(rotatedImage_h[y * width + x] - expectedOutput_h[y * width + x]) < TOLERANCE);\n            }\n        }\n    }\n    // Free device memory\n    CUDA_CHECK(cudaFree(image_d));\n    CUDA_CHECK(cudaFree(rotatedImage_d));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_rotateImage(  float* image_d, \n                                float* rotatedImage_d, \n                                float angle, \n                                int width, \n                                int height) {\n","test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2" }
{"task_id": "CUDA/97", "date": "2025-07-30", "prompt": "Write a CUDA kernel to calculate the Kronecker product of two matrices. Each thread should compute an element in the Kronecker product matrix.\n\nThe signature of the kernel is __global__ void k_kroneckerProduct(float* matrixA_d, float* matrixB_d, float* matrixKronProd_d, int matARowsize, int matAColsize, int matBRowsize, int matBColsize), where matrixA_d is the pointer to the input matrix A, matrixB_d is the pointer to the input matrix B, matrixKronProd_d is the Kronecker product output of matrix A and matrix B, matARowsize, matBRowsize are the number of rows of matrix A, matrix B respectively while matAColsize, matBColsize are the number of columns of matrix A, matrix B respectively.\n\n>>> k_kroneckerProduct({{1, 2}, {3, 4}}, {{0, 5}, {6, 7}}, matrixKronProd_d, 2, 2, 2, 2) -> matrixKronProd_d:{{0, 5, 0, 10}, {6, 7, 12, 14}, {0, 15, 0, 20}, {18, 21, 24, 28}}\n>>> k_kroneckerProduct({{1}, {2}, {3}}, {{1, 2}, {3, 4}},  matrixKronProd_d, 3, 1, 2, 2) -> matrixKronProd_d:{{1, 2}, {3, 4}, {2, 4}, {6, 8}, {3, 6}, {9, 12}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <assert.h>\n#include <cuda_runtime.h>\n#undef NDEBUG\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n\n#define BLOCK_SIZE 16\n\n__global__ void k_kroneckerProduct( float* matrixA_d, \n                                    float* matrixB_d, \n                                    float* matrixKronProd_d, \n                                    int    matARowsize, \n                                    int    matAColsize, \n                                    int    matBRowsize, \n                                    int    matBColsize);\n\nvoid launch() {\n    // Testcase 1-7\n    {\n        int numTestCases = 7;\n        int matrixAHeight[numTestCases] = {2, 2, 2, 2, 1, 3, 2};\n        int matrixAWidth[numTestCases] =  {2, 2, 2, 2, 3, 1, 3};\n        int matrixBHeight[numTestCases] = {2, 1, 2, 1, 2, 2, 4};\n        int matrixBWidth[numTestCases] =  {2, 2, 1, 3, 2, 2, 4};\n        \n        int maxMatrixAHeight = *std::max_element(matrixAHeight, matrixAHeight + numTestCases);\n        int maxMatrixAWidth = *std::max_element(matrixAWidth, matrixAWidth + numTestCases);\n        int maxMatrixBHeight = *std::max_element(matrixBHeight, matrixBHeight + numTestCases);\n        int maxMatrixBWidth = *std::max_element(matrixBWidth, matrixBWidth + numTestCases);\n\n        float matrixA_h[numTestCases][maxMatrixAHeight * maxMatrixAWidth] ={{1.0, 2.0, 3.0, 4.0},\n                                                                            {1.0, 0.0, 0.0, 1.0},\n                                                                            {1.0, 0.0, 0.0, 1.0},\n                                                                            {1.0, 2.0, 3.0, 4.0},\n                                                                            {1.0, 1.0, 1.0},\n                                                                            {1.0, 2.0, 3.0},\n                                                                            {1.0, -4.0, 7.0, -2.0, 3.0, 3.0}\n                                                                            }; \n\n        float matrixB_h[numTestCases][maxMatrixBHeight * maxMatrixBWidth] = {   {0.0, 5.0, 6.0, 7.0},\n                                                                                {1.0, 1.0},\n                                                                                {1.0, 1.0},\n                                                                                {1.0, 1.0, 1.0},\n                                                                                {1.0, 2.0, 3.0, 4.0},\n                                                                                {1.0, 2.0, 3.0, 4.0},\n                                                                                {8.0, -9.0, -6.0, 5.0, 1.0, -3.0, -4.0, 7.0, 2.0, 8.0, -8.0, -3.0, 1.0, 2.0, -5.0, -1.0}\n                                                                            };\n        \n        int matrixKronProdHeight = maxMatrixAHeight * maxMatrixBHeight;\n        int matrixKronProdWidth = maxMatrixAWidth * maxMatrixBWidth;\n        float* matrixKronProd_h = (float *)calloc(matrixKronProdHeight * matrixKronProdWidth, sizeof(float)); \n        \n        float expectedOutput_h[numTestCases][matrixKronProdHeight * matrixKronProdWidth] = {  {0, 5, 0, 10, 6, 7, 12, 14, 0, 15, 0, 20, 18, 21, 24, 28},\n                                                                                {1, 1, 0, 0, 0, 0, 1, 1},\n                                                                                {1, 0, 1, 0, 0, 1, 0, 1},\n                                                                                {1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4},\n                                                                                {1, 2, 1, 2, 1, 2, 3, 4, 3, 4, 3, 4},\n                                                                                {1, 2, 3, 4, 2, 4, 6, 8, 3, 6, 9, 12},\n                                                                                {8, -9, -6,  5, -32, 36, 24, -20, 56, -63, -42, 35, \n                                                                                 1, -3, -4,  7,  -4, 12, 16, -28,  7, -21, -28, 49,\n                                                                                 2,  8, -8, -3,  -8,-32, 32,  12, 14,  56, -56, -21, \n                                                                                 1,  2, -5, -1,  -4, -8, 20,   4,  7,  14, -35,  -7, \n                                                                                 -16, 18, 12, -10, 24, -27, -18, 15, 24, -27, -18, 15,\n                                                                                 -2,  6,  8, -14, 3, -9, -12, 21, 3, -9, -12, 21,\n                                                                                 -4, -16, 16,  6,  6, 24, -24,  -9, 6,  24,  -24,  -9,\n                                                                                 -2, -4, 10,  2,  3,  6, -15,  -3,  3,  6, -15, -3}\n                                                                             };\n        cudaStream_t stream;\n        CUDA_CHECK(cudaStreamCreate(&stream));\n        \n        // Grid: (1, 1, 1)\n        dim3 gridSize(ceil((matrixKronProdHeight + BLOCK_SIZE - 1) / BLOCK_SIZE), ceil((matrixKronProdWidth + BLOCK_SIZE - 1) / BLOCK_SIZE), 1);\n        \n        // Block: (16, 16, 1)\n        dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE, 1);\n        float* matrixA_d = nullptr;\n        float* matrixB_d = nullptr;\n        float* matrixKronProd_d = nullptr;\n\n        CUDA_CHECK(cudaMallocAsync(&matrixA_d, maxMatrixAHeight * maxMatrixAWidth * sizeof(float), stream));\n        CUDA_CHECK(cudaMallocAsync(&matrixB_d, maxMatrixBHeight * maxMatrixBWidth * sizeof(float), stream));\n        CUDA_CHECK(cudaMallocAsync(&matrixKronProd_d, matrixKronProdHeight * matrixKronProdWidth * sizeof(float), stream));\n\n        for (int tc=0; tc<numTestCases; tc++) {\n            matrixKronProdHeight = matrixAHeight[tc] * matrixBHeight[tc];\n            matrixKronProdWidth = matrixAWidth[tc] * matrixBWidth[tc];\n            CUDA_CHECK(cudaMemcpyAsync(matrixA_d, matrixA_h[tc], matrixAHeight[tc] * matrixAWidth[tc] * sizeof(float), cudaMemcpyHostToDevice, stream));\n            CUDA_CHECK(cudaMemcpyAsync(matrixB_d, matrixB_h[tc], matrixBHeight[tc] * matrixBWidth[tc] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n            void *args[] = {&matrixA_d, &matrixB_d, &matrixKronProd_d, &matrixAHeight[tc], &matrixAWidth[tc], &matrixBHeight[tc], &matrixBWidth[tc] };\n            CUDA_CHECK(cudaLaunchKernel((void*)k_kroneckerProduct, gridSize, blockSize, args, 0, stream));\n            CUDA_CHECK(cudaMemcpyAsync(matrixKronProd_h, matrixKronProd_d, matrixKronProdHeight * matrixKronProdWidth * sizeof(float), cudaMemcpyDeviceToHost, stream));\n            CUDA_CHECK(cudaStreamSynchronize(stream));\n            \n            for(int i = 0; i < matrixKronProdHeight * matrixKronProdWidth; i++) {\n                assert(matrixKronProd_h[i] == expectedOutput_h[tc][i]);\n            }\n        }\n\n        free(matrixKronProd_h);\n        CUDA_CHECK(cudaFreeAsync(matrixA_d, stream));\n        CUDA_CHECK(cudaFreeAsync(matrixB_d, stream));\n        CUDA_CHECK(cudaFreeAsync(matrixKronProd_d, stream));\n        CUDA_CHECK(cudaStreamDestroy(stream));\n    }\n}\n\n__global__ void k_kroneckerProduct( float* matrixA_d, \n                                    float* matrixB_d, \n                                    float* matrixKronProd_d, \n                                    int    matARowsize, \n                                    int    matAColsize, \n                                    int    matBRowsize, \n                                    int    matBColsize) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/98", "date": "2025-07-30", "prompt": "Implement a CUDA kernel to generate a heatmap. Each kernel thread should compute an element in the input array, using minimum and maximum boundary values to clamp and normalize the data.\n\nThe signature of the CUDA kernel is __global__ void k_generateHeatmap(float * input_d, unsigned char * output_d, const float minValue, const float maxValue, const int numElementsX, const int numElementsY), where minValue and maxValue are the boundaries of input data values.\n\n>>> k_generateHeatmap({-1500.0f, -1650.0f, -1800.0f, -1950.0f, -2100.0f, 2250.0f, 2400.0f, 2550.0f, 2700.0f, 2850.0f }, output_d, 1900.0f, 2800.0f, 10, 1) -> output_d:{ 0, 0, 0, 14, 57, 99, 142, 184, 227, 255 }\n>>> k_generateHeatmap({-1.0f, -0.8f, -0.6f, -0.4f, -0.2f, 0.0f, 0.2f, 0.4f, 0.6f, 0.8f }, output_d, -1.0f, 1.0f, 10, 1) -> output_d:{ 0, 25, 51, 77, 102, 128, 153, 179, 204, 230 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <vector>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call){                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess){                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\nconstexpr int NUM_ELEMENTS_X = 10;\nconstexpr int NUM_ELEMENTS_Y = 1;\nconstexpr int NUM_TOTAL_ELEMENTS = NUM_ELEMENTS_X * NUM_ELEMENTS_Y;\nconstexpr int SCALING_VALUE = 255;\nconstexpr int NUM_GRID_BLOCKS_X = 32;\nconstexpr int NUM_GRID_BLOCKS_Y = 8;\nconstexpr int NUM_BLOCK_THREADS_X = 16;\nconstexpr int NUM_BLOCK_THREADS_Y = 16;\n\n__global__ void k_generateHeatmap(  float * input_d, \n                                    unsigned char * output_d, \n                                    const float minValue, \n                                    const float maxValue,\n                                    const int numElementsX,\n                                    const int numElementsY);\n\nvoid launch() {\n    dim3 gridDim(NUM_GRID_BLOCKS_X, NUM_GRID_BLOCKS_Y, 1);\n    dim3 blockDim(NUM_BLOCK_THREADS_X, NUM_BLOCK_THREADS_Y, 1);\n    float * input_d;\n    unsigned char * output_d;\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    CUDA_CHECK(cudaMallocAsync(&input_d, sizeof(float) * NUM_TOTAL_ELEMENTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, sizeof(unsigned char) * NUM_TOTAL_ELEMENTS, stream));\n    std::vector<float> input(NUM_TOTAL_ELEMENTS);\n    std::vector<unsigned char> output(NUM_TOTAL_ELEMENTS);\n    std::vector<unsigned char> expectedOutput(NUM_TOTAL_ELEMENTS);\n    float * input_h = input.data();\n    unsigned char * output_h = output.data();\n    // Test 1: Continuously increasing values with instances of partial overflow and underflow.\n    {\n        constexpr float minValue = 1900.0f;\n        constexpr float maxValue = 2800.0f;\n        void * args[6] = { &input_d, &output_d, (void*)&minValue, (void*)&maxValue, (void*)&NUM_ELEMENTS_X, (void*)&NUM_ELEMENTS_Y };\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            input[i] = 1500 + i * 150.0f;\n            expectedOutput[i] = round(255 * (min(max(input[i], minValue), maxValue) - minValue) / (maxValue - minValue));\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, sizeof(float) * NUM_TOTAL_ELEMENTS, cudaMemcpyHostToDevice, stream));\n        // Block: (NUM_BLOCK_THREADS_X, NUM_BLOCK_THREADS_Y, 1)\n        // Grid: (NUM_GRID_BLOCKS_X, NUM_GRID_BLOCKS_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_generateHeatmap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(unsigned char) * NUM_TOTAL_ELEMENTS, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            assert(expectedOutput[i] == output[i]);\n        }\n    }\n    // Test 2: Duplicate Values.\n    {\n        constexpr float minValue = 1.0f;\n        constexpr float maxValue = 2.0f;\n        void * args[6] = { &input_d, &output_d, (void*)&minValue, (void*)&maxValue, (void*)&NUM_ELEMENTS_X, (void*)&NUM_ELEMENTS_Y };\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            input[i] = 1.5f;\n            expectedOutput[i] = round(255 * (min(max(input[i], minValue), maxValue) - minValue) / (maxValue - minValue));\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, sizeof(float) * NUM_TOTAL_ELEMENTS, cudaMemcpyHostToDevice, stream));\n        // Block: (NUM_BLOCK_THREADS_X, NUM_BLOCK_THREADS_Y, 1)\n        // Grid: (NUM_GRID_BLOCKS_X, NUM_GRID_BLOCKS_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_generateHeatmap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(unsigned char) * NUM_TOTAL_ELEMENTS, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            assert(expectedOutput[i] == output[i]);\n        }\n    }\n    // Test 3: Values beyond the allowed range.\n    {\n        constexpr float minValue = 10.0f;\n        constexpr float maxValue = 100.0f;\n        void * args[6] = { &input_d, &output_d, (void*)&minValue, (void*)&maxValue, (void*)&NUM_ELEMENTS_X, (void*)&NUM_ELEMENTS_Y };\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            input[i] = ((i & 1) ? 1.0f : 1000.0f);\n            expectedOutput[i] = round(255 * (min(max(input[i], minValue), maxValue) - minValue) / (maxValue - minValue));\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, sizeof(float) * NUM_TOTAL_ELEMENTS, cudaMemcpyHostToDevice, stream));\n        // Block: (NUM_BLOCK_THREADS_X, NUM_BLOCK_THREADS_Y, 1)\n        // Grid: (NUM_GRID_BLOCKS_X, NUM_GRID_BLOCKS_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_generateHeatmap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(unsigned char) * NUM_TOTAL_ELEMENTS, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            assert(expectedOutput[i] == output[i]);\n        }\n    }\n    // Test 4: Alternating values.\n    {\n        constexpr float minValue = 10.0f;\n        constexpr float maxValue = 100.0f;\n        void * args[6] = { &input_d, &output_d, (void*)&minValue, (void*)&maxValue, (void*)&NUM_ELEMENTS_X, (void*)&NUM_ELEMENTS_Y };\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            input[i] = ((i & 1) ? 20.0f : 80.0f);\n            expectedOutput[i] = round(255 * (min(max(input[i], minValue), maxValue) - minValue) / (maxValue - minValue));\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, sizeof(float) * NUM_TOTAL_ELEMENTS, cudaMemcpyHostToDevice, stream));\n        // Block: (NUM_BLOCK_THREADS_X, NUM_BLOCK_THREADS_Y, 1)\n        // Grid: (NUM_GRID_BLOCKS_X, NUM_GRID_BLOCKS_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_generateHeatmap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(unsigned char) * NUM_TOTAL_ELEMENTS, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            assert(expectedOutput[i] == output[i]);\n        }\n    }\n    // Test 5: Increasing values within boundaries.\n    {\n        constexpr float minValue = 10.0f;\n        constexpr float maxValue = 100.0f;\n        void * args[6] = { &input_d, &output_d, (void*)&minValue, (void*)&maxValue, (void*)&NUM_ELEMENTS_X, (void*)&NUM_ELEMENTS_Y };\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            input[i] = 10.0f + 10.0f * i;\n            expectedOutput[i] = round(255 * (min(max(input[i], minValue), maxValue) - minValue) / (maxValue - minValue));\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, sizeof(float) * NUM_TOTAL_ELEMENTS, cudaMemcpyHostToDevice, stream));\n        // Block: (NUM_BLOCK_THREADS_X, NUM_BLOCK_THREADS_Y, 1)\n        // Grid: (NUM_GRID_BLOCKS_X, NUM_GRID_BLOCKS_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_generateHeatmap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(unsigned char) * NUM_TOTAL_ELEMENTS, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            assert(expectedOutput[i] == output[i]);\n        }\n    }\n    // Test 6: Randomized inputs.\n    {\n        srand(1);\n        constexpr float minValue = 0.0f;\n        constexpr float maxValue = 1.0f;\n        void * args[6] = { &input_d, &output_d, (void*)&minValue, (void*)&maxValue, (void*)&NUM_ELEMENTS_X, (void*)&NUM_ELEMENTS_Y };\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            input[i] = 1.0f / rand();\n            expectedOutput[i] = round(255 * (min(max(input[i], minValue), maxValue) - minValue) / (maxValue - minValue));\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, sizeof(float) * NUM_TOTAL_ELEMENTS, cudaMemcpyHostToDevice, stream));\n        // Block: (NUM_BLOCK_THREADS_X, NUM_BLOCK_THREADS_Y, 1)\n        // Grid: (NUM_GRID_BLOCKS_X, NUM_GRID_BLOCKS_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_generateHeatmap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(unsigned char) * NUM_TOTAL_ELEMENTS, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            assert(expectedOutput[i] == output[i]);\n        }\n    }\n    // Test 7: A negative minimum value and a positive maximum value.\n    {\n        constexpr float minValue = -1.0f;\n        constexpr float maxValue = 1.0f;\n        void * args[6] = { &input_d, &output_d, (void*)&minValue, (void*)&maxValue, (void*)&NUM_ELEMENTS_X, (void*)&NUM_ELEMENTS_Y };\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            input[i] = -1.0f + i * 0.2f;\n            expectedOutput[i] = round(255 * (min(max(input[i], minValue), maxValue) - minValue) / (maxValue - minValue));\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, sizeof(float) * NUM_TOTAL_ELEMENTS, cudaMemcpyHostToDevice, stream));\n        // Block: (NUM_BLOCK_THREADS_X, NUM_BLOCK_THREADS_Y, 1)\n        // Grid: (NUM_GRID_BLOCKS_X, NUM_GRID_BLOCKS_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_generateHeatmap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(unsigned char) * NUM_TOTAL_ELEMENTS, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            assert(expectedOutput[i] == output[i]);\n        }\n    }\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n\n__global__ void k_generateHeatmap(  float * input_d, \n                                    unsigned char * output_d, \n                                    const float minValue, \n                                    const float maxValue,\n                                    const int numElementsX,\n                                    const int numElementsY) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/99", "date": "2025-07-30", "prompt": "Write a CUDA kernel that adjust brightness of an image leveraging data parallelism. Each thread processes a pixel by multiplying it with a brightness factor. If the resulting value exceeds the upper or lower thresholds, it is clamped to ensure the output remains within valid bounds, effectively saturating the result.\n\nThe signature of the function is __global__ void k_adjustBrightness(const float* inputImage, int upperThreshold, int lowerThreshold, float brightnessFactor, float *outputImage, int size), where inputImage is the pointer to the input image, UpperThreshold and lowerThreshold are the thresolds to ensure output is within valid values , brightnessFactor is a scaling factor to adjust brightness, outputImage is the pointer to the brightness adjusted output image, and size is total number of pixels in input image.\n\n>>> k_adjustBrightness({244.1642, 246.0466, 40.1913, 247.5012, 244.0776, 123.7708, 204.0715, 36.1810}, 250, 5, 0.7577, *outputImage, 8) -> ({185.0032, 186.4295, 30.4529, 187.5317, 184.9376, 93.7811, 154.6250, 27.4143})\n>>> k_adjustBrightness({107.5491, 233.5126, 202.0129, 244.6706, 167.2139, 9.1065, 216.5280, 238.1683, 189.4988, 100.0179}, 245, 10, 0.95, *outputImage, 10) -> ({102.1716, 221.8370, 191.9123, 232.4371, 158.8532, 10.0000, 205.7016, 226.2599, 180.0239, 95.0170})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <cuda_runtime.h>\n#include <assert.h>\n\n#undef  NDEBUG\n#define EPSILON     (1e-2)\n#define BLOCK_SIZE  (256)\n#define CUDA_CHECK(call) \\\ndo { \\\n       cudaError_t error = call; \\\n       if (error != cudaSuccess) { \\\n           fprintf(stderr, \"CUDA error at %s:%d %s\\n\", \\\n                   __FILE__, __LINE__, \\\n                   cudaGetErrorString(error)); \\\n           exit(EXIT_FAILURE); \\\n       } \\\n} while(0)\n\n__global__ void k_adjustBrightness(const float* inputImage, int upperThreshold, int lowerThreshold, float brightnessFactor, float *outputImage, int size);\n\nvoid launch() {\n     // Number of test cases\n    const int TEST_CASE_COUNT = 8;\n    // Sizes of the image in each test case\n    const int INPUT_DATA_LENGTH[TEST_CASE_COUNT] = {8, 10, 10, 12, 14, 15, 11, 13};\n    // Find the maximum image size\n    const int MAX_VECTOR_SIZE = *std::max_element(INPUT_DATA_LENGTH, INPUT_DATA_LENGTH + TEST_CASE_COUNT);\n\n    // Input vectors and configurations for the tests\n    const float inputImage_h[TEST_CASE_COUNT][MAX_VECTOR_SIZE] =  {\n        {244.1642, 246.0466, 40.1913, 247.5012, 244.0776, 123.7708, 204.0715, 36.1810},\n        {107.5491, 233.5126, 202.0129, 244.6706, 167.2139, 9.1065, 216.5280, 238.1683, 189.4988, 100.0179},\n        {140.2899, 158.7311, 149.6964, 52.9743, 76.8178, 120.0855, 58.7745, 215.2987, 49.6649, 57.6101},\n        {151.6985, 66.8640,\t153.7250, 181.3600,\t56.5454, 29.9415, 75.6523, 81.2885, 108.1625, 129.5039,\t21.8065, 66.9330},\n        {100.8564, 93.6963,\t251.9354, 9.6234, 225.7178, 232.8881, 203.0269, 25.1716, 66.7772, 85.5160, 173.3306, 34.8211, 183.9130, 27.2243},\n        {126.0144, 198.6582, 182.3345, 230.4487, 227.1852, 85.2116, 178.1802, 50.4415, 7.7879, 189.7389, 127.5057, 122.3801, 230.7042, 155.5160, 157.5049},\n        {58.0544, 111.1032, 79.3311, 235.4618, 109.7029, 47.1282, 230.7446, 249.8358,\t111.9118, 28.3354, 65.8065},\n        {7.4512,\t236.8578, 186.2344, 124.5953, 147.5239, 60.5073, 117.0065, 245.5876, 139.4355, 132.8896, 59.0566, 124.6689, 159.1353}\n    };\n    const int upperThresholds[TEST_CASE_COUNT] = {\n        250, 245, 247, 244, 241, 235, 230, 225\n    };\n    const int lowerThresholds[TEST_CASE_COUNT] = {\n        5, 10, 13, 15, 18, 20, 23, 27\n    };\n    const float brightnessFactor[TEST_CASE_COUNT] = {\n        0.7577, 0.95, 0.2219, 0.5313, 1.0413, 0.8829, 0.8499, 1.1173\n    };\n\n    // expected outputs\n    const float expectedOutputData[TEST_CASE_COUNT][MAX_VECTOR_SIZE] =  {\n        {185.0032, 186.4295, 30.4529, 187.5317, 184.9376, 93.7811, 154.6250, 27.4143},\n        {102.1716, 221.8370, 191.9123, 232.4371, 158.8532, 10.0000, 205.7016, 226.2599, 180.0239, 95.0170},\n        {31.1332, 35.2257, 33.2207, 13.0000, 17.0474, 26.6494, 13.0433, 47.7792, 13.0000, 13.0000},\n        {80.5974, 35.5248, 81.6741, 96.3566, 30.0426, 15.9079, 40.1941, 43.1886, 57.4667, 68.8054, 15.0000, 35.5615},\n        {105.0218, 97.5660,\t241.0000, 18.0000, 235.0399, 241.0000, 211.4119, 26.2112, 69.5351, 89.0478,\t180.4892, 36.2592, 191.5086, 28.3487},\n        {111.2581, 175.3953, 160.9831, 203.4632, 200.5818, 75.2333, 157.3153, 44.5348, 20.0000, 167.5205, 112.5748, 108.0494, 203.6887, 137.3051, 139.0611},\n        {49.3404, 94.4266, 67.4235, 200.1190, 93.2365, 40.0543, 196.1098, 212.3354, 95.1138, 24.0823, 55.9289},\n        {27.0000, 225.0000, 208.0797, 139.2103, 164.8285, 67.6048, 130.7314, 225.0000, 155.7913, 148.4776, 65.9839, 139.2926, 177.8019}\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Initialize results on the host\n    float *outputImage_h;\n    outputImage_h = (float*)malloc(MAX_VECTOR_SIZE * sizeof(float));\n\n    // Pointers for device memory (GPU)\n    float *intputImage_d, *outputImage_d;\n\n    // Allocate the memory on the device\n    CUDA_CHECK(cudaMallocAsync(&intputImage_d, MAX_VECTOR_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&outputImage_d, MAX_VECTOR_SIZE * sizeof(float), stream));\n\n    // Loop to execute each test case\n    for (int i = 0; i < TEST_CASE_COUNT; ++i) {\n        // Copy input data to the device\n        CUDA_CHECK(cudaMemcpyAsync(intputImage_d, inputImage_h[i], INPUT_DATA_LENGTH[i] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        // Initialize the result on the device\n        CUDA_CHECK(cudaMemsetAsync(outputImage_d, 0, INPUT_DATA_LENGTH[i] * sizeof(float), stream));\n\n        // Determine the number of threads and blocks\n        dim3 gridSize = dim3((INPUT_DATA_LENGTH[i] + BLOCK_SIZE - 1) / BLOCK_SIZE, 1, 1);\n        dim3 blockSize = dim3(BLOCK_SIZE, 1, 1);\n\n        // Execute the kernel\n        // Grid:  ((INPUT_DATA_LENGTH[i] + BLOCK_SIZE - 1) / BLOCK_SIZE, 1, 1)\n        // Block: (BLOCK_SIZE, 1, 1)\n        void *args[] = {&intputImage_d, (void*)&upperThresholds[i], (void*)&lowerThresholds[i], (void*)&brightnessFactor[i], &outputImage_d, (void*)&INPUT_DATA_LENGTH[i]};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_adjustBrightness, gridSize, blockSize, args, 0, stream));\n\n        // Copy the result back to the host (CPU)\n        CUDA_CHECK(cudaMemcpyAsync(outputImage_h, outputImage_d, INPUT_DATA_LENGTH[i] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify if the calculated dot product matches the expected result\n        for (int j = 0; j < INPUT_DATA_LENGTH[i]; j++) {\n            assert(fabs(outputImage_h[j] - expectedOutputData[i][j]) < EPSILON);\n        }\n    }\n    // Free device memory and stream\n    cudaFreeAsync(intputImage_d, stream);\n    cudaFreeAsync(outputImage_d, stream);\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    // Free host memories\n    free(outputImage_h);\n}\n\n__global__ void k_adjustBrightness(const float* inputImage, int upperThreshold, int lowerThreshold, float brightnessFactor, float *outputImage, int size) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/100", "date": "2025-07-30", "prompt": "Write a CUDA kernel that transforms from image positions to Geo coordinates, where each thread calculates the Geo coordinates for a corresponding pixel positions using geometric transformation:\n\nlatitude = extentWidth + imgX / (imgWidth-1) + extentLeft\nlongitude = extentHeight + imgY / (imgHeight-1) + extentTop.\n\nThe signature of the function is __global__ void k_imagePosToGeoCoord(float* imgX_d, float* imgY_d, float* lat_d, float* long_d, const float4 geoTransform, int width, int height, int imgWidth, int imgHeight), where imgX_d and imgY_d are arrays containing the image coordinates, lat_d and long_d are arrays in which the transformed world coordinates will be stored, width and height define the dimensions of the region to process within the image, while imgWidth and imgHeight represent the original image dimensions used for normalizing the coordinates.\n\n>>> k_imagePosToGeoCoord({0, 71, 142, 213, 284, 355, 426, 497, 568, 639}, {0, 53, 107, 160, 213, 267, 320, 373, 427, 479}, latitude, longitude, {73.25, 23.5, 0.35, 0.25}, 10, 1, 640, 480) -> (latitude: {73.25, 73.2889, 73.3278, 73.3667, 73.4056, 73.4444, 73.4833, 73.5222, 73.5611, 73.6}, longitude:{23.5, 23.5277, 23.5558, 23.5835, 23.6112, 23.6394, 23.667, 23.6947, 23.7229, 23.75})\n>>> k_imagePosToGeoCoord({0, 50, 100, 150, 200, 250, 300, 350, 400, 450}, {0, 50, 100, 150, 200, 250, 300, 350, 400, 450}, latitude, longitude, {73.75, 23.5, 0.35, 0.25}, 10, 1, 640, 480) -> (latitude: {73.75, 73.7774, 73.8048, 73.8322, 73.8595, 73.8869, 73.9143, 73.9417, 73.9691, 73.9965}, longitude:{23.5, 23.5261, 23.5522, 23.5783, 23.6044, 23.6305, 23.6566, 23.6827, 23.7088, 23.7349}) \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <cstdio>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n\n#define TOLERANCE 1E-3\n\n__global__ void k_imagePosToGeoCoord( float* imgX_d, float* imgY_d, float* lat_d, float* long_d, \n                                const float4 geoTransform, \n                                int width, int height, int imgWidth, int imgHeight);\n\nvoid launch() {\n    constexpr int BLOCK_SIZE_X = 32;\n    constexpr int BLOCK_SIZE_Y = 8;\n    constexpr int GRID_SIZE_X = 16;\n    constexpr int GRID_SIZE_Y = 16;\n    constexpr int NUM_POINTS = 10;\n    constexpr int TEST_WIDTH = 10;\n    constexpr int TEST_HEIGHT = 1;\n    constexpr int IMG_WIDTH = 640;\n    constexpr int IMG_HEIGHT = 480;\n\n    // Use a CUDA stream for asynchronous operations.\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate device memory\n    float* imgX_d, * imgY_d, * lat_d, * long_d;\n    CUDA_CHECK(cudaMallocAsync((void**)&imgX_d, NUM_POINTS * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&imgY_d, NUM_POINTS * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&lat_d, NUM_POINTS * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&long_d, NUM_POINTS * sizeof(float), stream));\n\n    //Test Case 1: \n    {\n        // Predefined 10 x, y image coordinates (for demonstration).\n        float imgX_h[NUM_POINTS] = { 0, 71, 142, 213, 284, 355, 426, 497, 568, 639 };\n        float imgY_h[NUM_POINTS] = { 0, 53, 107, 160, 213, 267, 320, 373, 427, 479 };\n\n        float expectedOutputLat_h[NUM_POINTS] = { \n            73.25, 73.2889, 73.3278, 73.3667, 73.4056, \n            73.4444, 73.4833, 73.5222, 73.5611, 73.6 \n        };\n        float expectedOutputLon_h[NUM_POINTS] = { \n            23.5, 23.5277, 23.5558, 23.5835, 23.6112, \n            23.6394, 23.667, 23.6947, 23.7229, 23.75 \n        };\n\n        // Array for world coordinates (latitude, longitude).\n        float latitude_h[NUM_POINTS], longitude_h[NUM_POINTS];\n\n        // Image coordinates to some Extent of lat/lon.\n        float4 geoTransform;\n        // Extent Left.\n        geoTransform.x = 73.25;\n        // Extent Top.\n        geoTransform.y = 23.50;\n        // Extent Width.\n        geoTransform.z = 0.35;\n        // Extent Height.\n        geoTransform.w = 0.25;\n\n        // Copy host data to device.\n        CUDA_CHECK(cudaMemcpyAsync( imgX_d, \n                                    imgX_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( imgY_d, \n                                    imgY_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n\n        // Define the block size and the grid size.\n        dim3 blockSize(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridSize(GRID_SIZE_X, GRID_SIZE_Y);\n\n        // Block: (32, 8, 1)\n        // Grid: (16, 16, 1)\n        void *args[] = {\n            &imgX_d, &imgY_d, &lat_d, &long_d, &geoTransform, \n            (void*)&TEST_WIDTH, (void*)&TEST_HEIGHT, (void*)&IMG_WIDTH, (void*)&IMG_HEIGHT\n        };\n        \n        CUDA_CHECK(cudaLaunchKernel((void*)k_imagePosToGeoCoord, \n                                    gridSize, \n                                    blockSize, \n                                    args, \n                                    0, \n                                    stream));\n\n        // Copy the results back to the host.\n        CUDA_CHECK(cudaMemcpyAsync( latitude_h, \n                                    lat_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( longitude_h, \n                                    long_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; i++) {\n            assert(abs(latitude_h[i] - expectedOutputLat_h[i]) < TOLERANCE);\n            assert(abs(longitude_h[i] - expectedOutputLon_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 2: \n    {\n        // Predefined 10 x, y image coordinates.\n        float imgX_h[NUM_POINTS] = { 0, 50, 100, 150, 200, 250, 300, 350, 400, 450 };\n        float imgY_h[NUM_POINTS] = { 0, 50, 100, 150, 200, 250, 300, 350, 400, 450 };\n\n        float expectedOutputLat_h[NUM_POINTS] = { \n            73.75, 73.7774, 73.8048, 73.8322, 73.8595, \n            73.8869, 73.9143, 73.9417, 73.9691, 73.9965 \n        };\n        float expectedOutputLon_h[NUM_POINTS] = { \n            23.5, 23.5261, 23.5522, 23.5783, 23.6044, \n            23.6305, 23.6566, 23.6827, 23.7088, 23.7349 \n        };\n\n        // Array for world coordinates (latitude, longitude).\n        float latitude_h[NUM_POINTS], longitude_h[NUM_POINTS];\n\n        // Image coordinates to some Extent of lat/lon.\n        float4 geoTransform;\n        // Extent Left (longitude).\n        geoTransform.x = 73.75;\n        // Extent Top (latitude).\n        geoTransform.y = 23.50;\n        // Extent Width.\n        geoTransform.z = 0.35;\n        // Extent Height.\n        geoTransform.w = 0.25;\n\n        // Copy host data to device\n        CUDA_CHECK(cudaMemcpyAsync( imgX_d, \n                                    imgX_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( imgY_d, \n                                    imgY_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n\n        // Define the block size and the grid size.\n        dim3 blockSize(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridSize(GRID_SIZE_X, GRID_SIZE_Y);\n\n        // Block: (32, 8, 1)\n        // Grid: (16, 16, 1)\n        void *args[] = {\n            &imgX_d, &imgY_d, &lat_d, &long_d, &geoTransform, \n            (void*)&TEST_WIDTH, (void*)&TEST_HEIGHT, (void*)&IMG_WIDTH, (void*)&IMG_HEIGHT\n        };\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_imagePosToGeoCoord, \n                                    gridSize, \n                                    blockSize, \n                                    args, \n                                    0, \n                                    stream));\n\n        // Copy the results back to the host.\n        CUDA_CHECK(cudaMemcpyAsync( latitude_h, \n                                    lat_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( longitude_h, \n                                    long_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; i++) {\n            assert(abs(latitude_h[i] - expectedOutputLat_h[i]) < TOLERANCE);\n            assert(abs(longitude_h[i] - expectedOutputLon_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 3: \n    {\n        float imgX_h[NUM_POINTS] = { 200, 250, 300, 350, 400, 450, 500, 550, 600, 639 };\n        float imgY_h[NUM_POINTS] = { 300, 320, 340, 360, 380, 400, 420, 440, 460, 479 };\n\n        float expectedOutputLat_h[NUM_POINTS] = { \n            73.3595, 73.3869, 73.4143, 73.4417, 73.4691, \n            73.4965, 73.5239, 73.5513, 73.5786, 73.6 \n        };\n        float expectedOutputLon_h[NUM_POINTS] = { \n            23.1566, 23.167, 23.1775, 23.1879, 23.1983, \n            23.2088, 23.2192, 23.2296, 23.2401, 23.25 \n        };\n\n        // Array for world coordinates (latitude, longitude).\n        float latitude_h[NUM_POINTS], longitude_h[NUM_POINTS];\n\n        // Image coordinates to some Extent of lat/lon.\n        float4 geoTransform;\n        // Extent Left (longitude)\n        geoTransform.x = 73.25;\n        // Extent Top (latitude)\n        geoTransform.y = 23.00;\n        // Extent Width\n        geoTransform.z = 0.35;\n        // Extent Height\n        geoTransform.w = 0.25;\n\n        // Copy host data to device.\n        CUDA_CHECK(cudaMemcpyAsync( imgX_d, \n                                    imgX_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( imgY_d, \n                                    imgY_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n\n        // Define the block size and the grid size.\n        dim3 blockSize(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridSize(GRID_SIZE_X, GRID_SIZE_Y);\n\n        // Block: (32, 8, 1)\n        // Grid: (16, 16, 1)\n        void *args[] = {\n            &imgX_d, &imgY_d, &lat_d, &long_d, &geoTransform, \n            (void*)&TEST_WIDTH, (void*)&TEST_HEIGHT, (void*)&IMG_WIDTH, (void*)&IMG_HEIGHT\n        };\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_imagePosToGeoCoord, \n                                    gridSize, \n                                    blockSize, \n                                    args, \n                                    0, \n                                    stream));\n\n        // Copy the results back to the host.\n        CUDA_CHECK(cudaMemcpyAsync( latitude_h, \n                                    lat_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( longitude_h, \n                                    long_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; i++) {\n            assert(abs(latitude_h[i] - expectedOutputLat_h[i]) < TOLERANCE);\n            assert(abs(longitude_h[i] - expectedOutputLon_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 4: \n    {\n        float imgX_h[NUM_POINTS] = { 100, 150, 200, 250, 300, 350, 400, 450, 500, 550 };\n        float imgY_h[NUM_POINTS] = { 120, 140, 160, 180, 200, 220, 240, 260, 280, 300 };\n\n        float expectedOutputLat_h[NUM_POINTS] = { \n            72.5548, 72.5822, 72.6095, 72.6369, 72.6643, \n            72.6917, 72.7191, 72.7465, 72.7739, 72.8013\n        };\n        float expectedOutputLon_h[NUM_POINTS] = { \n            23.5626, 23.5731, 23.5835, 23.5939, 23.6044, \n            23.6148, 23.6253, 23.6357, 23.6461, 23.6566\n        };\n\n        // Array for world coordinates (latitude, longitude).\n        float latitude_h[NUM_POINTS], longitude_h[NUM_POINTS];\n\n        // image coordinates to some Extent of lat/lon.\n        float4 geoTransform;\n        // Extent Left (longitude).\n        geoTransform.x = 72.50;\n        // Extent Top (latitude).\n        geoTransform.y = 23.50;\n        // Extent Width.\n        geoTransform.z = 0.35;\n        // Extent Height.\n        geoTransform.w = 0.25;\n\n        // Copy host data to device.\n        CUDA_CHECK(cudaMemcpyAsync( imgX_d, \n                                    imgX_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( imgY_d, \n                                    imgY_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n\n        // Define the block size and the grid size.\n        dim3 blockSize(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridSize(GRID_SIZE_X, GRID_SIZE_Y);\n\n        // Block: (32, 8, 1)\n        // Grid: (16, 16, 1)\n        void *args[] = {\n            &imgX_d, &imgY_d, &lat_d, &long_d, &geoTransform, \n            (void*)&TEST_WIDTH, (void*)&TEST_HEIGHT, (void*)&IMG_WIDTH, (void*)&IMG_HEIGHT\n        };\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_imagePosToGeoCoord, \n                                    gridSize, \n                                    blockSize, \n                                    args, \n                                    0, \n                                    stream));\n\n        // Copy the results back to the host.\n        CUDA_CHECK(cudaMemcpyAsync( latitude_h, \n                                    lat_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( longitude_h, \n                                    long_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; i++) {\n            assert(abs(latitude_h[i] - expectedOutputLat_h[i]) < TOLERANCE);\n            assert(abs(longitude_h[i] - expectedOutputLon_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 5: \n    {\n        float imgX_h[NUM_POINTS] = { 50, 120, 180, 240, 310, 370, 420, 500, 550, 600 };\n        float imgY_h[NUM_POINTS] = { 40, 90, 150, 200, 230, 290, 340, 390, 440, 470 };\n\n        float expectedOutputLat_h[NUM_POINTS] = { \n            73.7774, 73.8157, 73.8486, 73.8815, 73.9198, \n            73.9527, 73.98, 74.0239, 74.0513, 74.0786 \n        };\n        float expectedOutputLon_h[NUM_POINTS] = { \n            23.7709, 23.797, 23.8283, 23.8544, 23.87, \n            23.9014, 23.9275, 23.9535, 23.9796, 23.9953\n        };\n\n        // Array for world coordinates (latitude, longitude).\n        float latitude_h[NUM_POINTS], longitude_h[NUM_POINTS];\n\n        // Image coordinates to some Extent of lat/lon.\n        float4 geoTransform;\n        // Extent Left (longitude).\n        geoTransform.x = 73.75;\n        // Extent Top (latitude).\n        geoTransform.y = 23.75;\n        // Extent Width.\n        geoTransform.z = 0.35;\n        // Extent Height.\n        geoTransform.w = 0.25;\n\n        // Copy host data to device.\n        CUDA_CHECK(cudaMemcpyAsync( imgX_d, \n                                    imgX_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice,\n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( imgY_d, \n                                    imgY_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n\n        // Define the block size and the grid size.\n        dim3 blockSize(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridSize(GRID_SIZE_X, GRID_SIZE_Y);\n\n        // Block: (32, 8, 1)\n        // Grid: (16, 16, 1)\n        void *args[] = {\n            &imgX_d, &imgY_d, &lat_d, &long_d, &geoTransform, \n            (void*)&TEST_WIDTH, (void*)&TEST_HEIGHT, (void*)&IMG_WIDTH, (void*)&IMG_HEIGHT\n        };\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_imagePosToGeoCoord, \n                                    gridSize, \n                                    blockSize, \n                                    args, \n                                    0, \n                                    stream));\n\n        // Copy the results back to the host.\n        CUDA_CHECK(cudaMemcpyAsync( latitude_h, \n                                    lat_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( longitude_h, \n                                    long_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; i++) {\n            assert(abs(latitude_h[i] - expectedOutputLat_h[i]) < TOLERANCE);\n            assert(abs(longitude_h[i] - expectedOutputLon_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 6: \n    {\n        float imgX_h[NUM_POINTS] = { 100, 100, 100, 100, 100, 100, 100, 100, 100, 100 };\n        float imgY_h[NUM_POINTS] = { 0, 53, 107, 160, 213, 267, 320, 373, 427, 479 };\n\n        float expectedOutputLat_h[NUM_POINTS] = { \n            73.5548, 73.5548, 73.5548, 73.5548, 73.5548, \n            73.5548, 73.5548, 73.5548, 73.5548, 73.5548 \n        };\n        float expectedOutputLon_h[NUM_POINTS] = { \n            23.0, 23.0277, 23.0558, 23.0835, 23.1112, \n            23.1394, 23.167, 23.1947, 23.2229, 23.25\n        };\n\n        // Array for world coordinates (latitude, longitude).\n        float latitude_h[NUM_POINTS], longitude_h[NUM_POINTS];\n\n        // Image coordinates to some Extent of lat/lon.\n        float4 geoTransform;\n        // Extent Left (longitude)\n        geoTransform.x = 73.50;\n        // Extent Top (latitude)\n        geoTransform.y = 23.00;\n        // Extent Width\n        geoTransform.z = 0.35;\n        // Extent Height\n        geoTransform.w = 0.25;\n\n        // Copy host data to device.\n        CUDA_CHECK(cudaMemcpyAsync( imgX_d, \n                                    imgX_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( imgY_d, \n                                    imgY_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n\n        // Define the block size and the grid size.\n        dim3 blockSize(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridSize(GRID_SIZE_X, GRID_SIZE_Y);\n\n        // Block: (32, 8, 1)\n        // Grid: (16, 16, 1)\n        void *args[] = {\n            &imgX_d, &imgY_d, &lat_d, &long_d, &geoTransform, \n            (void*)&TEST_WIDTH, (void*)&TEST_HEIGHT, (void*)&IMG_WIDTH, (void*)&IMG_HEIGHT\n        };\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_imagePosToGeoCoord, \n                                    gridSize, \n                                    blockSize, \n                                    args, \n                                    0, \n                                    stream));\n\n        // Copy the results back to the host.\n        CUDA_CHECK(cudaMemcpyAsync( latitude_h, \n                                    lat_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( longitude_h, \n                                    long_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; i++) {\n            assert(abs(latitude_h[i] - expectedOutputLat_h[i]) < TOLERANCE);\n            assert(abs(longitude_h[i] - expectedOutputLon_h[i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 7: \n    {\n        float imgX_h[NUM_POINTS] = { 0, 71, 142, 213, 284, 355, 426, 497, 568, 639 };\n        float imgY_h[NUM_POINTS] = { 300, 300, 300, 300, 300, 300, 300, 300, 300, 300 };\n\n        float expectedOutputLat_h[NUM_POINTS] = { \n            72.75, 72.7889, 72.8278, 72.8667, 72.9056, \n            72.9444, 72.9833, 73.0222, 73.0611, 73.1 \n        };\n        float expectedOutputLon_h[NUM_POINTS] = { \n            23.1566, 23.1566, 23.1566, 23.1566, 23.1566, \n            23.1566, 23.1566, 23.1566, 23.1566, 23.1566\n        };\n\n        // Array for world coordinates (latitude, longitude).\n        float latitude_h[NUM_POINTS], longitude_h[NUM_POINTS];\n\n        // Image coordinates to some Extent of lat/lon.\n        float4 geoTransform;\n        // Extent Left (longitude).\n        geoTransform.x = 72.75;\n        // Extent Top (latitude).\n        geoTransform.y = 23.00;\n        // Extent Width.\n        geoTransform.z = 0.35;\n        // Extent Height.\n        geoTransform.w = 0.25;\n\n        // Copy host data to device.\n        CUDA_CHECK(cudaMemcpyAsync( imgX_d, \n                                    imgX_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( imgY_d, \n                                    imgY_h, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n\n        // Define the block size and the grid size.\n        dim3 blockSize(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        dim3 gridSize(GRID_SIZE_X, GRID_SIZE_Y);\n\n        // Block: (32, 8, 1)\n        // Grid: (16, 16, 1)\n        void *args[] = {\n            &imgX_d, &imgY_d, &lat_d, &long_d, &geoTransform, \n            (void*)&TEST_WIDTH, (void*)&TEST_HEIGHT, (void*)&IMG_WIDTH, (void*)&IMG_HEIGHT\n        };\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_imagePosToGeoCoord, \n                                    gridSize, \n                                    blockSize, \n                                    args, \n                                    0, \n                                    stream));\n\n        // Copy the results back to the host.\n        CUDA_CHECK(cudaMemcpyAsync( latitude_h, \n                                    lat_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( longitude_h, \n                                    long_d, \n                                    NUM_POINTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < NUM_POINTS; i++) {\n            assert(abs(latitude_h[i] - expectedOutputLat_h[i]) < TOLERANCE);\n            assert(abs(longitude_h[i] - expectedOutputLon_h[i]) < TOLERANCE);\n        }\n    }\n\n    // Free up memory space on the device memory.\n    CUDA_CHECK(cudaFreeAsync(imgX_d, stream));\n    CUDA_CHECK(cudaFreeAsync(imgY_d, stream));\n    CUDA_CHECK(cudaFreeAsync(lat_d, stream));\n    CUDA_CHECK(cudaFreeAsync(long_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream)); \n}\n\n__global__ void k_imagePosToGeoCoord( float* imgX_d, float* imgY_d, float* lat_d, float* long_d, \n                                const float4 geoTransform, \n                                int width, int height, int imgWidth, int imgHeight) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/101", "date": "2025-07-30", "prompt": "Write a CUDA kernel that performs in place bitonic sort on a dataset. The kernel will use each thread to load a single element and compare it with adjacent elements within a block.\n\nThe signature of the function is __global__ void k_bitonicSort(float* inputData, int size), where inputData is the pointer to the unsorted input and sorted output array, and size denotes the array length of inputData.\n\n>>> k_bitonicSort({4.5, 3, 7.2, 5, 2.1}, 5) => ({2.1, 3, 4.5, 5, 7.2})\n>>> k_bitonicSort({12, 9.5, 15.3, 3, 5.8, 8}, 6) => ({3, 5.8, 8, 9.5, 12, 15.3})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <cfloat>\n#include <algorithm>\n#include <cuda_runtime.h>\n#include <assert.h>\n\n#undef  NDEBUG\n#define BLOCK_SIZE  (256)\n#define CUDA_CHECK(call) \\\ndo { \\\n       cudaError_t error = call; \\\n       if (error != cudaSuccess) { \\\n           fprintf(stderr, \"CUDA error at %s:%d %s\\n\", \\\n                   __FILE__, __LINE__, \\\n                   cudaGetErrorString(error)); \\\n           exit(EXIT_FAILURE); \\\n       } \\\n} while(0)\n\n__global__ void k_bitonicSort(float* inputData, int size);\n\nvoid launch() {\n    // Number of test cases\n    const int TEST_CASE_COUNT = 8;\n    // Sizes of the image in each test case\n    const int INPUT_DATA_LENGTH[TEST_CASE_COUNT] = {5, 6, 5, 6, 7, 5, 6, 6};\n    // Find the maximum image size\n    const int MAX_VECTOR_SIZE = *std::max_element(INPUT_DATA_LENGTH, INPUT_DATA_LENGTH + TEST_CASE_COUNT);\n\n    // Input vectors and configurations for the tests\n    float inputImage[TEST_CASE_COUNT][MAX_VECTOR_SIZE] =  {\n        {4.5, 3, 7.2, 5, 2.1},\n        {12, 9.5, 15.3, 3, 5.8, 8},\n        {6.2, 10, 1.5, 7, 8.1},\n        {5, 4.2, 9.8, 3, 7.6, 2},\n        {9, 1.1, 3, 5.5, 6.8, 10, 7.3},\n        {13.5, 15, 7.2, 2.3, 9.9},\n        {14, 3.4, 1, 8.7, 5.3, 7.1},\n        {2, 7.4, 4.6, 6, 5, 3.1}\n    };\n    \n    // expected outputs\n    float expectedOutputData[TEST_CASE_COUNT][MAX_VECTOR_SIZE] =  {\n        {2.1, 3, 4.5, 5, 7.2}, \n        {3, 5.8, 8, 9.5, 12, 15.3}, \n        {1.5, 6.2, 7, 8.1, 10}, \n        {2, 3, 4.2, 5, 7.6, 9.8}, \n        {1.1, 3, 5.5, 6.8, 7.3, 9, 10}, \n        {2.3, 7.2, 9.9, 13.5, 15}, \n        {1, 3.4, 5.3, 7.1, 8.7, 14}, \n        {2, 3.1, 4.6, 5, 6, 7.4}\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Initialize results on the host\n    float *imageData_h;\n    imageData_h = (float*)malloc(MAX_VECTOR_SIZE * sizeof(float));\n\n    // Pointers for device memory (GPU)\n    float *imageData_d;\n\n    // Allocate the memory on the device\n    CUDA_CHECK(cudaMallocAsync(&imageData_d, MAX_VECTOR_SIZE * sizeof(float), stream));\n\n    // Loop to execute each test case\n    for (int i = 0; i < TEST_CASE_COUNT; ++i) {\n        int dataLength = INPUT_DATA_LENGTH[i];\n        // Copy input data to the device\n        CUDA_CHECK(cudaMemcpyAsync(imageData_d, inputImage[i], dataLength * sizeof(float), cudaMemcpyHostToDevice, stream));\n        \n        // Determine the number of threads and blocks\n        dim3 gridSize = dim3((dataLength + BLOCK_SIZE - 1) / BLOCK_SIZE, 1, 1);\n        dim3 blockSize = dim3(BLOCK_SIZE, 1, 1);\n\n        // Execute the kernel\n        // Grid:  (1, 1, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&imageData_d, (void*)&dataLength};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_bitonicSort, gridSize, blockSize, args, sizeof(float) * BLOCK_SIZE, stream));\n\n        // Copy the result back to the host (CPU)\n        CUDA_CHECK(cudaMemcpyAsync(imageData_h, imageData_d, dataLength * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify if the sorted data matches the expected output\n        for (int j = 0; j < dataLength; j++) {\n            assert(imageData_h[j] == expectedOutputData[i][j]);\n        }\n    }\n    // Free device memory and stream\n    CUDA_CHECK(cudaFreeAsync(imageData_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    // Free host memories\n    free(imageData_h);\n}\n\n__global__ void k_bitonicSort(float* inputData, int size) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/102", "date": "2025-07-30", "prompt": "Develop a CUDA kernel to convert the signal from the time domain to the frequency domain using the FFT(Fast Fourier Transform) algorithm. Utilize the device memory to reuse the data for every stage of the FFT.\n\nThe signature of the CUDA kernel is __global__ void k_fftButterfly(float *real_d, float *imag_d, int numElements, int logNumElements), where real_d and image_d are real and imaginary components of input complex number, numElements is the number of elements in the real_d & imag_d arrays, logNumElements is log of input size used for a number of stages in FFT.\n\n>>> k_fftButterfly(real_d{ 1,2,3,4 }, imag_d{ 0,0,0,0 }, 4, 2) --> {real_d{ 10.00000, -2.00000, -2.00000, -2.00000 } imag_d{ 0.00000, 2.00000, 0.00000, -2.00000 } }\n>>> k_fftButterfly(real_d{ 1, 2, 3, 4, 5, 6, 7, 8 }, imag_d{ 0, 0, 0, 0, 0, 0, 0, 0 }, 8, 3) --> {real_d{ 36.00000, -4.00000, -4.00000, -4.00000,-4.00000, -4.00000, -4.00000, -4.00000} imag_d{0.00000, 9.65685, 4.00000, 1.65685, 0.00000, -1.65685, -4.00000, -9.65685 } }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n\n#define CUDA_CHECK(call){                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess){                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n#undef NDEBUG\n\n// Tolerance for floating-point comparison.\n#define EPSILON    1e-5\n#define GRID_SIZE  5\n#define BLOCK_SIZE 128\n#define PI         3.14159265358979323846f\n\n// CUDA kernel for Fast Fourier Transform.\n__global__ void k_fftButterfly(float *real_d, float *imag_d, int numElements, int logNumElements);\n\nvoid launch() {    \n    auto performBitReversedOrdering = [](float* inputReal_h, float* inputImag_h, int numElements) {\n        int logn = log2f(numElements);\n        for (int i = 0; i < numElements; i++) {\n            unsigned int reversed_i = 0;\n            int value = i;\n            for (int j = 0; j < logn; j++) {\n                reversed_i = (reversed_i << 1) | (value & 1);\n                value >>= 1;\n            }\n\n            if (i < reversed_i) {\n                // Swap real and imaginary parts.\n                float temp_real = inputReal_h[i];\n                inputReal_h[i] = inputReal_h[reversed_i];\n                inputReal_h[reversed_i] = temp_real;\n\n                float temp_imag = inputImag_h[i];\n                inputImag_h[i] = inputImag_h[reversed_i];\n                inputImag_h[reversed_i] = temp_imag;\n            }\n        }\n    };\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Allocate memory on the device.\n    int maximumTestSize = 16;\n    float *real_d;\n    float *imag_d;\n    CUDA_CHECK(cudaMallocAsync(&real_d, maximumTestSize * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&imag_d, maximumTestSize * sizeof(float), stream));\n    \n    // Test case 1.\n    {\n        // Fast Fourier Transform size (must be a power of 2)\n        int numElements = 16;\n        float inputReal_h[numElements] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,16 };\n        float inputImag_h[numElements] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n\n        int logn = log2f(numElements);\n        performBitReversedOrdering(inputReal_h, inputImag_h, numElements);\n\n        // Copy input data (already bit-reversed) to the device\n        CUDA_CHECK(cudaMemcpyAsync( real_d, inputReal_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync( imag_d, inputImag_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n\n        // Grid: (GRID_SIZE, 1, 1)\n        // Block: (BLOCK_SIZE, 1, 1)\n        // Launch the FFT butterfly kernel\n        void* args[]={ &real_d, &imag_d, (void*)&numElements, &logn};\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_fftButterfly, \n                                                GRID_SIZE, \n                                                BLOCK_SIZE, \n                                                args, \n                                                0, \n                                                stream));\n\n        // Copy the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync( inputReal_h, real_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync( inputImag_h, imag_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        float expectedRealOutput[numElements] = {   136.00000, -8.00000, -8.00000, -8.00000,\n                                                    -8.00000, -8.00000, -8.00000, -8.00000,\n                                                    -8.00000, -8.00000, -8.00000, -8.00000,\n                                                    -8.00000, -8.00000, -8.00000, -8.00000 };\n        float expectedImagOutput[numElements] = {   0.00000, 40.21872, 19.31371, 11.97285,\n                                                    8.00000, 5.34543, 3.31371, 1.59130,\n                                                    0.00000, -1.59130, -3.31371, -5.34543,\n                                                    -8.00000, -11.97285, -19.31371, -40.21872 };\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputReal_h[j] - expectedRealOutput[j]) <= EPSILON);\n        }\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputImag_h[j] - expectedImagOutput[j]) <= EPSILON);\n        }\n    }\n\n    // Test case 2.\n    {\n        // Fast Fourier Transform size (must be a power of 2)\n        int numElements = 8;\n        float inputReal_h[numElements] = { 1, 2, 3, 4, 5, 6, 7, 8 };\n        float inputImag_h[numElements] = { 0, 0, 0, 0, 0, 0, 0, 0 };\n\n        int logn = log2f(numElements);\n        performBitReversedOrdering(inputReal_h, inputImag_h, numElements);\n\n        // Copy input data (already bit-reversed) to the device\n        CUDA_CHECK(cudaMemcpyAsync( real_d, inputReal_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync( imag_d, inputImag_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n\n        // Grid: (GRID_SIZE, 1, 1)\n        // Block: (BLOCK_SIZE, 1, 1)\n        // Launch the FFT butterfly kernel \n        void* args[]={ &real_d, &imag_d, (void*)&numElements, &logn};\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_fftButterfly, \n                                                GRID_SIZE, \n                                                BLOCK_SIZE, \n                                                args, \n                                                0, \n                                                stream));\n\n        // Copy the result back to the host.\n        CUDA_CHECK(cudaMemcpyAsync( inputReal_h, real_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync( inputImag_h, imag_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedRealOutput[numElements] = {   36.00000, -4.00000, -4.00000, -4.00000,\n                                                    -4.00000, -4.00000, -4.00000, -4.00000 };\n        float expectedImagOutput[numElements] = {   0.00000, 9.65685, 4.00000, 1.65685,\n                                                    0.00000, -1.65685, -4.00000, -9.65685 };\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputReal_h[j] - expectedRealOutput[j]) <= EPSILON);\n        }\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputImag_h[j] - expectedImagOutput[j]) <= EPSILON);\n        }\n    }\n\n    // Test case 3.\n    {\n        // Fast Fourier Transform size (must be a power of 2)\n        int numElements = 8;\n        float inputReal_h[numElements] = { 0.0, 0.707, 1.0, 0.707, 1.0, -0.707, -1.0, -0.707 };\n        float inputImag_h[numElements] = { 0, 0, 0, 0, 0, 0, 0, 0 };\n\n        int logn = log2f(numElements);\n        performBitReversedOrdering(inputReal_h, inputImag_h, numElements);\n\n        // Copy input data (already bit-reversed) to the device\n        CUDA_CHECK(cudaMemcpyAsync( real_d, inputReal_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync( imag_d, inputImag_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n\n        // Grid: (GRID_SIZE, 1, 1)\n        // Block: (BLOCK_SIZE, 1, 1)\n        // Launch the FFT butterfly kernel \n        void* args[]={ &real_d, &imag_d, (void*)&numElements, &logn};\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_fftButterfly, \n                                                GRID_SIZE, \n                                                BLOCK_SIZE, \n                                                args, \n                                                0, \n                                                stream));\n\n        // Copy the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync( inputReal_h, real_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync( inputImag_h, imag_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedRealOutput[numElements] = {   1.00000, -1.00000, 1.00000, -1.00000,\n                                                    1.00000, -1.00000, 1.00000, -1.00000 };\n        float expectedImagOutput[numElements] = {   0.00000, -3.99970, 0.00000, 0.00030,\n                                                    0.00000, -0.00030, 0.00000, 3.99970 };\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputReal_h[j] - expectedRealOutput[j]) <= EPSILON);\n        }\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputImag_h[j] - expectedImagOutput[j]) <= EPSILON);\n        }\n    }\n\n    // Test case 4.\n    {\n        // Fast Fourier Transform size (must be a power of 2)\n        int numElements = 16;\n        float inputReal_h[numElements] = { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0 };\n        float inputImag_h[numElements] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n\n        int logn = log2f(numElements);\n        performBitReversedOrdering(inputReal_h, inputImag_h, numElements);\n\n        // Copy input data (already bit-reversed) to the device\n        CUDA_CHECK(cudaMemcpyAsync( real_d, inputReal_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync( imag_d, inputImag_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n\n        // Grid: (GRID_SIZE, 1, 1)\n        // Block: (BLOCK_SIZE, 1, 1)\n        // Launch the FFT butterfly kernel \n        void* args[]={ &real_d, &imag_d, (void*)&numElements, &logn};\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_fftButterfly, \n                                                GRID_SIZE, \n                                                BLOCK_SIZE, \n                                                args, \n                                                0, \n                                                stream));\n\n        // Copy the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync( inputReal_h, real_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync( inputImag_h, imag_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedRealOutput[numElements] = {   8.00000, -1.00000, 0.00000, -1.00000,\n                                                    0.00000, -1.00000, 0.00000, -1.00000,\n                                                    0.00000, -1.00000, 0.00000, -1.00000,\n                                                    0.00000, -1.00000, 0.00000, -1.00000 };\n        float expectedImagOutput[numElements] = {   0.00000, 5.02734, 0.00000, 1.49661,\n                                                    0.00000, 0.66818, 0.00000, 0.19891,\n                                                    0.00000, -0.19891, 0.00000, -0.66818,\n                                                    0.00000, -1.49661, 0.00000, -5.02734 };\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputReal_h[j] - expectedRealOutput[j]) <= EPSILON);\n        }\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputImag_h[j] - expectedImagOutput[j]) <= EPSILON);\n        }\n    }\n\n    // Test case 5.\n    {\n        // Fast Fourier Transform size (must be a power of 2)\n        int numElements = 8;\n        float inputReal_h[numElements] = { 0.0, 0.142857, 0.285714, 0.428571, 0.571429, 0.714286, 0.857143, 1.0 };\n        float inputImag_h[numElements] = { 0, 0, 0, 0, 0, 0, 0, 0 };\n\n        int logn = log2f(numElements);\n        performBitReversedOrdering(inputReal_h, inputImag_h, numElements);\n\n        // Copy input data (already bit-reversed) to the device\n        CUDA_CHECK(cudaMemcpyAsync( real_d, inputReal_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync( imag_d, inputImag_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n\n        // Grid: (GRID_SIZE, 1, 1)\n        // Block: (BLOCK_SIZE, 1, 1)\n        // Launch the FFT butterfly kernel \n        void* args[]={ &real_d, &imag_d, (void*)&numElements, &logn};\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_fftButterfly, \n                                                GRID_SIZE, \n                                                BLOCK_SIZE, \n                                                args, \n                                                0, \n                                                stream));\n\n        // Copy the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync( inputReal_h, real_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync( inputImag_h, imag_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedRealOutput[numElements] = {   4.00000, -0.57143, -0.57143, -0.57143,\n                                                    -0.57143, -0.57143, -0.57143, -0.57143 };\n        float expectedImagOutput[numElements] = {   0.00000, 1.37955, 0.57143, 0.23669,\n                                                    0.00000, -0.23669, -0.57143, -1.37955 };\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputReal_h[j] - expectedRealOutput[j]) <= EPSILON);\n        }\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputImag_h[j] - expectedImagOutput[j]) <= EPSILON);\n        }\n    }\n\n    // Test case 6.\n    {\n        // Fast Fourier Transform size (must be a power of 2)\n        int numElements = 4;\n        float inputReal_h[numElements] = { 1, 2, 3, 4 };\n        float inputImag_h[numElements] = { 0, 0, 0, 0 };\n\n        int logn = log2f(numElements);\n        performBitReversedOrdering(inputReal_h, inputImag_h, numElements);\n\n        // Copy input data (already bit-reversed) to the device\n        CUDA_CHECK(cudaMemcpyAsync( real_d, inputReal_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync( imag_d, inputImag_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n\n        // Grid: (GRID_SIZE, 1, 1)\n        // Block: (BLOCK_SIZE, 1, 1)\n        // Launch the FFT butterfly kernel \n        void* args[]={ &real_d, &imag_d, (void*)&numElements, &logn};\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_fftButterfly, \n                                                GRID_SIZE, \n                                                BLOCK_SIZE, \n                                                args, \n                                                0, \n                                                stream));\n\n        // Copy the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync( inputReal_h, real_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync( inputImag_h, imag_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedRealOutput[numElements] = { 10.00000, -2.00000, -2.00000, -2.00000 };\n        float expectedImagOutput[numElements] = { 0.00000, 2.00000, 0.00000, -2.00000 };\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputReal_h[j] - expectedRealOutput[j]) <= EPSILON);\n        }\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputImag_h[j] - expectedImagOutput[j]) <= EPSILON);\n        }\n    }\n\n    // Test case 7.\n    {\n        // Fast Fourier Transform size (must be a power of 2)\n        int numElements = 8;\n        float inputReal_h[numElements] = { 0, 0, 0, 0, 1, 1, 1, 1 };\n        float inputImag_h[numElements] = { 0, 0, 0, 0, 0, 0, 0, 0 };\n\n        int logn = log2f(numElements);\n        performBitReversedOrdering(inputReal_h, inputImag_h, numElements);\n\n        // Copy input data (already bit-reversed) to the device\n        CUDA_CHECK(cudaMemcpyAsync( real_d, inputReal_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync( imag_d, inputImag_h, numElements * sizeof(float), \n                                    cudaMemcpyHostToDevice, stream));\n\n        // Grid: (GRID_SIZE, 1, 1)\n        // Block: (BLOCK_SIZE, 1, 1)\n        // Launch the FFT butterfly kernel \n        void* args[]={ &real_d, &imag_d, (void*)&numElements, &logn};\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_fftButterfly, \n                                                GRID_SIZE, \n                                                BLOCK_SIZE, \n                                                args, \n                                                0, \n                                                stream));\n\n        // Copy the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync( inputReal_h, real_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync( inputImag_h, imag_d, numElements * sizeof(float), \n                                    cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedRealOutput[numElements] = {   4.00000, -1.00000, 0.00000, -1.00000,\n                                                    0.00000, -1.00000, 0.00000, -1.00000 };\n        float expectedImagOutput[numElements] = {   0.00000, 2.41421, 0.00000, 0.41421,\n                                                    0.00000, -0.41421, 0.00000, -2.41421 };\n\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputReal_h[j] - expectedRealOutput[j]) <= EPSILON);\n        }\n        for (int j = 0; j < numElements; j++) {\n            assert (fabs(inputImag_h[j] - expectedImagOutput[j]) <= EPSILON);\n        }\n    }\n    \n    // Free device memory.\n    CUDA_CHECK(cudaFreeAsync(real_d, stream));\n    CUDA_CHECK(cudaFreeAsync(imag_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_fftButterfly(float *real_d, float *imag_d, int numElements, int logNumElements) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/103", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute the total electromagnetic energy and net acceleration of a system of charged particles using a parallel reduction technique in shared memory, where each thread is responsible for computing the properties of a single particle. Each particle is defined by its mass, charge, three position components, and three velocity components in 3D space.\n\nThe kernel should adhere to the following signature: __global__ void k_computeElectromagneticEnergy(float* mass_d, float* charge_d, float* posX_d, float* posY_d, float* posZ_d, float* velocityX_d, float* velocityY_d, float* velocityZ_d, float* accX_d, float* accY_d, float* accZ_d, float* energy_d, unsigned int particleCount, unsigned int blockSize). Here, mass_d and charge_d are pointers to arrays containing the mass and charge of each particle, respectively; posX_d, posY_d, and posZ_d are arrays containing the x, y, and z components of each particle's position; velocityX_d, velocityY_d, and velocityZ_d are arrays containing the x, y, and z components of each particle's velocity; accX_d, accY_d, and accZ_d are arrays for storing the computed x, y, and z components of each particle's acceleration; energy_d is a pointer to an array storing the electromagnetic energy of each particle; and particleCount is the total number of particles in the system.\n\n>>> k_computeElectromagneticEnergy(mass_d:{9.377778e+00, 4.188704e+00}, charge_d:{3.666271e-07, -1.343065e-07 }, PosX_d:{7.557432e+00, 5.920792e+00}, PosY_d:{2.663286e+00, 9.004688e+00}, PosZ_d:{7.831457e+00, -9.574142e+00}, VelX_d:{4.866964e+00, 1.766176e-01}, VelY_d:{3.788892e+00, 2.608461e+00}, VelZ_d:{3.423035e+00, 8.226066e-01}, accX_d, accY_d, accZ_d, energy_d,particleCount:{2}) -> AccX:{1.201183e-08, -2.689238e-08}, AccY:{-4.654157e-08, 1.041984e-07}, AccZ:{1.277452e-07, -2.859992e-07}, energy_d: 2.490527e+02\n>>> k_computeElectromagneticEnergy(mass_d:5.075351e+00, 7.386061e+00, 4.689754e+00}, charge_d:{4.102964e-07, -1.551322e-07, -9.556600e-07}, PosX_d:{-9.194314e+00, 3.681394e+00, 9.178165e+00}, PosY_d:{ -5.453526e+00, -8.182205e+00, 9.368402e+00}, PosZ_d:{6.816517e+00, -2.430100e+00, 1.251886e+00}, VelX_d:{2.445421e+00, -1.954804e+00, 3.721204e+00}, VelY_d:{3.764471e+00, -2.396704e+00, -4.160023e+00}, VelZ_d:{1.780605e-01, 4.350655e+00, -2.173305e+00}, accX_d, accY_d, accZ_d, energy_d,particleCount:{3}) -> AccX:{-1.243298e-06, 3.900122e-07, 7.312794e-07}, AccY:{-6.476981e-07, 4.291686e-07, 2.503966e-08}, AccZ:{5.214195e-07, -7.143858e-08, -4.517800e-07}, energy_d: 2.405715e+02\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n#define BLOCK_SIZE 256\n#define MAX_ITERATIONS 100\n#define REPORT_INTERVAL 10\n\n#undef NDEBUG\n#include <assert.h>\n#include <cuda_runtime.h>\n#include <iomanip>\n#include <iostream>\n#include <random>\n\n#define CUDA_CHECK(call)                                                                           \\\n    do {                                                                                           \\\n        cudaError_t error = call;                                                                  \\\n        if(error != cudaSuccess) {                                                                 \\\n            fprintf(stderr,                                                                        \\\n                    \"CUDA Error: %s at %s:%d\\n\",                                                   \\\n                    cudaGetErrorString(error),                                                     \\\n                    __FILE__,                                                                      \\\n                    __LINE__);                                                                     \\\n            exit(error);                                                                           \\\n        }                                                                                          \\\n    } while(0)\n\n// Global variables to track validation state\nbool g_validationPassed = true;\n\n// Physical constants\nconst float KCONSTANT = 8.99e9f; // Coulomb's constant [N  m ^2 /C^2]\nconst float MU0 = 1.256e-6f;     // Magnetic permeability [N/A^2]\nconst float PI = 3.14159265359f;\nconst float TOLERANCE = 1.0e-5f;\n\n__global__ void k_computeElectromagneticEnergy(float *mass_d,              // Particle masses\n                                               float *charge_d,            // Particle charges\n                                               float *posX_d,              // X-coordinates\n                                               float *posY_d,              // Y-coordinates\n                                               float *posZ_d,              // Z-coordinates\n                                               float *velocityX_d,         // X-velocities\n                                               float *velocityY_d,         // Y-velocities\n                                               float *velocityZ_d,         // Z-velocities\n                                               float *accX_d,              // X-accelerations\n                                               float *accY_d,              // Y-accelerations\n                                               float *accZ_d,              // Z-accelerations\n                                               float *energy_d,            // Output energy\n                                               unsigned int particleCount, // Number of particles\n                                               unsigned int blockSize      // Block size\n);\n\nvoid launch() {\n    const int NUM_TEST_CASES = 7;\n    const int PARTICLES_PER_CASE = 2;\n\n    const float VALIDATION_MASSES[7][2] = {\n        {3.631876e+00f, 9.131354e+00f},\n        {7.385075e+00f, 4.135964e+00f},\n        {9.751878e+00f, 2.960669e+00f},\n        {9.148198e+00f, 6.492680e+00f},\n        {8.672558e+00f, 7.371018e+00f},\n        {7.200110e+00f, 5.005208e+00f},\n        {5.579023e+00f, 5.783645e+00f},\n    };\n\n    const float VALIDATION_CHARGES[7][2] = {\n        {-4.155725e-07f, -1.651483e-07f},\n        {1.781762e-07f, 8.628020e-07f},\n        {-9.322497e-07f, -4.253100e-07f},\n        {-9.231530e-08f, -7.772045e-07f},\n        {9.564041e-07f, -1.491385e-07f},\n        {-8.315006e-07f, -5.551410e-07f},\n        {9.200888e-07f, -6.638066e-08f},\n    };\n\n    const float VALIDATION_POSITIONS_x[7][2] = {\n        {-8.544486e+00f, -5.482904e+00f},\n        {-7.841232e+00f, 7.503757e+00f},\n        {-4.843404e+00f, -9.931945e+00f},\n        {2.309865e+00f, -9.637021e+00f},\n        {3.924267e+00f, 3.997861e+00f},\n        {-7.573712e+00f, -6.023051e+00f},\n        {4.921102e+00f, 2.508144e+00f},\n    };\n\n    const float VALIDATION_POSITIONS_y[7][2] = {\n        {5.302430e+00f, -6.205835e-01f},\n        {-3.714027e-01f, -2.555189e-01f},\n        {-2.710483e+00f, -9.535002e+00f},\n        {-2.608790e+00f, -7.187396e+00f},\n        {7.878727e+00f, 6.448950e+00f},\n        {9.182911e-01f, -1.071891e+00f},\n        {-4.088163e+00f, 5.619675e+00f},\n    };\n\n    const float VALIDATION_POSITIONS_z[7][2] = {\n        {-4.939939e+00f, 5.555338e+00f},\n        {9.691526e+00f, -2.641640e+00f},\n        {-2.385830e+00f, -2.894819e+00f},\n        {-1.670827e+00f, -6.687011e+00f},\n        {6.146811e+00f, -7.164505e+00f},\n        {9.310335e+00f, 3.165870e+00f},\n        {2.570685e+00f, -3.728041e+00f},\n    };\n\n    const float VALIDATION_VELOCITIES_x[7][2] = {\n        {-1.090332e+00f, 4.086741e+00f},\n        {-4.921796e+00f, 2.353747e+00f},\n        {3.018028e+00f, -2.314307e+00f},\n        {1.160977e+00f, -2.353666e+00f},\n        {4.175297e+00f, 3.360822e+00f},\n        {4.767069e+00f, -2.337892e+00f},\n        {-2.360863e+00f, 2.492331e+00f},\n    };\n\n    const float VALIDATION_VELOCITIES_y[7][2] = {\n        {-3.572402e-01f, 2.205109e+00f},\n        {4.475070e+00f, -2.877292e+00f},\n        {-1.946185e+00f, 4.206961e+00f},\n        {2.165246e+00f, -8.155732e-01f},\n        {2.401717e+00f, 4.081059e-02f},\n        {2.426447e+00f, 2.710342e+00f},\n        {-4.826471e+00f, 3.618288e-01f},\n    };\n\n    const float VALIDATION_VELOCITIES_z[7][2] = {\n        {-4.849432e+00f, -4.564866e+00f},\n        {3.211321e+00f, -1.556432e+00f},\n        {-9.697361e-01f, 2.183446e+00f},\n        {1.992808e+00f, -3.939872e-01f},\n        {-2.246189e-02f, 2.275865e+00f},\n        {1.266311e+00f, -3.000744e+00f},\n        {7.873082e-01f, -1.649562e+00f},\n    };\n\n    const float VALIDATION_ACCELERATIONS_x[7][2] = {\n        {2.705547e-07f, -1.076096e-07f},\n        {3.763351e-07f, -6.719747e-07f},\n        {-2.998918e-06f, 9.877862e-06f},\n        {-3.245632e-07f, 4.573102e-07f},\n        {-4.534519e-09f, 5.335204e-09f},\n        {3.049669e-06f, -4.387020e-06f},\n        {1.437635e-07f, -1.386773e-07f},\n    };\n\n    const float VALIDATION_ACCELERATIONS_y[7][2] = {\n        {-5.234220e-07f, 2.081842e-07f},\n        {2.842045e-09f, -5.074686e-09f},\n        {-4.022012e-06f, 1.324774e-05f},\n        {-1.243878e-07f, 1.752626e-07f},\n        {8.809609e-08f, -1.036517e-07f},\n        {-3.914069e-06f, 5.630480e-06f},\n        {-5.783912e-07f, 5.579280e-07f},\n    };\n\n    const float VALIDATION_ACCELERATIONS_z[7][2] = {\n        {9.274770e-07f, -3.688917e-07f},\n        {-3.024703e-07f, 5.400835e-07f},\n        {-2.999712e-07f, 9.880479e-07f},\n        {-1.362756e-07f, 1.920125e-07f},\n        {8.201802e-07f, -9.650038e-07f},\n        {-1.208425e-05f, 1.738348e-05f},\n        {3.752769e-07f, -3.619998e-07f},\n    };\n\n    const float VALIDATION_TOTAL_ENERGIES[7] = {2.386897e+02f,\n                                                2.350626e+02f,\n                                                1.086523e+02f,\n                                                6.642221e+01f,\n                                                1.613333e+02f,\n                                                1.633776e+02f,\n                                                1.084686e+02f};\n\n    // Device pointers for particle data\n    float *mass_d, *charge_d;\n    float *posX_d, *posY_d, *posZ_d;\n    float *velocityX_d, *velocityY_d, *velocityZ_d;\n    float *accX_d, *accY_d, *accZ_d;\n    float *energy_d;\n    int particleCount = PARTICLES_PER_CASE;\n\n    // Create CUDA stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate device memory asynchronously\n    CUDA_CHECK(cudaMallocAsync(&mass_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&charge_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&posX_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&posY_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&posZ_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&velocityX_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&velocityY_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&velocityZ_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&accX_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&accY_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&accZ_d, PARTICLES_PER_CASE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&energy_d, sizeof(float), stream));\n\n    // Synchronize stream to ensure allocations are complete\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n\n    // Host arrays for validation\n    float accX_h[PARTICLES_PER_CASE];\n    float accY_h[PARTICLES_PER_CASE];\n    float accZ_h[PARTICLES_PER_CASE];\n    float gpuEnergy;\n\n    // properties of the device\n    cudaDeviceProp deviceProp;\n    int currentDevice;\n    CUDA_CHECK(cudaGetDevice(&currentDevice));\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, currentDevice));\n\n    int numSMs = deviceProp.multiProcessorCount;\n    int maxBlocksPerSM = deviceProp.maxBlocksPerMultiProcessor;\n    int numBlocks = numSMs * maxBlocksPerSM;\n    unsigned int blockSize = BLOCK_SIZE;\n\n    // Run tests for each case\n    for(int testCase = 0; testCase < NUM_TEST_CASES; testCase++) {\n        // Asynchronously copy test case data to device\n        CUDA_CHECK(cudaMemcpyAsync(mass_d,\n                                   VALIDATION_MASSES[testCase],\n                                   PARTICLES_PER_CASE * sizeof(float),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(charge_d,\n                                   VALIDATION_CHARGES[testCase],\n                                   PARTICLES_PER_CASE * sizeof(float),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(posX_d,\n                                   VALIDATION_POSITIONS_x[testCase],\n                                   PARTICLES_PER_CASE * sizeof(float),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(posY_d,\n                                   VALIDATION_POSITIONS_y[testCase],\n                                   PARTICLES_PER_CASE * sizeof(float),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(posZ_d,\n                                   VALIDATION_POSITIONS_z[testCase],\n                                   PARTICLES_PER_CASE * sizeof(float),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocityX_d,\n                                   VALIDATION_VELOCITIES_x[testCase],\n                                   PARTICLES_PER_CASE * sizeof(float),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocityY_d,\n                                   VALIDATION_VELOCITIES_y[testCase],\n                                   PARTICLES_PER_CASE * sizeof(float),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocityZ_d,\n                                   VALIDATION_VELOCITIES_z[testCase],\n                                   PARTICLES_PER_CASE * sizeof(float),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n\n        // Synchronize stream to ensure copies are complete before using the data\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Reset energy\n        float zero = 0.0f;\n        CUDA_CHECK(cudaMemcpy(energy_d, &zero, sizeof(float), cudaMemcpyHostToDevice));\n\n        // Launch kernel\n        // Grid: (numBlocks, 1, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&mass_d,\n                        &charge_d,\n                        &posX_d,\n                        &posY_d,\n                        &posZ_d,\n                        &velocityX_d,\n                        &velocityY_d,\n                        &velocityZ_d,\n                        &accX_d,\n                        &accY_d,\n                        &accZ_d,\n                        &energy_d,\n                        &particleCount,\n                        &blockSize};\n        dim3 grid(numBlocks);\n        dim3 block(BLOCK_SIZE);\n        size_t sharedMemSize = BLOCK_SIZE * sizeof(float);\n\n        CUDA_CHECK(cudaLaunchKernel((void *)k_computeElectromagneticEnergy,\n                                    grid,\n                                    block,\n                                    args,\n                                    sharedMemSize,\n                                    nullptr // Default stream\n                                    ));\n\n        // Asynchronously copy results back for validation\n        CUDA_CHECK(cudaMemcpyAsync(\n            accX_h, accX_d, PARTICLES_PER_CASE * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(\n            accY_h, accY_d, PARTICLES_PER_CASE * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(\n            accZ_h, accZ_d, PARTICLES_PER_CASE * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(\n            cudaMemcpyAsync(&gpuEnergy, energy_d, sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Synchronize stream to ensure copies are complete before validation\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Check accelerations using relative values\n        for(int i = 0; i < PARTICLES_PER_CASE; i++) {\n            assert(std::abs(accX_h[i] - VALIDATION_ACCELERATIONS_x[testCase][i]) /\n                       VALIDATION_ACCELERATIONS_x[testCase][i] <=\n                   TOLERANCE);\n            assert(std::abs(accY_h[i] - VALIDATION_ACCELERATIONS_y[testCase][i]) /\n                       VALIDATION_ACCELERATIONS_y[testCase][i] <=\n                   TOLERANCE);\n            assert(std::abs(accZ_h[i] - VALIDATION_ACCELERATIONS_z[testCase][i]) /\n                       VALIDATION_ACCELERATIONS_z[testCase][i] <=\n                   TOLERANCE);\n        }\n\n        // Check energy\n        assert(std::abs(gpuEnergy - VALIDATION_TOTAL_ENERGIES[testCase]) /\n                   VALIDATION_TOTAL_ENERGIES[testCase] <=\n               TOLERANCE);\n    }\n\n    // Cleanup\n    CUDA_CHECK(cudaFreeAsync(mass_d, stream));\n    CUDA_CHECK(cudaFreeAsync(charge_d, stream));\n    CUDA_CHECK(cudaFreeAsync(posX_d, stream));\n    CUDA_CHECK(cudaFreeAsync(posY_d, stream));\n    CUDA_CHECK(cudaFreeAsync(posZ_d, stream));\n    CUDA_CHECK(cudaFreeAsync(velocityX_d, stream));\n    CUDA_CHECK(cudaFreeAsync(velocityY_d, stream));\n    CUDA_CHECK(cudaFreeAsync(velocityZ_d, stream));\n    CUDA_CHECK(cudaFreeAsync(accX_d, stream));\n    CUDA_CHECK(cudaFreeAsync(accY_d, stream));\n    CUDA_CHECK(cudaFreeAsync(accZ_d, stream));\n    CUDA_CHECK(cudaFreeAsync(energy_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_computeElectromagneticEnergy(float *mass_d,              // Particle masses\n                                               float *charge_d,            // Particle charges\n                                               float *posX_d,              // X-coordinates\n                                               float *posY_d,              // Y-coordinates\n                                               float *posZ_d,              // Z-coordinates\n                                               float *velocityX_d,         // X-velocities\n                                               float *velocityY_d,         // Y-velocities\n                                               float *velocityZ_d,         // Z-velocities\n                                               float *accX_d,              // X-accelerations\n                                               float *accY_d,              // Y-accelerations\n                                               float *accZ_d,              // Z-accelerations\n                                               float *energy_d,            // Output energy\n                                               unsigned int particleCount, // Number of particles\n                                               unsigned int blockSize      // Block size\n) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/104", "date": "2025-07-30", "prompt": "Write a CUDA kernel to simulate temperature distribution across a 2D plate where each boundary point has a fixed temperature, and the plate has a thickness of one element. The kernel should utilize device memory to reuse data and update inner temperatures based on the average of the top, bottom, left, and right temperatures in each iteration of the simulation.\n\nThe signature of the function is __global__ void k_temperatureDistribution(float *temperatureValues, int numPlateElementsX, int numPlateElementsY, int numIterations), where temperatureValues is the array to the current temperature of plate, numPlateElementsX is the number of elements along the x-axis of the plate, numPlateElementsY is the number of elements along the y-axis of the plate, and numIterations is the number of iterations to simulate the temperature distribution.\n\n>>> k_temperatureDistribution(temperatureValues: {320.0, 320.0, 320.0, 320.0, 320.0, 0.0, 0.0, 320.0, 320.0, 0.0, 0.0, 320.0, 320.0, 320.0, 320.0, 320.0}, 4, 4, 3) -> temperatureValues: {320.0, 320.0, 320.00, 320.0, 320.0, 280.0, 280.0, 320.0, 320.0, 280.0, 280.0, 320.0, 320.0, 320.0, 320.0, 320.0}\n>>> k_temperatureDistribution(temperatureValues: {360.0, 366.667, 373.333, 380.0, 386.667, 460.0, 0.0, 0.0, 0.0, 393.333, 453.333, 0.0, 0.0, 0.0, 400.0, 446.667, 0.0, 0.0, 0.0, 406.667, 440.0, 433.333, 426.667, 420.0, 413.333}, 5, 5, 2) -> temperatureValues: {360.000000, 366.666656, 373.333313, 379.999969, 386.666626, 459.999847, 258.333282, 193.333313, 241.666641, 393.333282, 453.333191, 219.999939, 103.333313, 199.999969, 399.999939, 446.666534, 274.999939, 213.333282, 258.333282, 406.666595, 439.999878, 433.333221, 426.666565, 419.999908, 413.333252}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <cstring>\n#include <cuda_runtime.h>\n#include <assert.h>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\n\n#undef NDEBUG\n // Tolerance for floating-point comparison\n#define TOLERANCE               (1e-2)\n// Number of threads per block\n#define BLOCK_SIZE              (16)\n// Number of elements allocated for device memory\n#define NUM_DEVICE_MEMORY_ELEM  (1024)\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n__global__ void k_temperatureDistribution(float *temperatureValues, int numPlateElementsX, int numPlateElementsY, int numIterations);\n\n__device__ float plateCurrentTemperatures_d[NUM_DEVICE_MEMORY_ELEM];\n__device__ float alternateBuffer_d[NUM_DEVICE_MEMORY_ELEM];\n\nvoid launch() {\n    // Number of test cases\n    const int TEST_CASE_COUNT = 9;\n    // Input number of elements of the plate in x direction\n    int numPlateElementsX[TEST_CASE_COUNT] = {2, 3, 4, 5, 6, 7, 8, 9, 10};\n    int numPlateElementsY[TEST_CASE_COUNT];\n\n    float boundaryTemperatureElementTopLeft[TEST_CASE_COUNT] =  {\n        273.0,    // test case 1\n        300.0,    // test case 2\n        320.0,    // test case 3\n        360.0,    // test case 4\n        400.0,    // test case 5\n        410.0,    // test case 6\n        440.0,    // test case 7\n        450.0,    // test case 8\n        470.0};    // test case 9\n\n    float boundaryTemperatureElementBelowTopLeft[TEST_CASE_COUNT] =  {\n        373.0,    // test case 1\n        350.0,    // test case 2\n        320.0,    // test case 3\n        460.0,    // test case 4\n        500.0,    // test case 5\n        600.0,    // test case 6\n        700.0,    // test case 7\n        800.0,    // test case 8\n        900.0};   // test case 9\n\n    int numIterations[TEST_CASE_COUNT] = {4, 5, 3, 6, 4, 3, 5, 7, 8};\n\n    // Consider a 2D square plate, so numPlateElementsY will be same as numPlateElementsX\n    std::memcpy(numPlateElementsY, numPlateElementsX, TEST_CASE_COUNT * sizeof(int));\n    int maxNumPlateElementsX = *std::max_element(numPlateElementsX, numPlateElementsX + TEST_CASE_COUNT);\n    int maxNumPlateElementsY = maxNumPlateElementsX;\n    \n    //Number of elements are greater than allocated device memory, consider increasing NUM_DEVICE_MEMORY_ELEM value\n    if(maxNumPlateElementsX * maxNumPlateElementsY > NUM_DEVICE_MEMORY_ELEM) {\n        assert(false && \"Number of elements are greater than allocated device memory, consider increasing NUM_DEVICE_MEMORY_ELEM value\");\n    }\n\n    // Expected results for each test\n    float expectedTemperatureDistribution[TEST_CASE_COUNT][maxNumPlateElementsX * maxNumPlateElementsY] =  {\n        {273.0, 306.333, 373.0, 339.667},\n        {300.0, 307.143, 314.286, 350.0, 328.571, 321.429, 342.857, 335.714, 328.571},\n        {320.0, 320.0, 320.0, 320.0, 320.0, 280.0, 280.0, 320.0, 320.0, 280.0, 280.0, 320.0, 320.0, 320.0, 320.0, 320.0},\n        {360.0, 366.667, 373.333, 380.0, 386.667, 460.0, 374.583, 346.927, 355.573, 393.333, 453.333, 376.406, 335.833, 353.594, 400.0, 446.667, 393.594, 369.74, 374.583, 406.667, 440.0, 433.333, 426.667, 420.0, 413.333},\n        {400.0, 405.263, 410.526, 415.789, 421.053, 426.316, 500.0, 350.082, 273.335, 269.243, 330.14, 431.579, 494.737, 303.063, 187.418, 180.633, 276.933, 436.842, 489.474, 307.155, 194.202, 187.418, 283.121, 442.105, 484.21, 370.025, 299.465, 293.277, 350.082, 447.368, 478.947, 473.684, 468.421, 463.158, 457.895, 452.632},\n        {410.0, 418.261, 426.522, 434.783, 443.043, 451.304, 459.565, 600, 365.937, 242.853, 213.696, 237.69, 330.312, 467.826, 591.739, 298.098, 127.283, 71.0326, 114.891, 250.598, 476.087, 583.478, 279.524, 88.0706, 31.8206, 75.6793, 234.606, 484.348, 575.217, 303.261, 139.674, 83.4239, 127.283, 264.022, 492.609, 566.956, 401.562, 290.353, 258.614, 276.929, 365.937, 500.87, 558.696, 550.435, 542.174, 533.913, 525.652, 517.391, 509.13},\n        {440.0, 449.63, 459.259, 468.889, 478.519, 488.148, 497.778, 507.407, 700.0, 462.546, 337.473, 287.512, 286.59, 326.272, 408.304, 517.037, 690.371, 420.34, 239.132, 152.297, 146.599, 211.353, 342.936, 526.667, 680.741, 392.836, 186.754, 88.6921, 81.4511, 156.115, 316.081, 536.296, 671.111, 393.757, 192.452, 95.9332, 88.6921, 163.319, 324.375, 545.926, 661.482, 431.54, 266.911, 182.936, 175.732, 239.132, 370.443, 555.556, 651.852, 516.788, 414.876, 364.267, 355.972, 387.37, 462.546, 565.185, 642.222, 632.593, 622.963, 613.333, 603.704, 594.074, 584.445, 574.815},\n        {450.0, 461.29, 472.581, 483.871, 495.161, 506.452, 517.742, 529.032, 540.323, 800.0, 536.418, 402.995, 341.383, 323.29, 340.182, 385.078, 459.613, 551.613, 788.71, 519.57, 330.103, 223.077, 189.062, 208.688, 283.178, 405.597, 562.903, 777.42, 494.961, 277.621, 153.35, 108.613, 134.055, 222.989, 377.967, 574.194, 766.129, 482.877, 264.214, 133.421, 88.6845, 113.685, 210.585, 374.229, 585.484, 754.839, 496.163, 292.01, 172.645, 128.35, 153.35, 242.206, 398.508, 596.774, 743.549, 537.487, 377.029, 277.709, 242.692, 258.491, 330.103, 450.924, 608.065, 732.258, 613.223, 516.968, 458.377, 431.938, 437.836, 471.641, 536.418, 619.355, 720.968, 709.678, 698.387, 687.097, 675.807, 664.516, 653.226, 641.936, 630.645},\n        {470.0, 482.286, 494.571, 506.857, 519.143, 531.429, 543.714, 556.0, 568.286, 580.571, 900.0, 598.148, 450.604, 377.099, 350.257, 352.48, 377.998, 428.708, 502.461, 592.857, 887.714, 597.057, 387.671, 266.379, 212.1, 208.003, 245.626, 325.772, 451.425, 605.143, 875.428, 572.863, 337.611, 190.334, 123.72, 116.53, 160.923, 262.755, 420.587, 617.428, 863.143, 556.172, 311.15, 156.719, 84.6422, 76.7293, 125.475, 236.409, 411.804, 629.714, 850.857, 553.949, 315.248, 163.909, 92.5551, 84.6422, 133.571, 245.107, 422.125, 642, 838.571, 571.964, 358.365, 219.745, 154.964, 146.869, 190.334, 293.043, 453.394, 654.286, 826.285, 618.953, 449.57, 341.235, 286.841, 278.143, 310.948, 387.671, 512.438, 666.571, 814.0, 693.835, 596.235, 529.376, 494.625, 484.304, 496.569, 535.223, 598.148, 678.857, 801.714, 789.428, 777.143, 764.857, 752.571, 740.286, 728, 715.714, 703.428, 691.143}\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Declare host and device pointers\n    float *plateCurrentTemperatures_h, *deviceArray_d, *alternateDataArray_d;\n    plateCurrentTemperatures_h = (float*) malloc(maxNumPlateElementsX * maxNumPlateElementsY * sizeof(float));\n    \n    // Get pointers to the global __device__ array\n    cudaGetSymbolAddress((void**) &deviceArray_d, plateCurrentTemperatures_d);\n    cudaGetSymbolAddress((void**) &alternateDataArray_d, alternateBuffer_d);\n\n    // Loop to execute each test case\n    for (int testCaseId = 0; testCaseId < TEST_CASE_COUNT; testCaseId++) {\n\n        // Initialize inner plate temperatures to zero kelvin\n        for (int y = 1; y < numPlateElementsY[testCaseId] - 1; y++) {\n            memset(&plateCurrentTemperatures_h[y * numPlateElementsX[testCaseId] + 1], 0, (numPlateElementsX[testCaseId] - 2) * sizeof(float));\n        }\n\n        float baseGradient = 1.0f;\n        int numberOfEdges = 4;\n        int boundaryAdjustment = 5;\n        float temperatureGradient = baseGradient / (numberOfEdges * numPlateElementsX[testCaseId] - boundaryAdjustment);\n        float temperatureChange = (boundaryTemperatureElementBelowTopLeft[testCaseId] - boundaryTemperatureElementTopLeft[testCaseId]) * temperatureGradient;\n        float boundaryTemperature = boundaryTemperatureElementTopLeft[testCaseId];\n\n        // Initialize the boundary temperatures by constantly changing the temperature from the\n        // top-left corner in clockwise direction along the boundary till the element below top-left element\n        for (int j = 0; j < numPlateElementsX[testCaseId] - 1; j++) {\n            plateCurrentTemperatures_h[j] = boundaryTemperature;\n            boundaryTemperature += temperatureChange;\n        }\n\n        for (int j = 0; j < numPlateElementsY[testCaseId] - 1; j++) {\n            plateCurrentTemperatures_h[(j + 1) * numPlateElementsX[testCaseId] - 1] = boundaryTemperature;\n            boundaryTemperature += temperatureChange;\n        }\n\n        for (int j = numPlateElementsX[testCaseId] - 1; j >= 1; j--) {\n            plateCurrentTemperatures_h[(numPlateElementsY[testCaseId] - 1) * (numPlateElementsX[testCaseId]) + j] = boundaryTemperature;\n            boundaryTemperature += temperatureChange;\n        }\n\n        for (int j = numPlateElementsY[testCaseId] - 1; j >= 1; j--) {\n            plateCurrentTemperatures_h[j * numPlateElementsX[testCaseId]] = boundaryTemperature;\n            boundaryTemperature += temperatureChange;\n        }\n\n        // Copying data into device memory\n        CUDA_CHECK(cudaMemcpyAsync(deviceArray_d, plateCurrentTemperatures_h, numPlateElementsX[testCaseId] * numPlateElementsY[testCaseId] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(alternateDataArray_d, plateCurrentTemperatures_h, numPlateElementsX[testCaseId] * numPlateElementsY[testCaseId] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Determine the number of threads and blocks\n        dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE, 1);\n        dim3 gridSize((numPlateElementsX[testCaseId] + BLOCK_SIZE - 1) / BLOCK_SIZE, (numPlateElementsY[testCaseId] + BLOCK_SIZE - 1) / BLOCK_SIZE);\n\n        // Launch the kernel\n        // Grid: ((numPlateElementsX[testCaseId] + BLOCK_SIZE - 1) / BLOCK_SIZE, (numPlateElementsY[testCaseId] + BLOCK_SIZE - 1) / BLOCK_SIZE)\n        // Block: (BLOCK_SIZE, BLOCK_SIZE, 1)\n        void *args[] = {&deviceArray_d, (void*) &numPlateElementsX[testCaseId], (void*) &numPlateElementsY[testCaseId], (void*) &numIterations[testCaseId]};\n        CUDA_CHECK(cudaLaunchCooperativeKernel((void*)k_temperatureDistribution, gridSize, blockSize, args, 0, stream));\n\n        // Copy the output array plateUpdatedTemperatures_d from the device (GPU) to the host (CPU)\n        CUDA_CHECK(cudaMemcpyAsync(plateCurrentTemperatures_h, deviceArray_d, numPlateElementsX[testCaseId] * numPlateElementsY[testCaseId] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify whether the calculated plateCurrentTemperatures_h (computed by GPU) matches the expected result or not\n        for (int i = 0; i < numPlateElementsX[testCaseId] * numPlateElementsY[testCaseId]; i++) {\n            assert(fabs(plateCurrentTemperatures_h[i] - expectedTemperatureDistribution[testCaseId][i]) < TOLERANCE);\n        }\n    }\n\n    // Free host memories\n    free(plateCurrentTemperatures_h);\n\n    // Free stream\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_temperatureDistribution(float *temperatureValues, int numPlateElementsX, int numPlateElementsY, int numIterations) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/105", "date": "2025-07-30", "prompt": "Write a CUDA kernel to upscale a 2D grayscale image from specified input dimensions to target output dimensions using bilinear interpolation by leveraging shared memory to cache input data tiles that are reused across multiple threads within a block.\n\nThe signature of the function is __global__ void k_bilinearInterpolation(float *inputMat, int inputWidth, int inputHeight, float *outputMat, int outputWidth, int outputHeight), where inputMat_h is the pointer to the input image array, inputWidth and inputHeight are dimensions (width & height) of the input image, respectively, outputMat is the pointer to the output array, and outputWidth & outputHeight are dimensions (width & height) of interpolated output image array.\n\n>>> k_bilinearInterpolation({{0.0, 0.1, 0.2, 0.3, 0.4}, {0.5, 0.6, 0.7, 0.8, 0.9}, {0.10, 0.11, 0.12, 0.13, 0.14}, {0.15,0.16,0.17,0.18,0.19},{0.20,0.21,0.22,0.23,0.24}}, 5, 5, outputMat, 8, 8) -> outputMat:{{0.00, 0.06, 0.11, 0.17, 0.23, 0.29, 0.34, 0.40}, {0.29, 0.33, 0.44, 0.43, 0.54, 0.53, 0.64, 0.69}, {0.44, 0.49, 0.54, 0.59, 0.64, 0.69, 0.74, 0.79}, {0.21, 0.23, 0.26, 0.27, 0.30, 0.31, 0.34, 0.36}, {0.11, 0.12, 0.13, 0.13, 0.14, 0.14, 0.15, 0.15}, {0.14, 0.15, 0.16, 0.16, 0.17, 0.17, 0.18, 0.18}, {0.17, 0.18, 0.19, 0.19, 0.20, 0.20, 0.21, 0.21}, {0.20, 0.21, 0.21, 0.22, 0.22, 0.23, 0.23, 0.24}}\n>>> k_bilinearInterpolation({{0.0, 0.1, 0.2}, {0.3, 0.4, 0.5}, {0.6, 0.7, 0.8}, {0.9, 0.10, 0.11}, {0.12, 0.13, 0.14}}, 3, 5, outputMat, 7, 11) -> outputMat: {{0.00, 0.03, 0.07, 0.10, 0.13, 0.17, 0.20}, {0.12, 0.17, 0.17, 0.22, 0.27, 0.27, 0.32}, {0.24, 0.30, 0.28, 0.34, 0.40, 0.38, 0.44}, {0.36, 0.40, 0.42, 0.46, 0.50, 0.52, 0.56}, {0.48, 0.53, 0.53, 0.58, 0.63, 0.63, 0.68}, {0.60, 0.63, 0.67, 0.70, 0.73, 0.77, 0.80}, {0.72, 0.53, 0.65, 0.46, 0.48, 0.50, 0.52}, {0.84, 0.42, 0.64, 0.22, 0.23, 0.24, 0.25}, {0.74, 0.53, 0.32, 0.11, 0.11, 0.11, 0.12}, {0.43, 0.33, 0.22, 0.12, 0.12, 0.12, 0.13}, {0.12, 0.12, 0.13, 0.13, 0.13, 0.14, 0.14}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime_api.h>\n#include <float.h>\n#include <math.h>\n#include <cstdio>\n#include <time.h>\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n#define BLOCK_SIZE 16\n\n#undef NDEBUG\n#include <assert.h>\n\n__global__ void k_bilinearInterpolation(float *inputMat, int inputWidth, int inputHeight, float *outputMat, int outputWidth, int outputHeight);\n\nvoid launch() {\n    // Testcases count\n    int testcases = 10;\n    \n    float threshold = 0.5f;\n\n    // Input and output sizes\n    int inputSizeArray[2][testcases] =  { {5, 3, 31, 18, 6,  30, 20, 26, 40, 28 }, { 5, 5, 14, 28, 10, 33, 16, 18, 38, 12 }};\n    int outputSizeArray[2][testcases] = { {8, 7, 33, 44, 16, 32, 48, 43, 53, 33 }, { 8, 11, 37, 37, 29, 38, 29, 21, 50, 14 }};\n\n    float tcase_1[25] = { 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24 };\n    float tcase_2[15] = { 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.10, 0.11, 0.12, 0.13, 0.14 };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocating memory for the largest dataset\n    int maxWidth = 100; int maxHeight = 100;\n    float* inputMat_d; float* outputMat_d;\n    CUDA_CHECK(cudaMallocAsync(&inputMat_d, maxWidth * maxHeight * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&outputMat_d, maxWidth * maxHeight * sizeof(float), stream));\n\t\n\t// Allocate memory for input and output\n    float* outputMat_h = (float*)malloc(maxWidth * maxHeight * sizeof(float));\n    float* inputMat_h= (float*)malloc(maxWidth * maxHeight * sizeof(float));\n    float* outputMatExpected = (float*)malloc(maxWidth * maxHeight * sizeof(float));\n\n    // Running testcases\n    for (int tcase = 0; tcase < testcases; tcase++) {\n        // Settings input and output dimensions\n        int inputWidth = inputSizeArray[0][tcase];\n        int inputHeight = inputSizeArray[1][tcase];\n        int outputWidth = outputSizeArray[0][tcase];\n        int outputHeight = outputSizeArray[1][tcase];\n\n        if ((inputWidth > outputWidth) || (inputHeight > outputHeight)) {\n            assert(false && \"Output dimensions should be greater than input dimensions.\");\n        }\n\n        // Generate random inputs in the range [-10,10]\n        // Initializing random number state with present time stamp.\n        srand(time(NULL));\n        for (int y = 0; y < inputHeight * inputWidth; y++) {\n            if (tcase == 0) {\n                inputMat_h[y] = tcase_1[y];\n            } else if (tcase == 1) {\n                inputMat_h[y] = tcase_2[y];\n            } else {\n                inputMat_h[y] = (float)(rand() % 100) / 100.f;\n            }            \n        }\n\n        // Calling Bilinear interpolation on CPU\n        float xRatio = (inputWidth - 1) / (float)(outputWidth - 1);\n        float yRatio = (inputHeight - 1) / (float)(outputHeight - 1);\n        for (int y = 0; y < outputHeight; y++) {\n            for (int x = 0; x < outputWidth; x++) {\n                float dx = x * xRatio;\n                float dy = y * yRatio;\n\n                int dx_l = floorf(dx); int dx_h = ceilf(dx);\n                int dy_l = floorf(dy); int dy_h = ceilf(dy);\n\n                float p00 = inputMat_h[dy_l * inputWidth + dx_l];\n                float p01 = inputMat_h[dy_l * inputWidth + dx_h];\n                float p10 = inputMat_h[dy_h * inputWidth + dx_l];\n                float p11 = inputMat_h[dy_h * inputWidth + dx_h];\n\n                float tx = dx - dx_l;\n                float ty = dy - dy_l;\n                float tmpX1 = ((1 - tx) * p00) + (tx * p01);\n                float tmpX2 = ((1 - tx) * p11) + (tx * p10);\n                float outVal = (1 - ty) * tmpX1 + ty * tmpX2;\n\n                // Clip the outputs to the interval [0.0,1.0]\n                outVal = (outVal > 1.0) ? 1.0 : (outVal < 0.0) ? 0.0 : outVal;\n                outputMatExpected[y * outputWidth + x] = outVal;\n            }\n        }\n\n        // Calling Bilinear interpolation on GPU\n        // CUDA Initialization\n        size_t shMemX = ceilf(BLOCK_SIZE * xRatio) + 2;\n        size_t shMemY = ceilf(BLOCK_SIZE * yRatio) + 2;\n        size_t totalShMemBytes = shMemX * shMemY * sizeof(float);\n\n        dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE, 1);\n        size_t blockSizeX = outputWidth / BLOCK_SIZE + 1;\n        size_t blockSizeY = outputHeight / BLOCK_SIZE + 1;\n        dim3 gridDim(blockSizeX, blockSizeY, 1);\n\n        // Using pre-allocated memory to copy input to GPU memory\n        CUDA_CHECK(cudaMemcpyAsync(inputMat_d, inputMat_h, inputWidth * inputHeight * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // CUDA kernel Launch\n        void* args[] = { &inputMat_d, (void*)&inputWidth, (void*)&inputHeight, &outputMat_d, (void*)&outputWidth, (void*)&outputHeight };\n        CUDA_CHECK(cudaLaunchKernel((void*)k_bilinearInterpolation, gridDim, blockDim, args, totalShMemBytes, stream));\n        CUDA_CHECK(cudaMemcpyAsync(outputMat_h, outputMat_d, outputWidth * outputHeight * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Verification\n        for (int y = 0; y < outputHeight; y++) {\n            for (int x = 0; x < outputWidth; x++) {\n                assert(fabsf(outputMat_h[y * outputWidth + x] - outputMatExpected[y * outputWidth + x]) < threshold);\n            }\n        }\n    }\n    \n\t// Free allocated memory\n    free(inputMat_h);\n    free(outputMatExpected);\n    free(outputMat_h);\n\n    CUDA_CHECK(cudaFreeAsync(inputMat_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputMat_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_bilinearInterpolation(float *inputMat, int inputWidth, int inputHeight, float *outputMat, int outputWidth, int outputHeight) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/106", "date": "2025-07-30", "prompt": "Write a CUDA kernel to separate odd and even indexed elements of an array into different arrays using shared memory for coalesced global memory access while avoiding warp-divergence.\n\nThe signature of the CUDA kernel is __global__ void k_separateOddEven(int *input_d, int *oddData_d, int *evenData_d, int numElements), where input_d is the input vector containing all the input values, oddData_d is output vector for odd-indexed elements in the input_d vector, and evenData_d is output vector for even-indexed elements in the input_d vector.\n\n>>> k_separateOddEven({41, 18467, 6334, 26500, 19169, 15724, 11478, 29358, 26962, 24464}, oddData_d, evenData_d, 10) -> oddData_d: {18467, 26500, 15724, 29358, 24464 }, evenData_d:{ 41, 6334, 19169, 11478, 26962}\n>>> k_separateOddEven({5705, 28145, 23281, 16827, 9961, 491, 2995, 11942, 4827, 5436}, oddData_d, evenData_d, 10) -> oddData_d: {28145, 16827, 491, 11942, 5436 }, evenData_d:{ 5705, 23281, 9961, 2995, 4827}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call){                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess){                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\nconstexpr int NUM_BLOCKS_PER_GRID = 256;\n\n// The number of threads per block is a multiple of 64 for the algorithm to function efficiently.\nconstexpr int NUM_THREADS_PER_BLOCK = 256;\nconstexpr int NUM_ELEMENTS = 10;\nconstexpr int NUM_OUTPUT_ELEMENTS = (NUM_ELEMENTS & 1) ? (NUM_ELEMENTS / 2 + 1) : (NUM_ELEMENTS / 2);\n\n__global__ void k_separateOddEven(int *input_d, int *oddData_d, int *evenData_d, int numElements);\n\nint launch() {\n    // Host buffers for computations.\n    int input_h[NUM_ELEMENTS];\n    int oddData_h[NUM_OUTPUT_ELEMENTS];\n    int evenData_h[NUM_OUTPUT_ELEMENTS];\n    int oddDataExpected_h[NUM_OUTPUT_ELEMENTS];\n    int evenDataExpected_h[NUM_OUTPUT_ELEMENTS];\n\n    int *input_d;\n    int *oddData_d;\n    int *evenData_d;\n\n    cudaStream_t stream;\n\n    // Allocating resources.\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    CUDA_CHECK(cudaMallocAsync(&input_d, sizeof(int) * NUM_ELEMENTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&oddData_d, sizeof(int) * NUM_OUTPUT_ELEMENTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&evenData_d, sizeof(int) * NUM_OUTPUT_ELEMENTS, stream));\n    \n    dim3 gridDim(NUM_BLOCKS_PER_GRID, 1, 1);\n    dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n    void *args[4] = { &input_d, &oddData_d, &evenData_d, (void*)&NUM_ELEMENTS };\n    const int numTests = 7;\n    srand(1);\n\n    for(int test = 0; test < numTests; test++) {\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            input_h[i] = rand();\n\n            if(i & 1) {\n                oddDataExpected_h[i / 2] = input_h[i];\n            } else {\n                evenDataExpected_h[i / 2] = input_h[i];\n            }\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( input_d, \n                                    input_h, \n                                    sizeof(int) * NUM_ELEMENTS, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        // Block: (256, 1, 1)\n        // Grid: (256, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_separateOddEven, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync( oddData_h, \n                                    oddData_d, \n                                    sizeof(int) * NUM_OUTPUT_ELEMENTS, \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( evenData_h, \n                                    evenData_d, \n                                    sizeof(int) * NUM_OUTPUT_ELEMENTS, \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_OUTPUT_ELEMENTS; i++) {\n            // Number of odd-indexed elements is 1 less than number of even-indexed elements if input size is odd.\n            if((NUM_ELEMENTS & 1) && (i < NUM_OUTPUT_ELEMENTS - 1) || !(NUM_ELEMENTS & 1)){\n                assert(oddDataExpected_h[i] == oddData_h[i]);\n            }\n\n            assert(evenDataExpected_h[i] == evenData_h[i]);\n        }\n    }\n\n    // Releasing resources.\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(oddData_d, stream));\n    CUDA_CHECK(cudaFreeAsync(evenData_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n\n    return 0;\n}\n\n__global__ void k_separateOddEven(int *input_d, int *oddData_d, int *evenData_d, int numElements) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/107", "date": "2025-07-30", "prompt": "Implement a CUDA kernel to generate chaotic results from input elements that are initial conditions, utilizing the logistic map in a data-parallel way.\n\nThe signature of the CUDA kernel is __global__ void k_calculateLogisticMap(float *input_d, float *output_d, int iterations, float growthRate, int numElements), where input_d is a pointer to the array with the input values, output_d is the pointer to store the output array, iterations is the number of times that the logistic map equation is applied, growthRate is the constant to control the dynamics of the map, and numElements is the total number of elements to process them parallel.\n\n>>> k_calculateLogisticMap({0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09}, output_d, 10000, 3.9, 10) -> output_d: {0.0, 0.908595, 0.923597, 0.816154, 0.513786, 0.934181, 0.275205, 0.335843, 0.504806, 0.471532}\n>>> k_calculateLogisticMap({0.0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009}, output_d, 10000, 3.9, 10) -> output_d: {0.0, 0.551224, 0.884021, 0.829637, 0.967275, 0.764777, 0.410691, 0.685051, 0.666282, 0.191851}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call){                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess){                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// CUDA-related constants.\nconstexpr int NUM_BLOCKS_PER_GRID = 256;\nconstexpr int NUM_THREADS_PER_BLOCK = 256;\n\n// Error tolerance for comparing floating-point variables.\nconstexpr float EPSILON = 0.00001f;\n\n// Algorithm-related constants.\nconstexpr float GROWTH_RATE = 3.9f;\nconstexpr int NUM_ITERATIONS = 10000;\n\n__global__ void k_calculateLogisticMap(float *input_d, float *output_d, int iterations, float growthRate, int numElements);\n\nvoid launch() {\n    constexpr int NUM_ELEMENTS = 10;\n    float *input_d;\n    float *output_d;\n    float input_h[NUM_ELEMENTS];\n    float output_h[NUM_ELEMENTS];\n    float expectedOutput_h[NUM_ELEMENTS];\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    CUDA_CHECK(cudaMallocAsync(&input_d, NUM_ELEMENTS * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, NUM_ELEMENTS * sizeof(float), stream));\n\n    void *args[5] = { &input_d, &output_d, (void*)&NUM_ITERATIONS, (void*)&GROWTH_RATE, (void*)&NUM_ELEMENTS };\n    dim3 gridDim(NUM_BLOCKS_PER_GRID, 1, 1);\n    dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n\n    // Test 1: Distant values.\n    {\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            input_h[i] = 0.5f * i / (float) NUM_ELEMENTS;\n            float x = input_h[i];\n            \n            for(int j = 0; j < NUM_ITERATIONS; j++) {\n                x = GROWTH_RATE * x * (1.0f - x);\n            }\n\n            expectedOutput_h[i] = x;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( input_d, \n                                    input_h, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        // Block: (256, 1, 1)\n        // Grid: (256, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLogisticMap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync( output_h, \n                                    output_d, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            assert(fabsf(output_h[i] - expectedOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 2: Closer values.\n    {\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            input_h[i] = 0.1f * i / (float) NUM_ELEMENTS;\n            float x = input_h[i];\n            \n            for(int j = 0; j < NUM_ITERATIONS; j++) {\n                x = GROWTH_RATE * x * (1.0f - x);\n            }\n\n            expectedOutput_h[i] = x;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( input_d, \n                                    input_h, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        // Block: (256, 1, 1)\n        // Grid: (256, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLogisticMap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync( output_h, \n                                    output_d, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            assert(fabsf(output_h[i] - expectedOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 3: Very close values.\n    {\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            input_h[i] = 0.01f * i / (float) NUM_ELEMENTS;\n            float x = input_h[i];\n            \n            for(int j = 0; j < NUM_ITERATIONS; j++) {\n                x = GROWTH_RATE * x * (1.0f - x);\n            }\n\n            expectedOutput_h[i] = x;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( input_d, \n                                    input_h, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        // Block: (256, 1, 1)\n        // Grid: (256, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLogisticMap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync( output_h, \n                                    output_d, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            assert(fabsf(output_h[i] - expectedOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 4: Piecewise values.\n    {\n        input_h[0] = 0.010f;\n        input_h[1] = 0.015f;\n        input_h[2] = 0.030f;\n        input_h[3] = 0.035f;\n        input_h[4] = 0.050f;\n        input_h[5] = 0.055f;\n        input_h[6] = 0.070f;\n        input_h[7] = 0.075f;\n        input_h[8] = 0.090f;\n        input_h[9] = 0.095f;\n\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            float x = input_h[i];\n\n            for(int j = 0; j < NUM_ITERATIONS; j++) {\n                x = GROWTH_RATE * x * (1.0f - x);\n            }\n            \n            expectedOutput_h[i] = x;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( input_d, \n                                    input_h, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        // Block: (256, 1, 1)\n        // Grid: (256, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLogisticMap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync( output_h, \n                                    output_d, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            assert(fabsf(output_h[i] - expectedOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 5: Very close values in a different region.\n    {\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            input_h[i] = 0.5f + 0.01f * i / (float) NUM_ELEMENTS;\n            float x = input_h[i];\n\n            for(int j = 0; j < NUM_ITERATIONS; j++) {\n                x = GROWTH_RATE * x * (1.0f - x);\n            }\n\n            expectedOutput_h[i] = x;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( input_d, \n                                    input_h, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        // Block: (256, 1, 1)\n        // Grid: (256, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLogisticMap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync( output_h, \n                                    output_d, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            assert(fabsf(output_h[i] - expectedOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 6: Random values.\n    {\n        srand(1);\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            input_h[i] = 0.5f * (rand() / (float)RAND_MAX);\n            float x = input_h[i];\n            \n            for(int j = 0; j < NUM_ITERATIONS; j++) {\n                x = GROWTH_RATE * x * (1.0f - x);\n            }\n\n            expectedOutput_h[i] = x;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( input_d, \n                                    input_h, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        // Block: (256, 1, 1)\n        // Grid: (256, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLogisticMap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync( output_h, \n                                    output_d, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            assert(fabsf(output_h[i] - expectedOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 7: Exponentially changing values.\n    {\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            input_h[i] = 1.0f / (1.0f + abs(i * i));\n            float x = input_h[i];\n            \n            for(int j = 0; j < NUM_ITERATIONS; j++) {\n                x = GROWTH_RATE * x * (1.0f - x);\n            }\n\n            expectedOutput_h[i] = x;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( input_d, \n                                    input_h, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        // Block: (256, 1, 1)\n        // Grid: (256, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLogisticMap, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync( output_h, \n                                    output_d, \n                                    NUM_ELEMENTS * sizeof(float), \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_ELEMENTS; i++) {\n            assert(fabsf(output_h[i] - expectedOutput_h[i]) < EPSILON);\n        }\n    }\n\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_calculateLogisticMap(float *input_d, float *output_d, int iterations, float growthRate, int numElements) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/108", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute the nearest neighbors of every point in an array-A w.r.t array-B in N-Dimension, where N=3 for 3-Dimension, using shared memory to load array-B and compare it to every point in array-A to find the nearest neighbors.\n\nThe signature of the function is __global__ void k_nearestNeighbors(const float* inputVectorA_d, int nA, float* inputVectorB_d, int nB, int* nearestNeighborIndex), where for every 3D point in inputVectorA_d a nearest neighbour from inputVectorB_d is computed. The sizes of the inputs are given by nA and nB, and the number of nearest neighbors to be calculated is given by nearestNeighborIndex.\n\n>>> k_nearestNeighbors({{-0.38,-2.13,0.69},{-5.7,-4.5,-0.95},{9.23,7.69,9.65}}, 3, {{-9.57,-0.59,8.41},{-6.53,6.68,8.02},{5.11,-9.24,-1.19}},3, nearestNeighborIndex) -> nearestNeighborIndex : {2,0,1}\n>>> k_nearestNeighbors({{-3.44,-0.71,-5.57},{-1.28,-8.79,-5.71},{7.92,2.8,-8.56},{2.07,5.06,-8.3},{-2.45,-9.14,8.19}}, 5,\n    {{9.26,8.7,-4.33},{-8.07,0.91,7.17},{-5.81,-3.63,-7.7},{-8.4,2.07,-8.9}}, 4, nearestNeighborIndex) -> nearestNeighborIndex : {2,2,0,0,1}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <limits.h>\n#include <stdlib.h>\n#include <float.h>\n#include <cstdio>\n\n#define N_DIMS 3\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n#undef NDEBUG\n#include <assert.h>\n\n__global__ void k_nearestNeighbors(const float* inputVectorA_d, int nA, float* inputVectorB_d, int nB, int* nearestNeighborIndex);\n\nvoid launch() {\n    const int TEST_CASES = 9;\n    const int NUMEL_A = 10;\n    const int NUMEL_B = 10;\n    \n    // Variable allocations\n    int testCaseCount = TEST_CASES; // Number of test cases\n    int numberOfPointsA = NUMEL_A;\n    int numberOfPointsB = NUMEL_B;\n    int outputIndices[TEST_CASES][NUMEL_A] = { { 4,4,9,4,0,2,7,7,2,7 },{6,2,5,7,2,5,2,7,3,3},{9,4,9,1,4,1,9,7,2,4},\n        {0,6,2,1,7,2,9,9,2,2},{8,1,5,3,1,4,9,5,3,1},{0,2,4,4,3,0,7,5,6,4},{8,6,5,6,6,2,2,9,4,5},{0,5,3,6,0,4,7,6,1,4},{3,1,9,8,5,4,7,0,6,0} };\n\n    // Test-cases\n    float inputVectorA_h[TEST_CASES][NUMEL_A][N_DIMS] = {\n        { {-0.38,-2.13,0.69},{-5.7,-4.5,-0.95},{9.23,7.69,9.65},{-6.54,2.21,4.76},{-7.71,-7.99,8.89},{7.93,-6.5,0.11},{7.06,6.76,-8.3},{4.66,9.78,-9.06},{2.19,-5.55,-2.43},{2.74,-0.14,-7.55}},\n        { {7.84,7.03,-9.84},{-9.44,-6.52,6.45},{2.12,0.52,-4.83},{-7.45,2.44,0.11},{-5.57,-9.12,-4.06},{2.29,1.76,-5.07},{-0.76,-8.89,-5.96},{-5.9,-0.69,-4.59},{9.59,1.21,0.22},{2.45,0.02,-2.54}},\n        { {-0.58,2.59,-0.67},{-9.12,-6.22,4.25},{-8.42,7.6,2.66},{6.51,6.53,-7.43},{-7.46,-9.41,4.89},{3.84,5.8,0.96},{-5.66,7.97,-0.96},{6.59,-6.21,3.64},{6.01,-0.08,-1.41},{-2.13,-5.11,4.1}},\n        { {-9.23,-2.91,3.4},{-0.08,6.44,-4.75},{8.55,-0.77,4.66},{-9.89,-8.74,-7.01},{-7.96,0.95,7.43},{0.04,-0.93,2.77},{9.04,-2.37,-4.64},{6.26,-8.58,-9.26},{8.79,1.59,-0.54},{2.47,3.21,-2.82}},\n        { {-2.4,4.12,2.49 },{-4.5,-3.22,9.07},{8.23,3.75,6.46},{-1.19,5.46,-7.21},{-7.16,-3.16,-0.67},{-9.4,1.88,6.72 },{2.39,1.27,-10.0},{9.55,1.42,2.3},{-3.57,3.,-5.04},{-5.01,-4.98,0.11}},\n        { {9.87,7.55,-8.48},{-3.5,-6.02,-7.57},{3.4,5.82,4.63},{-3.2,-0.57,5.22},{-1.63,7.42,-7.59},{9.4,-2.44,-9.08},{-6.64,-7.42,3.47},{9.35,-2.07,7.19},{8.58,-9.92,-3.92},{2.,0.9,4.47}},\n        { {-1.31,8.34,2.35},{-8.78,-9.22,7.58},{7.15,-1.11,-6.23},{8.73,-9.34,4.52},{-0.18,-9.88,8.38},{-4.89,-9.52,-2.42},{6.98,-9.17,2.01},{3.5,9.11,2.65},{-6.53,1.64,-2.79},{2.38,-1.8,-6.29}},\n        { {-8.68,8.26,4.94},{7.49,-3.7,6.07},{-7.26,-0.92,1.11},{2.04,-4.78,2.27},{-0.77,8.68,0.12},{3.,0.83,-9.32},{-1.75,8.48,-6.31},{3.34,-7.74,-5.14},{1.82,-0.09,6.36},{9.18,-5.11,-8.6}},\n        { {-8.71,-4.18,3.85},{-9.03,-6.25,5.53},{-8.79,5.75,1.31},{-1.76,-1.4,-3.47},{-8.87,-3.7,-5.4},{0.95,9.43,5.59},{-7.39,-7.35,-8.36},{-0.81,5.47,-7.43},{-3.73,-4.33,-6.09},{-6.04,4.23,-8.14}}\n    };\n\n    float inputVectorB_h[TEST_CASES][NUMEL_B][N_DIMS] = {\n        { {-9.57,-0.59,8.41},{-6.53,6.68,8.02},{5.11,-9.24,-1.19},{-3.61,0.16,9.63},{-2.25,-1.03,5.41},{-6.86,7.33,9.88},{3.78,-8.85,8.71},{2.06,5.51,-6.19},{-8.9,-7.37,-8.82},{5.0,6.25,-2.29} },\n        { {-4.2,7.89,4.},{5.15,9.23,-9.98},{-6.59,-7.03,2.68},{4.27,2.22,2.26},{3.8,-8.31,6.78},{3.02,3.67,-9.28},{5.66,8.19,-8.3},{-6.92,5.01,-1.67},{-0.43,-3.39,3.41},{0.65,7.78,-0.58} },\n        { {8.49,-9.61,2.52},{6.22,9.2,-5.7},{3.58,-2.73,-8.28},{1.68,-9.99,-7.69},{-5.01,-3.29,6.66},{-1.97,-0.42,8.51},{-8.29,5.32,8.94},{5.01,-7.73,5.46},{8.08,9.91,-4.69},{-7.84,6.52,0.11} },\n        { {-3.65,9.13,0.49},{5.2,-9.82,-1.8},{5.15,3.55,-1.13},{9.52,8.68,-7.16},{7.89,8.92,-9.82},{5.88,7.32,-4.42},{0.16,9.05,-7.79},{-9.61,9.82,9.6},{3.9,5.6,6.54},{9.47,-2.26,-9.42} },\n        { {5.16,-9.4,7.01},{-6.74,-5.58,6.86},{-7.52,7.99,-4.3},{-2.87,0.84,-8.52},{-8.65,4.42,4.61},{2.04,-6.77,6.32},{0.66,-6.78,8.48},{1.47,-4.78,-6.51},{-5.37,6.01,-0.72},{1.88,-0.22,-6.58}},\n        { {9.26,8.7,-4.33},{-8.07,0.91,7.17},{-5.81,-3.63,-7.7},{-8.4,2.07,-8.9},{-0.73,0.83,4.67},{7.6,-4.02,6.84},{2.74,-7.79,-0.22},{-5.51,-8.62,-2.62},{4.26,-6.13,5.19},{8.43,6.77,-0.31}},\n        { {-2.45,3.65,3.51},{7.96,3.24,5.56},{1.05,-7.81,0.05},{7.89,-0.54,4.28},{-5.26,5.28,-8.05},{9.05,-5.79,-5.28},{0.76,-8.21,5.34},{-3.54,8.98,7.75},{-3.52,9.01,4.49},{7.9,3.74,1.18}},\n        { {-2.56,9.32,5.24},{-1.44,-2.26,9.79},{-3.15,-0.66,-5.08},{-9.61,-3.97,-0.63},{-0.69,1.23,-8.07},{2.89,-3.38,1.94},{1.59,-4.22,1.52},{-5.25,2.93,-4.45},{-7.22,8.42,-1.66},{-7.79,5.01,-3.55}},\n        { {-4.58,8.49,-9.35},{-8.96,-4.87,7.35},{4.99,-3.15,1.41},{-8.95,-5.62,2.89},{4.3,-0.67,5.26},{-9.95,-3.37,-5.77},{-4.,-3.29,-7.81},{-5.18,-3.62,-8.81},{-4.6,-4.07,-2.66},{-8.51,0.89,0.34}}\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    const int BLOCK_SIZE = 32; // number of threads per block\n\n    //Declaring device variables and allocating device memory for inputs\n    float* inputVectorA_d, * inputVectorB_d;\n    int* nearestNeighborIndex_h, * nearestNeighborIndex_d;\n    CUDA_CHECK(cudaMallocAsync(&inputVectorA_d, numberOfPointsA * N_DIMS * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&inputVectorB_d, numberOfPointsB * N_DIMS * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&nearestNeighborIndex_d, numberOfPointsA * sizeof(float), stream));\n    nearestNeighborIndex_h = (int*)malloc(numberOfPointsA * sizeof(int));\n\n    // Loop running through each test\n    size_t numBlocks = ((numberOfPointsA / BLOCK_SIZE) + 1);\n    for (int i = 0; i < testCaseCount; i++) {\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(inputVectorA_d, inputVectorA_h[i], numberOfPointsA * N_DIMS * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(inputVectorB_d, inputVectorB_h[i], numberOfPointsB * N_DIMS * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Calling nearest neighbor kernel\n        void* args[] = { &inputVectorA_d, (void*)&numberOfPointsA, &inputVectorB_d, (void*)&numberOfPointsB, &nearestNeighborIndex_d };\n        CUDA_CHECK(cudaLaunchKernel((void*)k_nearestNeighbors, numBlocks, BLOCK_SIZE, args, N_DIMS * numberOfPointsB * sizeof(float), stream));\n        \n        // Copying memory back to host from device\n        CUDA_CHECK(cudaMemcpyAsync(nearestNeighborIndex_h, nearestNeighborIndex_d, numberOfPointsA * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\t\t\n\t\t// Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify the test point with manually computed outputs.\n        for (int k = 0; k < numberOfPointsA; k++) {\n            assert(outputIndices[i][k] == nearestNeighborIndex_h[k]);\n        }\n    }\n\n    // Free allocated memory\n    CUDA_CHECK(cudaFreeAsync(inputVectorA_d, stream));\n    CUDA_CHECK(cudaFreeAsync(inputVectorB_d, stream));\n    CUDA_CHECK(cudaFreeAsync(nearestNeighborIndex_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    free(nearestNeighborIndex_h);\n}\n\n__global__ void k_nearestNeighbors(const float* inputVectorA_d, int nA, float* inputVectorB_d, int nB, int* nearestNeighborIndex) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/109", "date": "2025-07-30", "prompt": "Write a CUDA kernel that computes the Peak Signal-to-Noise Ratio (PSNR) between an original image and its compressed version using shared memory.\n\nThe signature of the kernel is __global__ void k_peakSignalToNoiseRatio(float *inputImage, float *compressedInputImage, float *peakToNoiseRatio, int imageWidth, int imageHeight), where inputImage is the pointer to the input image in row-major order, compressedInputImage is the pointer to compressed input image having same dimensions and format as inputImage, peakToNoiseRatio is the pointer to the output where computed PSNR value will be stored, imageWidth is the width in pixels, and imageHeight is the height in pixels.\n\n>>> k_peakSignalToNoiseRatio({0.452895, 0.403882, 0.69631, 0.452895, 0.403882, 0.69631, 0.452895, 0.403882, 0.69631}, {0.498184, 0.44427, 0.765941, 0.498184, 0.44427, 0.765941, 0.498184, 0.44427, 0.765941}, peakToNoiseRatio, 3, 3) -> peakToNoiseRatio : {25.4613}\n>>> k_peakSignalToNoiseRatio({0.452712, 0.747856, 0.605976, 0.452712, 0.747856, 0.605976, 0.452712, 0.747856}, {0.457239, 0.755335, 0.612036, 0.457239, 0.755335, 0.612036, 0.457239, 0.755335}, peakToNoiseRatio, 4, 2) -> peakToNoiseRatio : {44.2206}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <cuda_runtime.h>\n#include <assert.h>\n\n#undef NDEBUG\n#define BLOCK_SIZE  (32)\n#define EPSILON     (1e-3)\n#define CUDA_CHECK(call) \\\ndo {\\\n        cudaError_t error = call;\\\n        if (error != cudaSuccess) {\\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",\\\n                    __FILE__, __LINE__,\\\n                    cudaGetErrorString(error));\\\n                exit(EXIT_FAILURE);\\\n        }\\\n} while (0)\n\n// Warp-level reduction: each thread in the warp calls this to reduce its value.\n__device__ float warpReduceSum(float val) {\n    // Full mask: all 32 threads active in the warp.\n    int mask = 0xFFFFFFFF;\n\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(mask, val, offset);\n    }\n    \n    return val;\n}\n\n// Block-level reduction that uses shared memory.\n__device__ float blockReduceSum(float* sdata, int tid, int blockSize) {\n    int lane = tid % warpSize;\n    int warpId = tid / warpSize;\n\n    // Each thread's value is already stored in sdata[tid].\n    float val = sdata[tid];\n\n    // First, do warp-level reduction within each warp.\n    val = warpReduceSum(val);\n\n    // Write the reduced value of each warp to shared memory.\n    if (lane == 0) {\n        sdata[warpId] = val;\n    }\n\n    __syncthreads();\n\n    // Calculate the number of warps in the block.\n    int numWarps = (blockSize + warpSize - 1) / warpSize;\n    \n    // Only the first 'numWarps' threads need to participate in the final reduction.\n    if (tid < numWarps) {\n        val = sdata[tid];\n        val = warpReduceSum(val);\n    }\n\n    return val;\n}\n\n__global__ void k_peakSignalToNoiseRatio(float *inputImage, float *compressedInputImage, float *peakToNoiseRatio, const int imageWidth, const int imageHeight);\n\nvoid launch() {\n    const int NUM_TESTS = 7;\n    int imageWidth[NUM_TESTS] = { 3, 4, 5, 2, 3, 4, 5 };\n    int imageHeight[NUM_TESTS] = { 3, 2, 2, 4, 4, 4, 3};\n    float imageCompressionFactor[NUM_TESTS] = { 10.0f, 100.0f, 1000.0f, 10000.0f, 100000.0f, 1000000.0f, 10000000.0f};\n\n    // Calculating maximum image size (in pixels)\n    int maxImageSizeInPixels = 0;\n    for (int i = 0; i < NUM_TESTS; i++) {\n        int temp = imageWidth[i] * imageHeight[i];\n        if (temp > maxImageSizeInPixels)\n            maxImageSizeInPixels = temp;\n    }\n\n    // Input original image \n    float inputImage_h[NUM_TESTS][maxImageSizeInPixels] = {\n      {0.452895, 0.403882, 0.69631, 0.452895, 0.403882, 0.69631, 0.452895, 0.403882, 0.69631},\n      {0.452712, 0.747856, 0.605976, 0.452712, 0.747856, 0.605976, 0.452712, 0.747856},\n      {0.0344, 0.4387, 0.3816, 0.7655, 0.7952, 0.1869, 0.4898, 0.4456, 0.6463, 0.7094},\n      {0.2575, 0.8407, 0.2543, 0.8143, 0.2435, 0.9293, 0.3500, 0.1966},\n      {0.6892, 0.7482, 0.4505, 0.0838, 0.2290, 0.9133, 0.1524, 0.8258, 0.5383, 0.9961, 0.0782, 0.4427},\n      {0.8147, 0.9058, 0.1270, 0.9134, 0.6324, 0.0975, 0.2785, 0.5469, 0.9575, 0.9649, 0.1576, 0.9706, 0.9572, 0.4854, 0.8003, 0.1419},\n      {0.8308, 0.5853, 0.5497, 0.9172, 0.2858, 0.7572, 0.7537, 0.3804, 0.5678, 0.0759, 0.0540, 0.5308, 0.7792, 0.9340, 0.1299}\n    };\n\n    float expectedOutput[NUM_TESTS] = {25.4613, 44.2206, 65.299, 84.908, 104.43, 123.218, 144.059};\n\n    // Host Side Memory Allocation\n    float peakToNoiseRatio_h = 0.0f;\n    float* compressedInputImage_h = (float*)malloc(sizeof(float) * maxImageSizeInPixels);\n\n    // Device pointers initialization\n    float* inputImage_d = NULL;\n    float* compressedInputImage_d = NULL;\n    float* peakToNoiseRatio_d = NULL;\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate Memory on Device\n    CUDA_CHECK(cudaMallocAsync(&inputImage_d, sizeof(float) * maxImageSizeInPixels, stream));\n    CUDA_CHECK(cudaMallocAsync(&compressedInputImage_d, sizeof(float) * maxImageSizeInPixels, stream));\n    CUDA_CHECK(cudaMallocAsync(&peakToNoiseRatio_d, sizeof(float), stream));\n\n    for (int t = 0; t < NUM_TESTS; t++) {\n        //Initialising Host Input Memory\n        for (int j = 0; j < imageWidth[t]; j++) {\n            for (int k = 0; k < imageHeight[t]; k++) {\n                // Same As Original Image but Little Noise is added to the image \n                compressedInputImage_h[j + k * imageWidth[t]] = std::max(0.0f, std::min(1.0f, inputImage_h[t][j + k * imageWidth[t]] + inputImage_h[t][j + k * imageWidth[t]] / imageCompressionFactor[t]));\n            }\n        }\n\n        // Copy Input Images Data from Host To Device Memory\n        CUDA_CHECK(cudaMemcpyAsync(inputImage_d, &inputImage_h[t][0], sizeof(float) * imageWidth[t] * imageHeight[t], cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(compressedInputImage_d, compressedInputImage_h, sizeof(float) * imageWidth[t] * imageHeight[t], cudaMemcpyHostToDevice, stream));\n\n        // Determine the number of threads and blocks\n        dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE, 1);\n        dim3 gridSize((imageWidth[t] + BLOCK_SIZE - 1) / BLOCK_SIZE, (imageHeight[t] + BLOCK_SIZE - 1) / BLOCK_SIZE, 1);\n\n        // Launch Kernel\n        // Grid : ((imageWidth[t] + BLOCK_SIZE - 1) / BLOCK_SIZE, (imageHeight[t] + BLOCK_SIZE - 1) / BLOCK_SIZE, 1)\n        // Block : (BLOCK_SIZE, BLOCK_SIZE, 1);\n        void* args[] = { &inputImage_d, &compressedInputImage_d, &peakToNoiseRatio_d, (void*)&imageWidth[t], (void*)&imageHeight[t] };\n        CUDA_CHECK(cudaLaunchKernel((void*)k_peakSignalToNoiseRatio, gridSize, blockSize, args, sizeof(float) * blockSize.x * blockSize.y, stream));\n\n        // Copy Device Memory to Host Memory\n        CUDA_CHECK(cudaMemcpyAsync(&peakToNoiseRatio_h, peakToNoiseRatio_d, sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify the results\n        assert(fabs(peakToNoiseRatio_h - expectedOutput[t]) < EPSILON);\n    }\n\n    //Free Host Memory\n    free(compressedInputImage_h);\n\n    // Free device Side Memory\n    CUDA_CHECK(cudaFreeAsync(peakToNoiseRatio_d, stream));\n    CUDA_CHECK(cudaFreeAsync(inputImage_d, stream));\n    CUDA_CHECK(cudaFreeAsync(compressedInputImage_d, stream));\n\n    //Destroy CUDA stream\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_peakSignalToNoiseRatio(float *inputImage, float *compressedInputImage, float *peakToNoiseRatio, int imageWidth, int imageHeight) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/110", "date": "2025-07-30", "prompt": "Develop a CUDA kernel to iteratively solve the distance constraints of joints for a chain under gravitational forces with endpoints pinned to their initial positions. Use the device memory to retain the states of joints until multiple iterations are complete.\nThe signature of the CUDA kernel is __global__ void k_solveConstraintUnderGravity(float *x_d, float *y_d, float *distance_d, float *mass_d, float *xOld_d, float *yOld_d, int numJoints, TempStorage temp), where x_d is a pointer to array of x-coordinate of joints, y_d is a pointer to array of y-coordinate of joints, distance_d is a pointer to array of distances between connected joints, mass_d is a pointer to an array of mass values of joints, xOld_d is a pointer to an array of x-coordinate of joints from previous simulation iteration, yOld_d is a pointer to an array of y-coordinate of joints from previous simulation iteration, and the temp is a struct of temporary arrays used in the algorithm.\n\n>>> k_solveConstraintUnderGravity(\n    { 250.0, 250.366, 255.859, 279.663, 343.75, 478.882, 724.609, 1129.27 },\n    { 140.0, 111.346, 66.4371, 69.6125, 116.323, 139.648, 106.17, 63.8523 },\n    { 28.6559, 45.2441, 24.0146, 79.3034, 137.13, 247.998, 406.87 },\n    { 14.3279, 36.95, 34.6293, 51.659, 108.217, 192.564, 327.434, 203.435 },\n    { 250.0, 250.366, 255.859, 279.663, 343.75, 478.882, 724.609, 1129.27 },\n    { 140.0, 111.346, 66.4371, 69.6125, 116.323, 139.648, 106.17, 63.8523 },\n    8,\n    temp\n) -> x_d: { 250, 250.366, 255.86, 279.663, 343.75, 478.882, 724.609, 1129.27 }, y_d: { 140, 111.347, 66.4372, 69.6154, 116.326, 139.651, 106.173, 63.8523 }\n\n>>> k_solveConstraintUnderGravity(\n    { 0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0 },\n    { 0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0 },\n    { 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421 },\n    { 7.07107, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 14.1421, 7.07107 },\n    { 0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0 },\n    { 0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0 },\n    8,\n    temp\n) -> x_d: { 0.0, 9.99854, 19.9986, 29.9986, 39.9986, 49.9986, 59.9986, 70.0 }, y_d: { 0.0, 10.0015, 20.0015, 30.0015, 40.0015, 50.0015, 60.0015, 70.0 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <cooperative_groups.h>\n\n#define CUDA_CHECK(call) {                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Number of points that connect the segments to create the chain. There are NUM_JOINTS - 1 number of segments.\nconstexpr int NUM_JOINTS = 8;\nconstexpr int SIZE_SEGMENTS = ((NUM_JOINTS - 1) * sizeof(float));\nconstexpr int SIZE_JOINTS =  (NUM_JOINTS * sizeof(float));\n\n// Less than 1.2 pixel distance error is tolerated.\nconstexpr float MAXIMUM_ABSOLUTE_ERROR = 1.2f;\n\n// CUDA-related constants.\nconstexpr int NUM_THREADS_PER_BLOCK = 32;\nconstexpr int NUM_BLOCKS_PER_GRID = 2;\nconstexpr int GRID_STRIDE_SIZE = NUM_BLOCKS_PER_GRID * NUM_THREADS_PER_BLOCK;\nconstexpr int MAX_GRID_STRIDE_ITERATIONS = 1 + (NUM_JOINTS - 1) / GRID_STRIDE_SIZE;\nconstexpr int TEMP_ARRAY_SIZE = MAX_GRID_STRIDE_ITERATIONS * GRID_STRIDE_SIZE;\n\n// Struct of arrays for temporary storage in the algorithm with grid-stride loop.\nstruct TempStorage{\n    float *x_d;\n    float *y_d;\n    float *mass_d;\n    float *xLeft_d;\n    float *yLeft_d;\n    float *massLeft_d;\n    float *distanceLeft_d;\n    float *xRight_d;\n    float *yRight_d;\n    float *massRight_d;\n    float *distanceRight_d;\n    float *xOld_d;\n    float *yOld_d;\n    bool *pinned_d;\n};\n\n// The CUDA kernel to solve the constraints of a chain that is made of multiple joints using device memory to retain all states of the joints for all iterations.\n__global__ void k_solveConstraintUnderGravity(float *x_d, float *y_d, float *distance_d, float *mass_d, float *xOld_d, float *yOld_d, int numJoints, TempStorage temp);\n\nvoid launch() {\n    // Host data for joints.\n    float x_h[NUM_JOINTS];\n    float y_h[NUM_JOINTS];\n    float distance_h[NUM_JOINTS - 1];\n    float mass_h[NUM_JOINTS];\n    cudaStream_t stream;\n\n    // Device data for joints.\n    float *x_d;\n    float *y_d;\n    float *xOld_d;\n    float *yOld_d;\n    float *mass_d;\n    TempStorage temp;\n    \n    // Distance data between joint i and joint i + 1.\n    float *distance_d;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Device memory I/O array allocations.\n    CUDA_CHECK(cudaMallocAsync(&x_d, SIZE_JOINTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&y_d, SIZE_JOINTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&xOld_d, SIZE_JOINTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&yOld_d, SIZE_JOINTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&mass_d, SIZE_JOINTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&distance_d, SIZE_SEGMENTS, stream));\n\n    // Device memory temporary array allocations.\n    CUDA_CHECK(cudaMallocAsync(&temp.x_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.y_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.mass_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.xLeft_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.yLeft_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.massLeft_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.distanceLeft_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.xRight_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.yRight_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.massRight_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.distanceRight_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.xOld_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.yOld_d, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&temp.pinned_d, TEMP_ARRAY_SIZE * sizeof(unsigned char), stream));\n\n    CUDA_CHECK(cudaMemsetAsync(temp.x_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.y_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.mass_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.xLeft_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.yLeft_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.massLeft_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.distanceLeft_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.xRight_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.yRight_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.massRight_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.distanceRight_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.xOld_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.yOld_d, 0, TEMP_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(temp.pinned_d, 0, TEMP_ARRAY_SIZE * sizeof(unsigned char), stream));\n\n    for(int i = 0; i < NUM_JOINTS; i++) {\n        mass_h[i] = 0.0f;\n    }\n\n    // Test 1: initial condition = periodic wave shaped chain with variable segment length.\n    {\n        for(int index = 0;index < NUM_JOINTS; index++) {\n            float x = pow(index / (float)NUM_JOINTS, 4) * 1500 + 250;\n            float y = cos(40.0f * index / (float)(NUM_JOINTS)) * 40 + 100;\n            x_h[index] = x;\n            y_h[index] = y;\n\n            if(index > 0) {\n                float dx = x - x_h[index - 1];\n                float dy = y - y_h[index - 1];\n                distance_h[index - 1] = sqrt(dx * dx + dy * dy);\n            \n                // Segment mass depends on length. Joint mass is half of masses added from each segment on sides. Assuming 1 unit mass per unit distance.\n                mass_h[index - 1] += (distance_h[index - 1] * 0.5f);\n                mass_h[index] += (distance_h[index - 1] * 0.5f);\n            }\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(x_d, x_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(xOld_d, x_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_d, y_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yOld_d, y_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(distance_d, distance_h, SIZE_SEGMENTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        \n        // Resetting mass values for next initialization.\n        for(int i = 0; i < NUM_JOINTS; i++) {\n            mass_h[i] = 0.0f;\n        }\n\n        int numJoints = NUM_JOINTS;\n        void * args[8] = { &x_d, &y_d, &distance_d, &mass_d, &xOld_d, &yOld_d, &numJoints, &temp };\n        dim3 gridDim(NUM_BLOCKS_PER_GRID, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        \n        // Grid: (2, 1, 1)\n        // Block: (32, 1, 1)\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_solveConstraintUnderGravity, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync(x_h, x_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_h, y_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Checking the differences between the constrained distance and the current distance between joints.\n        for(int j = 1; j < NUM_JOINTS; j++) {\n            float dx = x_h[j - 1] - x_h[j];\n            float dy = y_h[j - 1] - y_h[j];\n            float distance = sqrt(dx * dx + dy * dy);\n            float expectedDistance = distance_h[j - 1];\n            float absoluteError = fabsf(distance - expectedDistance);\n            assert(absoluteError < MAXIMUM_ABSOLUTE_ERROR);\n        }\n    }\n\n    // Test 2: initial condition = chain with a diagonal line shape and uniform segment length.\n    {\n        for(int index = 0; index < NUM_JOINTS; index++) {\n            float x = index * 10.0f;\n            float y = index * 10.0f;\n            x_h[index] = x;\n            y_h[index] = y;\n            \n            if(index > 0) {\n                float dx = x - x_h[index - 1];\n                float dy = y - y_h[index - 1];\n                distance_h[index - 1] = sqrt(dx * dx + dy * dy);\n\n                // Segment mass depends on length. Joint mass is half of masses added from each segment on sides. Assuming 1 unit mass per unit distance.\n                mass_h[index - 1] += (distance_h[index - 1] * 0.5f);\n                mass_h[index] += (distance_h[index - 1] * 0.5f);\n            }\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(x_d, x_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(xOld_d, x_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_d, y_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yOld_d, y_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(distance_d, distance_h, SIZE_SEGMENTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        \n        // Resetting mass values for next initialization.\n        for(int i = 0; i < NUM_JOINTS; i++) {\n            mass_h[i] = 0.0f;\n        }\n        \n        int numJoints = NUM_JOINTS;\n        void * args[8] = { &x_d, &y_d, &distance_d, &mass_d, &xOld_d, &yOld_d, &numJoints, &temp };\n        dim3 gridDim(NUM_BLOCKS_PER_GRID, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        \n        // Grid: (2, 1, 1)\n        // Block: (32, 1, 1)\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_solveConstraintUnderGravity, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync(x_h, x_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_h, y_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Checking the differences between the constrained distance and the current distance between joints.\n        for(int j = 1; j < NUM_JOINTS; j++) {\n            float dx = x_h[j - 1] - x_h[j];\n            float dy = y_h[j - 1] - y_h[j];\n            float distance = sqrt(dx * dx + dy * dy);\n            float expectedDistance = distance_h[j - 1];\n            float absoluteError = fabsf(distance - expectedDistance);\n            assert(absoluteError < MAXIMUM_ABSOLUTE_ERROR);\n        }\n    }\n\n    // Test 3: initial condition = chain with a horizontal line shape and uniform segment length.\n    {\n        for(int index = 0; index < NUM_JOINTS; index++) {\n            float x = index * 10.0f;\n            float y = 400.0f;\n            x_h[index] = x;\n            y_h[index] = y;\n\n            if(index > 0) {\n                float dx = x - x_h[index - 1];\n                float dy = y - y_h[index - 1];\n                distance_h[index - 1] = sqrt(dx * dx + dy * dy);\n                \n                // Segment mass depends on length. Joint mass is half of masses added from each segment on sides. Assuming 1 unit mass per unit distance.\n                mass_h[index - 1] += (distance_h[index - 1] * 0.5f);\n                mass_h[index] += (distance_h[index - 1] * 0.5f);\n            }\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(x_d, x_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(xOld_d, x_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_d, y_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yOld_d, y_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(distance_d, distance_h, SIZE_SEGMENTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        \n        // Resetting mass values for next initialization.\n        for(int i = 0; i < NUM_JOINTS; i++) {\n            mass_h[i] = 0.0f;\n        }\n\n        int numJoints = NUM_JOINTS;\n        void * args[8] = { &x_d, &y_d, &distance_d, &mass_d, &xOld_d, &yOld_d, &numJoints, &temp };\n        dim3 gridDim(NUM_BLOCKS_PER_GRID, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        \n        // Grid: (2, 1, 1)\n        // Block: (32, 1, 1)\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_solveConstraintUnderGravity, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync(x_h, x_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_h, y_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Checking the differences between the constrained distance and the current distance between joints.\n        for(int j = 1; j < NUM_JOINTS; j++) {\n            float dx = x_h[j - 1] - x_h[j];\n            float dy = y_h[j - 1] - y_h[j];\n            float distance = sqrt(dx * dx + dy * dy);\n            float expectedDistance = distance_h[j - 1];\n            float absoluteError = fabsf(distance - expectedDistance);\n            assert(absoluteError < MAXIMUM_ABSOLUTE_ERROR);\n        }      \n    }\n\n    // Test 4: initial condition = chain with a vertical line shape and uniform segment length.\n    {\n        for(int index = 0; index < NUM_JOINTS; index++) {\n            float x = 500.0f;\n            float y = index * 10.0f;\n            x_h[index] = x;\n            y_h[index] = y;\n\n            if(index > 0) {\n                float dx = x - x_h[index - 1];\n                float dy = y - y_h[index - 1];\n                distance_h[index - 1] = sqrt(dx * dx + dy * dy);\n                \n                // Segment mass depends on length. Joint mass is half of masses added from each segment on sides. Assuming 1 unit mass per unit distance.\n                mass_h[index - 1] += (distance_h[index - 1] * 0.5f);\n                mass_h[index] += (distance_h[index - 1] * 0.5f);\n            }\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(x_d, x_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(xOld_d, x_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_d, y_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yOld_d, y_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(distance_d, distance_h, SIZE_SEGMENTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        \n        // Resetting mass values for next initialization.\n        for(int i = 0; i < NUM_JOINTS; i++) {\n            mass_h[i] = 0.0f;\n        }\n        \n        int numJoints = NUM_JOINTS;\n        void * args[8] = { &x_d, &y_d, &distance_d, &mass_d, &xOld_d, &yOld_d, &numJoints, &temp };\n        dim3 gridDim(NUM_BLOCKS_PER_GRID, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        \n        // Grid: (2, 1, 1)\n        // Block: (32, 1, 1)\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_solveConstraintUnderGravity, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync(x_h, x_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_h, y_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Checking the differences between the constrained distance and the current distance between joints.\n        for(int j = 1; j < NUM_JOINTS; j++) {\n            float dx = x_h[j - 1] - x_h[j];\n            float dy = y_h[j - 1] - y_h[j];\n            float distance = sqrt(dx * dx + dy * dy);\n            float expectedDistance = distance_h[j - 1];\n            float absoluteError = fabsf(distance - expectedDistance);\n            assert(absoluteError < MAXIMUM_ABSOLUTE_ERROR);\n        }      \n    }\n\n    // Test 5: initial condition = chain with a horizontal line shape, uniform segment length and one joint pulled far from equilibrium position.\n    {\n        for(int index = 0; index < NUM_JOINTS; index++) {\n            float x = index * 20.0f;\n            float y = 500.0f;\n            x_h[index] = x;\n            y_h[index] = y;\n\n            if(index > 0) {\n                float dx = x - x_h[index - 1];\n                float dy = y - y_h[index - 1];\n                distance_h[index - 1] = sqrt(dx * dx + dy * dy);\n                \n                // Segment mass depends on length. Joint mass is half of masses added from each segment on sides. Assuming 1 unit mass per unit distance.\n                mass_h[index - 1] += (distance_h[index - 1] * 0.5f);\n                mass_h[index] += (distance_h[index - 1] * 0.5f);\n            }\n        }\n\n        // Pulling middle joint 100 pixels in -y direction.\n        x_h[NUM_JOINTS / 2] = (NUM_JOINTS / 2) * 20.0f;\n        y_h[NUM_JOINTS / 2] = 400.0f;\n\n        CUDA_CHECK(cudaMemcpyAsync(x_d, x_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(xOld_d, x_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_d, y_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yOld_d, y_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(distance_d, distance_h, SIZE_SEGMENTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        \n        // Resetting mass values for next initialization.\n        for(int i = 0; i < NUM_JOINTS; i++) {\n            mass_h[i] = 0.0f;\n        }\n\n        int numJoints = NUM_JOINTS;\n        void * args[8] = { &x_d, &y_d, &distance_d, &mass_d, &xOld_d, &yOld_d, &numJoints, &temp };\n        dim3 gridDim(NUM_BLOCKS_PER_GRID, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        \n        // Grid: (2, 1, 1)\n        // Block: (32, 1, 1)\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_solveConstraintUnderGravity, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync(x_h, x_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_h, y_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Checking the differences between the constrained distance and the current distance between joints.\n        for(int j = 1; j < NUM_JOINTS; j++) {\n            float dx = x_h[j - 1] - x_h[j];\n            float dy = y_h[j - 1] - y_h[j];\n            float distance = sqrt(dx * dx + dy * dy);\n            float expectedDistance = distance_h[j - 1];\n            float absoluteError = fabsf(distance - expectedDistance);\n            assert(absoluteError < MAXIMUM_ABSOLUTE_ERROR);\n        }\n    }\n\n    // Test 6: initial condition = chain with a horizontal line shape, uniform segment length and two joints pulled far towards opposite directions.\n    {\n        for(int index = 0; index < NUM_JOINTS; index++) {\n            float x = index * 20.0f;\n            float y = 500.0f;\n            x_h[index] = x;\n            y_h[index] = y;\n            if(index > 0) {\n                float dx = x - x_h[index - 1];\n                float dy = y - y_h[index - 1];\n                distance_h[index - 1] = sqrt(dx * dx + dy * dy);\n                \n                // Segment mass depends on length. Joint mass is half of masses added from each segment on sides. Assuming 1 unit mass per unit distance.\n                mass_h[index - 1] += (distance_h[index - 1] * 0.5f);\n                mass_h[index] += (distance_h[index - 1] * 0.5f);\n            }\n        }\n\n        // Pulling 1 joint 100 pixels in -y direction.\n        x_h[5] = 5 * 20.0f;\n        y_h[5] = 400.0f;\n\n        // Pulling 1 joint 100 pixels in y direction.\n        x_h[NUM_JOINTS - 5] = (NUM_JOINTS - 5) * 20.0f;\n        y_h[NUM_JOINTS - 5] = 600.0f;\n\n        CUDA_CHECK(cudaMemcpyAsync(x_d, x_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(xOld_d, x_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_d, y_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yOld_d, y_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(distance_d, distance_h, SIZE_SEGMENTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        \n        // Resetting mass values for next initialization.\n        for(int i = 0; i < NUM_JOINTS; i++) {\n            mass_h[i] = 0.0f;\n        }\n        \n        int numJoints = NUM_JOINTS;\n        void * args[8] = { &x_d, &y_d, &distance_d, &mass_d, &xOld_d, &yOld_d, &numJoints, &temp };\n        dim3 gridDim(NUM_BLOCKS_PER_GRID, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        \n        // Grid: (2, 1, 1)\n        // Block: (32, 1, 1)\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_solveConstraintUnderGravity, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync(x_h, x_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_h, y_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Checking the differences between the constrained distance and the current distance between joints.\n        for(int j = 1; j < NUM_JOINTS; j++) {\n            float dx = x_h[j - 1] - x_h[j];\n            float dy = y_h[j - 1] - y_h[j];\n            float distance = sqrt(dx * dx + dy * dy);\n            float expectedDistance = distance_h[j - 1];\n            float absoluteError = fabsf(distance - expectedDistance);\n            assert(absoluteError < MAXIMUM_ABSOLUTE_ERROR);\n        }      \n    }\n\n    // Test 7: initial condition = chain with a vertical line shape, uniform segment length and one joint pulled far from equilibrium position.\n    {\n        for(int index = 0; index < NUM_JOINTS; index++) {\n            float x = 500.0f;\n            float y = index * 10.0f;\n            x_h[index] = x;\n            y_h[index] = y;\n            \n            if(index > 0) {\n                float dx = x - x_h[index - 1];\n                float dy = y - y_h[index - 1];\n                distance_h[index - 1] = sqrt(dx * dx + dy * dy);\n            \n                // Segment mass depends on length. Joint mass is half of masses added from each segment on sides. Assuming 1 unit mass per unit distance.\n                mass_h[index - 1] += (distance_h[index - 1] * 0.5f);\n                mass_h[index] += (distance_h[index - 1] * 0.5f);\n            }\n        }\n\n        // Pulling 1 joint 100 pixels in x direction.\n        x_h[5] = 600.0f;\n        y_h[5] = 5 * 10.0f;\n        CUDA_CHECK(cudaMemcpyAsync(x_d, x_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(xOld_d, x_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_d, y_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yOld_d, y_d, SIZE_JOINTS, cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(distance_d, distance_h, SIZE_SEGMENTS, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, SIZE_JOINTS, cudaMemcpyHostToDevice, stream));\n        \n        // Resetting mass values for next initialization.\n        for(int i = 0; i < NUM_JOINTS; i++) {\n            mass_h[i] = 0.0f;\n        }\n\n        int numJoints = NUM_JOINTS;\n        void * args[8] = { &x_d, &y_d, &distance_d, &mass_d, &xOld_d, &yOld_d, &numJoints, &temp };\n        dim3 gridDim(NUM_BLOCKS_PER_GRID, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        \n        // Grid: (2, 1, 1)\n        // Block: (32, 1, 1)\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_solveConstraintUnderGravity, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync(x_h, x_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_h, y_d, NUM_JOINTS * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Checking the differences between the constrained distance and the current distance between joints.\n        for(int j = 1; j < NUM_JOINTS; j++) {\n            float dx = x_h[j - 1] - x_h[j];\n            float dy = y_h[j - 1] - y_h[j];\n            float distance = sqrt(dx * dx + dy * dy);\n            float expectedDistance = distance_h[j - 1];\n            float absoluteError = fabsf(distance - expectedDistance);\n            assert(absoluteError < MAXIMUM_ABSOLUTE_ERROR);\n        }\n    }\n\n    CUDA_CHECK(cudaFreeAsync(x_d, stream));\n    CUDA_CHECK(cudaFreeAsync(y_d, stream));\n    CUDA_CHECK(cudaFreeAsync(xOld_d, stream));\n    CUDA_CHECK(cudaFreeAsync(yOld_d, stream));\n    CUDA_CHECK(cudaFreeAsync(distance_d, stream));\n    CUDA_CHECK(cudaFreeAsync(mass_d, stream));\n\n    CUDA_CHECK(cudaFreeAsync(temp.x_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.y_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.mass_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.xLeft_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.yLeft_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.massLeft_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.distanceLeft_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.xRight_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.yRight_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.massRight_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.distanceRight_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.xOld_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.yOld_d, stream));\n    CUDA_CHECK(cudaFreeAsync(temp.pinned_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_solveConstraintUnderGravity(float *x_d, float *y_d, float *distance_d, float *mass_d, float *xOld_d, float *yOld_d, int numJoints, TempStorage temp) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/111", "date": "2025-07-30", "prompt": "Write a CUDA kernel to find the max-score of alignment between two sequences using the Smith-Waterman algorithm. Each block will process one diagonal, and each thread will calculate a unique cell in diagonal based on previously computed neighboring values. Assume that the block size is computed based on the lengths of the sequences.\n\nThe signature of the function is __global__ void k_smithWatermanKernel(char *firstSequence_d, char *secondSequence_d, int length1, int length2, int *scoreMatrix_d, int *maxScore_d), where firstSequence_d is a pointer to the first genome sequence, secondSequence_d is a pointer to the second genome sequence, length1 and length2 are the lengths of first and second genome sequences, scoreMatrix_d is the pointer to integer matrix that stores the intermediate alignment scores and maxScore_d is the pointer to an integer that stores maximum alignment score between the two sequences.\n\n>>> k_smithWatermanKernel(\"GATTACA\", \"GCATGCU\", 7, 7, scoreMatrix_d, maxScore_d)-> maxScore_d: 4\n>>> k_smithWatermanKernel(\"AGCT\", \"CGTACG\", 4, 6, scoreMatrix_d, maxScore_d)-> maxScore_d: 2\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <vector>\n#include <string>\n#include <cstdio>\n#include <algorithm>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#undef  NDEBUG\n#include <assert.h>\n\nnamespace cg = cooperative_groups;\n\n#define MATCH         (2)\n#define MISMATCH      (-1)\n#define GAP           (-2)\n#define INDEX(row, col, length)   ((row) * ((length) + 1) + (col))\n#define BLOCK_SIZE    (16)\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\nstruct TestCase {\n    std::string sequence1;\n    std::string sequence2;\n    int expected_score;\n};\n\n__global__ void k_smithWatermanKernel(char *firstSequence_d, char *secondSequence_d, int length1, int length2, int *scoreMatrix_d, int *maxScore_d);\n\nvoid launch() {\n    // Total number of test cases\n    const int NUM_TEST_CASES = 11;\n\n    TestCase testCases[] = {\n        {\"GATTACA\", \"GCATGCU\", 4},                              // Test Case 1\n        {\"AGCT\", \"CGTACG\", 2},                                  // Test Case 2\n        {\"AAAA\", \"AAA\", 6},                                     // Test Case 3\n        {\"ACTG\", \"TGCA\", 4},                                    // Test Case 4\n        {\"ACCGTGA\", \"GTGAATA\", 8},                              // Test Case 5\n        {\"GCGT\", \"GCGT\", 8},                                    // Test Case 6\n        {\"ACGTACGT\", \"TGCATGCA\", 4},                            // Test Case 7\n        {\"GATTA\", \"CTAGG\", 4},                                  // Test Case 8\n        {\"ACGTACGTACGTACGTACGT\", \"ACGTACGTACGTACGTACGT\", 40},   // Test Case 9\n        {std::string(64, 'A'), std::string(64, 'A'), 128},      // Test Case 10\n        {std::string(100, 'C'), std::string(100, 'C'), 200}     // Test Case 11\n    };\n\n    // Determine the maximum lengths among all test cases\n    size_t maxLength1 = 0;\n    size_t maxLength2 = 0;\n    \n    for (int i = 0; i < NUM_TEST_CASES; ++i) {\n        maxLength1 = std::max(maxLength1, testCases[i].sequence1.length());\n        maxLength2 = std::max(maxLength2, testCases[i].sequence2.length());\n    }\n\n    // Allocate device memory asynchronously using the maximum sizes\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    size_t maxSize1 = maxLength1 * sizeof(char);\n    size_t maxSize2 = maxLength2 * sizeof(char);\n\n    size_t scoreMatrixSizeMax = (maxLength1 + 1) * (maxLength2 + 1) * sizeof(int);\n    char *firstSequence_d, *secondSequence_d;\n    int *scoreMatrix_d, *maxScore_d;\n    CUDA_CHECK(cudaMallocAsync((void**)&firstSequence_d, maxSize1, stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&secondSequence_d, maxSize2, stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&scoreMatrix_d, scoreMatrixSizeMax, stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&maxScore_d, sizeof(int), stream));\n\n    // Create a host vector for initializing the DP matrix.\n    std::vector<int> scoreMatrixInit((maxLength1 + 1) * (maxLength2 + 1), 0);\n\n    // For each test case, launch the kernel cooperatively.\n    for (int testIndex = 0; testIndex < NUM_TEST_CASES; testIndex++) {\n        std::string sequence1 = testCases[testIndex].sequence1;\n        std::string sequence2 = testCases[testIndex].sequence2;\n        int length1 = sequence1.length();\n        int length2 = sequence2.length();\n\n        // Copy sequences to device\n        CUDA_CHECK(cudaMemcpyAsync(firstSequence_d, sequence1.c_str(), length1 * sizeof(char), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(secondSequence_d, sequence2.c_str(), length2 * sizeof(char), cudaMemcpyHostToDevice, stream));\n\n        // Reset score matrix and max score for the current test case\n        std::vector<int> scoreMatrix_h((length1 + 1) * (length2 + 1), 0);\n        int maxScore_h = 0;\n        CUDA_CHECK(cudaMemcpyAsync(scoreMatrix_d, scoreMatrix_h.data(), (length1 + 1) * (length2 + 1) * sizeof(int), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(maxScore_d, &maxScore_h, sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        // Calculating hardware execution limit for concurrent blocks.\n        int maxBlocksPerGrid;\n        cudaOccupancyMaxActiveBlocksPerMultiprocessor(&maxBlocksPerGrid, k_smithWatermanKernel, BLOCK_SIZE, 0);\n        int numSMs;\n        cudaDeviceGetAttribute(&numSMs, cudaDevAttrMultiProcessorCount, 0);\n        int maxTotalBlocks = maxBlocksPerGrid * numSMs;\n        \n        // Determine the number of threads and blocks\n        dim3 blockSize(BLOCK_SIZE, 1, 1);\n        maxTotalBlocks = min(maxTotalBlocks, (length1 + length2 + BLOCK_SIZE - 1) / BLOCK_SIZE);\n        dim3 gridSize = dim3(maxTotalBlocks, 1, 1);\n        \n        //  Adding check for exceeding maximum grid dimensions\n        cudaDeviceProp props;\n        CUDA_CHECK(cudaGetDeviceProperties(&props, 0));\n        if (gridSize.x > props.maxGridSize[0] || gridSize.y > props.maxGridSize[1]) {\n            assert(false && \"Grid size exceeds device limits!\");\n        }\n\n        // Prepare kernel arguments for this test case.\n        void* args[] = {\n            (void*)&firstSequence_d,\n            (void*)&secondSequence_d,\n            (void*)&length1,\n            (void*)&length2,\n            (void*)&scoreMatrix_d,\n            (void*)&maxScore_d\n        };\n\n        // Launch the cooperative kernel for this test case.\n        CUDA_CHECK(cudaLaunchCooperativeKernel((const void*)k_smithWatermanKernel, gridSize, blockSize, args, 0, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Copy the computed max score back to host.\n        CUDA_CHECK(cudaMemcpyAsync(&maxScore_h, maxScore_d, sizeof(int), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        if(maxScore_h != testCases[testIndex].expected_score) {\n            std::vector<int> localDP((length1 + 1) * (length2 + 1), 0);\n            CUDA_CHECK(cudaMemcpyAsync(localDP.data(), scoreMatrix_d, (length1 + 1) * (length2 + 1) * sizeof(int), cudaMemcpyDeviceToHost, stream));\n            CUDA_CHECK(cudaStreamSynchronize(stream));\n            assert(maxScore_h == testCases[testIndex].expected_score);\n        }\n    }\n    \n    CUDA_CHECK(cudaFreeAsync(firstSequence_d, stream));\n    CUDA_CHECK(cudaFreeAsync(secondSequence_d, stream));\n    CUDA_CHECK(cudaFreeAsync(scoreMatrix_d, stream));\n    CUDA_CHECK(cudaFreeAsync(maxScore_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_smithWatermanKernel(char *firstSequence_d, char *secondSequence_d, int length1, int length2, int *scoreMatrix_d, int *maxScore_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/112", "date": "2025-07-30", "prompt": "Write a CUDA kernel to solve a partial differential equation using red black Gauss-Seidel method. The kernel should utilize device memory to load and update the solution array for each iteration.\n\nThe signature of the function is __global__ void k_solveRedBlackGaussSeidel( float *srcFunction, float *slnFunction, int size, int numIterations) where srcFunction is a pointer to the array containing right-hand side function f(x,y), slnFunction is the pointer to the solution array, size is the length of the square grid, and numIterations specifies how many iterations the solver should run.\n\n>>> k_solveRedBlackGaussSeidel({1, 1, 1, 1}, {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}, 2, 4) -> ({0, 0, 0, 0, 0, -0.0551, -0.0553, 0, 0, -0.0553, -0.0551, 0, 0, 0, 0, 0})\n>>> k_solveRedBlackGaussSeidel({3, 4, 5, 3.5, 4.5, 5.5, 4, 5, 6}, {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}, 3, 5) -> ({0, 0, 0, 0, 0, 0, -0.1522, -0.2250, -0.1968, 0, 0, -0.2116, -0.3010, -0.2652, 0, 0, -0.1745, -0.2518, -0.2191, 0, 0, 0, 0, 0, 0})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <cmath>\n#include <cuda_runtime.h>\n#include <assert.h>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\n\n#undef  NDEBUG\n // Tolerance for floating-point comparison\n#define TOLERANCE               (1e-4)\n// Number of threads per block\n#define BLOCK_SIZE              (16)\n// Number of elements allocated for device memory\n#define NUM_DEVICE_MEMORY_ELEM  (1024)\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n__global__ void k_solveRedBlackGaussSeidel( float *srcFunction, float *slnFunction, int size, int numIterations);\n\nvoid launch() {\n    // Number of test cases\n    constexpr int TEST_CASE_COUNT = 8;\n    // Single dimension of the square input grid\n    constexpr int functionGridDim[TEST_CASE_COUNT] = {2, 3, 4, 3, 4, 3, 3, 4};\n    // Number of iterations\n    constexpr int numIterations[TEST_CASE_COUNT] = {4, 5, 3, 6, 4, 3, 5, 7};\n    \n    // Find the maximum function grid dimension\n    constexpr int MAX_FUNC_GRID_DIM = *std::max_element(functionGridDim, functionGridDim + TEST_CASE_COUNT);\n    // Find the maximum function grid size\n    constexpr int MAX_FUNC_GRID_SIZE = MAX_FUNC_GRID_DIM * MAX_FUNC_GRID_DIM;\n    // Extra rows/columns \n    constexpr int EXTRA_ROW_COLUMN = 2;\n\n    // Input source function for the testcases\n    float fXy_h[TEST_CASE_COUNT][MAX_FUNC_GRID_SIZE] =  {\n        {1, 1, 1, 1},\n        {3, 4, 5, 3.5, 4.5, 5.5, 4, 5, 6},\n        {3.0000, 3.0625, 3.2500, 3.5625, 4.0000, 3.1250, 3.1875, 3.3750, 3.6875, 4.1250, 3.5000, 3.5625, 3.7500, 4.0625, 4.5000, 4.1250},\n        {0, 0, 0, 0, 1, 0, 0, 0, 0},\n        {0.1353, 0.3305, 0.3305, 0.1353, 0.3305, 0.8045, 0.8045, 0.3305, 0.3305, 0.8045, 0.8045, 0.3305, 0.1353, 0.3305, 0.3305, 0.1353},\n        {0, 0, 0, 0, 0.6065, 0, 0, 0, 0},\n        {7, 7, 7, 7, 7, 7, 7, 7, 7},\n        {0, 0.33, 0.66, 1, 0.33, 0.66, 0.99, 1.33, 0.66, 0.99, 1.32, 1.66, 1, 1.33, 1.66, 2}\n    };\n\n    // Expected outputs\n    float expectedSolution[TEST_CASE_COUNT][MAX_FUNC_GRID_SIZE] = {\n        {-0.0551, -0.0553, -0.0553, -0.0551},\n        {-0.1522, -0.2250, -0.1968, -0.2116, -0.3010, -0.2652, -0.1745, -0.2518, -0.2191},\n        {-0.0773, -0.1115, -0.1033, -0.0884, -0.1242, -0.1428, -0.1585, -0.1076, -0.1167, -0.1752, -0.1527, -0.1244, -0.0976, -0.1236, -0.1378, -0.0947},\n        {-0.0038, -0.0077, -0.0038, -0.0077, -0.0232, -0.0077, -0.0038, -0.0077, -0.0038},\n        {-0.0086, -0.0165, -0.0156, -0.0091, -0.0165, -0.0287, -0.0302, -0.0156, -0.0156, -0.0302, -0.0287, -0.0165, -0.0091, -0.0156, -0.0165, -0.0086},\n        {-0.0018, -0.0041, -0.0018, -0.0041, -0.0130, -0.0041, -0.0018, -0.0041, -0.0018},\n        {-0.2888, -0.3708, -0.2888, -0.3708, -0.4683, -0.3708, -0.2888, -0.3708, -0.2888},\n        {-0.0136, -0.0284, -0.0363, -0.0318, -0.0284, -0.0505, -0.0625, -0.0509, -0.0363, -0.0625, -0.0728, -0.0599, -0.0318, -0.0509, -0.0599, -0.0493}\n    };\n\n    unsigned maxSolutionGridSize = std::pow(MAX_FUNC_GRID_DIM + EXTRA_ROW_COLUMN, 2);\n    \n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Initialize results on the host\n    float *solution_h;\n    solution_h = (float*)malloc(maxSolutionGridSize * sizeof(float));\n    // Pointers for device memory (GPU)\n    float *pSolutionArrayU_d, *fXy_d;\n\n    CUDA_CHECK(cudaMallocAsync(&fXy_d, MAX_FUNC_GRID_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&pSolutionArrayU_d, maxSolutionGridSize * sizeof(float), stream));\n\n    // Loop to execute each test case\n    for (int testCaseId = 0; testCaseId < TEST_CASE_COUNT; testCaseId++) {\n        unsigned inputSlnGridSize = std::pow(functionGridDim[testCaseId] + EXTRA_ROW_COLUMN, 2);\n        // Initialize solution array with zeros\n        for (size_t i = 0; i < inputSlnGridSize; ++i) {\n            solution_h[i] = 0.0f;\n        }\n\n        // Copying data into device memory\n        CUDA_CHECK(cudaMemcpyAsync(pSolutionArrayU_d, solution_h, inputSlnGridSize * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(fXy_d, &fXy_h[testCaseId][0], functionGridDim[testCaseId] * functionGridDim[testCaseId] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Determine the number of threads and blocks\n        unsigned totalElemPerRowCol = functionGridDim[testCaseId] + EXTRA_ROW_COLUMN;\n        dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE, 1);\n        dim3 gridSize((totalElemPerRowCol + BLOCK_SIZE - 1) / BLOCK_SIZE, (totalElemPerRowCol + BLOCK_SIZE - 1) / BLOCK_SIZE);\n        \n        //  Adding check for exceeding maximum grid dimensions\n        cudaDeviceProp props;\n        CUDA_CHECK(cudaGetDeviceProperties(&props, 0));\n        if (gridSize.x > props.maxGridSize[0] || gridSize.y > props.maxGridSize[1]) {\n            assert(false && \"Grid size exceeds device limits!\");\n        }\n        \n        // Launch the kernel\n        // Grid:  ((totalElemPerRowCol + BLOCK_SIZE - 1) / BLOCK_SIZE, (totalElemPerRowCol + BLOCK_SIZE - 1) / BLOCK_SIZE)\n        // Block: (BLOCK_SIZE, BLOCK_SIZE, 1)\n        void *args[] = {&fXy_d, &pSolutionArrayU_d, (void*) &functionGridDim[testCaseId], (void*) &numIterations[testCaseId]};\n        CUDA_CHECK(cudaLaunchCooperativeKernel((void*)k_solveRedBlackGaussSeidel, gridSize, blockSize, args, 0, stream));\n\n        // Copy the output array pSolutionArrayU_d from the device (GPU) to the host (CPU)\n        CUDA_CHECK(cudaMemcpyAsync(solution_h, pSolutionArrayU_d, inputSlnGridSize * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify whether the computed solution array matches the expected result or not\n        unsigned resultIdx, referenceIdx; \n        // Skipping first and last element of each row\n        for (int i = 1; i < totalElemPerRowCol - 1; i++) {\n            // Skipping first and last element of each row \n            for (int j = 1; j < totalElemPerRowCol - 1; j++){\n                resultIdx = i*totalElemPerRowCol + j;\n                referenceIdx = (i-1)*(totalElemPerRowCol-2) + (j-1);\n                assert(fabs(solution_h[resultIdx] - expectedSolution[testCaseId][referenceIdx]) < TOLERANCE);\n            }\n        }\n    }\n    \n    // Free host memories\n    free(solution_h);\n\n    // Free stream\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_solveRedBlackGaussSeidel( float *srcFunction, float *slnFunction, int size, int numIterations) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/113", "date": "2025-07-30", "prompt": "Write a CUDA kernel to sort the elements in the array using merge sort method. Implement the kernel using two device functions, the first function should sort the elements within the blocks while the second function should merge all the sorted blocks in to a single sorted array.\n\nThe signature of the kernel is __global__ void k_mergeSort(float *input_d, float *sortedBlocks_d, float *output_d, int numElements), where input_d is the array which contains the elements to be sorted, sortedBlocks_d is the array which contains the sorted elements per block, output_d is the output array which contains the final sorted array elements of size numElements and numElements is the number of elements to be sorted.\n\nThe signature of the First device function is __device__ void d_mergeSortWithinBlock(float *input_d, float *sortedBlocks_d, int numElements), where input_d is the array which contains the elements to be sorted, sortedBlocks_d is the array which contains the sorted elements per block and numElements is the number of elements to be sorted. \n\nThe signature of the second device function is __device__ void d_mergeSortAcrossBlocks(float *sortedBlocks_d, float *output_d, int numElements), where sortedBlocks_d is the array which contains the intermediate sorted elements per block, output_d is the output array which contains the final sorted array elements of size numElements and numElements is the number of elements to be sorted. \n\n>>> k_mergeSort({25.0, 57.0, 2.0, 38.0, 49.0, 11.0, 79.0, 88.0, 5.0, 3.0}, sortedBlocks_d, output_d, 10) -> output_d: {2.0, 3.0, 5.0, 11.0, 25.0, 38.0, 49.0, 57.0, 79.0, 88.0}\n>>> d_mergeSortWithinBlock({25.0, 57.0, 2.0, 38.0, 49.0, 11.0, 79.0, 88.0, 5.0, 3.0}, sortedBlocks_d, 10) -> sortedBlocks_d: {2.0, 25.0, 38.0, 57.0, 11.0, 49.0, 79.0, 88.0, 3.0, 5.0}\n>>> d_mergeSortAcrossBlocks({2.0, 25.0, 38.0, 57.0, 11.0, 49.0, 79.0, 88.0, 3.0, 5.0}, output_d, 10) -> output_d: {2.0, 3.0, 5.0, 11.0, 25.0, 38.0, 49.0, 57.0, 79.0, 88.0}\n>>> k_mergeSort({17.0, 1.0, 15.0, 3.0, 18.0, 2.0, 11.0, 12.0}, sortedBlocks_d, output_d, 8) -> output_d: {1.0, 2.0, 3.0, 11.0, 12.0, 15.0, 17.0, 18.0}\n>>> d_mergeSortWithinBlock({17.0, 1.0, 15.0, 3.0, 18.0, 2.0, 11.0, 12.0}, sortedBlocks_d, 8) -> sortedBlocks_d: {1.0, 3.0, 15.0, 17.0, 2.0, 11.0, 12.0, 18.0}\n>>> d_mergeSortAcrossBlocks({1.0, 3.0, 15.0, 17.0, 2.0, 11.0, 12.0, 18.0}, output_d, 8) -> output_d: {1.0, 2.0, 3.0, 11.0, 12.0, 15.0, 17.0, 18.0}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <math.h>\n#include <algorithm>\n#include <assert.h>\n#include <float.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#undef NDEBUG\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n\n\n// Define the maximum number of threads per block (adjust as needed)\n#define MAX_THREADS 1024\n#define TOLERANCE   1e-2\n#define BLOCKSIZE   4\n#define MAXVALUE    9999\n\n// This device function divides the input array in to n blocks \n// and sorts the elements with in each blocks using merge sort method. This kernel uses shared memory to store the temporary sorted blocks\n__device__ void d_mergeSortWithinBlock(float *input_d, float *sortedBlocks_d, int numElements);\n\n// This device function merges the sorted blocks in to a single sorted array.\n__device__ void d_mergeSortAcrossBlocks(float *sortedBlocks_d, float *output_d, int numElements);\n\n// This kernel will sorts the elements using merge sort technique by calling two device functions d_mergeSortWithinBlock, d_mergeSortAcrossBlocks\n__global__ void k_mergeSort(float *input_d, float *sortedBlocks_d, float *output_d, int numElements);\n\nvoid launch() {\n    const int NUMTESTCASES = 7;\n    int numElements[NUMTESTCASES] = {6, 8, 9, 10, 11, 15, 7};       // Number of input elements in the list\n    int maxNumElements = *std::max_element(numElements, numElements + NUMTESTCASES);\n    // Host input and output arrays\n    float input_h[NUMTESTCASES][maxNumElements] ={  {6.0, 5.0, 4.0, 3.0, 2.0, 1.0},\n                                                    {17.0, 1.0, 15.0, 3.0, 18.0, 2.0, 11.0, 12.0},\n                                                    {5.0, 8.0, 2.0, 4.0, 10.0, 18.0, 9.0, 25.0, 1.0},\n                                                    {25.0, 57.0, 2.0, 38.0, 49.0, 11.0, 79.0, 88.0, 5.0, 3.0},\n                                                    {12.0, 32.0, 2.5, 1.3, 55.7, 38.2, 7.0, 15.5, 1.5, 22.5, 3.8},\n                                                    {125, 133, 145, 5.8, 38.3, 55.7, 125, 133, 77.5, 33.4, 55.7, 88.6, 77.5, 4.2, 2.0},\n                                                    {1.0, 2.0, 3.0, 1.0, 2.0, 3.0, 4.0}};\n    float expectedOutput_h[NUMTESTCASES][maxNumElements] = {{1.0, 2.0, 3.0, 4.0, 5.0, 6.0},\n                                                            {1.0, 2.0, 3.0, 11.0, 12.0, 15.0, 17.0, 18.0},\n                                                            {1.0, 2.0, 4.0, 5.0, 8.0, 9.0, 10.0, 18.0, 25.0},\n                                                            {2.0, 3.0, 5.0, 11.0, 25.0, 38.0, 49.0, 57.0, 79.0, 88.0},\n                                                            {1.3, 1.5, 2.5, 3.8, 7.0, 12.0, 15.5, 22.5, 32.0, 38.2, 55.7},\n                                                            {2.0, 4.2, 5.8, 33.4, 38.3, 55.7, 55.7, 77.5, 77.5, 88.6, 125, 125, 133, 133, 145},\n                                                            {1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0}};\n    float *output_h = (float *) calloc(maxNumElements, sizeof(float)); \n    \n    // Device input and output pointers\n    float *input_d;\n    float *output_d;\n    float *sortedBlocks_d;\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Allocate device memory\n    CUDA_CHECK(cudaMallocAsync((void**)&input_d, maxNumElements * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&output_d, maxNumElements * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&sortedBlocks_d, maxNumElements * sizeof(float), stream));\n\n    // Define block and grid sizes\n    int threadsPerBlock = BLOCKSIZE;\n    \n    // Blocks: (BLOCKSIZE, 1, 1)\n    dim3 blockSize(threadsPerBlock, 1, 1);\n    \n    // Grid: (ceil(maxNumElements / BLOCKSIZE), 1, 1)\n    dim3 gridSize(ceil(maxNumElements + blockSize.x - 1) / blockSize.x, 1, 1);\n\n    for(int tc = 0; tc < NUMTESTCASES; tc++){\n        // Copy input data to device\n        CUDA_CHECK(cudaMemcpyAsync(input_d, \n                                   input_h[tc], \n                                   numElements[tc] * sizeof(float), \n                                   cudaMemcpyHostToDevice, \n                                   stream));\n\n        // Launch the mergeSort kernel to sort the elements with in the block\n        void *args[] = {&input_d, &sortedBlocks_d, &output_d, &numElements[tc]};\n        CUDA_CHECK(cudaLaunchCooperativeKernel((void*)k_mergeSort, gridSize, blockSize, args, BLOCKSIZE * sizeof(float), stream));\n        \n        // Copy the output back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, \n                                   output_d, \n                                   numElements[tc] * sizeof(float), \n                                   cudaMemcpyDeviceToHost, \n                                   stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        //validate the results\n        for(int i = 0; i < numElements[tc]; i++) {\n            assert(fabs(output_h[i] - expectedOutput_h[tc][i]) < TOLERANCE);\n        }\n    }\n\n    // Free host and device memory\n    free(output_h);    \n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__device__ void d_mergeSortWithinBlock(float *input_d, float *sortedBlocks_d, int numElements) {\n    // Each block handles a portion of the array\n    int threadid = blockIdx.x * blockDim.x + threadIdx.x;\n    int blockSize = blockDim.x;\n\n    // Create shared memory for the block to work on\n    extern __shared__ float sharedData[];\n\n    // If the thread id is within bounds, load data into shared memory\n    if (threadid < numElements) {\n        sharedData[threadIdx.x] = input_d[threadid];\n    } else {\n        sharedData[threadIdx.x] = MAXVALUE;  // Fill with maximum value if out of bounds\n    }\n\n    // Synchronize threads in the block\n    __syncthreads();\n\n    // Perform the merge sort using shared memory\n    for (int divLen = 1; divLen <= blockSize / 2; divLen *= 2) {\n        // Thread ID within the block\n        int leftIdx = threadIdx.x * 2 * divLen + blockIdx.x * blockDim.x;\n        int blockStride = blockSize + blockIdx.x * blockDim.x;\n        int rightIdx = leftIdx + divLen;\n        int endIdx = leftIdx + 2 * divLen;\n\n        // Perform a merge of the two parts\n        if (leftIdx < blockStride) {\n            int i = leftIdx;\n            int j = rightIdx;\n\n            // Merge the elements\n            for (int k = leftIdx; k < endIdx; k++) {\n                float iValue = sharedData[i % blockSize];\n                float jValue = sharedData[j % blockSize];\n\n                // Merge in sorted order\n                if (i < rightIdx && (j >= endIdx || iValue <= jValue)) {\n                    sortedBlocks_d[k] = iValue;\n                    i++;\n                } else {\n                    sortedBlocks_d[k] = jValue;\n                    j++;\n                }\n            }\n        }\n\n        // Synchronize threads after each merge step\n        __syncthreads();\n\n        // Copy the sorted data from shared memory back to the output array\n        if (threadid < numElements) {\n            sharedData[threadIdx.x] = sortedBlocks_d[threadid];\n        }\n    }\n}\n\n__device__ void d_mergeSortAcrossBlocks(float *sortedBlocks_d, float *output_d, int numElements) {\n    auto grid = cooperative_groups::this_grid();\n    \n    //load the sortedBlocks in to the output\n    for(int idx = blockIdx.x * blockDim.x + threadIdx.x; idx < numElements; idx += gridDim.x * blockDim.x){\n        output_d[idx] = sortedBlocks_d[idx];\n    }\n    \n    // iterate each sorted block index from sortedBlocks_d and merge in to mergeSortedBlocks \n    for(int blockStride = blockDim.x; blockStride <= numElements; blockStride *= 2){\n        // calculate the left block start and end index.\n        int lblockStartIdx = 2 * blockIdx.x * blockStride;\n        int lblockEndIdx = min(numElements, blockStride + 2 * blockIdx.x * blockStride);\n        // calculate the right block start and end index.  \n        int rblockStartIdx = min(numElements, blockStride + 2 * blockIdx.x * blockStride);\n        int rblockEndIdx = min(numElements, blockStride * 2 + 2 * blockIdx.x * blockStride);\n        // initialize the left block and right block iterators \n        int i = lblockStartIdx, j = rblockStartIdx;\n        for(int k = lblockStartIdx; k < rblockEndIdx; k++){\n            // compare and merge the left blocks and right blocks data in to mergeSortedBlocks\n            if( i < lblockEndIdx && j < rblockEndIdx){\n                float lblockElement = output_d[i];\n                float rblockElement = output_d[j];\n                if(lblockElement < rblockElement){\n                    sortedBlocks_d[k] = lblockElement;\n                    i += 1;\n                } else {\n                    sortedBlocks_d[k] = rblockElement;\n                    j += 1;\n                }\n            } else if ( i < lblockEndIdx) {\n                float lblockElement = output_d[i]; \n                sortedBlocks_d[k] = lblockElement;\n                i += 1;\n            } else {\n                float rblockElement = output_d[j];\n                sortedBlocks_d[k] = rblockElement;\n                j += 1;\n            }\n        }\n\n        // synchronize all the threads in the grid after the merge of sorted block.\n        grid.sync();\n        \n        // copy the mergeSortedBlocks in to output array.\n        for(int idx = blockIdx.x * blockDim.x + threadIdx.x; idx < numElements; idx += gridDim.x * blockDim.x){\n            output_d[idx] = sortedBlocks_d[idx];\n        }\n        // synchronize all the threads in the grid after the copy of sortedblocks to output.\n        grid.sync();\n    }\n}\n\n// This module implements the merge sort by calling two device functions\n__global__ void k_mergeSort(float *input_d, float *sortedBlocks_d, float *output_d, int numElements){\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/114", "date": "2025-07-30", "prompt": "Write a cuda kernel to perform erosion operation over a binary image. Erosion operation needs to be performed for the given number of iterations. Outputs of each iteration need to be stored on device memory and reused across iterations. \n\nThe signature of the function is __global__ void k_imageErosion(const unsigned char *inputImage_d, const unsigned char *structuringElement_d, unsigned char *outputImage_d, unsigned char *outputImageIntermediateBuffer_d, int inputImageWidth, int inputImageHeight, int structuringElementWidth, int structuringElementHeight, int numberOfErosionIterations), where inputImage_d is a pointer to input image, structuringElement_d is a pointer to structuring element, outputImage_d is a pointer to output image, outputImageIntermediateBuffer_d is a pointer to the additional buffer that is used to store output image. The parameters inputImageWidth and inputImageHeight are the width and height of the input image, structuringElementWidth and structuringElementHeight are the width and height of the structuring element, numberOfErosionIterations specifies the number of iterations of erosion to be performed.\n\n>>> k_imageErosion({1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0},  {1,1,1,1,1,1,1,1,1}, outputImage_d, 5, 5, 3, 3) -> outputImage_d: ({1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0})\n>>> k_imageErosion({0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0},  {1,1,1,1,1,1,1,1,1}, outputImage_d, 5, 5, 3, 3) -> outputImage_d: ({0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <limits.h>\n#include <assert.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\nusing namespace cooperative_groups;\n\n#define MIN_IMAGE_ROW_INDEX 0 \n#define MIN_IMAGE_COLUMN_INDEX 0\n#define MIN_IMAGE_PIXEL_INDEX 0\n#define FOREGROUND_PIXEL 1\n#define FALSE 0\n#define SECOND_ITERATION 1\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n__global__ void k_imageErosion(const unsigned char *inputImage_d, const unsigned char *structuringElement_d, unsigned char *outputImage_d, unsigned char *outputImageIntermediateBuffer_d, int inputImageWidth, int inputImageHeight, int structuringElementWidth, int structuringElementHeight, int numberOfErosionIterations);\n\nvoid launch() {\n\n    //Initialize Constants\n    const int TEST_CASE_COUNT = 7;\n    const int MAX_INPUT_IMAGE_WIDTH = 9;\n    const int MAX_INPUT_IMAGE_HEIGHT = 9;\n    const int MAX_IMAGE_DIMENSIONS = 2;\n    const int IMAGE_HEIGHT_INDEX = 0;\n    const int IMAGE_WIDTH_INDEX = 1;\n    const int MIN_NUMBER_OF_THREADS_PER_BLOCK = 32;\n    const int MAX_NUMBER_OF_BLOCKS = 4;\n    \n    //Use CUDA Streams for Asynchronous Execution\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    //Allocate Device Memory\n    unsigned char *inputImage_d;\n    unsigned char *structuringElement_d;\n    unsigned char *outputImage_d;\n    unsigned char *outputImageIntermediateBuffer_d;\n    \n    CUDA_CHECK(cudaMallocAsync((void**)&inputImage_d, MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT * sizeof(unsigned char), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&structuringElement_d, MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT * sizeof(unsigned char), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&outputImage_d, MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT * sizeof(unsigned char), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&outputImageIntermediateBuffer_d, MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT * sizeof(unsigned char), stream));\n   \n    //Initialise Test Data\n    //Test Data Dimensions\n    int inputImageWidthHeight[TEST_CASE_COUNT][MAX_IMAGE_DIMENSIONS] = {\n      //Test Case - 1, {rows(height), columns(width)} \n      {4, 5},\n      //Test Case - 2\n      {5, 6},\n      //Test Case - 3\n      {6, 7},\n      //Test Case - 4\n      {7, 8},\n      //Test Case - 5\n      {8, 8},\n      //Test Case - 6\n      {9, 7},\n      //Test Case - 7\n      {9, 9}\n    };\n\n    int structuringElementWidthHeight[MAX_IMAGE_DIMENSIONS] = {3, 3};\n\n    //Input Data For Test\n    unsigned char inputImage_h[TEST_CASE_COUNT][MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT] = {\n      //Test Case - 1\n      {1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1},\n      //Test Case - 2\n      {0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 0},\n      //Test Case - 3 \n      {0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1},\n      //Test Case - 4 \n      {1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 1, 1, 1, 1, 0, 1},\n      //Test Case - 5 \n      {1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 0,\n       0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 0, 1, 1, 0, 1},\n      //Test Case - 6 \n      {1, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1},\n      //Test Case - 7 \n      {1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1}\n    };\n\n    //Expected Output for Test\n    unsigned char expectedOutputImage_h[TEST_CASE_COUNT][MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT] = {\n      //Test Case - 1 \n      {1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1},\n      //Test Case - 2 \n      {0, 0, 0, 1, 1, 1, \n       0, 0, 0, 1, 1, 1, \n       0, 0, 0, 0, 0, 0, \n       1, 1, 0, 0, 0, 0,\n       1, 1, 0, 0, 0, 0},\n      //Test Case - 3 \n      {0, 0, 0, 1, 1, 1, 1, \n       0, 0, 0, 0, 0, 0, 1, \n       0, 0, 0, 0, 0, 0, 1, \n       1, 0, 0, 0, 0, 0, 1, \n       1, 0, 0, 0, 0, 0, 1, \n       1, 0, 0, 0, 0, 0, 1},\n      //Test Case - 4 \n      {1, 1, 1, 1, 1, 1, 1, 1, \n       1, 1, 1, 1, 1, 1, 1, 1, \n       1, 1, 1, 1, 1, 1, 1, 1, \n       1, 1, 1, 1, 1, 1, 1, 1, \n       0, 0, 0, 1, 0, 0, 0, 0, \n       0, 0, 0, 1, 0, 0, 0, 0, \n       0, 0, 0, 1, 0, 0, 0, 0},\n      //Test Case - 5 \n      {1, 0, 0, 0, 0, 0, 0, 0, \n       1, 0, 0, 0, 0, 0, 0, 0, \n       1, 0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, 0},\n      //Test Case - 6 \n      {0, 0, 0, 0, 1, 1, 1, \n       0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, \n       1, 1, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0},\n      //Test Case - 7\n      {1, 0, 0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, 0, 0, \n       0, 0, 0, 0, 0, 0, 0, 0, 1, \n       0, 0, 0, 0, 0, 0, 1, 1, 1, \n       0, 0, 0, 0, 0, 0, 1, 1, 1, \n       0, 0, 0, 0, 0, 0, 1, 1, 1} \n    };\n\n    //Structuring Element\n    unsigned char structuringElement_h[MAX_INPUT_IMAGE_HEIGHT * MAX_INPUT_IMAGE_WIDTH] = {1, 1, 1, 1, 1, 1, 1, 1, 1};\n\n    \n    //Erosion Iterations\n    int erosionIterations[TEST_CASE_COUNT] = {1, 2, 2, 2, 2, 2, 3}; \n\n    //Output Image\n    unsigned char outputImage_h[MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT];\n\n    //Execute Test Cases\n    for (int testCase = 0; testCase < TEST_CASE_COUNT; testCase++){\n      int inputImageHeight = inputImageWidthHeight[testCase][IMAGE_HEIGHT_INDEX];  \n      int inputImageWidth = inputImageWidthHeight[testCase][IMAGE_WIDTH_INDEX];\n      int structuringElementHeight = structuringElementWidthHeight[IMAGE_HEIGHT_INDEX];\n      int structuringElementWidth = structuringElementWidthHeight[IMAGE_WIDTH_INDEX];\n      int numberOfErosionIterations = erosionIterations[testCase];\n      \n      //copy data from host to device\n      CUDA_CHECK(cudaMemcpyAsync(inputImage_d, inputImage_h[testCase], inputImageWidth * inputImageHeight * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n      CUDA_CHECK(cudaMemcpyAsync(structuringElement_d, structuringElement_h, structuringElementWidth * structuringElementHeight * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n      \n      //Set Kernel Configuration\n      int numThreadsPerBlock = MIN_NUMBER_OF_THREADS_PER_BLOCK;\n      if( ceil((float)(inputImageWidth * inputImageHeight) / numThreadsPerBlock) > MAX_NUMBER_OF_BLOCKS){\n          numThreadsPerBlock = ceil((float)(inputImageWidth * inputImageHeight) / MAX_NUMBER_OF_BLOCKS) ;\n      }\n\n      int numBlocks = ceil((float)(inputImageWidth * inputImageHeight) / numThreadsPerBlock);\n      dim3 block(numThreadsPerBlock, 1, 1);\n      dim3 grid(numBlocks, 1, 1);\n      \n      //Launch Kernel\n      // Grid:  ((inputImageWidth * inputImageHeight) / numThreadsPerBlock, 1, 1)\n      // Block: (32, 1, 1)\n      void *args[] = {&inputImage_d, &structuringElement_d, &outputImage_d, &outputImageIntermediateBuffer_d, &inputImageWidth, &inputImageHeight, &structuringElementWidth, &structuringElementHeight, &numberOfErosionIterations};\n      CUDA_CHECK(cudaLaunchCooperativeKernel((void*)k_imageErosion, grid, block, args, sizeof(unsigned char), stream));\n      \n      //Copy Data from device to host\n      CUDA_CHECK(cudaMemcpyAsync(outputImage_h, outputImage_d, inputImageWidth * inputImageHeight * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n      \n      //Sycnhronize tasks in the stream\n      CUDA_CHECK(cudaStreamSynchronize(stream));\n      \n      //Assert device output and expected output\n      for(int rowIndex = MIN_IMAGE_ROW_INDEX; rowIndex < inputImageHeight; rowIndex++) {\n        for(int columnIndex = MIN_IMAGE_COLUMN_INDEX; columnIndex < inputImageWidth; columnIndex++) {\n            int pixelIndex = rowIndex * inputImageWidth + columnIndex;\n            assert(outputImage_h[pixelIndex] == expectedOutputImage_h[testCase][pixelIndex]);\n        }\n      }\n    }\n    \n    //Deallocate Device Memory\n    CUDA_CHECK(cudaFreeAsync(inputImage_d, stream));\n    CUDA_CHECK(cudaFreeAsync(structuringElement_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputImage_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputImageIntermediateBuffer_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_imageErosion(const unsigned char *inputImage_d, const unsigned char *structuringElement_d, unsigned char *outputImage_d, unsigned char *outputImageIntermediateBuffer_d, int inputImageWidth, int inputImageHeight, int structuringElementWidth, int structuringElementHeight, int numberOfErosionIterations) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/115", "date": "2025-07-30", "prompt": "Create a CUDA kernel to simulate 2D wave propagation using the finite-difference method while utilizing device memory to access all intermediate states of the data. Implement reflective boundary conditions.\n\nThe signature of the CUDA kernel is __global__ void k_calculateWave2D(float *previousDisplacement_d, float *currentDisplacement_d, float *depth_d, float *previousTmp_d, float *currentTmp_d), where previousDisplacement_d is an array of previous time point states of the displacements, currentDisplacement_d is an array of current time point states of the displacements, depth_d is an array of depth values of each point that affect the speed of waves, previousTmp_d is an array for temporary storage, and currentTmp_d is another array for temporary storage.\n\n>>> k_calculateWave2D({\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.800000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f\n},\n{\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f\n},\n{\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f\n}) ->  currentDisplacement_d: {\n    0.000000f, 0.000000f, 0.000000f, 0.000011f, 0.000149f, 0.000835f, 0.000149f, 0.000011f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000009f, 0.000186f, 0.002062f, 0.009411f, 0.002062f, 0.000186f, 0.000009f, 0.000000f, 0.000000f,\n    0.000000f, 0.000009f, 0.000247f, 0.004107f, 0.036926f, 0.129077f, 0.036926f, 0.004107f, 0.000247f, 0.000009f, 0.000000f,\n    0.000011f, 0.000186f, 0.004107f, 0.055204f, 0.376970f, 0.893490f, 0.376969f, 0.055204f, 0.004107f, 0.000186f, 0.000011f,\n    0.000149f, 0.002062f, 0.036926f, 0.376970f, 1.713494f, 1.884681f, 1.713494f, 0.376970f, 0.036926f, 0.002062f, 0.000149f,\n    0.000835f, 0.009411f, 0.129077f, 0.893492f, 1.884684f, -1.106843f, 1.884684f, 0.893492f, 0.129077f, 0.009411f, 0.000835f,\n    0.000149f, 0.002062f, 0.036926f, 0.376976f, 1.713569f, 1.885096f, 1.713569f, 0.376976f, 0.036926f, 0.002062f, 0.000149f,\n    0.000011f, 0.000186f, 0.004116f, 0.055390f, 0.379030f, 0.902888f, 0.379030f, 0.055390f, 0.004116f, 0.000186f, 0.000011f,\n    0.000001f, 0.000019f, 0.000495f, 0.008213f, 0.073852f, 0.258153f, 0.073852f, 0.008213f, 0.000495f, 0.000019f, 0.000001f\n}\n\n>>> k_calculateWave2D({\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f\n},\n{\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f,\n    0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 1.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f, 0.000000f\n},\n{\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f,\n    1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f, 1.000000f\n}) ->  currentDisplacement_d: {\n    0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n    0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n    0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n    0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n    0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n    0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n    0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n    0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n    0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f\n}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Simulation-related constants.\nconstexpr int SIM_WIDTH = 11;\nconstexpr int SIM_HEIGHT = 9;\nconstexpr int SIM_SIZE_BYTES = SIM_WIDTH * SIM_HEIGHT * sizeof(float);\n// Distance between two neighbor points on the computed area for both dimensions.\nconstexpr float DELTA_DISTANCE = 0.5f;\n\n// CUDA-related constants.\nconstexpr int BLOCK_SIZE_X = 64;\nconstexpr int BLOCK_SIZE_Y = 4;\nconstexpr int GRID_SIZE_X = 3;\nconstexpr int GRID_SIZE_Y = 1;\nconstexpr int STRIDE_SIZE_X = BLOCK_SIZE_X * GRID_SIZE_X;\nconstexpr int STRIDE_SIZE_Y = BLOCK_SIZE_Y * GRID_SIZE_Y;\nconstexpr int REQUIRED_STRIDE_ITERATIONS_X = 1 + (SIM_WIDTH - 1) / STRIDE_SIZE_X;\nconstexpr int REQUIRED_STRIDE_ITERATIONS_Y = 1 + (SIM_HEIGHT - 1) / STRIDE_SIZE_Y;\n// Error tolerance for comparing floating-point variables.\nconstexpr float EPSILON = 0.001f;\n\n__global__ void k_calculateWave2D(  float *previousDisplacement_d, \n                                    float *currentDisplacement_d, \n                                    float *depth_d, \n                                    float *previousTmp_d, \n                                    float *currentTmp_d);\n\nvoid launch() {\n    // Arrays for simulation data.\n    float *depth_h;\n    float *currentDisplacement_h;\n    float *previousDisplacement_h;\n    float *depth_d;\n    float *currentDisplacement_d;\n    float *previousDisplacement_d;\n    // Temporary data arrays for preserving original input data during calculations.\n    float *previousTmp_d;\n    float *currentTmp_d;\n    cudaStream_t stream;\n\n    depth_h = new float[SIM_WIDTH * SIM_HEIGHT];\n    currentDisplacement_h = new float[SIM_WIDTH * SIM_HEIGHT];\n    previousDisplacement_h = new float[SIM_WIDTH * SIM_HEIGHT];\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    CUDA_CHECK(cudaMallocAsync(&depth_d, SIM_SIZE_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&currentDisplacement_d, SIM_SIZE_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&previousDisplacement_d, SIM_SIZE_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&currentTmp_d, SIM_SIZE_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&previousTmp_d, SIM_SIZE_BYTES, stream));\n\n    // Test 1: Pulse generation at point (5, 5).\n    {\n        for (int h = 0; h < SIM_HEIGHT; h++) {\n            for (int w = 0; w < SIM_WIDTH; w++) {\n                int index = w + h * SIM_WIDTH;\n                currentDisplacement_h[index] = 0.0f;\n                previousDisplacement_h[index] = 0.0f;\n                depth_h[index] = 1.0f;\n            }\n        }\n        currentDisplacement_h[5 + 5 * SIM_WIDTH] = 1.0f;\n        previousDisplacement_h[5 + 5 * SIM_WIDTH] = 0.8f;\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_d, \n                                    currentDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( previousDisplacement_d, \n                                    previousDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( depth_d, \n                                    depth_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        void * args[5] = {&previousDisplacement_d, &currentDisplacement_d, &depth_d, &previousTmp_d, &currentTmp_d };\n\n        // Grid: (3, 1, 1)\n        // Block: (64, 4, 1)\n        dim3 gridDim(GRID_SIZE_X, GRID_SIZE_Y, 1);\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y, 1);\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_calculateWave2D, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_h, \n                                    currentDisplacement_d, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedDisplacement[] = {\n            0.000000f, 0.000000f, 0.000000f, 0.000011f, 0.000149f, 0.000835f, 0.000149f, 0.000011f, 0.000000f, 0.000000f, 0.000000f,\n            0.000000f, 0.000000f, 0.000009f, 0.000186f, 0.002062f, 0.009411f, 0.002062f, 0.000186f, 0.000009f, 0.000000f, 0.000000f,\n            0.000000f, 0.000009f, 0.000247f, 0.004107f, 0.036926f, 0.129077f, 0.036926f, 0.004107f, 0.000247f, 0.000009f, 0.000000f,\n            0.000011f, 0.000186f, 0.004107f, 0.055204f, 0.376970f, 0.893490f, 0.376969f, 0.055204f, 0.004107f, 0.000186f, 0.000011f,\n            0.000149f, 0.002062f, 0.036926f, 0.376970f, 1.713494f, 1.884681f, 1.713494f, 0.376970f, 0.036926f, 0.002062f, 0.000149f,\n            0.000835f, 0.009411f, 0.129077f, 0.893492f, 1.884684f, -1.106843f, 1.884684f, 0.893492f, 0.129077f, 0.009411f, 0.000835f,\n            0.000149f, 0.002062f, 0.036926f, 0.376976f, 1.713569f, 1.885096f, 1.713569f, 0.376976f, 0.036926f, 0.002062f, 0.000149f,\n            0.000011f, 0.000186f, 0.004116f, 0.055390f, 0.379030f, 0.902888f, 0.379030f, 0.055390f, 0.004116f, 0.000186f, 0.000011f,\n            0.000001f, 0.000019f, 0.000495f, 0.008213f, 0.073852f, 0.258153f, 0.073852f, 0.008213f, 0.000495f, 0.000019f, 0.000001f\n        };\n\n        for(int i = 0; i < SIM_WIDTH * SIM_HEIGHT; i++) {\n            assert(fabs(expectedDisplacement[i] - currentDisplacement_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 2: Line-shaped wave generation at x = 5.\n    {\n        for (int h = 0; h < SIM_HEIGHT; h++) {\n            for (int w = 0; w < SIM_WIDTH; w++) {\n                int index = w + h * SIM_WIDTH;\n                currentDisplacement_h[index] = 0.0f;\n                previousDisplacement_h[index] = 0.0f;\n                depth_h[index] = 1.0f;\n            }\n        }\n        for(int i = 0; i < SIM_HEIGHT; i++) {\n            currentDisplacement_h[5 + i * SIM_WIDTH] = 1.0f;\n            previousDisplacement_h[5 + i * SIM_WIDTH] = 1.0f;\n        }\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_d, \n                                    currentDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( previousDisplacement_d, \n                                    previousDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( depth_d, \n                                    depth_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        void * args[5] = {&previousDisplacement_d, &currentDisplacement_d, &depth_d, &previousTmp_d, &currentTmp_d };\n\n        // Grid: (3, 1, 1)\n        // Block: (64, 4, 1)\n        dim3 gridDim(GRID_SIZE_X, GRID_SIZE_Y, 1);\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y, 1);\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_calculateWave2D, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_h, \n                                    currentDisplacement_d, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedDisplacement[] = {\n            0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n            0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n            0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n            0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n            0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n            0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n            0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n            0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f,\n            0.000407f, 0.004175f, 0.050254f, 0.284142f, 0.359115f, -0.395784f, 0.359115f, 0.284143f, 0.050254f, 0.004175f, 0.000407f\n        };\n\n        for(int i = 0; i < SIM_WIDTH * SIM_HEIGHT; i++) {\n            assert(fabs(expectedDisplacement[i] - currentDisplacement_h[i]) < EPSILON);\n        }\n    }\n    // Test 3: Superposition of two waves generated from center points (3, 3) and (5, 5).\n    {\n        for (int h = 0; h < SIM_HEIGHT; h++) {\n            for (int w = 0; w < SIM_WIDTH; w++) {\n                int index = w + h * SIM_WIDTH;\n                currentDisplacement_h[index] = 0.0f;\n                previousDisplacement_h[index] = 0.0f;\n                depth_h[index] = 1.0f;\n            }\n        }\n        currentDisplacement_h[3 + 3 * SIM_WIDTH] = 1.0f;\n        currentDisplacement_h[5 + 5 * SIM_WIDTH] = 1.0f;\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_d, \n                                    currentDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( previousDisplacement_d, \n                                    previousDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( depth_d, \n                                    depth_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        void * args[5] = {&previousDisplacement_d, &currentDisplacement_d, &depth_d, &previousTmp_d, &currentTmp_d };\n\n        // Grid: (3, 1, 1)\n        // Block: (64, 4, 1)\n        dim3 gridDim(GRID_SIZE_X, GRID_SIZE_Y, 1);\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y, 1);\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_calculateWave2D, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_h, \n                                    currentDisplacement_d, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedDisplacement[] = {\n            0.003420f, 0.030073f, 0.287207f, 1.076589f, 0.287718f, 0.033056f, 0.002225f, 0.000099f, 0.000003f, 0.000000f, 0.000000f,\n            0.030073f, 0.216047f, 1.584911f, 4.117199f, 1.592391f, 0.251925f, 0.022567f, 0.001285f, 0.000050f, 0.000001f, 0.000000f,\n            0.287207f, 1.584911f, 7.875597f, 10.093018f, 8.018077f, 2.115627f, 0.287205f, 0.022530f, 0.001112f, 0.000037f, 0.000002f,\n            1.076589f, 4.117199f, 10.093018f, -4.031080f, 11.653681f, 8.160119f, 2.115610f, 0.251240f, 0.016528f, 0.000685f, 0.000039f,\n            0.287718f, 1.592391f, 8.018077f, 11.653681f, 15.748361f, 11.653681f, 8.017787f, 1.584858f, 0.143859f, 0.007537f, 0.000515f,\n            0.033056f, 0.251925f, 2.115627f, 8.160119f, 11.653681f, -4.031123f, 10.091420f, 4.080718f, 0.538295f, 0.036522f, 0.003045f,\n            0.002225f, 0.022567f, 0.287205f, 2.115626f, 8.018075f, 10.093014f, 7.875318f, 1.577386f, 0.143604f, 0.007531f, 0.000515f,\n            0.000099f, 0.001286f, 0.022567f, 0.251924f, 1.592390f, 4.117192f, 1.584891f, 0.215405f, 0.015037f, 0.000643f, 0.000037f,\n            0.000006f, 0.000099f, 0.002225f, 0.033056f, 0.287719f, 1.076588f, 0.287205f, 0.030011f, 0.001710f, 0.000062f, 0.000003f\n        };\n\n        for(int i = 0; i < SIM_WIDTH * SIM_HEIGHT; i++) {\n            assert(fabs(expectedDisplacement[i] - currentDisplacement_h[i]) < EPSILON);\n        }\n    }\n    // Test 4: Point at (4, 4) with a positive displacement, surrounded by points with negative displacements.\n    {\n        for (int h = 0; h < SIM_HEIGHT; h++) {\n            for (int w = 0; w < SIM_WIDTH; w++) {\n                int index = w + h * SIM_WIDTH;\n                currentDisplacement_h[index] = 0.0f;\n                previousDisplacement_h[index] = 0.0f;\n                depth_h[index] = 1.0f;\n            }\n        }\n        currentDisplacement_h[4 + 4 * SIM_WIDTH] = 1.0f;\n        currentDisplacement_h[5 + 4 * SIM_WIDTH] = -1.0f;\n        currentDisplacement_h[3 + 4 * SIM_WIDTH] = -1.0f;\n        currentDisplacement_h[4 + 5 * SIM_WIDTH] = -1.0f;\n        currentDisplacement_h[4 + 3 * SIM_WIDTH] = -1.0f;\n\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_d, \n                                    currentDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( previousDisplacement_d, \n                                    previousDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( depth_d, \n                                    depth_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        void * args[5] = {&previousDisplacement_d, &currentDisplacement_d, &depth_d, &previousTmp_d, &currentTmp_d };\n\n        // Grid: (3, 1, 1)\n        // Block: (64, 4, 1)\n        dim3 gridDim(GRID_SIZE_X, GRID_SIZE_Y, 1);\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y, 1);\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_calculateWave2D, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_h, \n                                    currentDisplacement_d, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedDisplacement[] = {\n            -0.000244f, -0.002939f, -0.043876f, -0.346908f, -1.036736f, -0.346908f, -0.043876f, -0.002935f, -0.000122f, -0.000003f, -0.000000f,\n            -0.002939f, -0.029316f, -0.345737f, -2.003350f, -3.900983f, -2.003350f, -0.345736f, -0.029267f, -0.001469f, -0.000049f, -0.000002f,\n            -0.043876f, -0.345737f, -2.969965f, -10.735603f, -9.690741f, -10.735603f, -2.969943f, -0.344855f, -0.021938f, -0.000882f, -0.000049f,\n            -0.346908f, -2.003350f, -10.735603f, -15.433290f, -5.506227f, -15.433290f, -10.735305f, -1.994534f, -0.173454f, -0.008821f, -0.000590f,\n            -1.036736f, -3.900983f, -9.690741f, -5.506227f, -44.551655f, -5.506173f, -9.689205f, -3.865469f, -0.518370f, -0.035556f, -0.002985f,\n            -0.346908f, -2.003350f, -10.735603f, -15.433290f, -5.506227f, -15.433290f, -10.735305f, -1.994534f, -0.173454f, -0.008821f, -0.000590f,\n            -0.043876f, -0.345737f, -2.969965f, -10.735603f, -9.690741f, -10.735603f, -2.969938f, -0.344855f, -0.021938f, -0.000882f, -0.000049f,\n            -0.002939f, -0.029316f, -0.345737f, -2.003350f, -3.900983f, -2.003350f, -0.345736f, -0.029267f, -0.001469f, -0.000049f, -0.000002f,\n            -0.000244f, -0.002939f, -0.043876f, -0.346908f, -1.036736f, -0.346908f, -0.043876f, -0.002935f, -0.000122f, -0.000003f, -0.000000f\n        };\n\n        for(int i = 0; i < SIM_WIDTH * SIM_HEIGHT; i++) {\n            assert(fabs(expectedDisplacement[i] - currentDisplacement_h[i]) < EPSILON);\n        }\n    }\n    // Test 5: Refraction of a smooth wave among four mediums with increasing wave speeds.\n    {\n        for (int h = 0; h < SIM_HEIGHT; h++) {\n            for (int w = 0; w < SIM_WIDTH; w++) {\n                int index = w + h * SIM_WIDTH;\n                currentDisplacement_h[index] = 0.0f;\n                previousDisplacement_h[index] = 0.0f;\n                depth_h[index] = 1.0f;\n            }\n        }\n        for(int h = 0; h < SIM_HEIGHT; h++) {\n            for(int w = 0; w < SIM_WIDTH; w++) {\n                depth_h[w + h * SIM_WIDTH] = 1 + (w / (SIM_WIDTH / 4));\n            }\n        }\n        // Generating a smooth wave pattern centered at point (4, 4).\n        int size = 40;\n        int pointIdxX = 4;\n        int pointIdxY = 4;\n        constexpr float SIGMA = 10.0f * DELTA_DISTANCE;\n        constexpr float AMPLITUDE = 0.01f;\n        for (int i = -size; i <= size ; i++) {\n            for (int j = -size; j <= size; j++) {\n                float dx = j;\n                float dy = i;\n                if(i + pointIdxY >= 0 && i + pointIdxY < SIM_HEIGHT && j + pointIdxX >= 0 && j + pointIdxX < SIM_WIDTH) {\n                    float value = AMPLITUDE * exp(-(dx * dx + dy * dy) / (2.0f * SIGMA * SIGMA));\n                    currentDisplacement_h[(i + pointIdxY) * SIM_WIDTH + j + pointIdxX] += value;\n                    previousDisplacement_h[(i + pointIdxY) * SIM_WIDTH + j + pointIdxX] += value * 0.5f;\n                }\n            }\n        }\n        \n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_d, \n                                    currentDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( previousDisplacement_d, \n                                    previousDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( depth_d, \n                                    depth_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        void * args[5] = {&previousDisplacement_d, &currentDisplacement_d, &depth_d, &previousTmp_d, &currentTmp_d };\n\n        // Grid: (3, 1, 1)\n        // Block: (64, 4, 1)\n        dim3 gridDim(GRID_SIZE_X, GRID_SIZE_Y, 1);\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y, 1);\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_calculateWave2D, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_h, \n                                    currentDisplacement_d, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedDisplacement[] = {\n            0.347893f, 0.359615f, 0.384232f, 0.397982f, 0.400169f, 0.404647f, 0.407961f, 0.412704f, 0.414962f, 0.417527f, 0.417582f,\n            0.354402f, 0.362559f, 0.387474f, 0.399336f, 0.399395f, 0.403911f, 0.407326f, 0.412696f, 0.415485f, 0.418194f, 0.418206f,\n            0.379538f, 0.382708f, 0.392573f, 0.396739f, 0.395269f, 0.400201f, 0.407232f, 0.414392f, 0.416941f, 0.419505f, 0.419513f,\n            0.400508f, 0.401647f, 0.397311f, 0.391400f, 0.391836f, 0.399661f, 0.407328f, 0.415021f, 0.417777f, 0.420465f, 0.420582f,\n            0.408065f, 0.408764f, 0.399859f, 0.389530f, 0.390155f, 0.399948f, 0.407173f, 0.414455f, 0.417898f, 0.421080f, 0.421181f,\n            0.400508f, 0.401647f, 0.397311f, 0.391400f, 0.391836f, 0.399661f, 0.407328f, 0.415021f, 0.417777f, 0.420465f, 0.420582f,\n            0.379538f, 0.382708f, 0.392573f, 0.396739f, 0.395269f, 0.400201f, 0.407232f, 0.414392f, 0.416941f, 0.419505f, 0.419513f,\n            0.354402f, 0.362559f, 0.387474f, 0.399336f, 0.399395f, 0.403911f, 0.407326f, 0.412696f, 0.415485f, 0.418194f, 0.418206f,\n            0.347893f, 0.359615f, 0.384232f, 0.397982f, 0.400169f, 0.404647f, 0.407961f, 0.412704f, 0.414962f, 0.417527f, 0.417582f\n        };\n\n        for(int i = 0; i < SIM_WIDTH * SIM_HEIGHT; i++) {\n            assert(fabs(expectedDisplacement[i] - currentDisplacement_h[i]) < EPSILON);\n        }\n    }\n    // Test 6: Standing waves within a uniform medium.\n    {\n        for (int h = 0; h < SIM_HEIGHT; h++) {\n            for (int w = 0; w < SIM_WIDTH; w++) {\n                int index = w + h * SIM_WIDTH;\n                currentDisplacement_h[index] = 0.0f;\n                previousDisplacement_h[index] = 0.0f;\n                depth_h[index] = 1.0f;\n            }\n        }\n        float PI = acos(-1);\n        for (int h = 0; h < SIM_HEIGHT; h++) {\n            for (int w = 0; w < SIM_WIDTH; w++) {\n                depth_h[w + h * SIM_WIDTH] = 10.0f;\n                float xComponent = sin(w * 2.0f * PI / SIM_WIDTH);\n                float yComponent = sin(h * 2.0f * PI / SIM_HEIGHT);\n                currentDisplacement_h[w + h * SIM_WIDTH] = xComponent * yComponent;\n            }\n        }\n        \n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_d, \n                                    currentDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( previousDisplacement_d, \n                                    previousDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( depth_d, \n                                    depth_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        void * args[5] = {&previousDisplacement_d, &currentDisplacement_d, &depth_d, &previousTmp_d, &currentTmp_d };\n\n        // Grid: (3, 1, 1)\n        // Block: (64, 4, 1)\n        dim3 gridDim(GRID_SIZE_X, GRID_SIZE_Y, 1);\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y, 1);\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_calculateWave2D, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_h, \n                                    currentDisplacement_d, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedDisplacement[] = {\n            -4.565512f, -3.878747f, -1.352703f, 1.596967f, 2.593991f, 1.282479f, -0.361835f, -0.308868f, 1.661168f, 3.383909f, 3.707297f,\n            -4.873705f, -4.317383f, -2.096274f, 0.854752f, 1.993256f, 1.196660f, 0.096432f, 0.378078f, 2.431641f, 3.895500f, 4.085439f,\n            -5.979319f, -5.596576f, -3.891480f, -1.127063f, 0.685319f, 1.363383f, 1.661052f, 2.383622f, 3.897503f, 4.545934f, 4.444734f,\n            -4.805110f, -4.710963f, -3.891645f, -2.132775f, -0.293345f, 1.234980f, 2.387169f, 3.203460f, 3.713772f, 3.467847f, 3.112700f,\n            -1.215967f, -1.188277f, -1.145949f, -1.144635f, -0.479651f, 0.509417f, 1.352975f, 1.630776f, 1.140669f, 0.806318f, 0.720348f,\n            3.029806f, 2.984275f, 2.244284f, 0.528569f, -0.373716f, -0.489784f, -0.425310f, -0.816507f, -1.924535f, -2.082408f, -1.822046f,\n            6.205341f, 5.778104f, 4.005559f, 1.235420f, -0.550474f, -1.149731f, -1.375208f, -2.076431f, -3.577414f, -4.269567f, -4.205656f,\n            7.398588f, 6.672131f, 4.149539f, 1.107315f, -1.019596f, -1.528867f, -1.543732f, -2.293873f, -3.737435f, -4.954867f, -5.159369f,\n            8.097583f, 7.194371f, 4.261105f, 1.179894f, -1.265283f, -1.851095f, -1.825434f, -2.580025f, -3.684251f, -5.034657f, -5.354953f\n        };\n\n        for(int i = 0; i < SIM_WIDTH * SIM_HEIGHT; i++) {\n            assert(fabs(expectedDisplacement[i] - currentDisplacement_h[i]) < EPSILON);\n        }\n    }\n    // Test 7: Refraction through a depth gradient.\n    {\n        for (int h = 0; h < SIM_HEIGHT; h++) {\n            for (int w = 0; w < SIM_WIDTH; w++) {\n                int index = w + h * SIM_WIDTH;\n                currentDisplacement_h[index] = 0.0f;\n                previousDisplacement_h[index] = 0.0f;\n                depth_h[index] = 1.0f;\n            }\n        }\n        for (int h = 0; h < SIM_HEIGHT; h++) {\n            for (int w = 0; w < SIM_WIDTH; w++) {\n                depth_h[w + h * SIM_WIDTH] = 1 + 3.0f * (w / (float)SIM_WIDTH);\n            }\n        }\n        currentDisplacement_h[4 + 4 * SIM_WIDTH] = 1.0f;\n\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_d, \n                                    currentDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( previousDisplacement_d, \n                                    previousDisplacement_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( depth_d, \n                                    depth_h, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        void * args[5] = {&previousDisplacement_d, &currentDisplacement_d, &depth_d, &previousTmp_d, &currentTmp_d };\n        // Grid: (3, 1, 1)\n        // Block: (64, 4, 1)\n        dim3 gridDim(GRID_SIZE_X, GRID_SIZE_Y, 1);\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y, 1);\n        CUDA_CHECK(cudaLaunchCooperativeKernel( (void*)k_calculateWave2D, \n                                                gridDim, \n                                                blockDim, \n                                                args, \n                                                0, \n                                                stream));\n        CUDA_CHECK(cudaMemcpyAsync( currentDisplacement_h, \n                                    currentDisplacement_d, \n                                    SIM_SIZE_BYTES, \n                                    cudaMemcpyDeviceToHost, \n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        float expectedDisplacement[] = {\n            0.001134f, 0.022817f, 0.330711f, 1.576588f, 2.439878f, 3.204626f, 2.912915f, 1.867793f, 0.914622f, 0.384623f, 0.236438f,\n            0.006071f, 0.089629f, 0.878308f, 2.429212f, 2.309972f, 2.962719f, 3.398280f, 2.602556f, 1.433267f, 0.656104f, 0.420436f,\n            0.042618f, 0.428976f, 2.411370f, 2.028470f, 0.929674f, -0.156735f, 1.980986f, 3.070216f, 2.306219f, 1.271462f, 0.887846f,\n            0.174350f, 1.099788f, 2.657410f, -1.640502f, 2.095705f, -0.829313f, -0.723821f, 1.726273f, 2.314209f, 1.654315f, 1.288093f,\n            0.320152f, 1.235732f, 1.381921f, 1.126383f, 1.762368f, 2.024709f, 1.159134f, 2.069403f, 2.332343f, 1.727152f, 1.389491f,\n            0.174350f, 1.099788f, 2.657410f, -1.640502f, 2.095705f, -0.829310f, -0.723821f, 1.726273f, 2.314210f, 1.654315f, 1.288092f,\n            0.042618f, 0.428976f, 2.411370f, 2.028470f, 0.929674f, -0.156735f, 1.980986f, 3.070216f, 2.306216f, 1.271461f, 0.887846f,\n            0.006071f, 0.089629f, 0.878308f, 2.429213f, 2.309974f, 2.962719f, 3.398280f, 2.602556f, 1.433266f, 0.656103f, 0.420436f,\n            0.001134f, 0.022817f, 0.330712f, 1.576589f, 2.439879f, 3.204626f, 2.912915f, 1.867793f, 0.914622f, 0.384623f, 0.236438f\n        };\n\n        for(int i = 0; i < SIM_WIDTH * SIM_HEIGHT; i++) {\n            assert(fabs(expectedDisplacement[i] - currentDisplacement_h[i]) < EPSILON);\n        }\n    }\n\n    CUDA_CHECK(cudaFreeAsync(depth_d, stream));\n    CUDA_CHECK(cudaFreeAsync(currentDisplacement_d, stream));\n    CUDA_CHECK(cudaFreeAsync(previousDisplacement_d, stream));\n    CUDA_CHECK(cudaFreeAsync(previousTmp_d, stream));\n    CUDA_CHECK(cudaFreeAsync(currentTmp_d, stream));\n    // Deleting host arrays asynchronously to the freeing of device arrays.\n    delete [] depth_h;\n    delete [] currentDisplacement_h;\n    delete [] previousDisplacement_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_calculateWave2D(  float *previousDisplacement_d, \n                                    float *currentDisplacement_d, \n                                    float *depth_d, \n                                    float *previousTmp_d, \n                                    float *currentTmp_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/116", "date": "2025-07-30", "prompt": "Write a cuda kernel to find the number of occurrences of a pattern in a sequence where each thread checks for the pattern in the input sequence in parallel and utilizes the Atomic function to aggregate the count value. \n\nThe signature of the function is __global__ void k_patternMatch(char* sequence_d, char* pattern_d, int sequenceLength, int patternLength, int* count_d), where sequence refers to the input string, pattern is the string being searched within sequence, sequenceLength and patternLength represent the lengths of sequence and pattern respectively, and count_d denotes the number of times the pattern appears in the sequence.\n\n>>> k_patternMatch(\"ATCGTGATCGAAGCCT\", \"ATC\", 16, 3, count) -> count: 2\n>>> k_patternMatch(\"ATCGTGATTGAAGCCT\", \"ATCGA\", 16, 5, count) -> count: 0 \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n#include <limits>\n#include <string>\n#include <cstdio>\n#include <assert.h>\n#include <cuda.h>\n#include <curand_kernel.h>\n#include <cuda_runtime.h>\n\n#undef NDEBUG\n\nconst int THREADS_PER_BLOCK = 256;\n\n#define CUDA_CHECK(call)                                    \\\ndo {                                                        \\\n    cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                         \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",   \\\n                    __FILE__, __LINE__,                     \\\n                    cudaGetErrorString(error));             \\\n            exit(EXIT_FAILURE);                             \\\n    }                                                       \\\n} while (0)\n\n__global__ void k_patternMatch(char* sequence_d, char* pattern_d, int sequenceLength, int patternLength, int* count_d);\n\nvoid launch() {\n    const int testCaseCount = 7;\n\n    struct testCase {\n        std::string sequence;\n        std::string pattern;\n    };\n    \n    //Input sequences and patterns for testing\n    testCase input[] = {\n        {\"ATCGTGATCGAAGCCT\",\"ATC\"},\n        {\"ATCGTGATCGAAGCCT\",\"ATCG\"},\n        {\"ATGTGATGGAATGCT\",\"ATG\"},\n        {\"Thisisthesequence\",\"the\"},\n        {\"Thisisthesequence\",\"is\"},\n        {\"ATCGTGATTGAAGCCT\",\"ATCGA\"},\n        {\"Thisisthesequence\",\"th\"},\n    };\n\n    //Expected output\n    unsigned int expectedFrequency[] = {2,2,3,1,2,0,1};\n    \n    int maxSequenceLength = 0;\n    int maxPatternLength = 0;\n\n    for (int t = 0; t < testCaseCount; t++) {\n        std::string sequence = input[t].sequence;\n        std::string pattern = input[t].pattern;\n        int sequenceLength = sequence.length();\n        maxSequenceLength += sequenceLength;\n        int patternLength = pattern.length();\n        maxPatternLength += patternLength;\n    }\n\n    //Creating cuda streams\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    //Device Memory Allocation\n    int *count_d;\n    char *sequence_d, *pattern_d;\n    int* count_h = (int*)malloc(sizeof(int));\n    CUDA_CHECK(cudaMallocAsync((void **)&sequence_d, maxSequenceLength*sizeof(char), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&pattern_d, maxPatternLength*sizeof(char), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&count_d, sizeof(int), stream));\n\n    // Fetch GPU properties\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, 0);\n    int maxBlocks = (prop.maxThreadsPerMultiProcessor / THREADS_PER_BLOCK) * prop.multiProcessorCount;\n\n    //Declaration of test sequence and pattern\n    for (int i = 0; i < testCaseCount; i++) {\n        std::string sequence = input[i].sequence;\n        std::string pattern = input[i].pattern;\n        int sequenceLength = sequence.length();\n        int patternLength = pattern.length();\n\n        // Set the grid dimension to the minimum of needed blocks or GPU max blocks\n        int blocksPerGrid = min(maxBlocks, (sequenceLength + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK);\n              \n        // Copy sequences to device\n        CUDA_CHECK(cudaMemcpyAsync(sequence_d, sequence.c_str(), sequenceLength * sizeof(char), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(pattern_d, pattern.c_str(), patternLength * sizeof(char), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemsetAsync(count_d, 0, sizeof(int), stream));\n\n        //Launch the kernel function\n        //Grid: (sequenceLength + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK), 1, 1)\n        //Block: (THREADS_PER_BLOCK, 1, 1)\n        void *args[] = {&sequence_d, &pattern_d, &sequenceLength, &patternLength, &count_d};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_patternMatch, blocksPerGrid, THREADS_PER_BLOCK, args, THREADS_PER_BLOCK * sizeof(int), stream));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(count_h, count_d, sizeof(int), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        //Validate the results with the expected value\n        assert(expectedFrequency[i] == (*count_h)); \n    }\n\n    //Free Host and Device memory\n    free(count_h);\n    CUDA_CHECK(cudaFreeAsync(sequence_d, stream));\n    CUDA_CHECK(cudaFreeAsync(pattern_d, stream));\n    CUDA_CHECK(cudaFreeAsync(count_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_patternMatch(char* sequence_d, char* pattern_d, int sequenceLength, int patternLength, int* count_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/117", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform Sparse Matrix-Vector Multiplication (SpMV) using the Compressed Sparse Row (CSR) format with shared memory optimizations. The kernel should minimize global memory accesses and maximize multiprocessor utilization.\n\nThe k_spmvCsrOptimized kernel should have the signature: __global__ void k_spmvCsrOptimized(float* values, int* colIndices, int* rowPtr, float* x, float* y, int numRows). Here, values contains non-zero elements, colIndices stores column indices, rowPtr marks row boundaries, x is the input vector, y is the output vector, and numRows is the matrix dimension.\n\n>>> k_spmvCsrOptimized(values:{1.0f, 2.0f, 3.0f}, colIndices:{0, 1, 2}, rowPtr:{0, 1, 2, 3}, x:{1.0f, 2.0f, 3.0f}, y, numRows:{3}) -> y:{1.0f, 4.0f, 9.0f}\n>>> k_spmvCsrOptimized(values:{1.0f, 4.0f, 2.0f, 5.0f, 3.0f, 6.0f}, colIndices:{0, 2, 1, 3, 0, 1}, rowPtr:{0, 2, 4, 6, 6}, x:{1.0f, 2.0f, 3.0f, 4.0f}, y, numRows:{4}) -> y:{13.0f, 24.0f, 15.0f, 0.0f}\n", "cc_flags": "-arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <cmath>\n#include <cooperative_groups.h>\n#include <cstdio>\n#include <cstdlib>\n#include <cuda_runtime.h>\n#include <iostream>\n#include <vector>\n\nnamespace cg = cooperative_groups;\n\nconst float TOLERANCE = 1e-5f;\n\n#define CUDA_CHECK(call)                                                                           \\\n    do {                                                                                           \\\n        cudaError_t error = call;                                                                  \\\n        if(error != cudaSuccess) {                                                                 \\\n            fprintf(stderr,                                                                        \\\n                    \"CUDA Error: %s at %s:%d\\n\",                                                   \\\n                    cudaGetErrorString(error),                                                     \\\n                    __FILE__,                                                                      \\\n                    __LINE__);                                                                     \\\n            exit(error);                                                                           \\\n        }                                                                                          \\\n    } while(0)\n\n__global__ void k_spmvCsrOptimized(float *values_d,   // Non-zero elements array\n                                   int *colIndices_d, // Column indices array\n                                   int *rowPtr_d,     // Row pointers array\n                                   float *x_d,        // Input vector\n                                   float *y_d,              // Output vector\n                                   int numRows);            // Number of rows in the matrix\n\nstruct SpmvTestCase {\n    std::vector<float> values;    // Non-zero elements\n    std::vector<int> colIndices;  // Column indices\n    std::vector<int> rowPtr;      // Row pointers\n    std::vector<float> x;         // Input vector\n    std::vector<float> expectedY; // Expected output vector\n    int numRows;                  // Number of rows in the matrix\n};\n\nstd::vector<SpmvTestCase> testCases = {\n    // Test case 0: 3x3 diagonal matrix\n    {{1.0f, 2.0f, 3.0f}, {0, 1, 2}, {0, 1, 2, 3}, {1.0f, 2.0f, 3.0f}, {1.0f, 4.0f, 9.0f}, 3},\n    // Test case 1: 4x4 complex sparsity\n    {{1.0f, 4.0f, 2.0f, 5.0f, 3.0f, 6.0f},\n     {0, 2, 1, 3, 0, 1},\n     {0, 2, 4, 6, 6},\n     {1.0f, 2.0f, 3.0f, 4.0f},\n     {13.0f, 24.0f, 15.0f, 0.0f},\n     4},\n    // Test case 2: 4x4 diagonal matrix\n    {{1.0f, 2.0f, 3.0f, 4.0f},\n     {0, 1, 2, 3},\n     {0, 1, 2, 3, 4},\n     {2.0f, 3.0f, 4.0f, 5.0f},\n     {2.0f, 6.0f, 12.0f, 20.0f},\n     4},\n    // Test case 3: Upper triangular matrix\n    {{1.0f, 2.0f, 3.0f, 4.0f},\n     {0, 1, 2, 3},\n     {0, 1, 2, 3, 4},\n     {1.0f, 1.0f, 1.0f, 1.0f},\n     {1.0f, 2.0f, 3.0f, 4.0f},\n     4},\n    // Test case 4: Large sparse matrix\n    {{1.0f, 2.0f, 3.0f},\n     {0, 2, 3},\n     {0, 1, 2, 2, 3},\n     {1.0f, 2.0f, 3.0f, 4.0f},\n     {1.0f, 6.0f, 0.0f, 12.0f},\n     4},\n    // Test case 5: Symmetric matrix\n    {{1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f},\n     {0, 1, 1, 2, 0, 2},\n     {0, 2, 4, 6},\n     {1.0f, 2.0f, 3.0f},\n     {5.0f, 18.0f, 23.0f},\n     3},\n    // Test case 6: Single row matrix\n    {{1.0f, 2.0f, 3.0f}, \n      {0, 1, 2}, \n      {0, 3}, \n      {1.0f, 2.0f, 3.0f}, \n      {14.0f}, 1}};\n\nvoid launch() {\n    constexpr int BLOCK_SIZE = 256;   // 8 warps per block\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Get device properties for occupancy calculation\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n    int numberOfMultiProcessors = prop.multiProcessorCount;\n\n    const int rowsPerBlock = BLOCK_SIZE / 32; // 8 rows per block\n    const int sharedMemSizePerBlock = rowsPerBlock * 2 * sizeof(int); // 64 bytes\n\n    // Calculate occupancy-based max blocks per multiprocessor\n    int numberOfBlocksPerMultiProcessor;\n    CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n        &numberOfBlocksPerMultiProcessor,\n        k_spmvCsrOptimized,\n        BLOCK_SIZE,\n        sharedMemSizePerBlock));\n\n    int maxBlocks = numberOfMultiProcessors * numberOfBlocksPerMultiProcessor;\n\n    // Find maximum buffer dimensions across all test cases\n    size_t maxValuesSize = 0, maxColIndicesSize = 0, maxRowPtrSize = 0, maxXSize = 0;\n    int maxNumRows = 0;\n    for(const auto &tc : testCases) {\n        maxValuesSize = std::max(maxValuesSize, tc.values.size());\n        maxColIndicesSize = std::max(maxColIndicesSize, tc.colIndices.size());\n        maxRowPtrSize = std::max(maxRowPtrSize, tc.rowPtr.size());\n        maxXSize = std::max(maxXSize, tc.x.size());\n        maxNumRows = std::max(maxNumRows, tc.numRows);\n    }\n\n    // Allocate unified buffers once\n    float *values_d, *x_d, *y_d;\n    int *colIndices_d, *rowPtr_d;\n\n    CUDA_CHECK(cudaMallocAsync(&values_d, maxValuesSize * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&colIndices_d, maxColIndicesSize * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&rowPtr_d, maxRowPtrSize * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&x_d, maxXSize * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&y_d, maxNumRows * sizeof(float), stream));\n\n    for(const auto &tc : testCases) {\n        // Copy data to pre-allocated buffers\n        CUDA_CHECK(cudaMemcpyAsync(values_d,\n                                   tc.values.data(),\n                                   tc.values.size() * sizeof(float),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(colIndices_d,\n                                   tc.colIndices.data(),\n                                   tc.colIndices.size() * sizeof(int),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(rowPtr_d,\n                                   tc.rowPtr.data(),\n                                   tc.rowPtr.size() * sizeof(int),\n                                   cudaMemcpyHostToDevice,\n                                   stream));\n        CUDA_CHECK(cudaMemcpyAsync(\n            x_d, tc.x.data(), tc.x.size() * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemsetAsync(y_d, 0, tc.numRows * sizeof(float), stream));\n\n        // Compute grid size based on occupancy\n        int desiredGridSize = (tc.numRows + rowsPerBlock - 1) / rowsPerBlock;\n        int gridSize = std::min(desiredGridSize, maxBlocks);\n        // Ensure gridSize is at least the number of multiprocessors but does not exceed occupancy limits\n        gridSize = std::max(desiredGridSize, numberOfMultiProcessors);\n        gridSize = std::min(gridSize, maxBlocks);\n\n        // Launch optimized kernel\n        int numRows = tc.numRows;\n        void *args[] = {&values_d, &colIndices_d, &rowPtr_d, &x_d, &y_d, &numRows};\n        CUDA_CHECK(cudaLaunchKernel(\n            (void *)k_spmvCsrOptimized, gridSize, BLOCK_SIZE, args, sharedMemSizePerBlock, stream));\n\n        // Verify results\n        std::vector<float> y_h(tc.numRows);\n        CUDA_CHECK(cudaMemcpyAsync(\n            y_h.data(), y_d, tc.numRows * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < tc.numRows; ++i) {\n            assert(fabs(y_h[i] - tc.expectedY[i]) < TOLERANCE); // Correctness condition check\n        }\n    }\n\n    // Single cleanup after all test cases\n    CUDA_CHECK(cudaFreeAsync(values_d, stream));\n    CUDA_CHECK(cudaFreeAsync(colIndices_d, stream));\n    CUDA_CHECK(cudaFreeAsync(rowPtr_d, stream));\n    CUDA_CHECK(cudaFreeAsync(x_d, stream));\n    CUDA_CHECK(cudaFreeAsync(y_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n\n__global__ void k_spmvCsrOptimized(float *values_d,   // Non-zero elements array\n                                   int *colIndices_d, // Column indices array\n                                   int *rowPtr_d,     // Row pointers array\n                                   float *x_d,        // Input vector\n                                   float *y_d,        // Output vector\n                                   int numRows) {     // Number of rows in the matrix\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/118", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute the total angular momentum of a system of particles using a warp parallel reduction. Each thread should be responsible for computing the angular momentum contributed by one particle.\n\nThe kernel should have the following signature is __global__ void k_computeAngularMomentum(const float *mass_d, const float3 *pos_d, const float3 *vel_d, float3 *totalAM_d, unsigned int particleCount), where mass_d is an array of particle masses, pos_d is an array of float3 containing particle positions with respect to the origin, vel_d is an array of float3 containing particle velocities, totalAM_d is a pointer to a single float3 in global memory where the total angular momentum (Lx, Ly, Lz) is stored, and particleCount is the total number of particles in the system.\n\n>>> k_computeAngularMomentum(mass_d:{4.370861e+00f, 9.556429e+00f},\n                             pos_d:{{2.319939e+00f, 9.865848e-01f, -3.439814e+00f}, {-3.440055e+00f, -4.419164e+00f, 3.661761e+00f}},\n                             vel_d:{{2.022300e-01f, 4.161452e-01f, -9.588310e-01f}, {9.398197e-01f, 6.648853e-01f, -5.753218e-01f}},\n                             totalAM_d:{0.0f, 0.0f, 0.0f},\n                             particleCount:2) -> totalAM_d: {3.152112e+00, 2.065611e+01, 2.117977e+01}\n\n>>> k_computeAngularMomentum(mass_d:{7.752083e+00f, 2.799273e+00f},\n                             pos_d:{{-1.642612e+00f, -3.415288e+00f, -1.467934e+00f}, {-1.489164e+00f, -3.154448e+00f, -2.816532e-02f}},\n                             vel_d:{{-4.435090e-01f, -6.504617e-01f, 3.839987e-01f}, {-7.099691e-01f, -5.235986e-01f, -7.185734e-01f}},\n                             totalAM_d:{0.0f, 0.0f, 0.0f},\n                             particleCount:2) -> totalAM_d: {-1.126471e+01, 6.997188e+00, -7.545882e+00}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n#include <stdio.h>\n#include <stdlib.h>\n#include <cassert>\n\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                                                                   \\\ndo {                                                                                       \\\n    cudaError_t error = call;                                                              \\\n    if(error != cudaSuccess) {                                                             \\\n        fprintf(stderr,                                                                    \\\n            \"CUDA Error: %s at %s:%d\\n\",                                                   \\\n            cudaGetErrorString(error),                                                     \\\n            __FILE__,                                                                      \\\n            __LINE__);                                                                     \\\n        exit(error);                                                                       \\\n    }                                                                                      \\\n} while(0)\n\n//CUDA Kernel to compute total angular momentum using warp-level reduction signature\n__global__ void k_computeAngularMomentum(const float *mass_d, const float3 *pos_d, const float3 *vel_d, float3 *totalAM_d, unsigned int particleCount);\n\nvoid launch() {\n    // Test case constant params\n    const unsigned int NUM_TEST_CASES = 7;\n    const unsigned int MAX_PARTICLE_COUNT = 128;\n    const unsigned int BLOCK_SIZE = 256;\n    const float TOL = 1e-4f; // For floating point validation\n    const float3 ZERO_VEC = {0.0f, 0.0f, 0.0f}; // For initializing totalAM_d\n\n    //Declaring grid size using CUDA device properties\n    cudaDeviceProp deviceProp;\n    int currentDevice;\n    CUDA_CHECK(cudaGetDevice(&currentDevice));\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, currentDevice));\n\n    int numSMs = deviceProp.multiProcessorCount;\n    int maxBlocksPerSM = deviceProp.maxBlocksPerMultiProcessor;\n    int numBlocks = numSMs * maxBlocksPerSM;\n\n    // Util functions for vector operations\n    auto vecDiff = [](const float3 &a, const float3 &b) -> float3 {\n            return make_float3(a.x - b.x, a.y - b.y, a.z - b.z);\n    };\n    \n    auto vecNorm = [](const float3 &v) -> float {\n            return sqrtf(v.x * v.x + v.y * v.y + v.z * v.z);\n    };\n\n    // Test case params and validation results\n    unsigned int particleCountPerCase[NUM_TEST_CASES] = {2,4,8,16,32,33,128};\n    \n    // Array of particle masses\n    float mass_h[NUM_TEST_CASES][MAX_PARTICLE_COUNT] = {\n        {6.305726e+00f, 3.704031e+00f},\n        {6.305726e+00f, 3.704031e+00f, 3.452430e+00f, 2.186230e+00f},\n        {2.251249e+00f, 8.871612e+00f, 9.438993e+00f, 3.584668e+00f, 8.645597e+00f, 9.366398e+00f, 7.400962e+00f, 7.247701e+00f},\n        {4.331704e+00f, 4.898828e+00f, 2.548335e+00f, 5.171834e+00f, 5.511504e+00f, 6.763835e+00f, 5.561555e+00f, 8.112208e+00f, 9.472290e+00f, 8.975634e+00f, 4.200112e+00f, 6.374664e+00f, 9.971392e+00f, 8.030278e+00f, 6.658707e+00f, 8.125082e+00f},\n        {2.275266e+00f, 5.098696e+00f, 9.797214e+00f, 2.016401e+00f, 1.171289e+00f, 1.328442e+00f, 8.661542e+00f, 9.056560e+00f, 8.742530e+00f, 2.549934e+00f, 2.619827e+00f, 8.414488e+00f, 6.736075e+00f, 4.372297e+00f, 1.593920e+00f, 4.680943e+00f, 3.947108e+00f, 4.839671e+00f, 5.771972e+00f, 9.963538e+00f, 9.108752e+00f, 6.941498e+00f, 6.377804e+00f, 6.909125e+00f, 6.525471e+00f, 1.278987e+00f, 2.626591e+00f, 2.935518e+00f, 1.605198e+00f, 4.397502e+00f, 9.975583e+00f, 7.712607e+00f},\n        {2.849786e+00f, 7.465053e+00f, 7.576144e+00f, 9.680149e+00f, 6.867295e+00f, 8.921378e+00f, 3.516657e+00f, 3.632348e+00f, 2.191891e+00f, 1.503679e+00f, 1.340668e+00f, 6.695363e+00f, 1.525732e+00f, 5.048642e+00f, 7.834996e+00f, 2.906834e+00f, 7.071950e+00f, 9.457268e+00f, 7.231824e+00f, 1.217171e+00f, 6.151970e+00f, 8.008360e+00f, 9.859982e+00f, 2.863102e+00f, 6.103044e+00f, 4.954607e+00f, 7.174405e+00f, 9.687495e+00f, 5.436009e+00f, 3.928871e+00f, 2.751977e+00f, 9.776078e+00f, 5.185234e+00f},\n        {6.230005e+00f, 4.989229e+00f, 1.163098e+00f, 3.555043e+00f, 4.498519e+00f, 4.846625e+00f, 1.951974e+00f, 2.702023e+00f, 9.208075e+00f, 2.410063e+00f, 4.480112e+00f, 1.170238e+00f, 8.970317e+00f, 1.559257e+00f, 7.238318e+00f, 2.015014e+00f, 2.542270e+00f, 3.559504e+00f, 8.142887e+00f, 8.896988e+00f, 5.111900e+00f, 5.827071e+00f, 2.777854e+00f, 7.896147e+00f, 9.338067e+00f, 5.151153e+00f, 3.030848e+00f, 9.451649e+00f, 9.499514e+00f, 2.648732e+00f, 2.991516e+00f, 7.706479e+00f, 3.748563e+00f, 8.329482e+00f, 5.660578e+00f, 4.432099e+00f, 5.603123e+00f, 7.825382e+00f, 4.945275e+00f, 2.901776e+00f, 9.977221e+00f, 1.609963e+00f, 6.233970e+00f, 4.647338e+00f, 9.196743e+00f, 2.595348e+00f, 1.132503e+00f, 8.986875e+00f, 7.807049e+00f, 9.555057e+00f, 9.040251e+00f, 3.662513e+00f, 6.169474e+00f, 2.276883e+00f, 5.234349e+00f, 8.037710e+00f, 6.312180e+00f, 4.532503e+00f, 3.383346e+00f, 2.739691e+00f, 3.947001e+00f, 3.634520e+00f, 2.501058e+00f, 9.497878e+00f, 4.517690e+00f, 5.822381e+00f, 8.635795e+00f, 7.518073e+00f, 7.284304e+00f, 6.553366e+00f, 1.801319e+00f, 4.439465e+00f, 7.729893e+00f, 2.885712e+00f, 4.899245e+00f, 9.678103e+00f, 9.918964e+00f, 4.713537e+00f, 8.163313e+00f, 3.315258e+00f, 3.279986e+00f, 5.908838e+00f, 9.065317e+00f, 6.877554e+00f, 4.727919e+00f, 4.586776e+00f, 7.401623e+00f, 4.574008e+00f, 4.185090e+00f, 1.489334e+00f, 7.153626e+00f, 5.608767e+00f, 6.543734e+00f, 3.737551e+00f, 2.104480e+00f, 7.206819e+00f, 6.861120e+00f, 6.679341e+00f, 3.967432e+00f, 5.570528e+00f, 8.543704e+00f, 4.115218e+00f, 7.027076e+00f, 2.335130e+00f, 4.801848e+00f, 2.868979e+00f, 7.248571e+00f, 7.540494e+00f, 6.159638e+00f, 3.657510e+00f, 3.782389e+00f, 8.993936e+00f, 7.962289e+00f, 1.535879e+00f, 5.509607e+00f, 9.412314e+00f, 1.844778e+00f, 7.550896e+00f, 6.169221e+00f, 9.601748e+00f, 5.457664e+00f, 5.980261e+00f, 2.300332e+00f, 6.672713e+00f, 7.168782e+00f, 7.404363e+00f, 8.439940e+00f, 3.308550e+00f}\n    };\n\n    // Array of particle position vectors\n    float3 pos_h[NUM_TEST_CASES][MAX_PARTICLE_COUNT] = {\n        {{-2.275078e+00f, -3.681967e+00f, 2.129892e+00f}, {-4.756183e+00f, 4.598780e+00f, -4.431842e+00f}},\n        {{2.129892e+00f, -4.756183e+00f, 4.598780e+00f}, {-4.431842e+00f, -4.658658e+00f, 4.768967e+00f}, {3.881811e+00f, 4.415114e+00f, 3.964936e+00f}, {-3.184883e+00f, 4.143817e-01f, -2.354646e+00f}},\n        {{-3.071192e+00f, 2.937396e+00f, 8.205211e-01f}, {1.242876e+00f, -1.358968e+00f, -3.994811e+00f}, {-4.356758e+00f, 5.245913e-01f, 4.083174e+00f}, {1.313590e+00f, 2.095270e+00f, 4.966117e+00f},\n        {4.335111e+00f, -4.017012e+00f, -6.681991e-01f}, {-3.886086e+00f, -2.924219e+00f, 1.165533e+00f}, {-4.974482e+00f, 2.402400e+00f, 5.673252e-01f}, {-3.406768e+00f, -1.100228e+00f, -3.278599e+00f}},\n        {{-2.906986e+00f, -4.173940e+00f, 2.410393e+00f}, {-1.914883e+00f, 4.854140e+00f, -2.080884e+00f}, {-3.875106e+00f, 1.252599e+00f, 1.101421e+00f}, {-4.581864e+00f, -4.924020e+00f, -4.099555e+00f},\n        {-1.462398e+00f, -2.184415e+00f, -3.104460e+00f}, {1.138444e-01f, -1.776347e+00f, 2.393547e+00f}, {3.912554e+00f, 4.907492e+00f, -1.227119e+00f}, {-4.480234e+00f, 4.065798e+00f, 3.156210e+00f}, {2.829215e+00f, -1.974650e+00f, -4.268644e+00f}, {4.717871e+00f, -2.468360e+00f, 4.715707e+00f}, {4.874876e+00f, -1.138112e+00f, 5.252256e-02f}, {-1.714767e-01f, 4.900725e+00f, 3.261975e+00f}, {-1.070212e+00f, 4.825401e+00f, 4.790009e+00f}, {-3.998948e+00f, 3.996485e+00f, 4.894719e+00f}, {-1.724842e+00f, 2.894416e+00f, -2.670056e+00f}, {-1.700870e+00f, 1.938847e+00f, 4.078283e+00f}},\n        {{2.045208e+00f, -4.484117e+00f, -1.305178e+00f}, {4.618757e+00f, 1.410568e+00f, -4.537319e+00f}, {3.594376e+00f, -6.412062e-01f, -4.888710e+00f}, {3.844515e+00f, -1.948384e-01f, 2.155679e+00f}, {2.503676e+00f, 3.070157e-01f, 1.556055e+00f}, {-4.969348e+00f, 2.642597e+00f, 4.901001e+00f}, {-4.458138e+00f, 3.434931e-01f, 2.305594e+00f}, {3.688495e+00f, -3.629727e+00f, 1.376880e+00f}, {-3.937233e-01f, 3.843142e+00f, 1.715606e+00f}, {-2.193628e+00f, -1.210434e+00f, -1.187508e+00f}, {3.426227e+00f, -2.682326e+00f, -4.943444e+00f}, {-1.312654e+00f, -4.954494e+00f, -3.287589e+00f}, {4.579639e+00f, 4.650867e+00f, 3.170821e+00f}, {2.829236e+00f, -1.410877e+00f, -3.370172e+00f}, {4.568838e+00f, 5.432685e-01f, 4.057890e+00f}, {-4.001314e+00f, -1.396819e+00f, -1.475678e+00f}, {-1.201569e-01f, 4.549744e+00f, -1.772546e+00f}, {-3.229885e+00f, -4.624727e+00f, 1.155209e+00f}, {2.320020e+00f, -3.329764e+00f, -8.366543e-01f}, {6.806556e-01f, 4.099345e+00f, 3.124596e+00f}, {-1.683635e-01f, 5.244406e-01f,   4.795851e+00f}, {3.488241e+00f, 1.040715e+00f, -1.208681e+00f}, {3.500111e+00f, -3.804805e+00f, 4.918727e+00f}, {6.309077e-01f, -1.706381e-01f, 2.414668e+00f}, {-4.655891e+00f, 3.058487e+00f, 4.697628e+00f}, {-7.106831e-01f, 2.857911e+00f, 3.992337e+00f}, {2.471302e+00f, 3.314782e+00f, 2.378608e+00f}, {-3.606331e+00f, 5.921252e-01f, 6.570024e-01f}, {-2.641113e+00f, -4.378033e-01f, 2.399127e+00f}, {8.068738e-01f, 2.761862e+00f, -2.483472e+00f}, {-4.380830e+00f, -1.435362e+00f, 1.096672e+00f}, {-1.925850e+00f, -1.262309e+00f, 4.151771e+00f}},\n        {{4.035610e-01f, 2.763502e+00f, -3.329502e+00f}, {2.557337e+00f, -3.500076e+00f, -3.954062e+00f}, {-6.728450e-01f, 4.525556e+00f, -3.817499e+00f}, {-4.554858e+00f, -5.590220e-01f, 1.416127e+00f}, {-3.301355e+00f, -9.415395e-01f, 4.630525e+00f}, {4.306428e+00f, 1.479900e+00f, -1.221517e+00f}, {-3.594946e-01f, -1.016933e+00f, -2.850603e-01f}, {3.021212e+00f, -2.225819e+00f, 4.796696e+00f}, {-4.366450e+00f, -3.940294e+00f, 3.085883e+00f}, {-3.589154e-01f, 7.613556e-01f, -1.577444e+00f}, {4.400881e+00f, 2.563990e+00f, 1.134522e+00f}, {4.357833e+00f, 4.192998e+00f, -9.318069e-02f}, {2.300874e+00f, 3.324171e+00f, 1.381483e+00f}, {-4.251402e-01f, -2.429702e+00f, -2.319675e+00f}, {8.415338e-01f, 3.363436e+00f, 3.360623e+00f}, {-2.356934e+00f, 9.418337e-01f, -3.803297e+00f}, {-3.109041e+00f, -1.388294e+00f, -1.589252e+00f}, {-4.890564e+00f, 1.376279e+00f, 3.502676e+00f}, {2.491196e+00f, -3.448314e+00f, -2.246576e+00f}, {-7.879013e-01f, -4.377102e+00f, 3.056221e+00f}, {-4.501870e-01f, -2.726619e+00f, -4.453272e+00f}, {4.142271e+00f, -8.397415e-01f, 2.022879e+00f}, {4.284500e+00f, 1.688954e+00f, -1.847844e-01f}, {1.223494e+00f, 4.145685e+00f, 3.926884e+00f}, {-2.376592e-02f, 1.902846e+00f, -1.383373e+00f}, {-2.617380e+00f, 4.455689e+00f, -8.598321e-01f}, {2.698901e+00f, -2.097317e+00f, 1.216935e+00f}, {2.830530e+00f, 8.142686e-01f, 3.230621e+00f}, {3.013512e+00f, 2.999836e+00f, 4.116197e+00f}, {-4.115034e+00f, -2.169927e+00f, 4.326222e+00f}, {-3.288338e+00f, 6.965108e-01f, 4.149065e+00f}, {3.265583e+00f, 3.004221e+00f, 1.831369e+00f}, {7.552939e-01f, -6.938717e-01f, 4.316593e+00f}},\n        {{1.965909e+00f, -3.253533e+00f, -4.070023e+00f}, {-3.835147e+00f, 3.680952e+00f, 2.941125e+00f}, {1.081385e+00f, 1.339433e+00f, 2.524831e+00f}, {3.227284e+00f, 2.823747e+00f, 1.378452e+00f}, {-2.261339e+00f, 8.897815e-01f, 3.866131e+00f}, {-3.877760e+00f, 3.048854e+00f, 1.488789e+00f}, {-2.793684e+00f, 4.763263e+00f, 2.434178e+00f}, {-2.271716e+00f, -3.327556e+00f, 2.585653e+00f}, {8.532545e-01f, 4.434804e+00f, -2.415042e+00f}, {-2.074468e+00f, -4.904184e-01f, 8.827049e-01f}, {2.815784e+00f, 1.213077e+00f, -1.350096e+00f}, {1.845368e+00f, -3.166410e+00f, 4.272926e-01f}, {4.793953e+00f, 3.203107e+00f, 2.327675e+00f}, {3.548812e-01f, -4.538910e+00f, 1.490019e+00f}, {-3.045772e+00f, -3.822388e+00f, 2.982699e+00f}, {9.014031e-01f, -4.934777e+00f, 4.103786e+00f}, {4.994319e+00f, 4.700922e+00f, 5.473588e-01f}, {1.694579e+00f, 1.197840e+00f, -2.529696e+00f}, {-3.183652e+00f, -1.603417e+00f, -3.845008e+00f}, {2.217615e-02f, -5.643119e-01f, -2.532745e+00f}, {4.400327e+00f, -1.932102e+00f, 2.140548e+00f}, {-2.109469e+00f, -9.999188e-01f, 1.647233e-01f}, {1.301446e+00f, -1.341591e+00f, -3.154323e+00f}, {-2.000458e+00f, -4.747647e+00f, -4.430450e+00f}, {3.347463e+00f, -3.297064e+00f, 4.813834e+00f}, {4.335286e+00f, -6.877313e-01f, 3.508019e+00f}, {-1.024601e+00f, 5.114389e-01f, 4.931698e+00f}, {4.210388e+00f, 3.007665e+00f, -4.769697e+00f}, {-1.823522e+00f, -4.427250e-01f, -1.313068e+00f}, {1.267447e+00f, -8.828737e-01f, 3.453461e+00f}, {-1.506951e+00f, -3.453529e+00f, -1.918243e+00f}, {2.130241e+00f, -1.587020e+00f, -3.077999e-01f}, {-4.236825e+00f, 3.868386e+00f, 2.351874e+00f}, {1.592374e+00f, -4.553501e-01f, -1.472028e+00f}, {-2.489038e+00f, 2.037408e-01f, 1.945853e+00f}, {1.506494e+00f, 4.602473e+00f, 3.441171e+00f}, {-3.574834e+00f, 7.608652e-01f, 1.159596e+00f}, {-2.889530e+00f, 2.394566e+00f, 3.767891e+00f}, {3.329894e+00f, -3.753486e+00f, 4.462867e+00f}, {-3.930464e+00f, -2.968160e+00f, -7.344604e-01f}, {-1.528839e+00f, -5.057491e-01f, 4.273228e+00f}, {2.663799e+00f, -3.031203e+00f, -1.502557e+00f}, {-2.625264e+00f, 2.657936e+00f, -3.037730e+00f}, {3.713301e+00f, 3.879743e+00f, -2.218921e+00f}, {-5.613011e-01f, -4.238440e+00f, 2.364269e+00f}, {-1.689198e+00f, 1.050171e+00f, -4.407266e+00f}, {-1.400140e-01f, -1.974148e+00f, -4.046062e+00f}, {-4.788751e+00f, -2.517792e+00f, 1.754034e+00f}, {7.706456e-01f, 4.740491e-01f, 1.415437e+00f}, {-4.317611e+00f, 1.925294e+00f, 3.444596e+00f}, {3.225085e+00f, 6.611386e-01f, -1.525325e+00f}, {-4.494792e+00f, -3.887234e+00f, 1.967641e+00f}, {-3.567317e+00f, -8.982728e-01f, -2.022578e+00f}, {2.104661e+00f, -3.429204e+00f, -2.002287e+00f}, {2.212588e+00f, 1.394811e+00f, 1.392552e-01f}, {-4.474920e+00f, 3.097765e+00f, -1.583866e+00f}, {-7.123605e-01f, -5.362802e-01f, 2.461345e+00f}, {-2.086424e+00f, -1.235031e+00f, -4.424115e+00f}, {1.792487e+00f, -7.629645e-02f, -2.727298e+00f}, {3.102637e+00f, 4.978288e+00f, 2.924435e+00f}, {3.158732e+00f, -2.017574e+00f, 2.590222e+00f}, {-1.907420e-01f, 4.824571e+00f, -2.072636e+00f}, {1.756453e+00f, -4.995620e+00f, 1.822817e+00f}, {2.100254e+00f, 2.581627e+00f, 4.050212e+00f}, {-4.735670e+00f, -4.026481e+00f, 4.015223e+00f}, {-6.588210e-01f, 1.026186e+00f, 4.142759e+00f}, {-5.336795e-01f, -4.178545e+00f, 4.527218e+00f}, {4.672777e+00f, -1.706113e+00f, -1.658989e+00f}, {3.734219e+00f, -1.603940e+00f, -4.530660e+00f}, {-1.170577e+00f, -3.178735e+00f, 3.583523e+00f}, {-3.078864e+00f, 1.682949e-01f, -4.358176e+00f}, {2.251821e+00f, 4.947627e+00f, 2.950648e+00f}, {5.860130e-01f, 2.602646e+00f, -1.685745e+00f}, {2.462573e-01f, 7.674553e-01f, -1.477721e+00f}, {-1.103454e+00f, 1.420827e+00f, -2.836279e+00f}, {2.460136e+00f, -3.625235e+00f, 1.797549e+00f}, {2.908060e+00f, 3.827788e+00f, -5.290062e-01f}, {7.270028e-01f, 4.227972e+00f, -4.007816e+00f}, {-4.118544e+00f, 1.325535e+00f, -3.668687e+00f}, {7.106094e-01f, -3.621823e+00f, 2.538032e+00f}, {-4.280569e+00f, -3.960515e-01f, -4.505357e-01f}, {-4.737720e+00f, -4.491311e+00f, -1.350808e+00f}, {-5.157021e-01f, 1.128148e+00f, 1.862121e+00f}, {-1.723041e-01f, -5.870779e-01f, -3.189984e+00f}, {-2.232272e+00f, 1.665155e+00f, -6.127252e-01f}, {-2.045553e+00f, -1.781240e+00f, -2.120641e+00f}, {8.222538e-02f, 3.864646e+00f, 4.890926e+00f}, {-1.965659e+00f, 7.653501e-01f, -1.165723e+00f}, {-7.954939e-02f, -1.382593e+00f, -2.953230e+00f}, {2.902025e+00f, 1.266589e+00f, 4.025815e+00f}, {-4.069696e+00f, -3.334105e+00f, 4.276571e+00f}, {-2.434192e+00f, -3.088275e+00f, -4.500433e+00f}, {-4.528368e+00f, -2.631936e+00f, 2.484423e+00f}, {1.338998e+00f, -1.842518e+00f, 1.913905e+00f}, {-3.300430e+00f, 2.946509e+00f, 9.025896e-01f}, {1.750384e+00f, 3.813303e-01f, -3.524242e+00f}, {-4.881520e+00f, -3.099045e+00f, -2.016518e+00f}, {3.560684e+00f, 3.083459e-01f, -3.312994e+00f}, {-1.751002e+00f, -4.317727e+00f, -4.456614e+00f}, {2.259564e+00f, 2.144389e+00f, -2.132514e+00f}, {4.649236e+00f, -1.139263e+00f, -4.437564e+00f}, {-4.974045e+00f, -1.085910e+00f, 2.887737e+00f}, {-2.166686e+00f, -2.949709e-01f, 1.234197e+00f}, {-1.798021e+00f, 3.933870e-02f, 3.864961e+00f}, {9.022241e-01f, -1.694293e+00f, -3.245147e+00f}, {4.328768e+00f, 1.080880e-02f, 3.320421e+00f}, {-2.558163e+00f, 4.926327e+00f, 2.584890e-01f}, {1.845173e-01f, -4.858024e+00f, -4.514737e-01f}, {3.657390e+00f, -3.872755e+00f, 4.038770e+00f}, {6.156087e-01f, -3.543546e+00f, -4.335863e+00f}, {2.734746e+00f, -1.282358e+00f, -4.707343e+00f}, {3.592180e+00f, -3.871679e+00f, -1.028764e+00f}, {4.071579e+00f, 4.330583e+00f, -2.040866e+00f}, {3.342813e+00f, 3.736383e+00f, 2.270415e-01f}, {1.412291e+00f, -2.102275e-01f, 4.992956e+00f}, {-2.359104e+00f, 4.069505e+00f, 5.468698e-01f}, {-3.082861e+00f, -3.821312e+00f, -4.700109e+00f}, {2.623562e+00f, -1.107290e+00f, -2.678829e+00f}, {1.326333e+00f, -4.696346e+00f, -3.956976e+00f}, {-2.026752e+00f, 3.336470e+00f, 1.124633e+00f}, {4.557022e+00f, -1.730802e+00f, -2.257733e+00f}, {4.552738e+00f, 4.235761e+00f, -7.469592e-01f}, {-2.100109e+00f, -1.408197e+00f, -1.306132e+00f}, {2.702259e+00f, 1.358539e+00f, -3.925478e+00f}, {2.941458e+00f, -1.821444e+00f, 3.690075e+00f}, {3.603119e+00f, -4.574076e+00f, 4.387871e+00f}, {-3.438464e+00f, 1.427378e+00f, -6.694215e-01f}, {-4.629121e+00f, -2.420004e+00f, -2.514569e+00f}}\n    };\n\n    // Array of particle velocity vectors\n    float3 vel_h[NUM_TEST_CASES][MAX_PARTICLE_COUNT] = {\n        {{-9.317315e-01f, 9.537933e-01f, 7.763622e-01f}, {8.830227e-01f, 7.929872e-01f, -6.369766e-01f}},\n        {{2.858950e-01f, -8.132153e-01f, 9.140813e-01f}, {2.353761e-01f, 5.988926e-01f, -9.769579e-01f}, {-3.992917e-01f, -5.506736e-01f, -5.077804e-01f}, {1.830518e-02f, 4.711822e-01f, -2.127832e-01f}},\n        {{6.067519e-01f, 4.326032e-04f, -1.174910e-01f}, {-1.902734e-01f, 4.381490e-01f, -6.194583e-01f}, {7.214697e-02f, -3.084746e-01f, 4.023629e-01f}, {9.407850e-01f, -4.839484e-01f, 4.301702e-01f},\n        {-8.738118e-01f, 3.354260e-01f, 9.825423e-02f}, {1.966917e-01f, 6.346071e-01f, -7.291458e-01f}, {-1.768807e-01f, -3.884967e-01f, 5.398672e-01f}, {9.102120e-01f, -5.737484e-01f, -5.496022e-01f}},\n        {{-3.046651e-01f, 9.546707e-01f, 2.889966e-01f}, {-9.911433e-01f, -7.583087e-03f, -7.356965e-02f}, {-3.253570e-02f, -1.178742e-01f, -1.107645e-02f}, {6.266414e-01f, -9.510395e-01f, 3.074681e-01f},\n        {-4.737965e-01f, -5.279757e-01f, -9.723446e-01f}, {9.753135e-01f, -2.303998e-01f, 7.685216e-01f}, {-8.106697e-01f, -5.446122e-01f, 4.468894e-01f}, {7.726008e-01f, 3.953982e-01f, 2.322978e-01f}, {-9.583548e-01f, 8.045935e-01f, -5.386540e-01f}, {-3.171421e-01f, -3.223583e-01f, 7.973992e-01f}, {1.347935e-01f, -4.699046e-01f, 1.682705e-01f}, {-8.933615e-01f, 9.763501e-01f, 3.904319e-01f}, {-7.163578e-01f, 9.807975e-01f, 7.132301e-01f}, {-5.742055e-01f, 5.440878e-02f, -1.186284e-01f}, {3.697447e-01f, -8.991011e-01f, -9.729674e-02f}, {-9.282434e-01f, 2.513422e-01f, 6.472747e-01f}},\n        {{8.525970e-01f, -9.181121e-02f, 6.097633e-01f}, {2.808775e-02f, -1.699356e-01f, -7.463535e-01f}, {7.124113e-01f, 8.057005e-01f, 3.219395e-01f}, {-4.346353e-01f, 2.098522e-01f, -8.242064e-01f}, {-3.794995e-02f, 1.383328e-01f, 4.930081e-01f}, {-6.102952e-03f, -5.806683e-01f, 5.992542e-01f}, {3.920523e-01f, -1.296073e-01f, 5.180777e-01f}, {8.744481e-01f, 9.641043e-01f, 6.909258e-01f}, {-1.605179e-01f, -3.184043e-01f, 3.430406e-01f}, {3.751722e-02f, 2.301782e-01f, -3.556249e-02f}, {-8.166144e-01f, -2.259420e-01f, -3.144305e-01f}, {2.617904e-01f, 1.610814e-01f, 8.932514e-01f}, {-1.578655e-01f, 1.772121e-01f, 8.388495e-02f}, {-2.733801e-01f, 2.768214e-01f, 4.766717e-03f}, {-1.309701e-01f, -1.022093e-01f, -6.986226e-01f}, {-2.735330e-01f, -3.548982e-01f, 2.360768e-01f}, {7.457934e-02f, -8.898571e-04f, 2.033364e-01f}, {-2.643425e-01f, 3.650686e-02f, -8.201458e-01f}, {-5.480026e-02f, 7.225544e-01f, 6.722581e-01f}, {6.266480e-01f, 9.249440e-01f, -7.367876e-02f}, {7.528203e-01f, 5.752368e-01f, 1.414387e-01f}, {-8.348589e-01f, -7.541614e-01f, -4.926441e-01f}, {-7.023179e-01f, -7.789850e-02f, 8.081877e-01f}, {4.219343e-01f, -9.497321e-01f, 2.783211e-01f}, {5.446771e-01f, 4.830468e-01f, -9.594187e-01f}, {-9.431053e-01f, 3.354955e-01f, -1.033344e-01f}, {6.259166e-01f, 3.335942e-01f, 9.829645e-01f}, {4.246914e-01f, -8.863952e-01f, -8.797460e-01f}, {4.887730e-01f, 2.044909e-02f, -6.867790e-01f}, {9.724896e-01f, -7.735108e-01f, 2.725900e-01f}, {4.144672e-01f, -6.783517e-02f, 7.797414e-01f}, {4.302712e-01f, 6.261204e-01f, 5.720875e-01f}},\n        {{9.684361e-01f, 7.234718e-01f, -5.573957e-02f}, {8.474102e-02f, -8.187421e-01f, 8.380908e-01f}, {-2.493450e-01f, -5.069617e-01f, 7.534076e-01f}, {-2.008272e-01f, 2.060775e-01f, -8.724753e-01f}, {-3.634924e-01f, 4.717516e-01f, 5.452312e-01f}, {-7.708534e-01f, 1.684785e-01f, -4.237073e-01f}, {7.648034e-02f, -9.898344e-01f, 5.538969e-01f}, {-6.604468e-01f, 6.899662e-01f, -3.977065e-01f}, {-7.287515e-01f, 7.898627e-02f, 3.215552e-01f}, {-3.400924e-01f, -7.743736e-01f, -9.418629e-01f}, {5.509237e-01f, -5.615863e-02f, 7.927946e-02f}, {-8.431290e-01f, 8.548437e-01f, -3.983287e-01f}, {-8.336574e-01f, 4.839913e-01f, -8.766864e-01f}, {9.716983e-01f, -8.929457e-01f, 8.775479e-01f}, {6.835198e-01f, 9.587708e-02f, 6.084127e-03f}, {-5.626560e-01f, 5.978458e-01f, 1.111874e-01f}, {-8.074763e-01f, -4.008999e-01f, 5.303399e-01f}, {-4.921071e-01f, 7.347534e-01f, -5.582055e-01f}, {-6.788821e-01f, 6.675236e-01f, -8.891133e-02f}, {5.005561e-01f, 4.089506e-01f, 8.980394e-01f}, {9.221888e-01f, 3.301190e-01f, -2.017709e-01f}, {-8.815134e-01f, 7.769710e-01f, 9.323795e-01f}, {9.062560e-01f, 3.588009e-01f, -5.086121e-01f}, {-9.388563e-01f, 6.177755e-01f, -3.126098e-01f}, {7.829403e-01f, 9.607211e-01f, -2.794813e-01f}, {-8.895297e-02f, -7.347724e-01f, -9.518680e-01f}, {6.147623e-01f, -3.134223e-01f, 7.721085e-01f}, {-4.789447e-01f, -3.315623e-01f, 2.347221e-01f}, {2.799664e-01f, -9.377564e-01f, -2.348072e-01f}, {9.826531e-01f, -1.401322e-01f, 5.346533e-01f}, {4.958692e-01f, -9.714148e-01f, -3.532109e-01f}, {-9.228343e-01f, -9.044589e-01f, -1.161189e-03f}, {-8.329301e-01f, -7.048896e-01f, -5.459033e-01f}},\n        {{-9.075296e-01f, 9.842724e-01f, 5.079822e-01f}, {7.370959e-01f, 3.133695e-01f, -1.599016e-01f}, {-5.466570e-01f, 5.592505e-01f, -3.661158e-01f}, {9.068619e-02f, -3.021593e-01f, -6.610025e-01f}, {8.536022e-01f, -1.004034e-01f, -8.019088e-01f}, {8.909002e-02f, -7.037996e-01f, -5.310144e-01f}, {-7.654121e-01f, 7.069585e-02f, 8.477689e-01f}, {7.240063e-02f, -6.682866e-01f, 2.734422e-01f}, {6.976751e-01f, 2.055452e-01f, -4.004589e-01f}, {-2.285962e-01f, -6.958111e-01f, 2.366111e-01f}, {-3.365386e-01f, 6.111090e-01f, -1.009735e-01f}, {7.023978e-01f, -6.242963e-01f, -6.335863e-01f}, {1.756565e-01f, 6.747211e-02f, 3.334539e-01f}, {-4.702891e-01f, -1.782619e-01f, -8.862601e-01f}, {8.590546e-01f, 1.864208e-01f, -9.188985e-01f}, {4.858919e-01f, -9.039443e-02f, -9.306768e-01f}, {9.086179e-01f, 7.591340e-01f, -4.481277e-01f}, {8.440644e-01f, -4.165405e-01f, -6.373346e-01f}, {5.647987e-01f, 9.517160e-01f, 7.718711e-01f}, {-3.194496e-01f, 2.331052e-01f, -8.953381e-01f}, {2.702734e-01f, 1.486700e-01f, 8.992223e-01f}, {-9.957747e-02f, -2.346888e-01f, 7.453298e-01f}, {4.103107e-01f, -4.237243e-01f, 3.005492e-01f}, {-1.292491e-01f, 6.531071e-01f, -8.083789e-01f}, {3.681836e-01f, 9.299848e-01f, -9.444864e-01f}, {-6.628579e-01f, 2.371312e-01f, 5.310897e-01f}, {8.013173e-01f, -3.479197e-01f, 1.753383e-01f}, {-3.743907e-02f, 1.165725e-01f, -6.638402e-02f}, {-8.594336e-01f, 1.652132e-01f, 1.834068e-01f}, {-9.856852e-01f, -2.197727e-01f, 4.463081e-01f}, {-9.117802e-01f, -7.105815e-01f, -3.489779e-01f}, {-7.870656e-01f, 4.596592e-01f, -1.458405e-01f}, {-6.602142e-01f, 1.415942e-01f, 6.413746e-01f}, {-1.664620e-01f, -3.057376e-01f, -1.827227e-01f}, {9.001108e-01f, -8.020807e-01f, 1.635986e-01f}, {3.732434e-02f, -7.387534e-01f, 8.807275e-02f}, {-3.155561e-01f, -5.828751e-01f, 4.962806e-01f}, {6.227654e-01f, 9.324084e-01f, -7.818404e-01f}, {5.515871e-01f, 7.208932e-01f, 2.326277e-01f}, {8.743528e-01f, -4.359421e-01f, 9.739390e-01f}, {2.180200e-01f, 4.578781e-02f, 5.943890e-01f}, {-9.889423e-01f, -7.567531e-02f, -5.926664e-01f}, {3.275745e-01f, 4.298897e-01f, 5.468119e-01f}, {-4.639629e-01f, -5.302101e-01f, -7.701837e-01f}, {9.203755e-01f, -1.958658e-02f, -1.918448e-01f}, {-4.650514e-02f, -1.973444e-02f, -9.385314e-01f}, {3.483800e-01f, 5.867907e-01f, -4.924437e-01f}, {-3.164448e-01f, -9.282125e-01f, 1.436908e-01f}, {4.844939e-01f, 7.594075e-01f, -9.408363e-01f}, {4.788513e-01f, -4.895897e-01f, 4.832206e-01f}, {5.959802e-01f, 7.505259e-01f, 4.254862e-02f}, {7.982241e-01f, 5.903921e-01f, -8.571386e-01f}, {-9.904973e-01f, -4.983312e-01f, 3.045862e-01f}, {5.846962e-01f, -9.118406e-01f, 5.363070e-01f}, {5.335883e-01f, -1.204512e-01f, 1.014158e-01f}, {-8.672072e-01f, 7.278236e-01f, -3.665645e-01f}, {-2.665262e-01f, -4.793269e-01f, -3.445653e-01f}, {7.381179e-01f, 7.449852e-01f, -9.875329e-01f}, {-5.530674e-01f, 3.471784e-01f, 1.891114e-01f}, {1.926859e-01f, -6.706795e-01f, 4.954980e-02f}, {8.079341e-01f, -5.851161e-01f, -4.794894e-02f}, {-6.504124e-01f, -2.689075e-02f, -5.507391e-01f}, {8.114744e-01f, -7.052837e-01f, -2.083493e-02f}, {5.773110e-01f, -2.562691e-01f, -1.904774e-01f}, {3.263202e-01f, 1.481019e-01f, 5.927627e-02f}, {1.864418e-01f, 1.795782e-02f, -1.943076e-01f}, {-8.704956e-01f, 6.167983e-01f, 6.641350e-01f}, {8.702223e-01f, 4.033018e-01f, 7.274230e-01f}, {4.051882e-01f, 8.447918e-01f, 3.487469e-01f}, {2.658674e-01f, -3.010800e-03f, 7.619650e-01f}, {-6.061619e-01f, -3.017940e-01f, 2.311356e-01f}, {3.893036e-01f, 5.501674e-02f, 4.969019e-01f}, {6.536043e-01f, 4.697610e-01f, 7.082964e-01f}, {-2.665765e-01f, -1.813440e-01f, 5.320937e-01f}, {-3.331599e-01f, -2.941637e-01f, 9.723650e-01f}, {-4.537313e-01f, -3.540045e-01f, 5.605747e-01f}, {-8.615083e-01f, 6.645209e-01f, 9.809833e-02f}, {-5.629299e-01f, -9.456634e-01f, -8.850762e-01f}, {7.418134e-02f, -3.604313e-01f, 8.766578e-01f}, {1.816789e-01f, -7.634428e-01f, 3.647814e-01f}, {8.054800e-01f, 3.251360e-01f, -5.046228e-01f}, {-2.076218e-01f, 8.982510e-01f, 7.208526e-01f}, {3.887964e-01f, 1.501774e-01f, -9.263656e-01f}, {3.938331e-01f, 4.873082e-01f, -1.137143e-01f}, {-5.761319e-02f, -4.138619e-01f, -4.183334e-02f}, {2.747716e-01f, -1.509529e-02f, -5.831297e-01f}, {-8.147560e-01f, 7.235356e-01f, 4.148483e-01f}, {-3.976076e-01f, 1.508900e-01f, 8.645421e-01f}, {7.930680e-01f, -2.820343e-01f, 6.139995e-01f}, {2.735764e-02f, 4.620913e-01f, 6.319771e-01f}, {1.143838e-01f, 5.657853e-02f, 2.133671e-01f}, {-5.073606e-01f, -2.855733e-01f, -9.876691e-01f}, {5.088544e-01f, 7.067468e-02f, -2.635247e-01f}, {-2.802616e-01f, -9.392417e-01f, -9.326564e-01f}, {7.125054e-01f, -4.644208e-01f, -1.698428e-01f}, {9.780695e-01f, -6.236805e-01f, -7.716354e-01f}, {9.208031e-01f, -8.824762e-01f, 7.022352e-01f}, {2.818166e-01f, -9.164856e-01f, 4.769336e-01f}, {4.147383e-01f, 9.051742e-01f, 9.756441e-02f}, {7.200351e-02f, -7.401485e-01f, 9.269646e-01f}, {1.420399e-01f, -5.099404e-01f, 9.815450e-01f}, {3.635858e-01f, -9.523637e-01f, 7.410683e-01f}, {-7.317662e-01f, -1.043338e-01f, -5.692595e-01f}, {-7.227266e-02f, -3.570465e-01f, 7.219854e-01f}, {-7.162228e-01f, -2.895437e-01f, 4.832046e-02f}, {2.554188e-02f, -4.476272e-01f, -8.388043e-01f}, {7.410040e-01f, 5.541715e-01f, -4.596182e-01f}, {9.062824e-01f, 9.743793e-01f, -5.429886e-01f}, {3.968561e-01f, 4.342410e-01f, 6.220111e-02f}, {4.465133e-01f, -8.898095e-01f, -2.285851e-01f}, {-3.627377e-01f, -7.879012e-01f, 4.604100e-01f}, {3.091025e-01f, 4.588333e-01f, 5.825194e-02f}, {-2.871659e-03f, -2.973274e-01f, -5.648703e-01f}, {5.895313e-01f, -1.714089e-01f, 1.878830e-01f}, {1.627569e-01f, 9.316477e-01f, -7.941564e-01f}, {6.777208e-01f, 3.924180e-01f, 1.342747e-01f}, {-5.015327e-01f, 6.293574e-01f, 1.224798e-01f}, {-6.418504e-01f, -1.685388e-01f, -8.918917e-02f}, {-3.926910e-01f, 4.608928e-01f, 1.887488e-01f}, {-8.749324e-01f, 2.668775e-01f, 3.986096e-01f}, {-2.152847e-01f, -8.525114e-01f, -4.905011e-01f}, {4.515696e-01f, 8.095310e-01f, 4.572438e-01f}, {7.571378e-01f, -9.930643e-02f, 4.935106e-01f}, {5.184089e-01f, 1.292789e-01f, 5.520440e-01f}, {-3.932113e-01f, -9.712086e-01f, 5.891526e-01f}, {3.420169e-01f, 2.147453e-01f, 9.279557e-01f}, {-6.548538e-01f, 4.285311e-01f, -4.813838e-01f}, {6.029108e-01f, 9.912634e-01f, 4.690613e-01f}}\n    };\n\n    // Valid total angular momentum for each test case\n    float3 validTotalAM_h[NUM_TEST_CASES] = {\n        {-2.866796e+01, -2.709299e+01, -6.432711e+01},\n        {4.477622e+00, -1.610214e+01, -1.270816e+01},\n        {6.149367e+01, 8.838241e+00, 4.890832e+00},\n        {-4.621698e+01, -5.488707e+01, 8.224401e+01},\n        {-1.476430e+02, 4.598140e+01, 4.528759e+01},\n        {-1.153948e+02, -1.224836e+02, 3.769198e+01},\n        {-6.532889e+01, 8.917376e+01, 1.030329e+02}\n    };\n\n    // Test loop\n    for (unsigned int i = 0; i < NUM_TEST_CASES; ++i) {\n\n        float *mass_d;\n        float3 *pos_d, *vel_d, *totalAM_d;\n        unsigned int particleCount = particleCountPerCase[i];\n\n        float3 gpuTotalAM_h;\n\n        //Declare CUDA stream for Async operations\n        cudaStream_t stream;\n        CUDA_CHECK(cudaStreamCreate(&stream));\n\n        //Allocate memory on device\n        CUDA_CHECK(cudaMallocAsync(&mass_d, particleCount * sizeof(float), stream));\n        CUDA_CHECK(cudaMallocAsync(&pos_d, particleCount * sizeof(float3), stream));\n        CUDA_CHECK(cudaMallocAsync(&vel_d, particleCount * sizeof(float3), stream));\n        CUDA_CHECK(cudaMallocAsync(&totalAM_d, sizeof(float3), stream));\n\n        // Copy input data to device\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h[i], particleCount * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(pos_d, pos_h[i], particleCount * sizeof(float3), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(vel_d, vel_h[i], particleCount * sizeof(float3), cudaMemcpyHostToDevice, stream));\n\n        // Initialize total angular momentum to zero\n        CUDA_CHECK(cudaMemcpyAsync(totalAM_d, &ZERO_VEC, sizeof(float3), cudaMemcpyHostToDevice, stream));\n\n        // Configure kernel launch parameters\n        void *args[] = {&mass_d,\n                        &pos_d,\n                        &vel_d,\n                        &totalAM_d,\n                        &particleCount};\n\n        // Block: (256, 1, 1)\n        // Grid: (numBlocks, 1, 1)\n        dim3 gridDim(numBlocks);\n        dim3 blockDim(BLOCK_SIZE);\n\n        // Launch kernel\n        CUDA_CHECK(cudaLaunchKernel((void*)k_computeAngularMomentum,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    0,\n                                    stream\n                                    ));\n\n        // Wait for the kernel to complete.\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Copy result back to host\n        CUDA_CHECK(cudaMemcpyAsync(&gpuTotalAM_h, totalAM_d, sizeof(float3), cudaMemcpyDeviceToHost));\n\n        //Validate result\n        assert(vecNorm(vecDiff(gpuTotalAM_h, validTotalAM_h[i])) < TOL);\n\n        // Memory Cleanup\n        CUDA_CHECK(cudaFreeAsync(mass_d, stream));\n        CUDA_CHECK(cudaFreeAsync(pos_d, stream));\n        CUDA_CHECK(cudaFreeAsync(vel_d, stream));\n        CUDA_CHECK(cudaFreeAsync(totalAM_d, stream));\n        CUDA_CHECK(cudaStreamDestroy(stream));\n    }\n}\n\n// CUDA kernel to compute total angular momentum using warp-level reduction\n__global__ void k_computeAngularMomentum(const float *mass_d, const float3 *pos_d, const float3 *vel_d, float3 *totalAM_d, unsigned int particleCount) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/119", "date": "2025-07-30", "prompt": "Write a CUDA kernel to detect collisions between two polygons using the separating axes theorem method on 2D objects.\n\nThe signature of the kernel is __global__ void k_satCollisionDetectionKernel(const Point* firstShape, int numVertices1, const Point* secondShape, int numVertices2, int* collisionDetected), where firstShape is the pointer to an array of points representing the vertices of the first polygon in device memory, secondShape is the pointer to an array of points representing the vertices of the second polygon in device memory, numVertices1 is the number of vertices in the first polygon, numVertices2 is the number of vertices in the second polygon, and collisionDetected is a pointer to an integer flag in device memory that will be set to 1 if collision detected and 0 otherwise.\n\n>>> k_satCollisionDetectionKernel({{0, 0}, {4, 0}, {4, 4}, {0, 4}}, 4, {{2, 2}, {6, 2}, {6, 6}, {2, 6}}, 4, collisionDetected)-> collisionDetected: 1\n>>> k_satCollisionDetectionKernel({{0, 0}, {4, 0}, {4, 4}, {0, 4}}, 4, {{5, 5}, {9, 5}, {9, 9}, {5, 9}}, 4, collisionDetected)-> collisionDetected: 0 \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <float.h>\n#include <cstdio>\n#undef NDEBUG\n#include <assert.h>\n\n#define CUDA_CHECK(call)                                                                                  \\\ndo {                                                                                                      \\\n    cudaError_t error = call;                                                                         \\\n    if (error != cudaSuccess) {                                                                       \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\", __FILE__, __LINE__, cudaGetErrorString(error)); \\\n        exit(EXIT_FAILURE);                                                                           \\\n    }                                                                                                 \\\n} while (0)\n\n#define BLOCK_SIZE 128\n#define EPSILON 1e-5\n\n// Structure to represent a 2D point\nstruct Point {\n    float x;\n    float y;\n};\n\n// Structure to represent a Testcase\nstruct TestCase {\n    Point* firstShape;\n    int numVertices1;\n    Point* secondShape;\n    int numVertices2;\n    bool expectedCollision;\n};\n\n__global__ void k_satCollisionDetectionKernel(const Point* firstShape, int numVertices1, const Point* secondShape, int numVertices2, int* collisionDetected);\n\nvoid launch() {\n    \n    Point firstShape1[] = {{0, 0}, {4, 0}, {4, 4}, {0, 4}}; //Square 1\n    Point secondShape1[] = {{2, 2}, {6, 2}, {6, 6}, {2, 6}}; //Square 2\n\n    TestCase TestCase1 = {firstShape1, 4, secondShape1, 4, true}; // Squares Colliding\n    Point firstShape2[] = {{0, 0}, {4, 0}, {4, 4}, {0, 4}};\n    Point secondShape2[] = {{5, 5}, {9, 5}, {9, 9}, {5, 9}};\n\n    TestCase TestCase2 = {firstShape2, 4, secondShape2, 4, false}; // Squares Non Colliding\n    Point firstShape3[] = {{1, 1}, {3, 1}, {4, 2}, {3, 4}, {1, 4}, {0, 2}};\n    Point secondShape3[] = {{2, 3}, {5, 3}, {5, 5}, {2, 5}};\n\n    TestCase TestCase3 = {firstShape3, 6 , secondShape3, 4, true}; // Hexagon and Quadrilateral Colliding\n    Point firstShape4[] = {{1, 1}, {5, 1}, {5, 2}, {1, 2}};\n    Point secondShape4[] = {{3, 0}, {4, 1}, {3, 2}, {2, 1}};\n\n    TestCase TestCase4 = {firstShape4, 4, secondShape4, 4, true}; // Rectangle and Square Colliding\n    Point firstShape5[] = {{0, 0}, {2, 1}, {3, 3}, {1, 4}, {-1, 2}};\n    Point secondShape5[] = {{5, 5}, {7, 6}, {6, 8}};\n\n    TestCase TestCase5 = {firstShape5, 5, secondShape5, 3, false}; // Pentagon and Triangle Non-Colliding\n    Point firstShape6[] = {{0, 0}, {3, 0}, {3, 2}, {0, 2}};\n    Point secondShape6[] = {{5, 5}, {7, 5}, {8, 7}, {7, 9}, {5, 9}, {4, 7}, {4, 6}};\n\n    TestCase TestCase6 = {firstShape6, 4, secondShape6, 7, false}; // Quadrilateral and Heptagon Non-Colliding\n    Point firstShape7[] = {{0, 4}, {2, 4}, {3, 5}, {3, 7}, {2, 8}, {0, 8}, {-1, 7}, {-1, 5}};\n    Point secondShape7[] = {{5, 0}, {8, 2}, {6, 4}};\n\n    TestCase TestCase7 = {firstShape7, 8, secondShape7, 3, false}; // Octagon and Triangle Non-Colliding\n    Point firstShape8[] = {{0, 0}, {1, 0}, {2, 0}, {3, 0}, {4, 0}, {5, 0}, {6, 0}, {7, 0}, {8, 0}, {9, 0}, {9, 1}, {8, 1}, {7, 1}, {6, 1}, {5, 1}, {4, 1}, {3, 1}, {2, 1}, {1, 1}, {0, 1}, {0, 2}, {1, 2}, {2, 2}, {3, 2}, {4, 2}, {5, 2}, {6, 2}, {7, 2}, {8, 2}, {9, 2}, {9, 3}, {8, 3}, {7, 3}, {6, 3}, {5, 3}, {4, 3}, {3, 3}, {2, 3}, {1, 3}, {0, 3}};\n    Point secondShape8[] = {{15, 15}, {16, 15}, {17, 15}, {18, 15}, {19, 15}, {20, 15}, {21, 15}, {22, 15}, {23, 15}, {24, 15}, {24, 16}, {23, 16}, {22, 16}, {21, 16}, {20, 16}, {19, 16}, {18, 16}, {17, 16}, {16, 16}, {15, 16}, {15, 17}, {16, 17}, {17, 17}, {18, 17}, {19, 17}, {20, 17}, {21, 17}, {22, 17}, {23, 17}, {24, 17}, {24, 18}, {23, 18}, {22, 18}, {21, 18}, {20, 18}, {19, 18}, {18, 18}, {17, 18}, {16, 18}, {15, 18}};\n\n    TestCase TestCase8 = {firstShape8, 40, secondShape8, 40, false}; // Two Shapes with 40 points Non-Colliding\n    TestCase TestCases[] ={TestCase1, TestCase2, TestCase3, TestCase4, TestCase5, TestCase6, TestCase7, TestCase8};\n    \n    // Preallocate device memory for shapes and collision result\n    Point *firstShape_d, *secondShape_d;\n    int *collision_d;\n    \n    // Assuming maximum number of vertices in any test case is 40 for allocation\n    size_t maxVertices1 = 40;\n    size_t maxVertices2 = 40;\n\n    // Create CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    cudaStreamCreate(&stream);\n\n    // No dynamic shared memory is allocated during launch\n    size_t sharedMemSize = 0; \n\n    // Allocate maximum required device memory once asynchronously\n    CUDA_CHECK(cudaMallocAsync((void **)&firstShape_d, maxVertices1 * sizeof(Point), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&secondShape_d, maxVertices2 * sizeof(Point), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&collision_d, sizeof(int), stream));\n\n    int numTestCases = sizeof(TestCases) / sizeof(TestCase);\n    for (int tc = 0; tc < numTestCases; ++tc) {\n        TestCase currentTest = TestCases[tc];\n\n        // Initialize collision flag\n        int collision_h = 1;\n        CUDA_CHECK(cudaMemcpyAsync(collision_d, &collision_h, sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        // Copy shapes to device\n        CUDA_CHECK(cudaMemcpyAsync(firstShape_d, currentTest.firstShape, currentTest.numVertices1 * sizeof(Point), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(secondShape_d, currentTest.secondShape, currentTest.numVertices2 * sizeof(Point), cudaMemcpyHostToDevice, stream));\n\n        // Calculate number of normals\n        int numOfNormals = currentTest.numVertices1 + currentTest.numVertices2;\n\n        // Calculate number of blocks needed to process all axes\n        int blocksPerGrid = (numOfNormals + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n        // Define grid and block dimensions with dim3\n        dim3 gridSize(blocksPerGrid, 1, 1);\n        dim3 blockSize(BLOCK_SIZE, 1, 1);\n\n        // Prepare kernel arguments\n        void* kernelArgs[] = {(void*)&firstShape_d, (void*)&currentTest.numVertices1, (void*)&secondShape_d, (void*)&currentTest.numVertices2, (void*)&collision_d};\n\n        // Grid: (ceil(N/256), 1, 1) -> (1, 1, 1)\n        // Block: (256, 1, 1)\n        // Launch the kernel asynchronously without dynamic shared memory\n        CUDA_CHECK(cudaLaunchKernel((const void*)k_satCollisionDetectionKernel, gridSize, blockSize, kernelArgs, sharedMemSize, stream));\n\n        // Copy the result back to host\n        CUDA_CHECK(cudaMemcpyAsync(&collision_h, collision_d, sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Synchronize the stream to ensure kernel completion and memcpy completion\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Validate the result\n        assert(collision_h == currentTest.expectedCollision);\n    }\n\n    // Clean up\n    CUDA_CHECK(cudaFreeAsync(firstShape_d, stream));\n    CUDA_CHECK(cudaFreeAsync(secondShape_d, stream));\n    CUDA_CHECK(cudaFreeAsync(collision_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_satCollisionDetectionKernel(const Point* firstShape, int numVertices1, const Point* secondShape, int numVertices2, int* collisionDetected) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/120", "date": "2025-07-30", "prompt": "Write a CUDA kernel to count 2D points inside 2D polygons, given as vertex lists, by assigning each polygon to a block and each point to a thread, using warp-level reduction for local count accumulation and atomicAdd for global total.\n\nThe kernel signature is __global__ void k_countPointsInPolygons(const float2* points_d, const int* polyOffsets_d, const float2* polyVertices_d, int* counts_d, int numPoints, int numPolygons), where points_d is the array of points, polyOffsets_d defines the subset vertices in polyVertices_d for the respective polygon, polyVertices is the total vertices set of all polygons, counts_d is the computed result, numPoints is the total number of points, and numPolygons is the total number of polygons in the data set.\n\n>>> k_countPointsInPolygons(points_d:{{2.54f, 3.59f}, {93.56f, 77.84f}},\n                            polyOffsets_d:{0, 3, 6},\n                            polyVertices_d:{{108.63f, 88.34f}, {80.57f, 76.69f}, {-42.25f, 54.29f}, {94.08f, 163.08f}, {52.27f, -4.073f},{111.56f, 72.67f}},\n                            counts_d: {0},\n                            numPoints: 2,\n                            numPolygons: 2) -> counts_d: {0, 1};\n>>> k_countPointsInPolygons(points_d:{{52.75f, 88.66f}, {37.12f, 11.27f}},\n                            polyOffsets_d:{0, 7, 14},\n                            polyVertices_d:{{73.40f, 74.87f}, {63.12f, 73.70f}, {50.87f, 89.70f}, {12.73f, 115.39f}, {24.85f, 75.04f}, {-27.41f, 40.11f}, {-8.50f, 26.75f}, {68.34f, 60.59f}, {-67.15f, 59.18f}, {-29.74f, 36.99f}, {-19.55f, -62.04f}, {-6.79f, -71.06f}, {30.69f, -50.15f}, {110.93f, 7.72f}},\n                            counts_d: {0},\n                            numPoints: 2,\n                            numPolygons: 2) -> counts_d: {0, 1};\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n#include <stdio.h>\n#include <stdlib.h>\n#include <cassert>\n\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                                                                   \\\ndo {                                                                                       \\\n    cudaError_t error = call;                                                              \\\n    if(error != cudaSuccess) {                                                             \\\n        fprintf(stderr,                                                                    \\\n            \"CUDA Error: %s at %s:%d\\n\",                                                   \\\n            cudaGetErrorString(error),                                                     \\\n            __FILE__,                                                                      \\\n            __LINE__);                                                                     \\\n        exit(error);                                                                       \\\n    }                                                                                      \\\n} while(0)\n\n\n// CUDA Kernel signature to count points in polygons using warp-level reduction\n__global__ void k_countPointsInPolygons(const float2* points_d, const int* polyOffsets_d, const float2* polyVertices_d, int* counts_d, int numPoints, int numPolygons);\n\nvoid launch() {\n    const unsigned int NUM_TEST_CASES = 6;\n    const unsigned int MAX_POINTS_COUNT = 16;\n    const unsigned int MAX_POLYGONS_COUNT = 23;\n    const unsigned int MAX_TOTAL_VERTICES_COUNT = 119;\n\n    const unsigned int BLOCK_SIZE = 256;\n    int numPointsPerCase[NUM_TEST_CASES] = {0, 15, 10, 12, 16, 14};\n    int numPolygonsPerCase[NUM_TEST_CASES] = {0, 2, 10, 12, 8, 23};\n\n    float2 pointsPerCase[NUM_TEST_CASES][MAX_POINTS_COUNT] = {\n        {},\n        {{25.3935f, 15.9624f}, {78.6924f, 80.5324f}, {74.2756f, 1.8979f}, {72.6130f, 65.2378f}, {51.3578f, 80.0843f}, {19.1614f, 14.4334f}, {82.8588f, 62.6612f}, {44.3258f, 36.3087f}, {64.8938f, 93.8025f}, {43.0081f, 9.7263f}, {61.5176f, 97.9455f}, {63.7261f, 46.6297f}, {78.2126f, 18.8877f}, {51.3325f, 79.7097f}, {23.3491f, 25.6204f}},\n        {{84.0023f, 20.5858f}, {78.2397f, 80.0574f}, {16.2814f, 1.6055f}, {36.4122f, 58.2603f}, {45.8549f, 90.1331f}, {77.7861f, 15.7065f}, {65.9692f, 73.8676f}, {68.9573f, 84.1485f}, {9.9052f, 71.8982f}, {22.8545f, 65.1325f}},\n        {{68.9511f, 9.5930f}, {34.3738f, 8.4515f}, {72.2493f, 14.2196f}, {20.5987f, 7.7860f}, {99.2154f, 51.0257f}, {4.6489f, 51.3185f}, {81.9002f, 9.2764f}, {15.7800f, 3.3111f}, {29.6356f, 12.8318f}, {99.9346f, 35.9169f}, {36.7388f, 0.8057f}, {80.3509f, 82.8087f}},\n        {{5.9535f, 87.1826f}, {65.9460f, 68.5129f}, {39.0313f, 9.3470f}, {70.8545f, 25.8169f}, {5.5400f, 92.1123f}, {37.9600f, 12.1133f}, {86.6538f, 35.3229f}, {97.1403f, 40.4937f}, {0.8192f, 41.8948f}, {45.6392f, 59.6516f}, {80.8463f, 81.4288f}, {4.7672f, 73.0398f}, {94.5803f, 96.5074f}, {3.7246f, 22.8596f}, {82.4363f, 23.5031f}, {27.5142f, 42.0012f}},\n        {{61.3136f, 21.5347f}, {13.8689f, 94.5953f}, {94.1111f, 37.1791f}, {85.1030f, 92.3827f}, {10.1354f, 35.0836f}, {5.6592f, 40.2589f}, {72.4078f, 3.6739f}, {53.0263f, 83.1622f}, {72.6549f, 27.2877f}, {84.5994f, 77.4742f}, {21.9356f, 20.3580f}, {23.1267f, 1.1188f}, {38.6358f, 23.4979f}, {2.4899f, 49.1200f}}\n    };\n\n    int polyOffsetsPerCase[NUM_TEST_CASES][MAX_POLYGONS_COUNT+1] = {\n        {},\n        {0, 3, 11},\n        {0, 3, 11, 14, 20, 24, 31, 36, 44, 51, 58},\n        {0, 3, 7, 10, 17, 23, 31, 38, 44, 47, 53, 60, 65},\n        {0, 3, 8, 15, 18, 25, 29, 37, 42},\n        {0, 4, 10, 18, 22, 27, 30, 34, 37, 45, 48, 54, 60, 65, 72, 80, 86, 92, 95, 98, 103, 109, 113, 119}\n    };\n\n    float2 polyVerticesPerCase[NUM_TEST_CASES][MAX_TOTAL_VERTICES_COUNT] = {\n        {},\n        {{134.3617f, 67.3507f}, {46.8174f, 70.9466f}, {87.7588f, 22.4348f}, {-53.3826f, 79.5700f}, {-22.3968f, 75.9385f}, {-33.3050f, 64.6587f}, {-30.1813f, 63.1465f}, {0.1456f, 59.8312f}, {11.2200f, 48.6946f}, {19.4059f, 50.3045f}, {20.7879f, 78.7427f}},\n        {{56.6479f, 56.7658f}, {40.8065f, 55.8632f}, {80.9999f, 22.6961f}, {109.7568f, 24.4861f}, {75.7384f, 14.8199f}, {54.5859f, 43.8566f}, {65.9880f, 12.7185f}, {64.2804f, 11.1368f}, {64.2785f, -30.7154f}, {70.1765f, -10.9582f}, {99.9139f, 1.8788f}, {72.9123f, 97.6707f}, {42.3471f, 74.1175f}, {76.7311f, 72.6132f}, {139.3859f, 96.7507f}, {105.8097f, 110.3992f}, {104.6479f, 125.0762f}, {92.8866f, 130.7146f}, {87.2788f, 78.7460f}, {104.7586f, 87.7257f}, {89.0710f, 99.7146f}, {74.2983f, 102.4218f}, {63.6623f, 106.0967f}, {54.7968f, 60.1081f}, {44.7593f, 20.8099f}, {25.1003f, 14.5671f}, {19.4293f, 30.8442f}, {8.5566f, 42.3731f}, {-5.2437f, 40.2896f}, {10.5937f, -5.0472f}, {22.7524f, -18.6959f}, {41.5925f, 34.5525f}, {-3.9213f, 64.4598f}, {-7.7634f, 20.7071f}, {1.5913f, -0.8251f}, {26.5296f, 12.5197f}, {82.6740f, 116.5693f}, {72.2261f, 117.5951f}, {48.9520f, 94.9286f}, {48.4285f, 92.7039f}, {47.5101f, 90.9926f}, {33.2878f, 67.9994f}, {53.2091f, 62.1639f}, {57.0307f, 73.4961f}, {38.6922f, 108.9444f}, {28.8120f, 115.8366f}, {-5.7458f, 97.4224f}, {-6.8593f, 86.1534f}, {-2.5615f, 83.6504f}, {19.8517f, 46.1851f}, {23.2992f, 75.9435f}, {24.5139f, 80.3184f}, {41.2134f, 95.6397f}, {5.9310f, 114.0931f}, {-0.1396f, 77.0662f}, {-19.3324f, 68.8902f}, {14.5231f, 60.2121f}, {20.1672f, 69.3018f}},\n        {{55.2293f, 77.5417f}, {8.1647f, 34.0828f}, {37.0258f, 54.0557f}, {50.3118f, 50.0981f}, {6.2364f, 27.4681f}, {74.0279f, 0.2347f}, {59.4276f, 18.6317f}, {62.3479f, 59.9583f}, {51.9358f, 48.0523f}, {35.0476f, 0.8496f}, {67.0537f, 23.2495f}, {56.7724f, 51.3218f}, {43.0253f, 13.6831f}, {48.1178f, 8.3985f}, {52.8082f, 6.4274f}, {80.7712f, -4.0188f}, {90.2737f, -12.3314f}, {98.3461f, 101.2590f}, {77.8754f, 93.8470f}, {81.5063f, 111.8132f}, {57.5432f, 64.3428f}, {77.8353f, 78.8151f}, {94.3956f, 79.2274f}, {125.8321f, 50.9257f}, {109.9831f, 51.2148f}, {128.0208f, 76.2115f}, {99.6804f, 55.2562f}, {83.4562f, 85.8732f}, {99.5332f, 39.5124f}, {111.8878f, 10.3778f}, {117.6762f, 21.4250f}, {108.7409f, 69.2312f}, {97.3609f, 41.2653f}, {62.5299f, 46.4012f}, {70.4528f, 33.9400f}, {113.5160f, 14.7360f}, {124.3545f, 16.9632f}, {109.9968f, 27.0882f}, {63.8445f, 32.5492f}, {33.0780f, 17.3937f}, {41.3715f, 0.0632f}, {52.4508f, 3.4700f}, {81.3959f, -9.2477f}, {79.0691f, 1.3259f}, {92.0092f, 18.6972f}, {68.9770f, 13.5766f}, {85.7376f, -14.7095f}, {69.1550f, 59.1593f}, {55.9092f, 43.5145f}, {62.1752f, 35.3168f}, {54.5189f, 35.4208f}, {76.5204f, -3.2246f}, {102.1076f, 13.8841f}, {120.1010f, 40.7706f}, {87.2862f, 51.3812f}, {87.9691f, 49.4913f}, {71.7630f, 56.0025f}, {94.3277f, 36.7140f}, {82.6369f, 31.0020f}, {94.5882f, 21.7822f}, {29.0014f, 75.2209f}, {6.5746f, 100.2372f}, {-19.1631f, 69.1360f}, {-7.9727f, 67.8200f}, {50.7670f, 55.8857f}},\n        {{15.9238f, 33.6296f}, {-10.7460f, 42.7896f}, {-29.1164f, -1.1826f}, {108.0594f, 7.8457f}, {77.7694f, 53.0625f}, {77.1891f, 10.5083f}, {61.2914f, 1.1518f}, {61.4407f, -39.4808f}, {39.9643f, 84.4824f}, {16.2293f, 101.0130f}, {-5.1434f, 71.2431f}, {7.9201f, 59.1707f}, {-36.4552f, 27.7391f}, {-41.9747f, 10.7190f}, {57.7363f, 29.2877f}, {31.5345f, 22.9040f}, {4.9472f, 24.7763f}, {24.4523f, -5.4072f}, {24.5965f, 23.4213f}, {-1.7690f, 24.9921f}, {27.6597f, 11.5915f}, {-20.5477f, 12.5831f}, {35.9704f, -0.5741f}, {66.2204f, -6.1002f}, {62.2674f, 4.7513f}, {23.7963f, 78.5591f}, {39.9877f, 26.5168f}, {47.2572f, 9.0889f}, {48.3022f, 41.8915f}, {88.1789f, 86.2978f}, {58.4967f, 55.3120f}, {55.9288f, 53.3836f}, {12.0736f, 105.7440f}, {26.1890f, 56.5114f}, {-11.1118f, 46.5306f}, {15.6433f, -29.8749f}, {98.0036f, 6.0696f}, {136.6720f, 70.3806f}, {110.1534f, 49.4757f}, {130.9519f, 100.2367f}, {101.7868f, 94.4242f}, {142.3286f, 7.7358f}},\n        {{39.1525f, 136.0314f}, {58.9527f, 21.5976f}, {65.3684f, 48.2486f}, {94.4780f, 58.4242f}, {113.4197f, 104.9353f}, {113.5203f, 126.1892f}, {75.1015f, 69.9775f}, {35.7762f, 23.6532f}, {65.0925f, 27.7896f}, {35.1015f, -0.3997f}, {53.3435f, 36.4378f}, {18.7757f, 25.0511f}, {-53.6928f, 43.0815f}, {-30.1476f, -26.8485f}, {-6.6556f, -25.5359f}, {5.1446f, -36.1802f}, {19.9754f, 8.9401f}, {86.1090f, 1.6709f}, {84.0523f, 54.8230f}, {14.4555f, 18.0797f}, {-8.8044f, -18.3477f}, {61.9279f, -15.3647f}, {123.0983f, 81.0299f}, {107.5528f, 75.7157f}, {43.2018f, 74.8049f}, {28.0695f, -11.1196f}, {60.5985f, 16.9425f}, {51.9968f, 112.0940f}, {58.1027f, 30.0324f}, {91.8453f, 82.6109f}, {154.5059f, 54.5475f}, {90.7442f, 61.6303f}, {52.6430f, 72.0547f}, {130.3504f, 44.4099f}, {63.3580f, 61.7601f}, {54.8624f, 51.9889f}, {47.0117f, 31.9965f}, {118.7183f, 30.6116f}, {82.8192f, 73.8630f}, {57.0713f, 50.1101f}, {42.9553f, 84.8396f}, {51.9142f, 52.2758f}, {17.0512f, 58.4076f}, {25.6934f, 41.6060f}, {62.8683f, 22.4418f}, {-34.9576f, 104.2821f}, {-45.3225f, 59.2706f}, {6.3679f, 5.0496f}, {0.0253f, 153.6205f}, {-3.3306f, 165.4586f}, {-54.6575f, 156.1308f}, {-65.4280f, 110.2866f}, {-67.3530f, 82.1265f}, {32.0247f, 82.6117f}, {-30.2746f, 97.5766f}, {-10.4899f, 57.6101f}, {-26.6975f, 11.4278f}, {-10.3067f, -2.7274f}, {13.2934f, 43.7130f}, {81.1809f, 14.3672f}, {28.5567f, 19.9525f}, {40.3849f, -13.0171f}, {59.8539f, -31.8926f}, {62.8342f, 22.4079f}, {76.9268f, 19.4592f}, {34.6956f, 91.9640f}, {55.0788f, 93.2257f}, {51.9831f, 117.0970f}, {40.3579f, 154.7361f}, {-27.8643f, 154.3441f}, {-26.0759f, 58.6327f}, {-0.5718f, 66.9588f}, {69.4333f, 91.5448f}, {27.1546f, 91.1009f}, {74.2780f, 132.1854f}, {60.2099f, 139.3727f}, {46.6496f, 141.5777f}, {-19.4228f, 64.2178f}, {66.6184f, 78.3575f}, {68.8609f, 79.9441f}, {29.1608f, 74.5063f}, {25.3886f, 96.5535f}, {16.6441f, 113.7130f}, {-14.5048f, 62.2992f}, {-21.0423f, 27.7603f}, {24.4740f, 46.4402f}, {-6.2913f, 94.7375f}, {17.4469f, 54.7311f}, {11.3791f, 50.0966f}, {-27.6568f, 22.8939f}, {28.1530f, 3.7590f}, {35.4558f, 44.1193f}, {41.4080f, 153.0871f}, {110.5694f, 69.4201f}, {126.7183f, 68.1848f}, {20.6167f, 38.5994f}, {-24.0773f, -13.4292f}, {76.1802f, -8.1312f}, {16.1107f, 58.8548f}, {25.5474f, 41.0822f}, {21.1624f, 31.2073f}, {8.7804f, -5.4628f}, {54.0051f, 36.9842f}, {74.1395f, 117.6636f}, {65.3431f, 82.4008f}, {62.0612f, 78.9988f}, {-9.2448f, 67.3225f}, {-1.7858f, 61.1156f}, {140.9129f, 35.1002f}, {50.5060f, 80.4414f}, {16.9206f, 60.2410f}, {50.8593f, 20.8864f}, {75.9331f, 24.3188f}, {77.5725f, 113.3023f}, {61.7046f, 124.4735f}, {21.8253f, 101.6042f}, {-16.4761f, 142.8666f}, {2.7119f, 97.5965f}, {60.4517f, 57.3037f}}\n    };\n\n    int validCountsPerCasePerPolygon[NUM_TEST_CASES][MAX_POLYGONS_COUNT] = {\n        {},\n        {2, 0},\n        {0, 0, 2, 0, 2, 1, 0, 0, 1, 1},\n        {0, 0, 0, 2, 1, 1, 1, 2, 2, 2, 1, 0},\n        {0, 2, 5, 0, 3, 0, 9, 0},\n        {1, 1, 4, 5, 2, 1, 0, 0, 2, 0, 1, 2, 0, 1, 2, 3, 3, 0, 2, 2, 2, 1, 2}\n    };\n\n    cudaDeviceProp deviceProp;\n    int currentDevice;\n    CUDA_CHECK(cudaGetDevice(&currentDevice));\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, currentDevice));\n\n    int warpSize = deviceProp.warpSize;\n    int numSMs = deviceProp.multiProcessorCount;\n    int maxBlocksPerSM = deviceProp.maxBlocksPerMultiProcessor;\n    int maxNumBlocks = numSMs * maxBlocksPerSM;\n\n    for (int t = 0; t < NUM_TEST_CASES; t++) {\n        // Setting input params for test\n        int numPoints_h = numPointsPerCase[t];\n        int numPolygons_h = numPolygonsPerCase[t];\n        float2 *points_h = pointsPerCase[t];\n        float2 *polyVertices_h = polyVerticesPerCase[t];\n        int *polyOffsets_h = polyOffsetsPerCase[t];\n        int  *validCounts_h = validCountsPerCasePerPolygon[t];\n\n        // Allocate device memory for inputs and output counts.\n        float2 *points_d;\n        float2 *polyVertices_d;\n        int *polyOffsets_d;\n        int *counts_d;\n\n        size_t pointsSize = numPoints_h * sizeof(float2);\n        size_t offsetsSize = (numPolygons_h + 1) * sizeof(int);\n        size_t countsSize = numPolygons_h * sizeof(int);\n\n        int numVerts = polyOffsets_h[numPolygons_h];\n        size_t verticesSize = numVerts * sizeof(float2);\n\n        int gpuCounts_h[MAX_POLYGONS_COUNT] = {0};\n\n        cudaStream_t stream;\n        CUDA_CHECK(cudaStreamCreate(&stream));\n\n        // Locate memory space on device.\n        CUDA_CHECK(cudaMallocAsync(&points_d, pointsSize, stream));\n        CUDA_CHECK(cudaMallocAsync(&polyOffsets_d, offsetsSize, stream));\n        CUDA_CHECK(cudaMallocAsync(&polyVertices_d, verticesSize, stream));\n        CUDA_CHECK(cudaMallocAsync(&counts_d, countsSize, stream));\n\n        // Duplicate input data on to device.\n        CUDA_CHECK(cudaMemcpyAsync(points_d, points_h, pointsSize, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(polyOffsets_d, polyOffsets_h, offsetsSize, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(polyVertices_d, polyVertices_h, verticesSize, cudaMemcpyHostToDevice, stream));\n\n        CUDA_CHECK(cudaMemsetAsync(counts_d, 0, countsSize, stream));\n\n        void *args[] = {&points_d,\n                        &polyOffsets_d,\n                        &polyVertices_d,\n                        &counts_d,\n                        &numPoints_h,\n                        &numPolygons_h};\n\n        int numBlocks = max(1, min(numPolygons_h,maxNumBlocks));\n    \tint numWarps = (BLOCK_SIZE + warpSize - 1) / warpSize;\n    \tint shmemBytes = numWarps * sizeof(int);\n\n        // Launch kernel\n        // Grid: (numBlocks, 1, 1)\n        // Block: (256, 1, 1)\n        dim3 blockSize(BLOCK_SIZE);\n        dim3 gridSize(numBlocks);\n        CUDA_CHECK(cudaLaunchKernel((void*)k_countPointsInPolygons,\n                                    gridSize,\n                                    blockSize,\n                                    args,\n                                    shmemBytes,\n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validation\n        CUDA_CHECK(cudaMemcpyAsync(gpuCounts_h, counts_d, countsSize, cudaMemcpyDeviceToHost, stream));\n        for (int i = 0; i < numPolygons_h; i++) {\n           assert(gpuCounts_h[i] == validCounts_h[i]);\n        }\n\n        //Memory clean up\n        CUDA_CHECK(cudaFreeAsync(points_d, stream));\n        CUDA_CHECK(cudaFreeAsync(polyOffsets_d, stream));\n        CUDA_CHECK(cudaFreeAsync(polyVertices_d, stream));\n        CUDA_CHECK(cudaFreeAsync(counts_d, stream));\n        CUDA_CHECK(cudaStreamDestroy(stream));\n    }\n}\n\n\n__global__ void k_countPointsInPolygons(const float2* points_d, const int* polyOffsets_d, const float2* polyVertices_d, int* counts_d, int numPoints, int numPolygons) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/121", "date": "2025-07-30", "prompt": "Write a CUDA kernel to apply a bilateral filter for edgepreserving and smoothing on a 2D image.\n\nThe signature of the function is __global__ void k_bilateralFilterKernel(float *inputImage, float *filteredImage, int widthOfImage, int heightOfImage, int radiusOfFilter, float sigmaOfSpatial, float sigmaOfRange), where inputImage represents the input image pixel data, filteredImage represents the output image where the filtered result is stored, widthOfImage gives the width (number of pixels per row) of the input image, heightOfImage represents the height of the input image, radiusOfFilter represents the radius of the filter window, sigmaOfSpatial the standard deviation for the spatial (distance) weighting in the gaussian function, sigmaOfRange The standard deviation for the range (intensity difference) weighting in the gaussian function.\n\n>>> k_bilateralFilterKernel({4, 4}, filteredImage, 3, 3.0f, 25.0f) -> filteredImage: ({123.260002f, 7.377082f, 95.625015f, 95.624985f, 183.872910f})\n>>> k_bilateralFilterKernel({128, 128}, filteredImage, 5, 4.0f, 30.0f ) -> filteredImage: ({127.500031f, 2.302135f, 126.503929f, 126.503883f, 250.705582f})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cstdio>\n#include <cmath>\n#include <algorithm>\n#include <assert.h>\n#include <iostream>\n\n#define BLOCK_SIZE 16\n#define TOLERANCE 0.01f\n#define EPSILON 1e-5f\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n    cudaError_t error = call;                                                      \\\n    if (error != cudaSuccess) {                                                    \\\n        fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),    \\\n                __FILE__, __LINE__);                                               \\\n        exit(error);                                                               \\\n    }                                                                              \\\n} while (0)\n\n__global__ void k_bilateralFilterKernel(float *inputImage, float *filteredImage, int widthOfImage, int heightOfImage, int radiusOfFilter, float sigmaOfSpatial, float sigmaOfRange);\n\nvoid launch() {\n\n    // 2D image test cases\n    const int NUMBER_OF_TESTS = 8;\n    int widthOfImages[NUMBER_OF_TESTS] = {4, 64, 128, 128, 256, 256,  32, 100 };\n    int heightOfImages[NUMBER_OF_TESTS] = {4, 64, 128, 128, 256, 256,  32, 100 };\n    int radii[NUMBER_OF_TESTS] = {3, 2, 3, 5, 3, 5, 1, 2 };\n    float sigmaOfSpatials[NUMBER_OF_TESTS] = {3.0f, 2.0f, 3.0f, 4.0f, 3.0f, 4.0f, 1.0f, 2.0f};\n    float sigmaOfRanges[NUMBER_OF_TESTS] = {25.0f, 20.0f, 25.0f, 30.0f, 25.0f, 30.0f, 15.0f, 20.0f};\n\n    // For each test case, we store 5 expected values center, topLeft, topRight, bottomLeft, bottomRight\n    float expectedOutput[NUMBER_OF_TESTS][5] = {\n        {123.260002f, 7.377082f, 95.625015f, 95.624985f, 183.872910f},  \n        {127.499992f, 2.067940f, 125.507812f, 125.507812f, 248.947693f},  \n        {127.499985f, 1.517415f, 126.503914f, 126.503883f, 251.490356f},  \n        {127.500031f, 2.302135f, 126.503929f, 126.503883f, 250.705582f}, \n        {127.499992f, 0.761391f, 127.001984f, 127.001968f, 253.242493f},  \n        {127.500046f, 1.157711f, 127.001938f, 127.001915f, 252.846146f}, \n        {127.500000f, 2.099976f, 123.515625f, 123.515640f, 244.931259f},  \n        {127.499985f, 1.335109f, 126.224991f, 126.225006f, 251.114883f}   \n    };\n    \n    // Find the maximum image size\n    int maxWidth = *std::max_element(widthOfImages, widthOfImages + NUMBER_OF_TESTS);\n    int maxHeight = *std::max_element(heightOfImages, heightOfImages + NUMBER_OF_TESTS);\n    int maxSize = maxWidth * maxHeight;\n\n    // Allocate device memory asynchronously\n    float *inputImage_d, *filteredImage_d;\n    float *inputImage_h, *filteredImage_h;\n\n    // Create a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate host memory\n    inputImage_h = new float[maxSize];\n    filteredImage_h = new float[maxSize];\n\n    // Allocate device memory asynchronously\n    CUDA_CHECK(cudaMallocAsync(&inputImage_d, maxSize * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&filteredImage_d, maxSize * sizeof(float), stream));\n     \n    // Make sure allocations are complete before proceeding\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n\n    // Run the 2D image test cases\n    for (int testCase = 0; testCase < NUMBER_OF_TESTS; testCase++) {\n        int width = widthOfImages[testCase];\n        int height = heightOfImages[testCase];\n        int size = width * height;\n        \n        // Initialize input with a gradient pattern\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                inputImage_h[y * width + x] = (float)(x + y) / (width + height) * 255.0f;\n            }\n        }\n\n        // Async copy from host to device\n        CUDA_CHECK(cudaMemcpyAsync(inputImage_d, inputImage_h, size * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Define block and grid sizes \n        dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE, 1);\n        \n        // Calculate grid size using fewer blocks\n        int gridX = (width + BLOCK_SIZE - 1) / BLOCK_SIZE;\n        int gridY = (height + BLOCK_SIZE - 1) / BLOCK_SIZE;\n        \n        // Cap the grid dimensions to reduce number of blocks\n        gridX = min(gridX, 16);\n        gridY = min(gridY, 16);\n        \n        dim3 gridSize(gridX, gridY, 1);\n\n        // Create argument array for kernel launch\n        int radius = radii[testCase];\n        float sigmaS = sigmaOfSpatials[testCase];\n        float sigmaR = sigmaOfRanges[testCase];\n        \n        //Kernel Launch \n        void* kernelArgs[] = {(void*)&inputImage_d, (void*)&filteredImage_d, (void*)&width, (void*)&height, (void*)&radius,         (void*)&sigmaS, (void*)&sigmaR};\n        \n        // Launch the bilateral filter kernel asynchronously using cudaLaunchKernel\n        CUDA_CHECK(cudaLaunchKernel((const void*)k_bilateralFilterKernel, gridSize, blockSize, kernelArgs, 0, stream));\n\n        // Async copy from device to host\n        CUDA_CHECK(cudaMemcpyAsync(filteredImage_h, filteredImage_d, size * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        \n        // Synchronize the stream to ensure results are ready\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Define indices for key pixels\n        int centerIdx = (height/2) * width + (width/2);\n        int topLeftIdx = 0;\n        int topRightIdx = width - 1;\n        int bottomLeftIdx = (height - 1) * width;\n        int bottomRightIdx = (height - 1) * width + (width - 1);\n        \n        // Check sample pixels against expected values\n        assert(fabs(filteredImage_h[centerIdx] - expectedOutput[testCase][0]) < TOLERANCE);\n        assert(fabs(filteredImage_h[topLeftIdx] - expectedOutput[testCase][1]) < TOLERANCE);\n        assert(fabs(filteredImage_h[topRightIdx] - expectedOutput[testCase][2]) < TOLERANCE);\n        assert(fabs(filteredImage_h[bottomLeftIdx] - expectedOutput[testCase][3]) < TOLERANCE);\n        assert(fabs(filteredImage_h[bottomRightIdx] - expectedOutput[testCase][4]) < TOLERANCE);\n    }\n\n    // Cleanup with async free operations\n    CUDA_CHECK(cudaFreeAsync(inputImage_d, stream));\n    CUDA_CHECK(cudaFreeAsync(filteredImage_d, stream));\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n// Bilateral Filter Kernel Applies an edge-preserving and smoothing filter,\n// each output pixel is computed as a weighted average of its neighborhood,\n// where the weights depend on both the spatial distance and the intensity difference.\n__global__ void k_bilateralFilterKernel(float *inputImage, float *filteredImage, int widthOfImage, int heightOfImage, int radiusOfFilter, float sigmaOfSpatial, float sigmaOfRange) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/122", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform matrix multiplication by using tensor cores by using mma ptx instruction of dimension m16n8k8 for Ampere Architecture.\nConfigure it in row major by column major format, using bf16 data type for input matrices A (row major) and B (column major), and f32 for accumulator matrix C(row major), all input\nmatrices will have m16n8k8 compatible dimensions.\n\nThe signature of the function is __global__ void k_mmaTensorMatMul(__nv_bfloat16 *inputLayerA_d, __nv_bfloat16 *inputLayerB_d, float *outputLayer_d, int mDim, int nDim, int kDim), where inputLayerA_d is first input layer matrix with dimension mDim x kDim, inputLayerB_d is second input layer matrix with dimension kDim x nDim, outputLayer_d is output matrix with dimension mDim x nDim.\n\n>>> k_mmaTensorMatMul({3, 6, 7, 5}, {3, 5}, outputLayer_d, 2, 1, 2)-> outputLayer_d: ({44, 43})\n>>> k_mmaTensorMatMul({3, 6, 17, 15}, {13, 15}, outputLayer_d, 2, 1, 2)-> outputLayer_d: ({294, 303})\n", "cc_flags": "-arch=sm_80", "ld_flags": "", "declaration": "\n#include <cstdio>\n#include <cassert>\n#include <cstdint>\n#include <random>\n#include <cuda_runtime.h>\n#include <mma.h>\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n#undef NDEBUG\n\n#define MMA_M 16\n#define MMA_N 8\n#define MMA_K 8\n\n__global__ void k_mmaTensorMatMul(__nv_bfloat16 *inputLayerA_d, __nv_bfloat16 *inputLayerB_d, float *outputLayer_d, int mDim, int nDim, int kDim);\n\n// Function to compute valid reference result\nvoid cpuMatMulReference(const __nv_bfloat16* A,\n                        const __nv_bfloat16* B,\n                        float* cpuRefC,\n                        int M,\n                        int N,\n                        int K) {\n    for (int i = 0; i < M; i++) {\n        for (int j = 0; j < N; j++) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; k++) {\n                float a_val = static_cast<float>(A[i*K + k]);\n                float b_val = static_cast<float>(B[k*N + j]);\n                sum += a_val * b_val;\n            }\n            cpuRefC[i*N + j] = sum;\n        }\n    }\n}\n\nvoid launch() {\n    const int TEST_CASE_COUNT = 7;\n    //Test case dimensions {M, N, K}\n    const int TEST_CASES_DIMS[TEST_CASE_COUNT][3] = {{16,16,16}, {512,512,512}, {32,16,32}, {256, 256, 256}, {64, 64, 64} , {64, 32, 32}, {128, 128, 128}};\n\n    //Tolerance for validation, set to 1% due to nature of half precision operations\n    const float TOLERANCE  = 0.01;\n    const int BLOCK_SIZE = 256;\n\n    //Set up random number generation\n    std::random_device randomSeedSource; // Automatically configure high quality seed using system info\n    std::mt19937 randEngine(randomSeedSource());\n\n    // Bounded random distribution for test case initialization\n    std::uniform_real_distribution<float> randDist(1.0f, 100.0f);\n\n    for (int i = 0; i < TEST_CASE_COUNT; i++) {\n        // Dimensions of the input and output layers\n        int M = TEST_CASES_DIMS[i][0]; //Number of Rows in Matrix A\n        int N = TEST_CASES_DIMS[i][1]; //Number of Columns in Matrix B\n        int K = TEST_CASES_DIMS[i][2]; //Number of Columns in Matrix A and Rows in Matrix B\n\n        //Pointers for Host Memory\n        __nv_bfloat16* A_h =(__nv_bfloat16*)malloc(M * K * sizeof(__nv_bfloat16));\n        __nv_bfloat16* B_h =(__nv_bfloat16*)malloc(K * N * sizeof(__nv_bfloat16));\n\n        float* cpuC_h =(float*)malloc(M * N * sizeof(float)); // Reference Matrix space allocation on host\n        float* gpuC_h = (float*)malloc(M * N * sizeof(float));// GPU result Matrix space allocation on host\n\n        //Pointers for device memory (GPU)\n        __nv_bfloat16* A_d;\n        __nv_bfloat16* B_d;\n        float* C_d;\n\n        //Populating input matrices with random values\n        for (int i = 0; i < M * K; i++) {\n            float val = randDist(randEngine);\n            A_h[i] = __nv_bfloat16(val);\n        }\n\n        for (int i = 0; i < K * N; i++) {\n            float val = randDist(randEngine);\n            B_h[i] = __nv_bfloat16(val);\n        }\n\n        // Use a CUDA stream for asynchronous operations\n        cudaStream_t stream;\n        CUDA_CHECK(cudaStreamCreate(&stream));\n\n        // Allocate the memory on the device\n        CUDA_CHECK(cudaMallocAsync(&A_d, M * K * sizeof(__nv_bfloat16), stream));\n        CUDA_CHECK(cudaMallocAsync(&B_d, K * N * sizeof(__nv_bfloat16), stream));\n        CUDA_CHECK(cudaMallocAsync(&C_d, M * N * sizeof(float), stream));\n\n        //Load Test Cases\n        CUDA_CHECK(cudaMemcpyAsync(A_d, A_h, M * K * sizeof(__nv_bfloat16), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(B_d, B_h, K * N * sizeof(__nv_bfloat16), cudaMemcpyHostToDevice, stream));\n\n        // Initialize the result on the device\n        CUDA_CHECK(cudaMemsetAsync(C_d, 0, M * N * sizeof(float), stream));\n\n        //Check if the dimensions are divisible by the block tile dimensions\n        assert(M % MMA_M == 0);\n        assert(N % MMA_N == 0);\n        assert(K % MMA_K == 0);\n\n        dim3 gridDim((N + MMA_N - 1) / MMA_N, (M + MMA_M - 1) / MMA_M);\n        dim3 blockDim(BLOCK_SIZE);  // one warp per block\n        int shmemBytes = (MMA_M * MMA_K + MMA_K * MMA_N) * sizeof(__nv_bfloat16);\n\n        // Launch kernel\n        // Grid: ((N + MMA_N - 1/ MMA_N), (M + MMA_M - 1)/ MMA_M, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&A_d,\n                        &B_d,\n                        &C_d,\n                        (void*)&M,\n                        (void*)&N,\n                        (void*)&K};\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_mmaTensorMatMul,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    shmemBytes,\n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        cpuMatMulReference(A_h, B_h, cpuC_h, M, N, K);\n\n        //Copying the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync(gpuC_h, C_d, M * N * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        //Validate the result, with in 1% tolerance\n        for(int t = 0; t < M*N; ++t) {\n            assert(std::fabs((gpuC_h[t] - cpuC_h[t]) / std::fabs(cpuC_h[t])) <= TOLERANCE);\n        }\n\n        //Free up resources\n        CUDA_CHECK(cudaFreeAsync(A_d, stream));\n        CUDA_CHECK(cudaFreeAsync(B_d, stream));\n        CUDA_CHECK(cudaFreeAsync(C_d, stream));\n        CUDA_CHECK(cudaStreamDestroy(stream));\n        free(A_h);\n        free(B_h);\n        free(cpuC_h);\n        free(gpuC_h);\n    }\n}\n\n// Storing 16x8 Matrix Tile\n__device__ __forceinline__ void d_storeMatrixTile16x8(__nv_bfloat16* dst, __nv_bfloat16* (&reg)[4], int dstStrideBytes) {\n   int lane = threadIdx.x % 32;\n\n    //Casting 2x bf16 elements into 4 byte space of uint32_t\n    uint32_t (&regInt)[2] = reinterpret_cast<uint32_t(&)[2]>(reg);\n    uint32_t* dstPtr = reinterpret_cast<uint32_t*>(dst);\n    dstStrideBytes /= sizeof(uint32_t);\n\n    int fragmentRow = lane / 4;\n    int fragmentCol = lane % 4;\n\n    // Adjacent Threads store 4 bytes each\n    dstPtr[fragmentRow * dstStrideBytes + fragmentCol] = regInt[0];\n    fragmentRow += 8;\n    dstPtr[fragmentRow * dstStrideBytes + fragmentCol] = regInt[1];\n}\n\n__device__ __forceinline__ uint32_t d_cvtaToSharedU32(const void* ptr) {\n    unsigned long long address;\n    asm volatile(\"cvta.to.shared.u64 %0, %1;\" : \"=l\"(address) : \"l\"(ptr));\n    return static_cast<uint32_t>(address);\n}\n\n__device__ __forceinline__ void d_storeMatrixTile16x8_f32(float* dst, float reg[4], int n) {\n    int lane = threadIdx.x % 32;  // 0..31\n    int r = lane / 4;        // 0..7 for the top half\n    int c = (lane % 4) * 2;    // columns: 0,2,4,6\n    dst[r * n + c] = reg[0];\n    dst[r * n + c + 1] = reg[1];\n    dst[(r + 8) * n + c] = reg[2];\n    dst[(r + 8) * n + c + 1] = reg[3];\n}\n\n// Kernel: Multiply bf16 matrices inputLayerA_d (mDimx kDim) and inputLayerB_d (kDim x nDim) to produce an output outputLayer_d (mDim x nDim) in f32.\n// Each block (a single warp) computes one 168 output tile using MMA instructions.\n__global__ void k_mmaTensorMatMul(__nv_bfloat16 *inputLayerA_d, __nv_bfloat16 *inputLayerB_d, float *outputLayer_d, int mDim, int nDim, int kDim) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/123", "date": "2025-07-30", "prompt": "Write a CUDA kernel to simulate a logic circuit such as a MIMD (multiple instruction multiple data) pipeline, using shared-memory lookup table to accelerate the redundant lookups. Use circuit selection codes for each stage in the instruction code and compute the circuit output by a single lookup for each stage the data goes through.\n\nSignature of the CUDA kernel is __global__ void k_simulateLogicCircuit(uint32_t* input_d, uint8_t* output_d, uint8_t* lookupTable_d), where input_d is pointer to input array with each element representing 24 bit instruction and 8bit data, output_d is pointer to output array of 8 bit results of simulated logic circuit, and lookupTable_d is pointer to the truth table of all circuits used in the simulated MIMD pipeline.\n\n>>> k_simulateLogicCircuit({\n    256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270\n}, output_d,{\n    0, 0, 0, 0, 0, 1, 2, 3, 0, 2, 4, 6, 0, 3, 6, 9, \n    0, 4, 8, 12, 0, 5, 10, 15, 0, 6, 12, 18, 0, 7, 14, 21\n}) -> output_d: {\n    0, 0, 0, 0, 0, 1, 2, 3, 0, 2, 4, 6, 0, 3, 6\n}\n\n>>> k_simulateLogicCircuit({\n    256, 257, 258, 259, 260, 261, 262, 263, 256, 257, 258, 259, 260, 261, 262\n}, output_d,{\n    0, 1, 1, 2, 2, 3, 3, 4\n}) -> output_d: {\n    0, 1, 1, 2, 2, 3, 3, 4, 0, 1, 1, 2, 2, 3, 3\n}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <cstdio>\n#include <cstdint>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Algorithm-related constants.\n// Number of instances of pipelines to simulate. Each CUDA thread simulates a single pipeline in the MIMD pipeline.\nconstexpr int NUM_PIPELINES_OF_CIRCUIT = 15;\nconstexpr int NUM_STAGES_PER_PIPELINE = 12;\n// 2-bit selector value to end computations.\nconstexpr int NO_OPERATION = 0;\n// Input data and output data are 8-bit integers. Maximum 256 unique cases are possible.\nconstexpr int NUM_ALL_POSSIBLE_STATES = 256;\n// This is supposed to be constant for this test due to using only 2-bit selector code per pipeline stage.\nconstexpr int NUM_CIRCUITS = 3;\n// Each data is single byte inside a 32bit instruction input.\nconstexpr int NUM_BITS_OF_DATA = 8;\nconstexpr int MASK_SELECT_DATA = 0b11111111;\nconstexpr int INSERT_INSTRUCTION_0 = 0;\nconstexpr int INSERT_INSTRUCTION_1 = 2;\nconstexpr int INSERT_INSTRUCTION_2 = 4;\nconstexpr int INSERT_INSTRUCTION_3 = 6;\nconstexpr int INSERT_INSTRUCTION_4 = 8;\nconstexpr int INSERT_INSTRUCTION_5 = 10;\nconstexpr int INSERT_INSTRUCTION_6 = 12;\nconstexpr int INSERT_INSTRUCTION_7 = 14;\nconstexpr int INSERT_INSTRUCTION_8 = 16;\nconstexpr int INSERT_INSTRUCTION_9 = 18;\nconstexpr int INSERT_INSTRUCTION_10 = 20;\nconstexpr int INSERT_INSTRUCTION_11 = 22;\n\nconstexpr int INTERRUPT_DIVIDE_BY_ZERO = 0b11111111;\nconstexpr int NUM_LOOKUP_TABLE_BYTES = sizeof(uint8_t) * NUM_ALL_POSSIBLE_STATES * NUM_CIRCUITS;\nconstexpr int NUM_INPUT_BYTES = sizeof(uint32_t) * NUM_PIPELINES_OF_CIRCUIT;\n\n\n// CUDA-related constans.\nconstexpr int MINIMUM_ALLOWED_NUMBER_OF_THREADS_PER_BLOCK = 4;\nconstexpr int MINIMUM_ALLOWED_NUMBER_OF_BLOCKS_PER_GRID = 1;\n\n__global__ void k_simulateLogicCircuit(uint32_t* input_d, uint8_t* output_d, uint8_t* lookupTable_d);\n\nvoid launch() {\n    cudaDeviceProp props;\n    CUDA_CHECK(cudaGetDeviceProperties(&props, 0));\n    // Dynamically scaling the number of CUDA threads for the workload size.\n    int numThreadsPerBlock = NUM_PIPELINES_OF_CIRCUIT / props.multiProcessorCount;\n    numThreadsPerBlock = (numThreadsPerBlock / 32) * 32;\n    if(numThreadsPerBlock > props.maxThreadsPerBlock) {\n        numThreadsPerBlock = props.maxThreadsPerBlock;\n    }\n    if(numThreadsPerBlock < MINIMUM_ALLOWED_NUMBER_OF_THREADS_PER_BLOCK) {\n        numThreadsPerBlock = MINIMUM_ALLOWED_NUMBER_OF_THREADS_PER_BLOCK;\n    }\n    \n    int numBlocksPerGrid = NUM_PIPELINES_OF_CIRCUIT / numThreadsPerBlock;\n    if(numBlocksPerGrid > props.maxBlocksPerMultiProcessor * props.multiProcessorCount) {\n        numBlocksPerGrid = props.maxBlocksPerMultiProcessor * props.multiProcessorCount;\n    }\n    if(numBlocksPerGrid < MINIMUM_ALLOWED_NUMBER_OF_BLOCKS_PER_GRID) {\n        numBlocksPerGrid = MINIMUM_ALLOWED_NUMBER_OF_BLOCKS_PER_GRID;\n    }\n\n    dim3 gridDim(numBlocksPerGrid, 1, 1);\n    dim3 blockDim(numThreadsPerBlock, 1, 1);\n\n    // Dynamically allocating host arrays.\n    uint32_t* input_h = new uint32_t[NUM_PIPELINES_OF_CIRCUIT];\n    uint8_t* output_h = new uint8_t[NUM_PIPELINES_OF_CIRCUIT];\n    uint8_t* lookupTable_h = new uint8_t[NUM_ALL_POSSIBLE_STATES * NUM_CIRCUITS];\n    cudaStream_t stream;\n    uint32_t* input_d;\n    uint8_t* output_d;\n    uint8_t* lookupTable_d;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    // Dynamically allocating device arrays.\n    CUDA_CHECK(cudaMallocAsync(&input_d, NUM_INPUT_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, sizeof(uint8_t) * NUM_PIPELINES_OF_CIRCUIT, stream));\n    CUDA_CHECK(cudaMallocAsync(&lookupTable_d, NUM_LOOKUP_TABLE_BYTES, stream));\n    void* args[3] = { &input_d, &output_d, &lookupTable_d };\n    // Dynamic size of shared memory.\n    int sharedMemorySize = NUM_CIRCUITS * NUM_ALL_POSSIBLE_STATES * sizeof(uint8_t);\n\n    auto hToD = cudaMemcpyHostToDevice;\n    auto dToH = cudaMemcpyDeviceToHost;\n\n    // Test 1: 2-stage pipeline. Passing the data through a 2-bit adder (adds 2 bit parts into 4 bit parts) and then a 4-bit adder (2 nibbles into one 8-bit integer).\n    {\n        for(int i = 0; i < NUM_CIRCUITS * NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i] = 0;\n        }\n        uint32_t circuit1 = 1;\n        uint32_t circuit2 = 2;\n        // LUT for for 2-bit adder working on 4 parts of the 8-bit input.\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n            uint8_t input = i;\n            uint8_t variable1 = input & 0b11;\n            uint8_t variable2 = (input >> 2) & 0b11;\n            uint8_t variable3 = (input >> 4) & 0b11;\n            uint8_t variable4 = (input >> 6) & 0b11;\n            uint8_t sum1 = (variable1 + variable2);\n            uint8_t sum2 = (variable3 + variable4);\n            uint8_t output = sum1 | (sum2 << 4);\n            lookupTable_h[i + (circuit1 - 1) * NUM_ALL_POSSIBLE_STATES] = output;\n        }\n        // LUT for for 4-bit adder working on 2 parts of the 8-bit input.\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n            uint8_t input = i;\n            uint8_t variable1 = input & 0b1111;\n            uint8_t variable2 = (input >> 4) & 0b1111;\n            uint8_t sum = variable1 + variable2;\n            uint8_t output = sum;\n            lookupTable_h[i + (circuit2 - 1) * NUM_ALL_POSSIBLE_STATES] = output;\n        }\n        for (uint32_t i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            // Instruction is made of two selector codes. Circuit1 and circuit2. Circuit1 will be selected during the first stage of pipeline and circuit2 will be selected during the second stage of pipeline.\n            uint32_t instruction = (circuit1 << INSERT_INSTRUCTION_0) | (circuit2 << INSERT_INSTRUCTION_1);\n            uint32_t inputData = i % 256;\n            // Instruction and the data is encoded as a single input.\n            input_h[i] = ((instruction << NUM_BITS_OF_DATA) | inputData);\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, NUM_INPUT_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(lookupTable_d, lookupTable_h, NUM_LOOKUP_TABLE_BYTES, hToD, stream));\n        // Grid:(3, 1, 1)\n        // Block: (4, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_simulateLogicCircuit, gridDim, blockDim, args, sharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(uint8_t) * NUM_PIPELINES_OF_CIRCUIT, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            uint8_t data = input_h[i] & MASK_SELECT_DATA;\n            uint8_t value1 = data & 0b11;\n            uint8_t value2 = (data>>2) & 0b11;\n            uint8_t value3 = (data>>4) & 0b11;\n            uint8_t value4 = (data>>6) & 0b11;\n            uint8_t sum = value1 + value2 + value3 + value4;\n            assert(sum == output_h[i]);\n        }\n    }\n    // Test 2: Single operation. Reversing the bits of all odd-indexed elements and inverting the bits of all even-indexed elements.\n    {\n        for(int i = 0; i < NUM_CIRCUITS * NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i] = 0;\n        }\n        uint32_t circuit1 = 1;\n        uint32_t circuit2 = 2;\n        // LUT for reversed bits.\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n            uint32_t x = ((i & 0xF0) >> 4) | ((i & 0x0F) << 4);\n            x = ((x & 0xCC) >> 2) | ((x & 0x33) << 2);\n            x = ((x & 0xAA) >> 1) | ((x & 0x55) << 1);\n            lookupTable_h[i + (circuit1 - 1) * NUM_ALL_POSSIBLE_STATES] = x;\n        }\n        // LUT for inverted bits\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i + (circuit2 - 1) * NUM_ALL_POSSIBLE_STATES] = ~i;\n        }\n        for (uint32_t i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            // Instruction is made of only one of selector codes. Circuit1 or circuit2. Pipeline will run only single stage for any input.\n            uint32_t instruction = (i % 2 == 1) ? circuit1 : circuit2;\n            uint32_t inputData = i % 256;\n            input_h[i] = ((instruction << NUM_BITS_OF_DATA) | inputData);\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, NUM_INPUT_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(lookupTable_d, lookupTable_h, NUM_LOOKUP_TABLE_BYTES, hToD, stream));\n        // Grid:(3, 1, 1)\n        // Block: (4, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_simulateLogicCircuit, gridDim, blockDim, args, sharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(uint8_t) * NUM_PIPELINES_OF_CIRCUIT, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            uint8_t data = input_h[i] & MASK_SELECT_DATA;\n            if ((i % 2) == 1) {\n                assert(lookupTable_h[data + (circuit1 - 1) * NUM_ALL_POSSIBLE_STATES] == output_h[i]);\n            }\n            else {\n                assert(lookupTable_h[data + (circuit2 - 1) * NUM_ALL_POSSIBLE_STATES] == output_h[i]);\n            }\n        }\n    }\n    // Test 3: 5-stage pipeline with 3 different circuits used. x = x * 2, x = x - 1, x = x * 2, x = x - 1, and majority voting applied.\n    {\n        for(int i = 0; i < NUM_CIRCUITS * NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i] = 0;\n        }\n        uint32_t circuit1 = 1;\n        uint32_t circuit2 = 2;\n        uint32_t circuit3 = 3;\n        // LUT for x = x * 2\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i + (circuit1 - 1) * NUM_ALL_POSSIBLE_STATES] = i * 2;\n        }\n        // LUT for x = x - 1\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i + (circuit2 - 1) * NUM_ALL_POSSIBLE_STATES] = i - 1;\n        }\n        // LUT for majority voting.\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n#if defined(_WIN64) || defined(_WIN32)\n            lookupTable_h[i + (circuit3 - 1) * NUM_ALL_POSSIBLE_STATES] = (__popcnt(i) > 4 ? 1 : 0);\n#endif\n#ifdef __unix__  \n            lookupTable_h[i + (circuit3 - 1) * NUM_ALL_POSSIBLE_STATES] = (__builtin_popcount(i) > 4 ? 1 : 0);\n#endif\n        }\n        for (uint32_t i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            // Instruction is made of five selector codes. Circuit1, circuit2, circuit1, circuit2, circuit3. Pipeline will run five stages for five different operations for all data in parallel.\n            uint32_t instruction = (circuit1 << INSERT_INSTRUCTION_0) | (circuit2 << INSERT_INSTRUCTION_1) | (circuit1 << INSERT_INSTRUCTION_2) | (circuit2 << INSERT_INSTRUCTION_3) | (circuit3 << INSERT_INSTRUCTION_4);\n            uint32_t inputData = i % 256;\n            input_h[i] = ((instruction << NUM_BITS_OF_DATA) | inputData);\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, NUM_INPUT_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(lookupTable_d, lookupTable_h, NUM_LOOKUP_TABLE_BYTES, hToD, stream));\n        // Grid:(3, 1, 1)\n        // Block: (4, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_simulateLogicCircuit, gridDim, blockDim, args, sharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(uint8_t) * NUM_PIPELINES_OF_CIRCUIT, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            uint8_t data = input_h[i] & MASK_SELECT_DATA;\n            uint8_t hostResult = data;\n            hostResult = hostResult * 2 - 1;\n            hostResult = hostResult * 2 - 1;\n#if defined(_WIN64) || defined(_WIN32)\n            hostResult = (__popcnt(hostResult) > 4 ? 1 : 0);\n#endif\n#ifdef __unix__ \n            hostResult = (__builtin_popcount(hostResult) > 4 ? 1 : 0);\n#endif\n            assert(hostResult == output_h[i]);\n        }\n    }\n    // Test 4: 1 bit + 2 bit adder using single circuit.\n    {\n        for(int i = 0; i < NUM_CIRCUITS * NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i] = 0;\n        }\n        uint32_t circuit1 = 1;\n        // LUT for 1 bit + 2 bit adder, returning 3 bits.\n        constexpr int NUM_STATES_FOR_1BIT_2BIT_ADDER = 8;\n        for (uint32_t i = 0; i < NUM_STATES_FOR_1BIT_2BIT_ADDER; i++) {\n            uint8_t value1Bit = i & 1; \n            uint8_t value2Bit = (i / 2) & 3; \n            lookupTable_h[i + (circuit1 - 1) * NUM_ALL_POSSIBLE_STATES] = value1Bit + value2Bit;\n        }\n        for (uint32_t i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            uint32_t inputData = i % NUM_STATES_FOR_1BIT_2BIT_ADDER;\n            uint32_t instruction = (circuit1 << INSERT_INSTRUCTION_0);\n            input_h[i] = ((instruction << NUM_BITS_OF_DATA) | inputData);\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, NUM_INPUT_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(lookupTable_d, lookupTable_h, NUM_LOOKUP_TABLE_BYTES, hToD, stream));\n        // Grid:(3, 1, 1)\n        // Block: (4, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_simulateLogicCircuit, gridDim, blockDim, args, sharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(uint8_t) * NUM_PIPELINES_OF_CIRCUIT, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            uint8_t data = input_h[i] & MASK_SELECT_DATA;\n            uint8_t value1Bit = data & 1; \n            uint8_t value2Bit = (data / 2) & 3; \n            assert(value1Bit + value2Bit == output_h[i]);\n        }\n    }\n    // Test 5: 8-stage pipeline. Four times repeated linear congruential generator that calculates seed = (5 * seed + 7) mod 256 where modulo is automatically calculated for the 8bit variable assignment.\n    {\n        for(int i = 0; i < NUM_CIRCUITS * NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i] = 0;\n        }\n        uint32_t circuit1 = 1;\n        uint32_t circuit2 = 2;\n        // LUT for x = x * 5\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i + (circuit1 - 1) * NUM_ALL_POSSIBLE_STATES] = i * 5;\n        }\n        // LUT for x = x + 7\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i + (circuit2 - 1) * NUM_ALL_POSSIBLE_STATES] = i + 7;\n        }\n        for (uint32_t i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            // Instruction is made of eight selector codes. Circuit1, circuit2, circuit1, circuit2,circuit1, circuit2,circuit1, circuit2. Pipeline will run eight stages for eight different operations for all data in parallel.\n            uint32_t instruction = (circuit1 << INSERT_INSTRUCTION_0) | (circuit2 << INSERT_INSTRUCTION_1) | (circuit1 << INSERT_INSTRUCTION_2) | (circuit2 << INSERT_INSTRUCTION_3) | (circuit1 << INSERT_INSTRUCTION_4) | (circuit2 << INSERT_INSTRUCTION_5) | (circuit1 << INSERT_INSTRUCTION_6) | (circuit2 << INSERT_INSTRUCTION_7);\n            uint32_t inputData = i % 256;\n            input_h[i] = ((instruction << NUM_BITS_OF_DATA) | inputData);\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, NUM_INPUT_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(lookupTable_d, lookupTable_h, NUM_LOOKUP_TABLE_BYTES, hToD, stream));\n        // Grid:(3, 1, 1)\n        // Block: (4, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_simulateLogicCircuit, gridDim, blockDim, args, sharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(uint8_t) * NUM_PIPELINES_OF_CIRCUIT, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            uint8_t data = input_h[i] & MASK_SELECT_DATA;\n            uint8_t hostResult = ((((data * 5 + 7) * 5 + 7) * 5 + 7) * 5 + 7);\n            assert(hostResult == output_h[i]);\n        }\n    }\n    // Test 6: Two-stage pipeline. Right-shift for (data % 4) bits, twice.\n    {\n        for(int i = 0; i < NUM_CIRCUITS * NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i] = 0;\n        }\n        uint32_t circuit1 = 1;\n        // LUT for x = x >> (x % 4).\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n            uint8_t value = i >> (i % 4);\n            lookupTable_h[i + (circuit1 - 1) * NUM_ALL_POSSIBLE_STATES] = value;\n        }\n        for (uint32_t i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            // Two-stage pipeline is defined.\n            uint32_t instruction = (circuit1 << INSERT_INSTRUCTION_0) | (circuit1 << INSERT_INSTRUCTION_1);\n            uint32_t inputData = i % 256;\n            input_h[i] = ((instruction << NUM_BITS_OF_DATA) | inputData);\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, NUM_INPUT_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(lookupTable_d, lookupTable_h, NUM_LOOKUP_TABLE_BYTES, hToD, stream));\n        // Grid:(3, 1, 1)\n        // Block: (4, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_simulateLogicCircuit, gridDim, blockDim, args, sharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(uint8_t) * NUM_PIPELINES_OF_CIRCUIT, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            uint8_t data = input_h[i] & MASK_SELECT_DATA;\n            uint8_t value1 = data >> (data % 4);\n            uint8_t value2 = value1 >> (value1 % 4);\n            uint8_t hostResult = value2;\n            assert(hostResult == output_h[i]);\n        }\n    }\n    // Test 7: Passing the data through a 4-bit divider. Lower half of 8-bit input is divided by the higher half of 8-bit input and the result is written to the output. If division by zero is requested, it returns INTERRUPT_DIVIDE_BY_ZERO to define an undefined 4-bit division result.\n    {\n        for(int i = 0; i < NUM_CIRCUITS * NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i] = 0;\n        }\n        uint32_t circuit1 = 1;\n        // LUT for for 4-bit divider.\n        for (uint32_t i = 0; i < NUM_ALL_POSSIBLE_STATES; i++) {\n            uint8_t input = i;\n            uint8_t value1 = input & 0b1111;\n            uint8_t value2 = (input >> 2) & 0b1111;\n            uint8_t division = value2 == 0 ? INTERRUPT_DIVIDE_BY_ZERO : (value1 / value2);\n            lookupTable_h[i + (circuit1 - 1) * NUM_ALL_POSSIBLE_STATES] = division;\n        }\n        for (uint32_t i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            // Single operation is defined which is the division operation of a 4-bit value by another 4-bit value within the 8-bit integer input.\n            uint32_t instruction = circuit1;\n            uint32_t inputData = i % 256;\n            input_h[i] = ((instruction << NUM_BITS_OF_DATA) | inputData);\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, NUM_INPUT_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(lookupTable_d, lookupTable_h, NUM_LOOKUP_TABLE_BYTES, hToD, stream));\n        // Grid:(3, 1, 1)\n        // Block: (4, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_simulateLogicCircuit, gridDim, blockDim, args, sharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(uint8_t) * NUM_PIPELINES_OF_CIRCUIT, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            uint8_t data = input_h[i] & MASK_SELECT_DATA;\n            uint8_t value1 = data & 0b1111;\n            uint8_t value2 = (data>>2) & 0b1111;\n            uint8_t division = value2 == 0 ? INTERRUPT_DIVIDE_BY_ZERO : (value1 / value2);\n            assert(division == output_h[i]);\n        }\n    }\n    // Test 8: 2 bit * 3 bit multiplier using single circuit.\n    {\n        for(int i = 0; i < NUM_CIRCUITS * NUM_ALL_POSSIBLE_STATES; i++) {\n            lookupTable_h[i] = 0;\n        }\n        uint32_t circuit1 = 1;\n        // LUT for 2 bit + 3 bit multiplier, returning 5 bits.\n        constexpr int NUM_STATES_FOR_2BIT_3BIT_MULTIPLIER = 32;\n        for (uint32_t i = 0; i < NUM_STATES_FOR_2BIT_3BIT_MULTIPLIER; i++) {\n            uint8_t value2Bit = i & 3; \n            uint8_t value3Bit = (i / 4) & 7; \n            lookupTable_h[i + (circuit1 - 1) * NUM_ALL_POSSIBLE_STATES] = value2Bit * value3Bit;\n        }\n        for (uint32_t i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            uint32_t inputData = i % NUM_STATES_FOR_2BIT_3BIT_MULTIPLIER;\n            uint32_t instruction = (circuit1 << INSERT_INSTRUCTION_0);\n            input_h[i] = ((instruction << NUM_BITS_OF_DATA) | inputData);\n        }\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, NUM_INPUT_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(lookupTable_d, lookupTable_h, NUM_LOOKUP_TABLE_BYTES, hToD, stream));\n        // Grid:(3, 1, 1)\n        // Block: (4, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_simulateLogicCircuit, gridDim, blockDim, args, sharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(uint8_t) * NUM_PIPELINES_OF_CIRCUIT, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < NUM_PIPELINES_OF_CIRCUIT; i++) {\n            uint8_t data = input_h[i] & MASK_SELECT_DATA;\n            uint8_t value2Bit = data & 3; \n            uint8_t value3Bit = (data / 4) & 7; \n            assert(value2Bit * value3Bit == output_h[i]);\n        }\n    }\n    // Freeing the unused memory space.\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    CUDA_CHECK(cudaFreeAsync(lookupTable_d, stream));\n    // Deleting host arrays while device arrays are freed asynchronously.\n    delete[] input_h;\n    delete[] output_h;\n    delete[] lookupTable_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n// This CUDA kernel simulates up to 12 stages of a MIMD pipeline where each parallel pipeline is computed by a different CUDA thread.\n// The MIMD pipeline works in parallel to process all input elements and writes results to the output elements.\n// The logic calculation of each circuit is made by a single lookup from a shared-memory table, inside each stage.\n__global__ void k_simulateLogicCircuit(uint32_t* input_d, uint8_t* output_d, uint8_t* lookupTable_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/124", "date": "2025-07-30", "prompt": "Write a cuda kernel to sort integers using parallel bubble sort algorithm. This algorithm sorts the integers in N iterations, where N is the size of the input array. In each iteration the adjacent pairs of integers across the array are compared in two steps, in first step pairs with even index are compared and in second step pairs with odd index are compared. \n\nThe signature of the function is __global__ void k_bubbleSort(int *inputOutput_d, int numElements), where inputOutput_d is a pointer to an array of unsorted integers that are sorted inplace, numElements denotes the number of integers in the input. \n\n>>> k_bubbleSort({9, 3, 4, 6, 2, 5}, 6) -> inputOutput_d: ({2, 3, 4, 5, 6, 9})\n>>> k_bubbleSort({19, 17, 13, 18, 21, 24}, 6) -> inputOutput_d: ({13, 17, 18, 19, 21, 24})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <limits.h>\n#include <assert.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\nusing namespace cooperative_groups;\n\n#define SET_TO_ZERO 0\n#define DIVIDE_BY_TWO 2\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n__global__ void k_bubbleSort(int *inputOutput_d, int numElements);\n\nvoid launch() {\n\n    //Initialize Constants\n    const int TEST_CASE_COUNT = 7;\n    const int NUMBER_OF_THREADS_PER_BLOCK = 16;\n    const int SHARED_MEMORY_SIZE_IN_BYTES = 0;\n  \n    //Initialise Test Data\n    //Test Data Dimensions\n    int inputArraySize[TEST_CASE_COUNT] = {9, 12, 16, 24, 32, 48, 64};\n\n    //Identify max input size\n    int maxInputSize = 0;\n    for(int index = 0; index < TEST_CASE_COUNT; index++) {\n        maxInputSize = max(maxInputSize, inputArraySize[index]);\n    }\n\n    //Input Data For Test\n    int input_h[TEST_CASE_COUNT][maxInputSize] = {\n        //Test Case - 1\n        {19, 4, 1, 5, 25, 24, 13, 17, 16},\n        //Test Case - 2\n        {4, 5, 28, 36, 20, 21, 37, 10, 6, 45, 16, 18},\n        //Test Case - 3\n        {36, 9, 27, 2, 8, 13, 7, 14, 29, 20, 19, 5, 37, 22, 24, 12},\n        //Test Case - 4\n        {30, 33, 55, 8, 9, 16, 43, 58, 57, 54, 34, 56, 51, 21, 26, 25, 24, 48, 14, 50, 15, 44, 18, 41},\n        //Test Case - 5\n        {16, 24, 17, 11, 3, 1, 28, 23, 46, 45, 26, 48, 37, 22, 34, 43, 51, 15, 39, 40, 13, 58, 54, 41, 6, 29, 4, 50, 56, 32, 38, 27},\n        //Test Case - 6\n        {30, 46, 65, 13, 25, 53, 52, 34, 16, 69, 39, 55, 61, 35, 57, 48, 23, 9, 31, 36, 11, 38, 68, 24, 50, 19, 62, 27, 4, 37, 60, 49, 20, 44, 43, 33, 21, 17, 64, 18, 29, 45, 66, 40, 63, 6, 12, 10},\n        //Test Case - 7\n        {56, 48, 90, 99, 2, 27, 26, 38, 8, 3, 20, 75, 55, 93, 51, 28, 64, 30, 16, 82, 53, 49, 11, 54, 17, 67, 24, 44, 71, 86, 87, 95, 94, 18, 78, 42, 25, 34, 60, 1, 88, 52, 80, 5, 14, 91, 23, 96, 47, 15, 59, 58, 6, 36, 79, 12, 74, 85, 37, 31, 21, 46, 33, 92}\n    };\n    \n    //Expected Output for Test\n    int expectedOutput_h[TEST_CASE_COUNT][maxInputSize] = {\n        //Test Case - 1\n        {1, 4, 5, 13, 16, 17, 19, 24, 25},\n        //Test Case - 2\n        {4, 5, 6, 10, 16, 18, 20, 21, 28, 36, 37, 45},\n        //Test Case - 3\n        {2, 5, 7, 8, 9, 12, 13, 14, 19, 20, 22, 24, 27, 29, 36, 37},\n        //Test Case - 4\n        {8, 9, 14, 15, 16, 18, 21, 24, 25, 26, 30, 33, 34, 41, 43, 44, 48, 50, 51, 54, 55, 56, 57, 58},\n        //Test Case - 5\n        {1, 3, 4, 6, 11, 13, 15, 16, 17, 22, 23, 24, 26, 27, 28, 29, 32, 34, 37, 38, 39, 40, 41, 43, 45, 46, 48, 50, 51, 54, 56, 58},\n        //Test Case - 6\n        {4, 6, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 23, 24, 25, 27, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 50, 52, 53, 55, 57, 60, 61, 62, 63, 64, 65, 66, 68, 69},\n        //Test Case - 7\n        {1, 2, 3, 5, 6, 8, 11, 12, 14, 15, 16, 17, 18, 20, 21, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 36, 37, 38, 42, 44, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 58, 59, 60, 64, 67, 71, 74, 75, 78, 79, 80, 82, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 99}\n    };\n\n    //Output of device on host\n    int output_h[maxInputSize] = {};\n\n    //Use CUDA Streams for Asynchronous Execution\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n \n    //Allocate Device Memory\n    int *inputOutput_d;\n\n    CUDA_CHECK(cudaMallocAsync((void**)&inputOutput_d, maxInputSize * sizeof(int), stream));\n    \n    //Execute Test Cases\n    for (int testCase = 0; testCase < TEST_CASE_COUNT; testCase++) {\n        \n        int numElements = inputArraySize[testCase];\n\n        //Reset Output\n        CUDA_CHECK(cudaMemsetAsync(inputOutput_d, SET_TO_ZERO, maxInputSize * sizeof(int), stream));\n\n        //Copy input data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(inputOutput_d, input_h[testCase], numElements * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        //Get Device Properties\n        cudaDeviceProp prop;\n        int device;\n        cudaGetDevice(&device);\n        cudaGetDeviceProperties(&prop, device);\n\n        //Set Kernel Configuration\n        int numThreadsPerBlock = NUMBER_OF_THREADS_PER_BLOCK;\n        numThreadsPerBlock = min(numThreadsPerBlock, prop.maxThreadsDim[0]);\n        \n        //Only numElements / 2 threads are required\n        int halfNumElements = numElements / DIVIDE_BY_TWO;\n        int numBlocks = ceil((float)(halfNumElements) / numThreadsPerBlock);\n        numBlocks = min(numBlocks, prop.maxGridSize[0]);\n\n        dim3 block(numThreadsPerBlock, 1, 1);\n        dim3 grid(numBlocks, 1, 1);\n\n        //Launch Kernel\n        //Grid: (numElements/16, 1, 1)\n        //Block: (16, 1, 1)\n        void *args[] = {&inputOutput_d, &numElements};\n        CUDA_CHECK(cudaLaunchCooperativeKernel((void*)k_bubbleSort, grid, block, args, SHARED_MEMORY_SIZE_IN_BYTES, stream));\n        \n        //Copy Data from device to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, inputOutput_d, numElements * sizeof(int), cudaMemcpyDeviceToHost, stream));\n        \n        //Synchronize tasks in the stream\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        //Assert device output and expected output\n        for(int index = 0; index < numElements; index++) {\n            assert(output_h[index] == expectedOutput_h[testCase][index]);\n        }\n      \n    }\n    \n    //Deallocate Device Memory\n    CUDA_CHECK(cudaFreeAsync(inputOutput_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n\n}\n\n__global__ void k_bubbleSort(int *inputOutput_d, int numElements) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/125", "date": "2025-07-30", "prompt": "Write a CUDA kernel to detect presence of a signal using energy detection algorithm. Given an input signal vector $a$ with $n$ elements and an energy window of length $m$ ($m < $n). The kernel should use warp shuffles, reductions and atomics to determine the index corresponding to the peak output.\n\nThe signature of the function is __global__ void k_calculateMovingEnergy(int *inputVector, int windowLength, int *maxResult, int *maxResultIdx, int size), where inputVector is the input signal array, windowLength is the length of the window to calculate energy, maxResult is the peak energy window output, maxResultIdx contains index of peak energy and size is the length of the input signal array.\n\n>>> k_calculateMovingEnergy({1, 2, 3, 4, 5, 6, 7}, 2, maxResult, maxResultIdx, 7) -> maxResult : 85, maxResultIdx : 5\n>>> k_calculateMovingEnergy({3, 4, 2, 7, 9, 8, 1}, 3, maxResult, maxResultIdx, 7) -> maxResult : 194, maxResultIdx : 3\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <cuda_runtime.h>\n#undef    NDEBUG\n#include <assert.h>\n\n#define BLOCK_SIZE        (512)\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n__global__ void k_calculateMovingEnergy(int *inputVector, int windowLength, int *maxResult, int *maxResultIdx, int size);\n\nvoid launch() {\n    // Number of test cases\n    constexpr int TEST_CASE_COUNT = 10;\n    int inputDataLength[TEST_CASE_COUNT] = {7, 7, 10, 11, 12, 13, 14, 15, 35, 520}; // Sizes of the vectors in each test case\n    const int MAX_VECTOR_SIZE = *std::max_element(inputDataLength, inputDataLength + TEST_CASE_COUNT);\n\n    // Input vectors for the tests\n    int inputData_h[TEST_CASE_COUNT][MAX_VECTOR_SIZE] =  {\n        {1,2,3,4,5,6,7},                  // test case 1\n        {3,4,2,7,9,8,1},                  // test case 2\n        {5,2,10,4,3,1,3,1,6,8},           // test case 3\n        {7,1,1,8,10,6,2,9,4,3,8},         // test case 4\n        {1,1,7,7,6,8,8,8,3,7,6,4},        // test case 5\n        {1,8,4,7,8,2,2,6,5,9,8,8,1},      // test case 6\n        {1,1,8,10,7,2,8,2,2,7,4,7,8,6},   // test case 7\n        {8,3,8,10,9,1,4,4,7,6,8,4,3,1,8}, // test case 8\n        {5,1,3,10,2,9,6,10,1,5,2,10,1,8,9,9,1,4,3,9,5,10,2,3,2,2,9,6,6,2,9,7,4,6,5}, // test case 9\n        {5,5,4,8,7,8,10,10,2,2,7,1,6,6,9,5,4,7,8,6,4,2,6,3,1,8,3,5,7,4,8,4,7,8,5,1,4,5,3,2,9,5,9,4,8,4,9,8,4,3,8,10,4,7,5,9,8,2,9,10,6,9,6,2,2,5,8,9,8,4,6,1,2,2,7,5,2,5,2,1,9,6,10,7,6,9,9,10,1,9,7,10,6,5,9,3,5,10,6,9,8,6,3,7,1,7,7,8,9,10,8,6,10,6,1,2,9,5,9,3,6,7,1,7,4,1,5,2,2,3,2,2,1,7,3,6,7,5,6,5,2,5,9,9,3,3,6,7,5,3,10,1,2,2,2,7,6,1,10,8,8,1,9,10,10,9,8,6,2,4,2,1,10,4,3,4,5,7,1,9,6,9,4,5,1,2,7,4,9,2,10,6,8,10,3,5,5,8,9,2,2,4,1,6,4,2,3,10,7,5,10,2,8,8,6,2,6,3,2,3,9,1,3,1,5,1,9,2,1,4,5,2,10,4,3,1,3,1,6,8,7,1,1,8,10,6,2,9,4,3,8,1,1,7,7,6,8,8,8,3,7,6,4,1,8,4,7,8,2,2,6,5,9,8,8,1,1,1,8,10,7,2,8,2,2,7,4,7,8,6,8,3,8,10,9,1,4,4,7,6,8,4,3,1,8,3,4,6,3,7,5,2,8,2,3,3,6,1,5,2,2,8,3,7,10,5,7,8,5,7,2,10,2,3,8,5,8,4,3,1,7,5,5,7,1,4,8,7,2,2,1,1,5,7,8,6,2,7,2,2,1,2,2,2,4,4,3,3,9,8,6,2,3,1,10,8,6,4,2,7,10,2,3,4,1,7,5,10,5,7,2,4,2,8,9,4,7,3,6,9,6,4,3,5,5,4,6,8,5,5,2,1,3,4,7,10,10,5,3,8,8,8,8,2,7,5,3,1,9,2,2,7,9,6,8,2,10,6,7,1,9,8,2,6,4,6,4,5,2,3,1,10,7,10,2,10,8,6,5,3,8,3,1,8,7,8,7,5,4,9,4,9,8,9,6,7,10,5,1,9,7,4,10,3,7,7,4,2,1,5,2,8,4,9,8,6,2,10,3,10,3,4,1,7,2,1,8,4,7,4,7,1,10,9,8,9,4,7,6,6}\n    };\n\n    int windowLength_h[TEST_CASE_COUNT] = {\n        2, 3, 3, 3, 3, 3, 4, 4, 4, 5\n    };\n\n    // expected outputs\n    int expectedMaxIndex[TEST_CASE_COUNT] = {5, 3, 0, 3, 5, 9, 2, 1, 11, 162};\n\n    int expectedMaxEnergy[TEST_CASE_COUNT] = {\n      85, 194, 129, 200, 192, 209, 217, 254, 246, 426\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Initialize result on the host\n    int *maxResultIdx_h, *maxResult_h;\n    maxResultIdx_h = (int*)malloc(sizeof(int));\n    maxResult_h = (int*)malloc(sizeof(int));\n\n    // Pointers for device memory (GPU)\n    int *inputVector_d, *maxResultIdx_d, *maxResult_d;\n\n    // Allocate the memory on the device\n    CUDA_CHECK(cudaMallocAsync(&inputVector_d, MAX_VECTOR_SIZE * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&maxResultIdx_d, sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&maxResult_d, sizeof(int), stream));\n\n    // Loop to execute each test case\n    for (int i = 0; i < TEST_CASE_COUNT; ++i) {\n\n        // Copy input data to the device\n        CUDA_CHECK(cudaMemcpyAsync(inputVector_d, inputData_h[i], inputDataLength[i] * sizeof(int), cudaMemcpyHostToDevice, stream));\n        // Initialize the result on the device\n        CUDA_CHECK(cudaMemsetAsync(maxResultIdx_d, 0, sizeof(int), stream));\n        CUDA_CHECK(cudaMemsetAsync(maxResult_d, 0, sizeof(int), stream));\n\n        // Determine the number of threads and blocks\n        dim3 gridSize = dim3((inputDataLength[i] + BLOCK_SIZE - 1) / BLOCK_SIZE, 1, 1);\n        dim3 blockSize = dim3(BLOCK_SIZE, 1, 1);\n\n        //  Adding check for exceeding maximum grid dimensions\n        cudaDeviceProp props;\n        CUDA_CHECK(cudaGetDeviceProperties(&props, 0));\n        if (gridSize.x > props.maxGridSize[0] || gridSize.y > props.maxGridSize[1]) {\n            assert(false && \"Grid size exceeds device limits!\");\n        }\n        int warpSize = props.warpSize;\n\n        // Execute the kernel\n        // Grid:  ((inputDataLength[i] + BLOCK_SIZE - 1) / BLOCK_SIZE, 1, 1)\n        // Block: (BLOCK_SIZE, 1, 1)\n        size_t sharedMemorySize =  2 * ((BLOCK_SIZE + warpSize - 1)/warpSize) * sizeof(int);\n        void *args[] = {&inputVector_d, (void*)&windowLength_h[i], &maxResult_d, &maxResultIdx_d, (void*)&inputDataLength[i]};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateMovingEnergy, gridSize, blockSize, args, sharedMemorySize, stream));\n\n        // Copy the result back to the host (CPU)\n        CUDA_CHECK(cudaMemcpyAsync(maxResultIdx_h, maxResultIdx_d, sizeof(int), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(maxResult_h, maxResult_d, sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        assert(maxResultIdx_h[0] == expectedMaxIndex[i]);\n        assert(maxResult_h[0] == expectedMaxEnergy[i]);\n    }\n    // Free device memory and stream\n    CUDA_CHECK(cudaFreeAsync(inputVector_d, stream));\n    CUDA_CHECK(cudaFreeAsync(maxResultIdx_d, stream));\n    CUDA_CHECK(cudaFreeAsync(maxResult_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    // Free host memories\n    free(maxResultIdx_h);\n    free(maxResult_h);\n}\n\n// Each thread is computing energy within a window, each warp is calculating peak index of the output and the\n// result is saved in shared memory, a block level reduction is then performed to find the final maximum across\n// different warps in a block\n__global__ void k_calculateMovingEnergy(int *inputVector, int windowLength, int *maxResult, int *maxResultIdx, int size) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/126", "date": "2025-07-30", "prompt": "This CUDA kernel computes the Histogram of Oriented Gradients (HOG) for images using signed gradients (0 to 180) and 20 bins (9 bins total). Each thread processes a single pixel, computes gradients via the Sobel operator, and updates a block-level shared memory histogram, which is then stored in global memory.\n\nThe signature of the function is __global__ void k_computeHOG(unsigned char *image, int *hogDescriptor, int width, int height), where the image is the input grayscale image stored in a 1D array, hogDescriptor is the output histogram array (9-bin histogram per block),\nwidth and height are image dimensions.\n\n>>> k_computeHOG({50,55,60,65,70,75,80,85,90,95,100,105,110,115,120,125,130,135,140,145,150,155,160,165,170,175,180,185,190,195,200,205,210,215,220,225,230,235,240,245,250,255,245,235,225,215,205,195,185,175,\n165,155,145,135,125,115,105,95,85,75,65,55,45,35}, hogDescriptor, 8, 8)->hogDescriptor:{0, 16, 0, 304, 672, 32, 0, 0, 0}\n>>> k_computeHOG({200,180,160,140,40,20,200,180,80,60,40,20,120,100,80,60,160,140,120,100,200,180,160,140,40,20,200,180,80,60,40,20}, hogDescriptor, 4, 8)->hogDescriptor:{0, 0, 0, 64, 128, 224, 32, 64, 0}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <cfloat>\n#include <algorithm>\n#include <cuda_runtime.h>\n#include <assert.h>\n#include <iostream>\n\n#undef  NDEBUG\n#define BLOCK_SIZE  16  // 16x16 thread block\n\nconst int HOG_BINS = 9;\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n__global__ void k_computeHOG(unsigned char *image, int *hogDescriptor, int width, int height);\n\nvoid launch() {\n    const int maxImageWidth = 8;\n    const int maxImageHeight = 8;\n    const int MAX_IMAGE_SIZE = maxImageWidth * maxImageHeight;\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate device memory\n    unsigned char *image_d;\n    int *hogDescriptor_d;\n    CUDA_CHECK(cudaMallocAsync(&image_d, MAX_IMAGE_SIZE * sizeof(unsigned char), stream));\n    CUDA_CHECK(cudaMallocAsync(&hogDescriptor_d, HOG_BINS * sizeof(int), stream));\n\n    // Copy results back to host\n    int hogResult_h[HOG_BINS];\n\n    cudaDeviceProp deviceProp;\n    cudaGetDeviceProperties(&deviceProp, 0);  // Assuming device 0\n\n    int maxGridX = deviceProp.maxGridSize[0];  // Max blocks in x-direction\n    int maxGridY = deviceProp.maxGridSize[1];  // Max blocks in y-direction\n\n    // Test Case 1\n    {\n      int imageWidth = 8;\n      int imageHeight = 8;\n      int imageSize = imageWidth * imageHeight;\n\n      unsigned char imageData[]= {\n        50, 55, 60, 65, 70, 75, 80, 85,\n        90, 95, 100, 105, 110, 115, 120, 125,\n        130, 135, 140, 145, 150, 155, 160, 165,\n        170, 175, 180, 185, 190, 195, 200, 205,\n        210, 215, 220, 225, 230, 235, 240, 245,\n        250, 255, 245, 235, 225, 215, 205, 195,\n        185, 175, 165, 155, 145, 135, 125, 115,\n        105, 95, 85, 75, 65, 55, 45, 35\n      };\n\n      int expectedHOG[HOG_BINS] = {0, 16, 0, 304, 672, 32, 0, 0, 0};\n\n      // Copy input image to device\n      CUDA_CHECK(cudaMemcpyAsync(image_d, imageData, imageSize * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n      CUDA_CHECK(cudaMemsetAsync(hogDescriptor_d, 0, HOG_BINS * sizeof(int), stream));\n\n      int gridX = (imageWidth + BLOCK_SIZE - 1) / BLOCK_SIZE;  // Ceiling division\n      int gridY = (imageHeight + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n      if (gridX > maxGridX) {\n          gridX = maxGridX;\n      }\n      if (gridY > maxGridY) {\n          gridY = maxGridY;\n      }\n\n      // Launch kernel\n      dim3 gridSize(gridX, gridY);        \n      dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n\n      // Execute the kernel\n      // Grid:  (ceil(imageWidth/16), ceil(imageHeight/16), 1)\n      // Block: (16, 16, 1)\n      void *args[] = {&image_d, &hogDescriptor_d, &imageWidth, &imageHeight};\n      CUDA_CHECK(cudaLaunchKernel((void*)k_computeHOG, gridSize, blockSize, args, sizeof(int) * HOG_BINS, stream));\n\n      CUDA_CHECK(cudaMemcpyAsync(hogResult_h, hogDescriptor_d, HOG_BINS * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n      // Check tasks in the stream has completed\n      CUDA_CHECK(cudaStreamSynchronize(stream));\n\n      for (int j = 0; j < HOG_BINS; j++) {\n        assert(hogResult_h[j] == expectedHOG[j]);\n      }\n    }\n\n    // Test Case 2\n    {\n      int imageWidth = 8;\n      int imageHeight = 8;\n      int imageSize = imageWidth * imageHeight;\n\n      unsigned char imageData[]= {\n        200, 180, 160, 140, 120, 100, 80, 60,\n        40, 20, 200, 180, 160, 140, 120, 100,\n        80, 60, 40, 20, 200, 180, 160, 140,\n        120, 100, 80, 60, 40, 20, 200, 180,\n        160, 140, 120, 100, 80, 60, 40, 20,\n        200, 180, 160, 140, 120, 100, 80, 60,\n        40, 20, 200, 180, 160, 140, 120, 100,\n        80, 60, 40, 20, 200, 180, 160, 140\n      };\n\n      int expectedHOG[HOG_BINS] = {16, 0, 0, 144, 96, 496, 144, 128, 0};\n\n      // Copy input image to device\n      CUDA_CHECK(cudaMemcpyAsync(image_d, imageData, imageSize * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n      CUDA_CHECK(cudaMemsetAsync(hogDescriptor_d, 0, HOG_BINS * sizeof(int), stream));\n\n      int gridX = (imageWidth + BLOCK_SIZE - 1) / BLOCK_SIZE;  // Ceiling division\n      int gridY = (imageHeight + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n      if (gridX > maxGridX) {\n          gridX = maxGridX;\n      }\n      if (gridY > maxGridY) {\n          gridY = maxGridY;\n      }\n\n      // Launch kernel\n      dim3 gridSize(gridX, gridY);        \n      dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n\n      // Execute the kernel\n      // Grid:  (ceil(imageWidth/16), ceil(imageHeight/16), 1)\n      // Block: (16, 16, 1)\n      void *args[] = {&image_d, &hogDescriptor_d, &imageWidth, &imageHeight};\n      CUDA_CHECK(cudaLaunchKernel((void*)k_computeHOG, gridSize, blockSize, args, sizeof(int) * HOG_BINS, stream));\n\n      CUDA_CHECK(cudaMemcpyAsync(hogResult_h, hogDescriptor_d, HOG_BINS * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n      // Check tasks in the stream has completed\n      CUDA_CHECK(cudaStreamSynchronize(stream));\n\n      for (int j = 0; j < HOG_BINS; j++) {\n        assert(hogResult_h[j] == expectedHOG[j]);\n      }\n    }\n\n    // Test Case 3\n    {\n      int imageWidth = 8;\n      int imageHeight = 8;\n      int imageSize = imageWidth * imageHeight;\n\n      unsigned char imageData[]= {\n        10, 20, 30, 40, 50, 60, 70, 80,\n        20, 30, 40, 50, 60, 70, 80, 90,\n        30, 40, 50, 60, 70, 80, 90, 100,\n        40, 50, 60, 70, 80, 90, 100, 110,\n        50, 60, 70, 80, 90, 100, 110, 120,\n        60, 70, 80, 90, 100, 110, 120, 130,\n        70, 80, 90, 100, 110, 120, 130, 140,\n        80, 90, 100, 110, 120, 130, 140, 150\n      };\n\n      int expectedHOG[HOG_BINS] = {0, 192, 640, 192, 0, 0, 0, 0, 0};\n\n      // Copy input image to device\n      CUDA_CHECK(cudaMemcpyAsync(image_d, imageData, imageSize * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n      CUDA_CHECK(cudaMemsetAsync(hogDescriptor_d, 0, HOG_BINS * sizeof(int), stream));\n\n      int gridX = (imageWidth + BLOCK_SIZE - 1) / BLOCK_SIZE;  // Ceiling division\n      int gridY = (imageHeight + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n      if (gridX > maxGridX) {\n          gridX = maxGridX;\n      }\n      if (gridY > maxGridY) {\n          gridY = maxGridY;\n      }\n\n      // Launch kernel\n      dim3 gridSize(gridX, gridY);        \n      dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n\n      // Execute the kernel\n      // Grid:  (ceil(imageWidth/16), ceil(imageHeight/16), 1)\n      // Block: (16, 16, 1)\n      void *args[] = {&image_d, &hogDescriptor_d, &imageWidth, &imageHeight};\n      CUDA_CHECK(cudaLaunchKernel((void*)k_computeHOG, gridSize, blockSize, args, sizeof(int) * HOG_BINS, stream));\n\n      CUDA_CHECK(cudaMemcpyAsync(hogResult_h, hogDescriptor_d, HOG_BINS * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n      // Check tasks in the stream has completed\n      CUDA_CHECK(cudaStreamSynchronize(stream));\n\n      for (int j = 0; j < HOG_BINS; j++) {\n        assert(hogResult_h[j] == expectedHOG[j]);\n      }\n    }\n\n    // Test Case 4\n    {\n      int imageWidth = 8;\n      int imageHeight = 8;\n      int imageSize = imageWidth * imageHeight;\n\n      unsigned char imageData[]= {\n        50, 55, 60, 65, 70, 75, 80, 85,\n        90, 95, 100, 105, 110, 115, 120, 125,\n        130, 135, 140, 145, 150, 155, 160, 165,\n        170, 175, 180, 185, 190, 195, 200, 205,\n        210, 215, 220, 225, 230, 235, 240, 245,\n        250, 255, 245, 235, 225, 215, 205, 195,\n        185, 175, 165, 155, 145, 135, 125, 115,\n        105, 95, 85, 75, 65, 55, 45, 35\n      };\n\n      int expectedHOG[HOG_BINS] = {0, 16, 0, 304, 672, 32, 0, 0, 0};\n\n      // Copy input image to device\n      CUDA_CHECK(cudaMemcpyAsync(image_d, imageData, imageSize * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n      CUDA_CHECK(cudaMemsetAsync(hogDescriptor_d, 0, HOG_BINS * sizeof(int), stream));\n\n      int gridX = (imageWidth + BLOCK_SIZE - 1) / BLOCK_SIZE;  // Ceiling division\n      int gridY = (imageHeight + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n      if (gridX > maxGridX) {\n          gridX = maxGridX;\n      }\n      if (gridY > maxGridY) {\n          gridY = maxGridY;\n      }\n\n      // Launch kernel\n      dim3 gridSize(gridX, gridY);        \n      dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n\n      // Execute the kernel\n      // Grid:  (ceil(imageWidth/16), ceil(imageHeight/16), 1)\n      // Block: (16, 16, 1)\n      void *args[] = {&image_d, &hogDescriptor_d, &imageWidth, &imageHeight};\n      CUDA_CHECK(cudaLaunchKernel((void*)k_computeHOG, gridSize, blockSize, args, sizeof(int) * HOG_BINS, stream));\n\n      CUDA_CHECK(cudaMemcpyAsync(hogResult_h, hogDescriptor_d, HOG_BINS * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n      // Check tasks in the stream has completed\n      CUDA_CHECK(cudaStreamSynchronize(stream));\n\n      for (int j = 0; j < HOG_BINS; j++) {\n        assert(hogResult_h[j] == expectedHOG[j]);\n      }\n    }\n\n    // Test Case 5\n    {\n      int imageWidth = 4;\n      int imageHeight = 8;\n      int imageSize = imageWidth * imageHeight;\n\n      unsigned char imageData[]= {\n        200, 180, 160, 140,\n        40, 20, 200, 180,\n        80, 60, 40, 20,\n        120, 100, 80, 60,\n        160, 140, 120, 100,\n        200, 180, 160, 140,\n        40, 20, 200, 180,\n        80, 60, 40, 20\n      };\n\n      int expectedHOG[HOG_BINS] = {0, 0, 0, 64, 128, 224, 32, 64, 0};\n\n      // Copy input image to device\n      CUDA_CHECK(cudaMemcpyAsync(image_d, imageData, imageSize * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n      CUDA_CHECK(cudaMemsetAsync(hogDescriptor_d, 0, HOG_BINS * sizeof(int), stream));\n\n      int gridX = (imageWidth + BLOCK_SIZE - 1) / BLOCK_SIZE;  // Ceiling division\n      int gridY = (imageHeight + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n      if (gridX > maxGridX) {\n          gridX = maxGridX;\n      }\n      if (gridY > maxGridY) {\n          gridY = maxGridY;\n      }\n\n      // Launch kernel\n      dim3 gridSize(gridX, gridY);        \n      dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n\n      // Execute the kernel\n      // Grid:  (ceil(imageWidth/16), ceil(imageHeight/16), 1)\n      // Block: (16, 16, 1)\n      void *args[] = {&image_d, &hogDescriptor_d, &imageWidth, &imageHeight};\n      CUDA_CHECK(cudaLaunchKernel((void*)k_computeHOG, gridSize, blockSize, args, sizeof(int) * HOG_BINS, stream));\n\n      CUDA_CHECK(cudaMemcpyAsync(hogResult_h, hogDescriptor_d, HOG_BINS * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n      // Check tasks in the stream has completed\n      CUDA_CHECK(cudaStreamSynchronize(stream));\n\n      for (int j = 0; j < HOG_BINS; j++) {\n        assert(hogResult_h[j] == expectedHOG[j]);\n      }\n    }\n\n    // Test Case 6\n    {\n      int imageWidth = 8;\n      int imageHeight = 4;\n      int imageSize = imageWidth * imageHeight;\n\n      unsigned char imageData[]= {\n        10, 20, 30, 40, 50, 60, 70, 80,\n        20, 30, 40, 50, 60, 70, 80, 90,\n        30, 40, 50, 60, 70, 80, 90, 100,\n        40, 50, 60, 70, 80, 90, 100, 110\n      };\n\n      int expectedHOG[HOG_BINS] = {0, 192, 256, 64, 0, 0, 0, 0, 0};\n\n      // Copy input image to device\n      CUDA_CHECK(cudaMemcpyAsync(image_d, imageData, imageSize * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n      CUDA_CHECK(cudaMemsetAsync(hogDescriptor_d, 0, HOG_BINS * sizeof(int), stream));\n\n      int gridX = (imageWidth + BLOCK_SIZE - 1) / BLOCK_SIZE;  // Ceiling division\n      int gridY = (imageHeight + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n      if (gridX > maxGridX) {\n          gridX = maxGridX;\n      }\n      if (gridY > maxGridY) {\n          gridY = maxGridY;\n      }\n\n      // Launch kernel\n      dim3 gridSize(gridX, gridY);        \n      dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n\n      // Execute the kernel\n      // Grid:  (ceil(imageWidth/16), ceil(imageHeight/16), 1)\n      // Block: (16, 16, 1)\n      void *args[] = {&image_d, &hogDescriptor_d, &imageWidth, &imageHeight};\n      CUDA_CHECK(cudaLaunchKernel((void*)k_computeHOG, gridSize, blockSize, args, sizeof(int) * HOG_BINS, stream));\n\n      CUDA_CHECK(cudaMemcpyAsync(hogResult_h, hogDescriptor_d, HOG_BINS * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n      // Check tasks in the stream has completed\n      CUDA_CHECK(cudaStreamSynchronize(stream));\n\n      // Verify results\n      for (int j = 0; j < HOG_BINS; j++) {\n        assert(hogResult_h[j] == expectedHOG[j]);\n      }\n    }\n\n    // Test Case 7\n    {\n      int imageWidth = 5;\n      int imageHeight = 8;\n      int imageSize = imageWidth * imageHeight;\n\n      unsigned char imageData[]= {\n        5, 15, 25, 35, 45,\n        15, 25, 35, 45, 55,\n        25, 35, 45, 55, 65,\n        35, 45, 55, 65, 75,\n        45, 55, 65, 75, 85,\n        55, 65, 75, 85, 95,\n        65, 75, 85, 95, 105,\n        75, 85, 95, 105, 115\n      };\n\n      int expectedHOG[HOG_BINS] = {0, 96, 352, 192, 0, 0, 0, 0, 0};\n\n      // Copy input image to device\n      CUDA_CHECK(cudaMemcpyAsync(image_d, imageData, imageSize * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n      CUDA_CHECK(cudaMemsetAsync(hogDescriptor_d, 0, HOG_BINS * sizeof(int), stream));\n\n      int gridX = (imageWidth + BLOCK_SIZE - 1) / BLOCK_SIZE;  // Ceiling division\n      int gridY = (imageHeight + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n      if (gridX > maxGridX) {\n          gridX = maxGridX;\n      }\n      if (gridY > maxGridY) {\n          gridY = maxGridY;\n      }\n\n      // Launch kernel\n      dim3 gridSize(gridX, gridY);        \n      dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n\n      // Execute the kernel\n      // Grid:  (ceil(imageWidth/16), ceil(imageHeight/16), 1)\n      // Block: (16, 16, 1)\n      void *args[] = {&image_d, &hogDescriptor_d, &imageWidth, &imageHeight};\n      CUDA_CHECK(cudaLaunchKernel((void*)k_computeHOG, gridSize, blockSize, args, sizeof(int) * HOG_BINS, stream));\n\n      CUDA_CHECK(cudaMemcpyAsync(hogResult_h, hogDescriptor_d, HOG_BINS * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n      // Check tasks in the stream has completed\n      CUDA_CHECK(cudaStreamSynchronize(stream));\n\n      for (int j = 0; j < HOG_BINS; j++) {\n        assert(hogResult_h[j] == expectedHOG[j]);\n      }\n    }\n\n    // Test Case 8\n    {\n      int imageWidth = 8;\n      int imageHeight = 6;\n      int imageSize = imageWidth * imageHeight;\n\n      unsigned char imageData[]= {\n        255, 245, 235, 225, 215, 205, 195, 185,\n        175, 165, 155, 145, 135, 125, 115, 105,\n        95, 85, 75, 65, 55, 45, 35, 25,\n        15, 5, 255, 245, 235, 225, 215, 205,\n        195, 185, 175, 165, 155, 145, 135, 125,\n        115, 105, 95, 85, 75, 65, 55, 45\n      };\n\n      int expectedHOG[HOG_BINS] = {0, 32, 0, 192, 384, 160, 0, 0, 0};\n\n      // Copy input image to device\n      CUDA_CHECK(cudaMemcpyAsync(image_d, imageData, imageSize * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n      CUDA_CHECK(cudaMemsetAsync(hogDescriptor_d, 0, HOG_BINS * sizeof(int), stream));\n\n      int gridX = (imageWidth + BLOCK_SIZE - 1) / BLOCK_SIZE;  // Ceiling division\n      int gridY = (imageHeight + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n      if (gridX > maxGridX) {\n          gridX = maxGridX;\n      }\n      if (gridY > maxGridY) {\n          gridY = maxGridY;\n      }\n\n      // Launch kernel\n      dim3 gridSize(gridX, gridY);        \n      dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n\n      // Execute the kernel\n      // Grid:  (ceil(imageWidth/16), ceil(imageHeight/16), 1)\n      // Block: (16, 16, 1)\n      void *args[] = {&image_d, &hogDescriptor_d, &imageWidth, &imageHeight};\n      CUDA_CHECK(cudaLaunchKernel((void*)k_computeHOG, gridSize, blockSize, args, sizeof(int) * HOG_BINS, stream));\n\n      CUDA_CHECK(cudaMemcpyAsync(hogResult_h, hogDescriptor_d, HOG_BINS * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n      // Check tasks in the stream has completed\n      CUDA_CHECK(cudaStreamSynchronize(stream));\n\n      for (int j = 0; j < HOG_BINS; j++) {\n        assert(hogResult_h[j] == expectedHOG[j]);\n      }\n    }\n\n    // Free device memory\n    CUDA_CHECK(cudaFreeAsync(image_d, stream));\n    CUDA_CHECK(cudaFreeAsync(hogDescriptor_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_computeHOG(unsigned char *image, int *hogDescriptor, int width, int height) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/127", "date": "2025-07-30", "prompt": "Write a CUDA kernel to find the maximum value in a large integer array using parallel reduction with warp shuffle operations and shared memory optimizations.\n\nThe signature of the kernel is __global__ void k_findMax(int* input_d, int* result_d, size_t numElements, int warpsPerBlock, int warpSize), where input_d is a device pointer to the input array of 32-bit integers, result_d is a single-element device pointer to store the maximum value, numElements is the total number of elements in the input array, warpsPerBlock is the number of warps per block (calculated from device properties) and warpSize is the Size of the warp (calculated from device properties)\n\nThe implementation must use __shfl_down_sync for warp-level reduction, utilize shared memory for block-level aggregation, implement grid-stride loops for scalable input handling, use atomicMax for final global maximum update, and handle arbitrary input sizes including non-multiples of block size.\n\n>>> k_findMax({1, 2, 3, 98765}, result_d, 4,  warpsPerBlock, warpSize) -> 98765\n>>> k_findMax({-5, -3, 0, -10, -8}, result_d, 5,   warpsPerBlock, warpSize) -> 0\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <assert.h>\n#include <limits>\n#include <cstdio>\n#include <limits>\n#include <vector>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                                                                   \\\ndo {                                                                                       \\\n    cudaError_t error = call;                                                              \\\n    if(error != cudaSuccess) {                                                             \\\n        fprintf(stderr,                                                                    \\\n            \"CUDA Error: %s at %s:%d\\n\",                                                   \\\n            cudaGetErrorString(error),                                                     \\\n            __FILE__,                                                                      \\\n            __LINE__);                                                                     \\\n        exit(error);                                                                       \\\n    }                                                                                      \\\n} while(0)\n\n\n__global__ void k_findMax(\n    int* input_d,            // [in] Device pointer to input array of integers to search\n    int* result_d,           // [out] Single-element device pointer to store global maximum result\n    size_t numElements,      // [in] Total number of elements in input array\n    int warpsPerBlock,      // [in] Number of warps per block (calculated from device properties)\n    int warpSize              // [in] Size of the warp (calculated from device properties)\n);\n\nconstexpr int BLOCK_SIZE = 256;\nconstexpr int INITIAL_MAX = std::numeric_limits<int>::min();\n\nstruct TestCase {               // [in] Total number of elements in test case\n    size_t numElements;         // [in] Input data for test case\n    std::vector<int> input;     // [in] Reference maximum value for validation\n    int expectedMax;            // [in] Expected result\n};\n\nstd::vector<TestCase> testCases = {\n    {4, {1, 2, 3, 98765}, 98765},\n    {5, {-5, -3, 0, -10, -8}, 0},\n    {513, std::vector<int>(513, 314), 314},  \n    {1<<20, [](){ \n        std::vector<int> vec(1<<20, 0);\n        vec[1000] = 42;\n        return vec;\n    }(), 42},\n    {1000, std::vector<int>(1000, 100), 100},\n    {1, {-42}, -42},\n    {7, {-5, 3, -2, 10, -1, 9, 0}, 10}\n};\n\nvoid launch() {\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n    \n    // Calculate warps per block and shared memory per block\n    int warpSize = prop.warpSize;\n    int warpsPerBlock = BLOCK_SIZE / warpSize;\n    size_t sharedMemPerBlock = warpsPerBlock * sizeof(int);\n    \n    // Calculate occupancy parameters once\n    int numBlocksPerSM = 0;\n    CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n        &numBlocksPerSM,\n        k_findMax,\n        BLOCK_SIZE,\n        sharedMemPerBlock  // Shared memory per block\n    ));\n    \n    int maxBlocks = prop.multiProcessorCount * numBlocksPerSM;\n    \n    // Create a stream per test case for concurrent execution\n    constexpr int MAX_CONCURRENT_TESTS = 4;  // Limit concurrent tests to avoid resource exhaustion\n    const int numTests = static_cast<int>(testCases.size());  // All 7 test cases\n    const int numStreams = std::min(numTests, MAX_CONCURRENT_TESTS);\n    \n    // Create CUDA streams for concurrent execution\n    cudaStream_t streams[MAX_CONCURRENT_TESTS];\n    for (int i = 0; i < numStreams; ++i) {\n        CUDA_CHECK(cudaStreamCreate(&streams[i]));\n    }\n    \n    // Allocate resources for results\n    std::vector<int*> input_d(numTests, nullptr);\n    std::vector<int*> result_d(numTests, nullptr);\n    std::vector<int> result_h(numTests);\n    \n    // Asynchronously launch all test cases\n    for (int tc = 0; tc < numTests; ++tc) {\n        // Select stream in round-robin fashion\n        cudaStream_t stream = streams[tc % numStreams];\n        TestCase& test = testCases[tc];\n        \n        // Asynchronously allocate device memory\n        CUDA_CHECK(cudaMallocAsync(&input_d[tc], test.numElements * sizeof(int), stream));\n        CUDA_CHECK(cudaMallocAsync(&result_d[tc], sizeof(int), stream));\n        \n        // Initialize result with minimum value\n        int initial = INITIAL_MAX;\n        CUDA_CHECK(cudaMemcpyAsync(result_d[tc], &initial, sizeof(int), \n                                  cudaMemcpyHostToDevice, stream));\n        \n        // Copy input data asynchronously\n        CUDA_CHECK(cudaMemcpyAsync(input_d[tc], test.input.data(), \n                                  test.numElements * sizeof(int), \n                                  cudaMemcpyHostToDevice, stream));\n        \n        // Optimized grid configuration\n        int desiredBlocks = (test.numElements + BLOCK_SIZE - 1) / BLOCK_SIZE;\n        dim3 block(BLOCK_SIZE);\n        dim3 grid(std::min(desiredBlocks, maxBlocks));\n        \n        // Prepare kernel arguments array\n        void* args[] = { \n            &input_d[tc],      // Device input array pointer\n            &result_d[tc],     // Device output pointer\n            &test.numElements, // Number of elements in input array\n            &warpsPerBlock,   // Warps per block calculated from device properties\n            &warpSize           // Size of the warp as per device properties\n        };\n        \n        // Launch kernel with explicit error checking\n        CUDA_CHECK(cudaLaunchKernel(\n            (const void*)k_findMax,  // Kernel function pointer\n            grid,                    // Grid dimensions (blocks per grid)\n            block,                   // Block dimensions (threads per block)\n            args,                    // Array of kernel arguments\n            sharedMemPerBlock,       // Dynamic shared memory size\n            stream                   // Stream ID\n        ));\n            \n        // Queue asynchronous memory transfer of result\n        CUDA_CHECK(cudaMemcpyAsync(&result_h[tc], result_d[tc], sizeof(int),\n                                  cudaMemcpyDeviceToHost, stream));\n    }\n    \n    // Synchronize all streams and validate results\n    for (int tc = 0; tc < numTests; ++tc) {\n        cudaStream_t stream = streams[tc % numStreams];\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Silent C-style assertion for validation\n        assert(result_h[tc] == testCases[tc].expectedMax);\n        \n        // Free allocated resources asynchronously\n        CUDA_CHECK(cudaFreeAsync(input_d[tc], stream));\n        CUDA_CHECK(cudaFreeAsync(result_d[tc], stream));\n    }\n    \n    // Synchronize all streams before destroying them to ensure all operations complete\n    for (int i = 0; i < numStreams; ++i) {\n        CUDA_CHECK(cudaStreamSynchronize(streams[i]));\n        CUDA_CHECK(cudaStreamDestroy(streams[i]));\n    }\n}\n\n\n__global__ void k_findMax(int* input_d, int* result_d, size_t numElements, int warpsPerBlock, int warpSize) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/128", "date": "2025-07-30", "prompt": "Write a CUDA kernel to search for elements in a sorted array using the binary search method, ensuring communication between threads in a block using shared memory.\n\nThe signature of the CUDA kernel is __global__ void k_binarySearch(int *arr_d, int *queries_d, int *results_d, int arrSize, int querySize), where arr_d is a device pointer to the sorted array, queries_d is a device pointer to the queries, results_d is a device pointer to store the results (index of found element, or -1 if not found), arrSize is the total number of elements in arr_d and querySize is the total number of queries in queries_d.\n\n>>> k_binarySearch({1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21}, {5, 10, 15, 20, 21}, results_d, 11, 5) -> {2, -1, 7, -1, 10}\n>>> k_binarySearch({2, 4, 6, 8, 10}, {6, 3, 10}, results_d, 5, 3) -> {2, -1, 4}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <stdio.h>\n#include <stdlib.h>\n#include <cuda.h>\n#include <iostream>\n#include <vector>\n#include <cassert>\n#include <algorithm>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\nnamespace cg = cooperative_groups;\n\n// Constants\n#define THREADS_PER_BLOCK 256\n#define SHARED_MEM_SIZE 256  // Shared memory size\n\n// CUDA Error Checking Macro\n#define CUDA_CHECK(call)  \\\n    do { \\\n        cudaError_t error = call; \\\n        if (error != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\", \\\n                    __FILE__, __LINE__, \\\n                    cudaGetErrorString(error)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while (0)\n\n__global__ void k_binarySearch(int *arr_d, int *queries_d, int *results_d, int arrSize, int querySize);\n\nvoid launch() {\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Define test cases (sorted arrays) and corresponding query cases.\n    std::vector<std::vector<int>> testCases = {\n        {1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21},\n        {2, 4, 6, 8, 10},\n        {10, 20, 30, 40, 50, 60, 70, 80, 90, 100},\n        {100, 200, 300, 400, 500, 600, 700, 800, 900, 1000},\n        {9, 18, 27, 36, 45, 54, 63, 72, 81, 90},\n        {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n    };\n\n    std::vector<std::vector<int>> queryCases = {\n        {5, 10, 15, 20, 21},\n        {6, 3, 10},\n        {25, 40, 100, 200},\n        {700, 500, 1000, 1100},\n        {45, 100, 9, 81},\n        {1, 10, 5}\n    };\n\n    // Expected results for each test case.\n    std::vector<std::vector<int>> expectedResults = {\n        {2, -1, 7, -1, 10},\n        {2, -1, 4},\n        {-1, 3, 9, -1},\n        {6, 4, 9, -1},\n        {4, -1, 0, 8},\n        {0, 9, 4}\n    };\n\n    // Process each test case.\n    for (size_t testCaseIndex = 0; testCaseIndex < testCases.size(); testCaseIndex++) {\n        int arraySize = testCases[testCaseIndex].size();\n        int querySize = queryCases[testCaseIndex].size();\n        if (arraySize == 0 || querySize == 0) continue; // Skip empty test cases\n\n        // Allocate device pointers.\n        int *arr_d, *queries_d, *results_d;\n\n        // Allocate device memory.\n        CUDA_CHECK(cudaMallocAsync(&arr_d, arraySize * sizeof(int), stream));\n        CUDA_CHECK(cudaMallocAsync(&queries_d, querySize * sizeof(int), stream));\n        CUDA_CHECK(cudaMallocAsync(&results_d, querySize * sizeof(int), stream));\n\n        // Copy input data.\n        CUDA_CHECK(cudaMemcpyAsync(arr_d, testCases[testCaseIndex].data(), arraySize * sizeof(int), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(queries_d, queryCases[testCaseIndex].data(), querySize * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        // --- Begin occupancy and grid dimension setup for this test case ---\n        cudaDeviceProp deviceProps;\n        CUDA_CHECK(cudaGetDeviceProperties(&deviceProps, 0));\n        cudaDeviceProp cooperativeProps;\n        CUDA_CHECK(cudaGetDeviceProperties(&cooperativeProps, 0));\n        assert(cooperativeProps.cooperativeLaunch && \"Error: This device does not support cooperative kernel launches.\");\n\n        // Calculate the total number of Streaming Multiprocessors (SMs).\n        int totalSMs = deviceProps.multiProcessorCount;\n\n        // Determine the maximum number of blocks per SM that can be active for the binary search kernel.\n        int maxBlocksPerSM;\n        CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(&maxBlocksPerSM, k_binarySearch, THREADS_PER_BLOCK, 0));\n\n        // Calculate the maximum number of cooperative blocks allowed on the device.\n        int maxCoopBlocks = maxBlocksPerSM * totalSMs;\n\n        // Compute the number of blocks required based on the current query dataset.\n        int numBlocks = (querySize + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n        numBlocks = std::min(numBlocks, maxCoopBlocks); // Clamp block count to cooperative launch limits.\n        numBlocks = std::max(numBlocks, 1); // Ensure at least one block is launched.\n\n        // Define grid and block dimensions.\n        dim3 gridSize(numBlocks, 1, 1);\n        dim3 blockSize(THREADS_PER_BLOCK, 1, 1);\n\n        // Verify that grid dimensions do not exceed device limits.\n        if (gridSize.x > deviceProps.maxGridSize[0] || gridSize.y > deviceProps.maxGridSize[1]) {\n            assert(false && \"Grid size exceeds device limits!\");\n        }\n\n        // Prepare kernel arguments.\n        void* kernelArgs[] = {\n            (void*)&arr_d,\n            (void*)&queries_d,\n            (void*)&results_d,\n            (void*)&arraySize,\n            (void*)&querySize\n        };\n\n        // Launch the cooperative kernel.\n        CUDA_CHECK(cudaLaunchCooperativeKernel((void*)k_binarySearch, gridSize, blockSize, kernelArgs, 0, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Copy results back.\n        std::vector<int> output_h(querySize);\n        CUDA_CHECK(cudaMemcpyAsync(output_h.data(), results_d, querySize * sizeof(int), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify the result for this test case.\n        for (int index = 0; index < querySize; index++) {\n            assert(output_h[index] == expectedResults[testCaseIndex][index]);\n        }\n\n        // Free device memory.\n        CUDA_CHECK(cudaFreeAsync(arr_d, stream));\n        CUDA_CHECK(cudaFreeAsync(queries_d, stream));\n        CUDA_CHECK(cudaFreeAsync(results_d, stream));\n    }\n\n    // Destroy CUDA stream.\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n// Each block cooperatively loads a portion of the sorted array from global memory into its private shared memory.\n// Each thread in the block copies elements starting at its thread index and then increments by blockDim.x (i += blockDim.x) to cover additional elements.\n// For each query, a thread performs a binary search by repeatedly computing a mid index and comparing the target with the mid element.\n// The binary search uses the data from shared memory if the mid index is within the cached region; otherwise, it falls back to accessing global memory.\n// Each thread writes the resulting index (or -1 if the target is not found) back to global memory for retrieval by the host.\n\n__global__ void k_binarySearch(int *arr_d, int *queries_d, int *results_d, int arrSize, int querySize) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/129", "date": "2025-07-30", "prompt": "Write a CUDA kernel to find the closest intersection points for a group of rays against a group of spheres and assign the colors of the intersected spheres to the output of the rays. Use shared memory tiling to load sphere data and perform redundant accesses to the spheres only through shared memory. Use grayscale colors for the spheres and orthographic projection for the rays to enhance simplicity.\n\nThe signature of the CUDA kernel is __global__ void k_calculateRaySphereIntersection(float *rayOriginX_d, float *rayOriginY_d, float *rayOriginZ_d, float *rayDirectionX_d, float *rayDirectionY_d, float *rayDirectionZ_d, float *sphereX_d, float *sphereY_d, float *sphereZ_d, float *sphereRadius_d, float *sphereColor_d, float *rayColorOutput_d), where rayOriginX_d is a pointer to the array of x-components of rays origin vectors, rayOriginY_d is a pointer to the array of y-components of rays origin vectors, rayOriginZ_d is a pointer to the array of z-components of rays origin vectors, rayDirectionX_d is a pointer to the array of x-components of rays direction vectors, rayDirectionY_d is a pointer to the array of y-components of rays direction vectors, rayDirectionZ_d is a pointer to the array of z-components of rays direction vectors, sphereX_d is a pointer to the array of x-components of spheres centers, sphereY_d is a pointer to the array of y-components of spheres centers, sphereZ_d is a pointer to the array of z-components of spheres centers, sphereRadius_d is a pointer to the array of radii of spheres, sphereColor_d is a pointer to the array of colors of spheres, and rayColorOutput_d is a pointer to the array of output colors of rays.\n\nThe k_calculateRaySphereIntersection CUDA kernel can make use of device function d_intersect:\n\nThe signature of the function to calculate the intersection between a single ray and a single sphere is inline __device__ void d_intersect(Ray & ray, Sphere & sphere), where the ray is the ray data, and the sphere is the sphere data.\n\n>>> k_calculateRaySphereIntersection({ index % 4 }, { index / 4 }, { -40.00f for 16 elements }, { 0.00f for 16 elements }, { 0.00f for 16 elements }, { 10.00f for 16 elements }, { 1.500000f, 1.500000f }, { 1.500000f, 1.500000f }, { 0.000000f, 0.000000f }, { 0.800000f, 1.200000f}, { 0.000000f, 0.100000f }, \n    rayColorOutput_d) -> rayColorOutput_d: { 0.500000f, 0.500000f, 0.500000f, 0.500000f, 0.500000f, 0.100000f, 0.100000f, 0.500000f, 0.500000f, 0.100000f, 0.100000f, 0.500000f, 0.500000f, 0.500000f, 0.500000f, 0.500000f }\n\n>>> k_calculateRaySphereIntersection({ index % 4 }, { index / 4 }, { -40.00f for 16 elements }, { 0.00f for 16 elements }, { 0.00f for 16 elements }, { 10.00f for 16 elements }, { 0.000000f, 3.000000f }, { 0.000000f, 3.000000f }, { 0.000000f, 0.000000f }, { 2.000000f, 2.000000f }, { 0.250000f, 0.750000f }, \n    rayColorOutput_d) -> rayColorOutput_d: { 0.250000f, 0.250000f, 0.250000f, 0.500000f, 0.250000f, 0.250000f, 0.500000f, 0.750000f, 0.250000f, 0.500000f, 0.750000f, 0.750000f, 0.500000f, 0.750000f, 0.750000f, 0.750000f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <cstdio>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call){                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess){                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Testing-related constants. Screen size is only used during generation of rays on a 2D plane.\nconstexpr int SCREEN_WIDTH = 4;\nconstexpr int SCREEN_HEIGHT = 4;\nconstexpr int NUM_TOTAL_SPHERES = 2;\nconstexpr float EPSILON = 0.0001f;\n\n// Algorithm-related constants.\n// A ray is constructed for each pixel on the screen. This is solely for preparing inputs for the CUDA kernel, which does not utilize the screen dimensions.\nconstexpr int NUM_TOTAL_RAYS = SCREEN_WIDTH * SCREEN_HEIGHT;\nconstexpr float FLOAT_MAX = (3.4e38);\nconstexpr float BACKGROUND_COLOR = 0.5f;\n\n// Allocation-related constants.\nconstexpr int RAY_BYTES = NUM_TOTAL_RAYS * sizeof(float);\nconstexpr int SPHERE_BYTES = NUM_TOTAL_SPHERES * sizeof(float);\n\n// CUDA-related constant.\nconstexpr int MINIMUM_ALLOWED_NUMBER_OF_THREADS_PER_BLOCK = 4;\nconstexpr int MINIMUM_ALLOWED_NUMBER_OF_BLOCKS_PER_GRID = 1;\n// Ray information with 3D origin coordinates, 3D direction, current distance to closest intersection, and current color fields.\nstruct Ray;\n// Sphere information with 3D center coordinates, radius and color fields.\nstruct Sphere;\n\ninline __device__ void d_intersect(Ray & ray, Sphere & sphere);\n\n__global__ void k_calculateRaySphereIntersection(float *rayOriginX_d, float *rayOriginY_d, float *rayOriginZ_d, float *rayDirectionX_d, float *rayDirectionY_d, float *rayDirectionZ_d, float *sphereX_d, float *sphereY_d, float *sphereZ_d, float *sphereRadius_d, float *sphereColor_d, float *rayColorOutput_d);\n\nint getSizeOfSphere();\n\nvoid launch() {\n    cudaDeviceProp props;\n    CUDA_CHECK(cudaGetDeviceProperties(&props, 0));\n\n    // Limiting the number of threads per block after dynamically scaling it to the size of the workload.\n    int minimumRequiredThreadsPerBlock = NUM_TOTAL_RAYS / props.multiProcessorCount;\n    minimumRequiredThreadsPerBlock = (minimumRequiredThreadsPerBlock / 32) * 32;\n    if(minimumRequiredThreadsPerBlock > props.maxThreadsPerBlock){\n        minimumRequiredThreadsPerBlock = props.maxThreadsPerBlock;\n    }\n    \n    if(minimumRequiredThreadsPerBlock < MINIMUM_ALLOWED_NUMBER_OF_THREADS_PER_BLOCK){\n        minimumRequiredThreadsPerBlock = MINIMUM_ALLOWED_NUMBER_OF_THREADS_PER_BLOCK;\n    }\n\n    // Limiting the number of blocks per grid after dynamically scaling it to the size of the workload.\n    int numThreadsPerBlock = minimumRequiredThreadsPerBlock;\n    int numBlocksPerGrid = NUM_TOTAL_RAYS / numThreadsPerBlock;\n    if(numBlocksPerGrid > props.maxBlocksPerMultiProcessor * props.multiProcessorCount){\n        numBlocksPerGrid = props.maxBlocksPerMultiProcessor * props.multiProcessorCount;\n    }\n\n    if(numBlocksPerGrid < MINIMUM_ALLOWED_NUMBER_OF_BLOCKS_PER_GRID){\n        numBlocksPerGrid = MINIMUM_ALLOWED_NUMBER_OF_BLOCKS_PER_GRID;\n    }\n\n    int numTileSpheres = numThreadsPerBlock;\n\n    // Used for querying size of an incomplete type, in run-time.\n    int sizeOfSphere = getSizeOfSphere();\n\n    // Allocating memory for host arrays.\n    float *rayOriginX_h = new float[NUM_TOTAL_RAYS];\n    float *rayOriginY_h = new float[NUM_TOTAL_RAYS];\n    float *rayOriginZ_h = new float[NUM_TOTAL_RAYS];\n    float *rayDirectionX_h = new float[NUM_TOTAL_RAYS];\n    float *rayDirectionY_h = new float[NUM_TOTAL_RAYS];\n    float *rayDirectionZ_h = new float[NUM_TOTAL_RAYS];\n    float *sphereX_h = new float[NUM_TOTAL_SPHERES];\n    float *sphereY_h = new float[NUM_TOTAL_SPHERES];\n    float *sphereZ_h = new float[NUM_TOTAL_SPHERES];\n    float *sphereRadius_h = new float[NUM_TOTAL_SPHERES];\n    float *sphereColor_h = new float[NUM_TOTAL_SPHERES];\n    float *rayColorOutput_h = new float[NUM_TOTAL_RAYS];\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    float *rayOriginX_d;\n    float *rayOriginY_d;\n    float *rayOriginZ_d;\n    float *rayDirectionX_d;\n    float *rayDirectionY_d;\n    float *rayDirectionZ_d;\n    float *sphereX_d;\n    float *sphereY_d;\n    float *sphereZ_d;\n    float *sphereRadius_d;\n    float *sphereColor_d;\n    float *rayColorOutput_d;\n    \n    // Allocating memory for device arrays.\n    CUDA_CHECK(cudaMallocAsync(&rayOriginX_d, RAY_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&rayOriginY_d, RAY_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&rayOriginZ_d, RAY_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&rayDirectionX_d, RAY_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&rayDirectionY_d, RAY_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&rayDirectionZ_d, RAY_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&rayColorOutput_d, RAY_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&sphereX_d, SPHERE_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&sphereY_d, SPHERE_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&sphereZ_d, SPHERE_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&sphereRadius_d, SPHERE_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&sphereColor_d, SPHERE_BYTES, stream));\n    void * args[12] = {\n        &rayOriginX_d,\n        &rayOriginY_d,\n        &rayOriginZ_d,\n        &rayDirectionX_d,\n        &rayDirectionY_d,\n        &rayDirectionZ_d,\n        &sphereX_d,\n        &sphereY_d,\n        &sphereZ_d,\n        &sphereRadius_d,\n        &sphereColor_d,\n        &rayColorOutput_d\n    };\n\n    auto hToD = cudaMemcpyHostToDevice;\n    auto dToH = cudaMemcpyDeviceToHost;\n\n    dim3 gridDim(numBlocksPerGrid, 1, 1);\n    dim3 blockDim(numThreadsPerBlock, 1, 1);\n\n    // Test 1: Spheres nested within each other; only the largest sphere remains visible.\n    {\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            // For simplicity, orthographic projection is utilized for the input rays.\n            float x = i % SCREEN_WIDTH;\n            float y = i / SCREEN_WIDTH;\n            rayOriginX_h[i] = x;\n            rayOriginY_h[i] = y;\n            rayOriginZ_h[i] = -SCREEN_WIDTH * 10.0f;\n            rayDirectionX_h[i] = 0.0f;\n            rayDirectionY_h[i] = 0.0f;\n            rayDirectionZ_h[i] = 10.0f;\n        }\n\n        for(int i = 0; i < NUM_TOTAL_SPHERES; i++){\n            sphereX_h[i] = -0.5f + SCREEN_WIDTH / 2.0f;\n            sphereY_h[i] = -0.5f + SCREEN_HEIGHT / 2.0f;\n            sphereZ_h[i] = 0.0f;\n            sphereRadius_h[i] = i == 0 ? 0.8f : 1.2f;\n            sphereColor_h[i] = 0.1f * i;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginX_d, rayOriginX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginY_d, rayOriginY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginZ_d, rayOriginZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionX_d, rayDirectionX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionY_d, rayDirectionY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionZ_d, rayDirectionZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereX_d, sphereX_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereY_d, sphereY_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereZ_d, sphereZ_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereRadius_d, sphereRadius_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereColor_d, sphereColor_h, SPHERE_BYTES, hToD, stream));\n        \n        // Grid: (3, 1, 1)\n        // Block: (8, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateRaySphereIntersection, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    numTileSpheres * sizeOfSphere, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayColorOutput_h, rayColorOutput_d, RAY_BYTES, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        float expectedColor[NUM_TOTAL_RAYS] = {\n            0.500000f, 0.500000f, 0.500000f, 0.500000f,\n            0.500000f, 0.100000f, 0.100000f, 0.500000f,\n            0.500000f, 0.100000f, 0.100000f, 0.500000f,\n            0.500000f, 0.500000f, 0.500000f, 0.500000f\n        };\n\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            assert(fabsf(expectedColor[i] - rayColorOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 2: Spheres are positioned diagonally side by side.\n    {\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            // For simplicity, orthographic projection is utilized for the input rays.\n            float x = i % SCREEN_WIDTH;\n            float y = i / SCREEN_WIDTH;\n            rayOriginX_h[i] = x;\n            rayOriginY_h[i] = y;\n            rayOriginZ_h[i] = -SCREEN_WIDTH * 10.0f;\n            rayDirectionX_h[i] = 0.0f;\n            rayDirectionY_h[i] = 0.0f;\n            rayDirectionZ_h[i] = 10.0f;\n        }\n\n        for(int i = 0; i < NUM_TOTAL_SPHERES; i++){\n            sphereX_h[i] = (i == 0 ? 0 : (SCREEN_WIDTH - 1));\n            sphereY_h[i] = (i == 0 ? 0 : (SCREEN_HEIGHT - 1));\n            sphereZ_h[i] = 0.0f;\n            sphereRadius_h[i] = SCREEN_WIDTH / 2.0f;\n            sphereColor_h[i] = i == 0 ? 0.25f : 0.75f;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginX_d, rayOriginX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginY_d, rayOriginY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginZ_d, rayOriginZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionX_d, rayDirectionX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionY_d, rayDirectionY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionZ_d, rayDirectionZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereX_d, sphereX_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereY_d, sphereY_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereZ_d, sphereZ_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereRadius_d, sphereRadius_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereColor_d, sphereColor_h, SPHERE_BYTES, hToD, stream));\n        \n        // Grid: (3, 1, 1)\n        // Block: (8, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateRaySphereIntersection, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    numTileSpheres * sizeOfSphere, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayColorOutput_h, rayColorOutput_d, RAY_BYTES, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        float expectedColor[NUM_TOTAL_RAYS] = {\n            0.250000f, 0.250000f, 0.250000f, 0.500000f,\n            0.250000f, 0.250000f, 0.500000f, 0.750000f,\n            0.250000f, 0.500000f, 0.750000f, 0.750000f,\n            0.500000f, 0.750000f, 0.750000f, 0.750000f\n        };\n\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            assert(fabsf(expectedColor[i] - rayColorOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 3: Spheres arranged behind one another with progressively increasing radii.\n    {\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            // For simplicity, orthographic projection is utilized for the input rays.\n            float x = i % SCREEN_WIDTH;\n            float y = i / SCREEN_WIDTH;\n            rayOriginX_h[i] = x;\n            rayOriginY_h[i] = y;\n            rayOriginZ_h[i] = -SCREEN_WIDTH * 10.0f;\n            rayDirectionX_h[i] = 0.0f;\n            rayDirectionY_h[i] = 0.0f;\n            rayDirectionZ_h[i] = 10.0f;\n        }\n\n        for(int i = 0; i < NUM_TOTAL_SPHERES; i++){\n            sphereX_h[i] = SCREEN_WIDTH / 2.0f - 0.5f;\n            sphereY_h[i] = SCREEN_HEIGHT / 2.0f - 0.5f;\n            sphereZ_h[i] = i * SCREEN_WIDTH;\n            sphereRadius_h[i] = (i + 1.0f) *  SCREEN_WIDTH / 4.0f;\n            sphereColor_h[i] = i * 0.1f;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginX_d, rayOriginX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginY_d, rayOriginY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginZ_d, rayOriginZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionX_d, rayDirectionX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionY_d, rayDirectionY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionZ_d, rayDirectionZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereX_d, sphereX_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereY_d, sphereY_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereZ_d, sphereZ_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereRadius_d, sphereRadius_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereColor_d, sphereColor_h, SPHERE_BYTES, hToD, stream));\n        \n        // Grid: (3, 1, 1)\n        // Block: (8, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateRaySphereIntersection, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    numTileSpheres * sizeOfSphere, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayColorOutput_h, rayColorOutput_d, RAY_BYTES, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        float expectedColor[NUM_TOTAL_RAYS] = {\n            0.500000f, 0.100000f, 0.100000f, 0.500000f,\n            0.100000f, 0.000000f, 0.000000f, 0.100000f,\n            0.100000f, 0.000000f, 0.000000f, 0.100000f,\n            0.500000f, 0.100000f, 0.100000f, 0.500000f\n        };\n\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            assert(fabsf(expectedColor[i] - rayColorOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 4: Spheres are visibly overlapping each other on the screen.\n    {\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            // For simplicity, orthographic projection is utilized for the input rays.\n            float x = i % SCREEN_WIDTH;\n            float y = i / SCREEN_WIDTH;\n            rayOriginX_h[i] = x;\n            rayOriginY_h[i] = y;\n            rayOriginZ_h[i] = -SCREEN_WIDTH * 10.0f;\n            rayDirectionX_h[i] = 0.0f;\n            rayDirectionY_h[i] = 0.0f;\n            rayDirectionZ_h[i] = 10.0f;\n        }\n\n        for(int i = 0; i < NUM_TOTAL_SPHERES; i++){\n            sphereX_h[i] = ((i + 0.5f) / (float)NUM_TOTAL_SPHERES) * SCREEN_WIDTH;\n            sphereY_h[i] = SCREEN_HEIGHT / 2.0f;\n            sphereZ_h[i] = 0;\n            sphereRadius_h[i] = SCREEN_WIDTH / 3.0f;\n            sphereColor_h[i] = i * 0.1f;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginX_d, rayOriginX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginY_d, rayOriginY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginZ_d, rayOriginZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionX_d, rayDirectionX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionY_d, rayDirectionY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionZ_d, rayDirectionZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereX_d, sphereX_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereY_d, sphereY_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereZ_d, sphereZ_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereRadius_d, sphereRadius_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereColor_d, sphereColor_h, SPHERE_BYTES, hToD, stream));\n        \n        // Grid: (3, 1, 1)\n        // Block: (8, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateRaySphereIntersection, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    numTileSpheres * sizeOfSphere, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayColorOutput_h, rayColorOutput_d, RAY_BYTES, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        float expectedColor[NUM_TOTAL_RAYS] = {\n            0.500000f, 0.500000f, 0.500000f, 0.500000f,\n            0.500000f, 0.000000f, 0.500000f, 0.100000f,\n            0.000000f, 0.000000f, 0.000000f, 0.100000f,\n            0.500000f, 0.000000f, 0.500000f, 0.100000f\n        };\n\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            assert(fabsf(expectedColor[i] - rayColorOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 5: The spheres are perfectly overlapping.\n    {\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            // For simplicity, orthographic projection is utilized for the input rays.\n            float x = i % SCREEN_WIDTH;\n            float y = i / SCREEN_WIDTH;\n            rayOriginX_h[i] = x;\n            rayOriginY_h[i] = y;\n            rayOriginZ_h[i] = -SCREEN_WIDTH * 10.0f;\n            rayDirectionX_h[i] = 0.0f;\n            rayDirectionY_h[i] = 0.0f;\n            rayDirectionZ_h[i] = 10.0f;\n        }\n\n        for(int i = 0; i < NUM_TOTAL_SPHERES; i++){\n            sphereX_h[i] = SCREEN_WIDTH / 2.0f;\n            sphereY_h[i] = SCREEN_HEIGHT / 2.0f;\n            sphereZ_h[i] = 0;\n            sphereRadius_h[i] = SCREEN_WIDTH / 3.0f;\n            sphereColor_h[i] = i * 0.1;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginX_d, rayOriginX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginY_d, rayOriginY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginZ_d, rayOriginZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionX_d, rayDirectionX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionY_d, rayDirectionY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionZ_d, rayDirectionZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereX_d, sphereX_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereY_d, sphereY_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereZ_d, sphereZ_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereRadius_d, sphereRadius_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereColor_d, sphereColor_h, SPHERE_BYTES, hToD, stream));\n        \n        // Grid: (3, 1, 1)\n        // Block: (8, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateRaySphereIntersection, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    numTileSpheres * sizeOfSphere, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayColorOutput_h, rayColorOutput_d, RAY_BYTES, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        float expectedColor[NUM_TOTAL_RAYS] = {\n            0.500000f, 0.500000f, 0.500000f, 0.500000f,\n            0.500000f, 0.500000f, 0.000000f, 0.500000f,\n            0.500000f, 0.000000f, 0.000000f, 0.000000f,\n            0.500000f, 0.500000f, 0.000000f, 0.500000f\n        };\n\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            assert(fabsf(expectedColor[i] - rayColorOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 6: All spheres are outside the visible volume in the x-direction -> Spheres are not visible.\n    {\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            // For simplicity, orthographic projection is utilized for the input rays.\n            float x = i % SCREEN_WIDTH;\n            float y = i / SCREEN_WIDTH;\n            rayOriginX_h[i] = x;\n            rayOriginY_h[i] = y;\n            rayOriginZ_h[i] = -SCREEN_WIDTH * 10.0f;\n            rayDirectionX_h[i] = 0.0f;\n            rayDirectionY_h[i] = 0.0f;\n            rayDirectionZ_h[i] = 10.0f;\n        }\n\n        for(int i = 0; i < NUM_TOTAL_SPHERES; i++){\n            sphereX_h[i] = -SCREEN_WIDTH * 2;\n            sphereY_h[i] = SCREEN_HEIGHT / 2.0f;\n            sphereZ_h[i] = 0;\n            sphereRadius_h[i] = SCREEN_WIDTH / 3.0f;\n            sphereColor_h[i] = i * 0.1;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginX_d, rayOriginX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginY_d, rayOriginY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginZ_d, rayOriginZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionX_d, rayDirectionX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionY_d, rayDirectionY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionZ_d, rayDirectionZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereX_d, sphereX_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereY_d, sphereY_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereZ_d, sphereZ_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereRadius_d, sphereRadius_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereColor_d, sphereColor_h, SPHERE_BYTES, hToD, stream));\n        \n        // Grid: (3, 1, 1)\n        // Block: (8, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateRaySphereIntersection, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    numTileSpheres * sizeOfSphere, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayColorOutput_h, rayColorOutput_d, RAY_BYTES, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        float expectedColor[NUM_TOTAL_RAYS] = {\n            0.500000f, 0.500000f, 0.500000f, 0.500000f,\n            0.500000f, 0.500000f, 0.500000f, 0.500000f,\n            0.500000f, 0.500000f, 0.500000f, 0.500000f,\n            0.500000f, 0.500000f, 0.500000f, 0.500000f\n        };\n\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            assert(fabsf(expectedColor[i] - rayColorOutput_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 7: All spheres are located behind the plane containing the origin points of the rays, which means the spheres are not visible.\n    {\n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            // For simplicity, orthographic projection is utilized for the input rays.\n            float x = i % SCREEN_WIDTH;\n            float y = i / SCREEN_WIDTH;\n            rayOriginX_h[i] = x;\n            rayOriginY_h[i] = y;\n            rayOriginZ_h[i] = -SCREEN_WIDTH * 10.0f;\n            rayDirectionX_h[i] = 0.0f;\n            rayDirectionY_h[i] = 0.0f;\n            rayDirectionZ_h[i] = 10.0f;\n        }\n\n        for(int i = 0; i < NUM_TOTAL_SPHERES; i++){\n            sphereX_h[i] = SCREEN_WIDTH / 2.0f;\n            sphereY_h[i] = SCREEN_HEIGHT / 2.0f;\n            sphereZ_h[i] = -SCREEN_WIDTH * 20.0f;\n            sphereRadius_h[i] = SCREEN_WIDTH / 3.0f;\n            sphereColor_h[i] = i * 0.1;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginX_d, rayOriginX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginY_d, rayOriginY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayOriginZ_d, rayOriginZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionX_d, rayDirectionX_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionY_d, rayDirectionY_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayDirectionZ_d, rayDirectionZ_h, RAY_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereX_d, sphereX_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereY_d, sphereY_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereZ_d, sphereZ_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereRadius_d, sphereRadius_h, SPHERE_BYTES, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sphereColor_d, sphereColor_h, SPHERE_BYTES, hToD, stream));\n        \n        // Grid: (3, 1, 1)\n        // Block: (8, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateRaySphereIntersection, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    numTileSpheres * sizeOfSphere, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(rayColorOutput_h, rayColorOutput_d, RAY_BYTES, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        float expectedColor[NUM_TOTAL_RAYS] = {\n            0.500000f, 0.500000f, 0.500000f, 0.500000f,\n            0.500000f, 0.500000f, 0.500000f, 0.500000f,\n            0.500000f, 0.500000f, 0.500000f, 0.500000f,\n            0.500000f, 0.500000f, 0.500000f, 0.500000f\n        };\n        \n        for(int i = 0; i < NUM_TOTAL_RAYS; i++){\n            assert(fabsf(expectedColor[i] - rayColorOutput_h[i]) < EPSILON);\n        }\n    }\n\n    CUDA_CHECK(cudaFreeAsync(rayOriginX_d, stream));\n    CUDA_CHECK(cudaFreeAsync(rayOriginY_d, stream));\n    CUDA_CHECK(cudaFreeAsync(rayOriginZ_d, stream));\n    CUDA_CHECK(cudaFreeAsync(rayDirectionX_d, stream));\n    CUDA_CHECK(cudaFreeAsync(rayDirectionY_d, stream));\n    CUDA_CHECK(cudaFreeAsync(rayDirectionZ_d, stream));\n    CUDA_CHECK(cudaFreeAsync(sphereX_d, stream));\n    CUDA_CHECK(cudaFreeAsync(sphereY_d, stream));\n    CUDA_CHECK(cudaFreeAsync(sphereZ_d, stream));\n    CUDA_CHECK(cudaFreeAsync(sphereRadius_d, stream));\n    CUDA_CHECK(cudaFreeAsync(sphereColor_d, stream));\n    CUDA_CHECK(cudaFreeAsync(rayColorOutput_d, stream));\n\n    delete [] rayOriginX_h;\n    delete [] rayOriginY_h;\n    delete [] rayOriginZ_h;\n    delete [] rayDirectionX_h;\n    delete [] rayDirectionY_h;\n    delete [] rayDirectionZ_h;\n    delete [] sphereX_h;\n    delete [] sphereY_h;\n    delete [] sphereZ_h;\n    delete [] sphereRadius_h;\n    delete [] sphereColor_h;\n    delete [] rayColorOutput_h;\n\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\nstruct Ray{\n    float3 origin;\n    float3 direction;\n    float currenttDistance;\n    float currentColor;\n};\nstruct Sphere{\n    float x, y, z, r;\n    float color;\n};\n\nint getSizeOfSphere() {\n    Sphere* ptr = new Sphere();\n    int size = sizeof(*ptr);\n    delete ptr;\n    return size;\n}\n\n// Solving the quadratic formula that equates two points of the sphere and the ray. \n// A * t + B * t + C = 0 where A = d . d, B = 2 * (m . d), and C = (m . m) * (R), where R is the radius of the sphere, m is the vector from the ray's origin to the sphere's center, and d is the direction of the ray. \n// solution 1: origin + t1 * direction \n// solution 2: origin + t2 * direction \n// t1 = (-B - (B - 4AC)^(0.5)) / (2A) \n// t2 = (-B + (B - 4AC)^(0.5)) / (2A)\ninline __device__ void d_intersect(Ray & ray, Sphere & sphere){\n    // Distance from the ray's origin to the sphere's center.\n    float3 difference = make_float3(ray.origin.x - sphere.x, ray.origin.y - sphere.y, ray.origin.z - sphere.z);\n    // Project the ray's direction onto the vector that points from the ray's origin to the sphere's center. Projection = (m . d).\n    float projection = ray.direction.x * difference.x + ray.direction.y * difference.y + ray.direction.z * difference.z;\n    // Dot product of m with itself.\n    float squaredLength = difference.x * difference.x + difference.y * difference.y + difference.z * difference.z;\n    // Ray's direction has a length of 1.0, which makes the dot product of A equal to 1.0.\n    float a = 1.0f; \n    float b = 2.0f * projection;\n    float c = squaredLength - (sphere.r * sphere.r);\n    // B - 4AC\n    float rootCalc = b * b - 4.0f * a * c;\n    float resultDistance = ray.currenttDistance;\n    float resultColor = ray.currentColor;\n    // The ray is intersecting the sphere.\n    if((sphere.r > 0.0f) && !(rootCalc < 0.0f)){\n        // Computing the roots.\n        float solution1 = (-b - sqrtf(rootCalc)) / (2.0f * a);\n        float solution2 = (-b + sqrtf(rootCalc)) / (2.0f * a);\n        float solution = solution1 < solution2 ? solution1 : solution2;\n        if(solution < ray.currenttDistance){\n            resultColor = sphere.color;\n            resultDistance = solution;\n        }\n    }\n    ray.currenttDistance = resultDistance;\n    ray.currentColor = resultColor;\n}\n\n// Calculating the intersections of a total of NUM_TOTAL_RAYS rays with NUM_TOTAL_SPHERES spheres.\n__global__ void k_calculateRaySphereIntersection(float *rayOriginX_d, float *rayOriginY_d, float *rayOriginZ_d, float *rayDirectionX_d, float *rayDirectionY_d, float *rayDirectionZ_d, float *sphereX_d, float *sphereY_d, float *sphereZ_d, float *sphereRadius_d, float *sphereColor_d, float *rayColorOutput_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/130", "date": "2025-07-30", "prompt": "Write a CUDA kernel to apply the Sobel filter for edge detection on grayscale images using shared memory. The implementation must use a 33 convolution kernel to compute the gradient in both the x and y directions. For pixels near the image borders that do not have a complete 33 neighborhood, assume a clamp-to-border strategy and use the nearest valid pixel value for any missing neighbors. The kernel should load image tiles into shared memory, perform the convolution efficiently, and write the resulting edge-detected image to global memory.\n\nThe signature of the function is __global__ void k_sobelEdgeDetection(const unsigned char *inputImage, unsigned char *outputImage, int widthOfImage, int heightOfImage). Where inputImage is the pointer to the input image (grayscale pixel values ranging from 0 to 255) stored in device memory, outputImage is the pointer to the output image (gradient magnitudes with the same dimension as inputImage) stored in device memory, widthOfImage is the number of pixels per row, heightOfImage is the number of rows in the image.\n\n>>> k_sobelEdgeDetection({{128,128,128,128,128}, {128,128,128,128,128}, {128,128,128,128,128}, {128,128,128,128,128}, {128,128,128,128,128}}, outputImage, 5, 5) -> outputImage:({{0,0,0,0,0},{0,0,0,0,0}, {0,0,0,0,0}, {0,0,0,0,0}, {0,0,0,0,0}})\n>>> k_sobelEdgeDetection({{0,0,0,0,0}, {255,255,255,255,255}, {255,255,255,255,255}, {0,0,0,0,0}, {0,0,0,0,0}}, outputImage, 5, 5) -> outputImage:({{0,0,0,0,0}, {0,255,255,255,0}, {0,255,255,255,0}, {0,255,255,255,0}, {0,0,0,0,0}})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <assert.h>\n\n#define BLOCK_SIZE 16\n#define MAX_IMAGE_SIZE 32\n#define RADIUS 1  \n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n    cudaError_t error = call;                                                      \\\n    if (error != cudaSuccess) {                                                    \\\n        fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),     \\\n                __FILE__, __LINE__);                                               \\\n        exit(error);                                                               \\\n    }                                                                              \\\n} while (0)\n\n__global__ void k_sobelEdgeDetection(const unsigned char* inputImage, unsigned char* outputImage, int widthOfImage, int heightOfImage);\n\nstruct testCase {\n    int widthOfImage;\n    int heightOfImage;\n    unsigned char inputImage[MAX_IMAGE_SIZE * MAX_IMAGE_SIZE];         \n    unsigned char expectedOutputImage[MAX_IMAGE_SIZE * MAX_IMAGE_SIZE];\n};\n\n\nvoid launch() {\n\n    // Create a CUDA stream.\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Determine the largest image size among all test cases.\n    size_t maxImageSize = MAX_IMAGE_SIZE * MAX_IMAGE_SIZE * sizeof(unsigned char);\n\n    // Allocate device memory for the largest image.\n    unsigned char *inputImage_d, *outputImage_d;\n    CUDA_CHECK(cudaMallocAsync((void **)&inputImage_d, maxImageSize, stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&outputImage_d, maxImageSize, stream));\n\n    // Define block dimensions.\n    dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n    size_t sharedMemSize = (BLOCK_SIZE + 2 * RADIUS) * (BLOCK_SIZE + 2 * RADIUS) * sizeof(unsigned char);\n    dim3 gridSize;\n\n    // Initialize device output memory to zero.\n    CUDA_CHECK(cudaMemsetAsync(outputImage_d, 0, maxImageSize, stream));\n\n    // Define test cases.\n    testCase testCases[] = {\n        // Test Case 1: 5x5 uniform image with no edges.\n        {\n            5, 5,\n            {\n                128, 128, 128, 128, 128,\n                128, 128, 128, 128, 128,\n                128, 128, 128, 128, 128,\n                128, 128, 128, 128, 128,\n                128, 128, 128, 128, 128\n            },\n\n            // Expected Output of Test Case 1\n            {\n                0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0\n            }\n        },\n        // Test Case 2: 5x5 image with horizontal edges.\n        {\n            5, 5,\n            {\n                0, 0, 0, 0, 0,\n                255, 255, 255, 255, 255,\n                255, 255, 255, 255, 255,\n                0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0                \n            },\n\n            // Expected Output of Test Case 2\n            {\n                0, 0, 0, 0, 0,\n                0, 255, 255, 255, 0,\n                0, 255, 255, 255, 0,\n                0, 255, 255, 255, 0,\n                0, 0, 0, 0, 0    \n            }\n        },\n        // Test Case 3: 5x5 image with diagonal edges.\n        {\n            5, 5,\n            {\n                255, 0, 0, 0, 0,\n                0, 255, 0, 0, 0,\n                0, 0, 255, 0, 0,\n                0, 0, 0, 255, 0,\n                0, 0, 0, 0, 255 \n            },\n\n            // Expected Output of Test Case 3\n            {\n                0, 0, 0, 0, 0,\n                0, 0, 255, 255, 0,\n                0, 255, 0, 255, 0,\n                0, 255, 255, 0, 0,\n                0, 0, 0, 0, 0      \n            }\n        },\n        // Test Case 4: 7x7 vertical edges.\n        {\n            7, 7,\n            {\n                0, 255, 0, 0, 255, 255, 0,\n                0, 255, 0, 0, 255, 255, 0,\n                0, 255, 0, 0, 255, 255, 0,\n                0, 255, 0, 0, 255, 255, 0,\n                0, 255, 0, 0, 255, 255, 0,\n                0, 255, 0, 0, 255, 255, 0,\n                0, 255, 0, 0, 255, 255, 0\n            },\n\n            // Expected Output of Test Case 4\n            {\n                0, 0, 0, 0, 0, 0, 0,\n                0, 0, 255, 255, 255, 255, 0,\n                0, 0, 255, 255, 255, 255, 0,\n                0, 0, 255, 255, 255, 255, 0,\n                0, 0, 255, 255, 255, 255, 0,\n                0, 0, 255, 255, 255, 255, 0,\n                0, 0, 0, 0, 0, 0, 0\n            }\n        },\n        // Test Case 5: 5x5 image with consecutive dots pattern.\n        {\n            5, 5,\n            {\n                0, 255, 0, 255, 0,\n                255, 0, 255, 0, 255,\n                0, 255, 0, 255, 0,\n                255, 0, 255, 0, 255,\n                0, 255, 0, 255, 0\n            },\n\n            // Expected Output of Test Case 5\n            {\n                0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0  \n            }\n        },\n        // Test Case 6: 5x5 image with random white pixels.\n        {\n            5, 5,\n            {\n                255, 0, 0, 0, 0,\n                0, 0, 0, 255, 0,\n                0, 0, 255, 0, 0,\n                255, 0, 0, 0, 0,\n                0, 255, 0, 0, 0\n            },\n\n            // Expected Output of Test Case 6\n            {\n                0, 0, 0, 0, 0,\n                0, 0, 255, 255, 0,\n                0, 255, 255, 255, 0,\n                0, 255, 255, 255, 0,\n                0, 0, 0, 0, 0          \n            }\n        },\n        // Test Case 7: 5x5 image with horizontal gradient.\n        {\n            5, 5,\n            {\n                0, 64, 128, 192, 255,\n                0, 64, 128, 192, 255,\n                0, 64, 128, 192, 255,\n                0, 64, 128, 192, 255,\n                0, 64, 128, 192, 255\n            },\n\n            // Expected Output of Test Case 7\n            {\n                0, 0, 0, 0, 0,\n                0, 255, 255, 255, 0,\n                0, 255, 255, 255, 0,\n                0, 255, 255, 255, 0,\n                0, 0, 0, 0, 0\n            }\n        },\n        // Test Case 8: 5x5 image with vertical gradient.\n        {\n            5, 5,\n            {\n                0, 0, 0, 0, 0,\n                64, 64, 64, 64, 64,\n                128, 128, 128, 128, 128,\n                192, 192, 192, 192, 192,\n                255, 255, 255, 255, 255\n            },\n\n            // Expected Output of Test Case 8\n            {\n                0, 0, 0, 0, 0,\n                0, 255, 255, 255, 0,\n                0, 255, 255, 255, 0,\n                0, 255, 255, 255, 0,\n                0, 0, 0, 0, 0\n            }\n        },\n        //Test Case 9: 7x10 image with vertical transition.\n        {\n            7, 10,\n            {\n                0, 0, 0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0, 0, 0,\n                255, 255, 255, 255, 255, 255, 255,\n                255, 255, 255, 255, 255, 255, 255,\n                255, 255, 255, 255, 255, 255, 255,\n                0, 0, 0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0, 0, 0\n            },\n\n            // Expected Output of Test Case 9\n            {\n                0, 0, 0, 0, 0, 0, 0,\n                0, 255, 255, 255, 255, 255, 0,\n                0, 255, 255, 255, 255, 255, 0,\n                0, 0, 0, 0, 0, 0, 0,\n                0, 255, 255, 255, 255, 255, 0,\n                0, 255, 255, 255, 255, 255, 0,\n                0, 0, 0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0, 0, 0,\n                0, 0, 0, 0, 0, 0, 0\n            }\n        }\n    };\n\n    // Number of test cases.\n    const int numTests = sizeof(testCases) / sizeof(testCases[0]);\n\n    // Iterate through each test case.\n    for (int testIdx = 0; testIdx < numTests; ++testIdx) {\n        testCase currentTest = testCases[testIdx];\n        int widthOfImage = currentTest.widthOfImage;\n        int heightOfImage = currentTest.heightOfImage;\n        \n        // Prepare host output array (initialize to zero).\n        unsigned char outputImage_h[MAX_IMAGE_SIZE * MAX_IMAGE_SIZE] = {0};\n        \n        // Copy input data to device asynchronously.\n        CUDA_CHECK(cudaMemcpyAsync(inputImage_d, currentTest.inputImage, widthOfImage * heightOfImage * sizeof(unsigned char),\n                                   cudaMemcpyHostToDevice, stream));\n\n        // Reinitialize device output image to zero for the current test case.\n        CUDA_CHECK(cudaMemsetAsync(outputImage_d, 0, widthOfImage * heightOfImage * sizeof(unsigned char), stream));\n        \n        // Set grid size based on current test dimensions.\n        gridSize = dim3((widthOfImage + BLOCK_SIZE - 1) / BLOCK_SIZE, (heightOfImage + BLOCK_SIZE - 1) / BLOCK_SIZE);\n        \n        // Launch the kernel asynchronously.\n        void* kernelArgs[] = { (void*)&inputImage_d, (void*)&outputImage_d, (void*)&widthOfImage, (void*)&heightOfImage };\n        CUDA_CHECK(cudaLaunchKernel((const void*)k_sobelEdgeDetection, gridSize, blockSize,\n                                    kernelArgs, sharedMemSize, stream));\n\n        // Copy the output data back to host asynchronously.\n        CUDA_CHECK(cudaMemcpyAsync(outputImage_h, outputImage_d, widthOfImage * heightOfImage * sizeof(unsigned char),\n                                   cudaMemcpyDeviceToHost, stream));\n        \n        // Synchronize the stream to ensure completion.\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Verify the results using assert.\n        for (int i = 0; i < widthOfImage * heightOfImage; ++i) {\n            if (outputImage_h[i] != currentTest.expectedOutputImage[i]) {\n                assert(outputImage_h[i] == currentTest.expectedOutputImage[i]);\n            }\n        }\n    }\n\n    // Free allocated memory asynchronously.\n    CUDA_CHECK(cudaFreeAsync(inputImage_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputImage_d, stream));\n\n    // Destroy the CUDA stream.\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    \n    // Synchronize the device.\n    CUDA_CHECK(cudaDeviceSynchronize());\n}\n\n__global__ void k_sobelEdgeDetection(const unsigned char* inputImage, unsigned char* outputImage, int widthOfImage, int heightOfImage) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/131", "date": "2025-07-30", "prompt": "Write a CUDA kernel that measures the similarity between two signals as a function of time shift using shared memory. The kernel performs cross-correlation in shared memory. The input vector contains $n$ elements and the reference vector contains $m$ elements where $m < $n.\n\nThe signature of the function is __global__ void k_measureSimilarity(int *inputVector, int inputSize, int *referenceVector, int refVectorSize, int *output), where inputVector is the pointer to the input data array, inputSize represents input data length, referenceVector is the pointer to the reference data for cross-correlation, refVectorSize is the size of the reference data array, and output is the pointer to the result array.\n\n>>> k_measureSimilarity({1, 2, 3, 4, 5, 6, 7, 8}, 8, {5,6}, 2, *output) -> output: ({6, 17, 28, 39, 50, 61, 72, 83, 40})\n>>> k_measureSimilarity({3, 4, 2, 7, 9, 8, 1}, 7, {1,2,3}, 3, *output) -> output: ({9, 18, 17, 29, 43, 49, 28, 10, 1})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <cuda_runtime.h>\n#include <algorithm>\n#undef    NDEBUG\n#include <assert.h>\n\n// Number of threads per block\n#define BLOCK_SIZE        (256)\n#define CUDA_CHECK(call)                              \\\ndo {                                                  \\\n       cudaError_t error = call;                      \\\n       if (error != cudaSuccess) {                    \\\n           fprintf(stderr, \"CUDA error at %s:%d %s\\n\",\\\n                   __FILE__, __LINE__,                \\\n                   cudaGetErrorString(error));        \\\n           exit(EXIT_FAILURE);                        \\\n       }                                              \\\n} while(0)\n\n__global__ void k_measureSimilarity(int *inputVector, int inputSize, int *referenceVector, int refVectorSize, int* output);\n\nvoid launch() {\n    // Number of test cases\n    constexpr int TEST_CASE_COUNT = 7;\n    // Sizes of the input vectors in each test case\n    int inputDataLength[TEST_CASE_COUNT] = {8, 7, 25, 25, 30, 30, 35};\n    // Sizes of the reference vectors in each test case\n    int referenceDataLength[TEST_CASE_COUNT] = {2, 3, 6, 6, 8, 8, 9};\n    // Maximum sizes\n    const int MAX_INPUT_VECTOR_SIZE = *std::max_element(inputDataLength, inputDataLength + TEST_CASE_COUNT);\n    const int MAX_PREAMBLE_VECTOR_SIZE = *std::max_element(referenceDataLength, referenceDataLength + TEST_CASE_COUNT);\n    const int MAX_OUT_VECTOR_SIZE = MAX_INPUT_VECTOR_SIZE + MAX_PREAMBLE_VECTOR_SIZE - 1;\n    const int MAX_INPUT_ZERO_PAD_SIZE = MAX_INPUT_VECTOR_SIZE + 2 * (MAX_PREAMBLE_VECTOR_SIZE - 1);\n\n    // Input vectors for the tests\n    int inputData[TEST_CASE_COUNT][MAX_INPUT_VECTOR_SIZE] = {\n        {1, 2, 3, 4, 5, 6, 7, 8}, // test case 1\n        {3, 4, 2, 7, 9, 8, 1}, // test case 2\n        {2, 2, 1, 5, 2, 9, 1, 1, 6, 8, 4, 2, 4, 3, 6, 10, 7, 2, 4, 1, 6, 5, 10, 9, 5}, // test case 3\n        {7, 4, 10, 2, 8, 7, 9, 4, 8, 9, 4, 6, 10, 6, 4, 7, 4, 8, 5, 5, 7, 10, 4, 9, 8}, // test case 4\n        {8, 7, 6, 7, 1, 4, 5, 3, 8, 9, 3, 8, 2, 9, 2, 6, 4, 9, 6, 5, 9, 4, 5, 10, 1, 10, 2, 7, 6, 7}, // test case 5\n        {5, 2, 3, 3, 4, 2, 4, 2, 9, 1, 10, 4, 1, 4, 8, 8, 6, 7, 9, 1, 4, 1, 2, 8, 8, 9, 6, 1, 10, 9}, // test case 6\n        {3, 8, 6, 7, 4, 2, 5, 4, 8, 8, 7, 2, 1, 6, 4, 10, 10, 3, 9, 9, 6, 9, 10, 6, 8, 6, 1, 5, 7, 6, 4, 10, 9, 9, 4} // test case 7\n    };\n\n    // reference vectors for processing\n    int referenceData[TEST_CASE_COUNT][MAX_PREAMBLE_VECTOR_SIZE] = {\n        {5, 6},                           // test case 1\n        {1, 2, 3},                        // test case 2\n        {18, 3, 8, 19, 19, 15},           // test case 3\n        {20, 1, 8, 14, 6, 5},             // test case 4\n        {8, 13, 17, 1, 2, 20, 14, 5},     // test case 5\n        {6, 11, 20, 15, 17, 9, 10, 12},   // test case 6\n        {12, 18, 19, 14, 5, 14, 2, 9, 14} // test case 7\n    };\n\n    int expectedOutputData[TEST_CASE_COUNT][MAX_OUT_VECTOR_SIZE] = {\n        {6, 17, 28, 39, 50, 61, 72, 83, 40},\n        {9, 18, 17, 29, 43, 49, 28, 10, 1},\n        {30, 68, 91, 148, 166, 318, 303, 254, 296, 324, 499, 327, 274, 323, 395, 431, 481, 482, 383, 323, 402, 372, 415, 543, 512, 469, 287, 247, 177, 90},\n        {35, 62, 172, 182, 231, 335, 305, 438, 294, 388, 367, 428, 302, 375, 428, 309, 296, 400, 309, 286, 343, 290, 383, 370, 337, 356, 388, 153, 188, 160},\n        {40, 147, 288,  275, 245, 329, 344, 431, 444, 387, 449, 426, 356, 463, 476, 482, 411, 396, 493, 470, 457, 429, 530, 474, 497, 544, 420, 484, 456, 460, 497, 331, 245, 216, 253, 139, 56},\n        {60, 74, 101, 169, 214, 272, 295, 307, 363, 349, 407, 466, 390, 505, 494, 554, 503, 536, 603, 597, 620, 575, 476, 501, 432, 454, 470, 484, 645, 714, 603, 533, 443, 382, 296, 159, 54},\n        {42, 139, 162, 210, 258, 244, 393, 486, 598, 665, 569, 503, 433, 537, 571, 654, 728, 512, 556, 615, 611, 813, 864, 804, 813, 806, 728, 760, 777, 715, 663, 655, 636, 596, 634, 586, 587, 561, 545, 509, 346, 180, 48}\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Initialize and allocate result memory on the host\n    int *inputVectorZeroPad_h, *output_h;\n    inputVectorZeroPad_h = (int*)malloc(MAX_INPUT_ZERO_PAD_SIZE * sizeof(int));\n    output_h = (int*)malloc(MAX_OUT_VECTOR_SIZE * sizeof(int));\n\n    // Pointers for device memory (GPU)\n    int *inputVectorZeroPad_d, *referenceVector_d, *output_d;\n    // Allocating memory on device\n    CUDA_CHECK(cudaMallocAsync(&inputVectorZeroPad_d, MAX_INPUT_ZERO_PAD_SIZE * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&referenceVector_d, MAX_PREAMBLE_VECTOR_SIZE * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, MAX_OUT_VECTOR_SIZE* sizeof(int), stream));\n\n    int outputSize = 0, inputZeroPadSize = 0;\n    // Loop to execute each test case\n    for (int i = 0; i < TEST_CASE_COUNT; ++i) {\n        outputSize = inputDataLength[i] + referenceDataLength[i] - 1;\n        inputZeroPadSize = inputDataLength[i] + 2 * (referenceDataLength[i] - 1);\n\n        // preparing zero padding input vector for cross-correlation\n        for (unsigned j = 0; j < inputZeroPadSize; j++) {\n            inputVectorZeroPad_h[j] = 0;\n        }\n        for (unsigned j = referenceDataLength[i] - 1; j < outputSize; j++) {\n            inputVectorZeroPad_h[j] = inputData[i][j - referenceDataLength[i] + 1];\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(inputVectorZeroPad_d, inputVectorZeroPad_h, inputZeroPadSize * sizeof(int), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(referenceVector_d, referenceData[i], referenceDataLength[i] * sizeof(int), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemsetAsync(output_d, 0, outputSize * sizeof(int), stream));\n\n        // Determine the number of threads and blocks\n        dim3 gridSize = dim3((inputZeroPadSize + BLOCK_SIZE - 1) / BLOCK_SIZE, 1, 1);\n        dim3 blockSize = dim3(BLOCK_SIZE, 1, 1);\n\n        //  Adding check for exceeding maximum grid dimensions\n        cudaDeviceProp props;\n        CUDA_CHECK(cudaGetDeviceProperties(&props, 0));\n        if (gridSize.x > props.maxGridSize[0] || gridSize.y > props.maxGridSize[1]) {\n            assert(false && \"Grid size exceeds device limits!\");\n        }\n        \n        // shared memory required for inputVectorZeroPad_d ->  BLOCK_SIZE + referenceDataLength[i] - 1\n        // shared memory required for referenceVector_d    ->  referenceDataLength[i]\n        size_t sharedMemorySize = (BLOCK_SIZE + 2 * referenceDataLength[i]) * sizeof(int);\n        // Execute the kernel\n        // Grid:  ((inputZeroPadSize + BLOCK_SIZE - 1) / BLOCK_SIZE, 1, 1)\n        // Block: (BLOCK_SIZE, 1, 1)\n        void *args[] = {&inputVectorZeroPad_d, (void*)&inputDataLength[i], &referenceVector_d, (void*)&referenceDataLength[i], &output_d};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_measureSimilarity, gridSize, blockSize, args, sharedMemorySize, stream));\n\n        // Copying data back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, outputSize * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify if the calculated dot product matches the expected result\n        for (int j = 0; j < outputSize; j++) {\n            assert(output_h[j] == expectedOutputData[i][j]);\n        }\n    }\n    // Free host memories\n    free(inputVectorZeroPad_h);\n    free(output_h);\n\n    // Free device memories and stream\n    CUDA_CHECK(cudaFree(inputVectorZeroPad_d));\n    CUDA_CHECK(cudaFree(referenceVector_d));\n    CUDA_CHECK(cudaFree(output_d));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_measureSimilarity(int *inputVector, int inputSize, int *referenceVector, int refVectorSize, int *output) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/132", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform matrix multiplication by using tensor cores by using mma ptx instruction of dimension m16n8k8 for Ampere Architecture.\nConfigure it in row major by column major format, using f16 data type for input matrices A (row major) and B (column major), and f32 for accumulator matrix C(row major), all input\nmatrices will have m16n8k8 compatible dimensions.\n\nThe signature of the function is __global__ void k_mmaTensorMatMulM16N8k8Fp16(half *inputMatrixA, half *inputMatrixB, float *mmaOutMatrixC_d, int mDim, int nDim, int kDim), where inputMatrixA is first input layer matrix with dimension mDim x kDim, inputMatrixB is second input layer matrix with dimension kDim x nDim, mmaOutMatrixC_d is output matrix with dimension mDim x nDim.\n\n>>> k_mmaTensorMatMulM16N8k8Fp16({1, 1, 1, 1}, {2, 2}, mmaOutMatrixC_d, 2, 1, 2)-> mmaOutMatrixC_d: ({4, 4})\n>>> k_mmaTensorMatMulM16N8k8Fp16({1, 2, 2, 1}, {3, 3}, mmaOutMatrixC_d, 2, 1, 2)-> mmaOutMatrixC_d: ({9, 9})\n", "cc_flags": "-arch=sm_80", "ld_flags": "", "declaration": "\n#include <cstdio>\n#include <cassert>\n#include <cstdint>\n#include <random>\n\n#include <cuda_runtime.h>\n#include <mma.h>\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n#undef NDEBUG\n\n#define MMA_M 16\n#define MMA_N 8\n#define MMA_K 8\n\n__global__ void k_mmaTensorMatMulM16N8k8Fp16(half *inputMatrixA, half *inputMatrixB, float *mmaOutMatrixC_d, int mDim, int nDim, int kDim);\n\n// Function to compute valid reference result\nvoid cpuMatMulReference(const half* A,\n                        const half* B,\n                        float* cpuRefC,\n                        int M,\n                        int N,\n                        int K) {\n    for (int i = 0; i < M; i++) {\n        for (int j = 0; j < N; j++) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; k++) {\n                float a_val = static_cast<float>(A[i*K + k]);\n                float b_val = static_cast<float>(B[k*N + j]);\n                sum += a_val * b_val;\n            }\n            cpuRefC[i*N + j] = sum;\n        }\n    }\n}\n\nvoid launch() {\n    const int TEST_CASE_COUNT = 7;\n\n    //Test case dimensions {M, N, K}\n    const int TEST_CASES_DIMS[TEST_CASE_COUNT][3] = {{16,16,16}, {512,512,512}, {32,16,32}, {256, 256, 256}, {64, 64, 64} , {64, 32, 32}, {128, 128, 128}};\n\n    //Tolerance for validation, set to 1% due to nature of half precision operations\n    const float TOLERANCE  = 0.01;\n    const int BLOCK_SIZE = 256;\n\n    //Set up random number generation\n    std::mt19937 randEngine(static_cast<unsigned int>(time(nullptr)));\n    // Bounded random distribution for test case initialization\n    std::uniform_real_distribution<float> randDist(1.0f, 100.0f);\n\n    for (int i = 0; i < TEST_CASE_COUNT; i++) {\n        // Dimensions of the input and output layers\n        int M = TEST_CASES_DIMS[i][0]; //Number of Rows in Matrix A\n        int N = TEST_CASES_DIMS[i][1]; //Number of Columns in Matrix B\n        int K = TEST_CASES_DIMS[i][2]; //Number of Columns in Matrix A and Rows in Matrix B\n\n        //Pointers for Host Memory\n        half* A_h =(half*)malloc(M * K * sizeof(half));\n        half* B_h =(half*)malloc(K * N * sizeof(half));\n\n        float* cpuC_h =(float*)malloc(M * N * sizeof(float)); // Reference Matrix space allocation on host\n        float* gpuC_h = (float*)malloc(M * N * sizeof(float));// GPU result Matrix space allocation on host\n\n        //Pointers for device memory (GPU)\n        half* A_d;\n        half* B_d;\n        float* C_d;\n\n        //Populating input matrices with random values\n        for (int i = 0; i < M * K; i++) {\n            float val = randDist(randEngine);\n            A_h[i] = half(val);\n        }\n\n        for (int i = 0; i < K * N; i++) {\n            float val = randDist(randEngine);\n            B_h[i] = half(val);\n        }\n\n        // Use a CUDA stream for asynchronous operations\n        cudaStream_t stream;\n        CUDA_CHECK(cudaStreamCreate(&stream));\n\n        // Allocate the memory on the device\n        CUDA_CHECK(cudaMallocAsync(&A_d, M * K * sizeof(half), stream));\n        CUDA_CHECK(cudaMallocAsync(&B_d, K * N * sizeof(half), stream));\n        CUDA_CHECK(cudaMallocAsync(&C_d, M * N * sizeof(float), stream));\n\n        //Load Test Cases\n        CUDA_CHECK(cudaMemcpyAsync(A_d, A_h, M * K * sizeof(half), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(B_d, B_h, K * N * sizeof(half), cudaMemcpyHostToDevice, stream));\n\n        // Initialize the result on the device\n        CUDA_CHECK(cudaMemsetAsync(C_d, 0, M * N * sizeof(float), stream));\n\n        //Check if the dimensions are divisible by the block tile dimensions\n        assert(M % MMA_M == 0);\n        assert(N % MMA_N == 0);\n        assert(K % MMA_K == 0);\n\n        dim3 gridDim((N + MMA_N - 1) / MMA_N, (M + MMA_M - 1) / MMA_M);\n        dim3 blockDim(BLOCK_SIZE);  // one warp per block\n        int shmemBytes = (MMA_M * MMA_K + MMA_K * MMA_N) * sizeof(half);\n\n        // Launch kernel\n        // Grid: ((N + MMA_N - 1/ MMA_N), (M + MMA_M - 1)/ MMA_M, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&A_d,\n                        &B_d,\n                        &C_d,\n                        (void*)&M,\n                        (void*)&N,\n                        (void*)&K};\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_mmaTensorMatMulM16N8k8Fp16,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    shmemBytes,\n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        cpuMatMulReference(A_h, B_h, cpuC_h, M, N, K);\n\n        //Copying the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync(gpuC_h, C_d, M * N * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        //Validate the result, with in 1% tolerance\n        for(int t = 0; t < M*N; ++t) {\n            assert(std::fabs((gpuC_h[t] - cpuC_h[t]) / std::fabs(cpuC_h[t])) <= TOLERANCE);\n        }\n\n        //Free up resources\n        CUDA_CHECK(cudaFreeAsync(A_d, stream));\n        CUDA_CHECK(cudaFreeAsync(B_d, stream));\n        CUDA_CHECK(cudaFreeAsync(C_d, stream));\n        CUDA_CHECK(cudaStreamDestroy(stream));\n        free(A_h);\n        free(B_h);\n        free(cpuC_h);\n        free(gpuC_h);\n    }\n}\n\n// Storing 16x8 Matrix Tile\n__device__ __forceinline__ void d_storeMatrixTile16x8(half* dst, half* (&reg)[4], int dstStrideBytes) {\n   int lane = threadIdx.x % 32;\n\n    //Casting 2x f16 elements into 4 byte space of uint32_t\n    uint32_t (&regInt)[2] = reinterpret_cast<uint32_t(&)[2]>(reg);\n    uint32_t* dstPtr = reinterpret_cast<uint32_t*>(dst);\n    dstStrideBytes /= sizeof(uint32_t);\n\n    int fragmentRow = lane / 4;\n    int fragmentCol = lane % 4;\n\n    // Adjacent Threads store 4 bytes each\n    dstPtr[fragmentRow * dstStrideBytes + fragmentCol] = regInt[0];\n    fragmentRow += 8;\n    dstPtr[fragmentRow * dstStrideBytes + fragmentCol] = regInt[1];\n}\n\n__device__ __forceinline__ uint32_t d_cvtaToSharedU32(const void* ptr) {\n    unsigned long long address;\n    asm volatile(\"cvta.to.shared.u64 %0, %1;\" : \"=l\"(address) : \"l\"(ptr));\n    return static_cast<uint32_t>(address);\n}\n\n__device__ __forceinline__ void d_storeMatrixTile16x8_f32(float* dst, float reg[4], int n) {\n    int lane = threadIdx.x % 32;  // 0..31\n    int r = lane / 4;        // 0..7 for the top half\n    int c = (lane % 4) * 2;    // columns: 0,2,4,6\n    dst[r * n + c] = reg[0];\n    dst[r * n + c + 1] = reg[1];\n    dst[(r + 8) * n + c] = reg[2];\n    dst[(r + 8) * n + c + 1] = reg[3];\n}\n\n// Each block computes one 168 output tile using MMA instructions.\n__global__ void k_mmaTensorMatMulM16N8k8Fp16(half *inputMatrixA, half *inputMatrixB, float *mmaOutMatrixC_d, int mDim, int nDim, int kDim) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/133", "date": "2025-07-30", "prompt": "Implement the N-body algorithm's force calculation using CUDA. Utilize shared memory tiling to optimize the accesses. For simplicity, perform calculations in 2D space.\n\nThe signature of the CUDA kernel is __global__ void k_calculateNBodyForce(float *xPosition_d, float *yPosition_d, float *mass_d, float *xForce_d, float *yForce_d), where xPosition_d is a pointer to the array of x-coordinate values of particles, yPosition_d is a pointer to the array of y-coordinate values of particles, mass_d is a pointer to the array of mass values of particles, xForce_d is a pointer to the array of x components of the computed forces, and yForce_d is a pointer to the array of y components of the computed forces.\n\n>>> k_calculateNBodyForce({ 100.0000f, 200.0000f, 300.0000f, 400.0000f, 500.0000f }, { 100.0000f, 200.0000f, 300.0000f, 400.0000f, 500.0000f }, { 1.0000f, 1.0000f, 1.0000f, 1.0000f, 1.0000f }, xForce_d, yForce_d) -> xForce_d: { 0.5000f, 0.1300f, -0.0000f, -0.1300f, -0.5000f }, yForce_d: { 0.5000f, 0.1300f, -0.0000f, -0.1300f, -0.5000f }\n>>> k_calculateNBodyForce({ 500.0000f, 975.5283f, 793.8926f, 206.1072f, 24.4718f }, { 1000.0000f, 654.5085f, 95.4915f, 95.4916f, 654.5085f }, { 1.0000f, 1.0000f, 1.0000f, 1.0000f, 1.0000f }, xForce_d, yForce_d) -> xForce_d: { -0.0000f, -0.0524f, -0.0324f, 0.0324f, 0.0524f }, yForce_d: { -0.0551f, -0.0170f, 0.0445f, 0.0445f, -0.0170f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <random>\n#include <ctime>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) {                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Algorithm-related constants.\nconstexpr int NUM_PARTICLES = 5;\nconstexpr int ALLOCATION_SIZE = NUM_PARTICLES * sizeof(float);\n\n// x, y, mass => 3 input properties\nconstexpr int NUMBER_OF_INPUT_PROPERTIES_OF_PARTICLE = 3;\nconstexpr float GRAVITATIONAL_FORCE_CONSTANT = 10000.0f;\nconstexpr float VERY_CLOSE = 1e-15f;\nconstexpr float SMOOTHING = 1e-5f;\n\n// CUDA-related constants.\n// Tile size is independent of the number of threads per block. Each block can load a flexible amount of particles into the shared memory tile.\nconstexpr int NUM_PARTICLES_PER_TILE = 2;\nconstexpr int SHARED_MEM_ALLOCATION_SIZE = NUM_PARTICLES_PER_TILE * NUMBER_OF_INPUT_PROPERTIES_OF_PARTICLE * sizeof(float);\n\n// Test-related constants.\n// Tolerance for errors in comparisons.\nconstexpr float EPSILON = 0.001f;\n\nconstexpr float ACCEPTABLE_MAGNITUDE = 1e12f;\n\n__global__ void k_calculateNBodyForce(float *xPosition_d, float *yPosition_d, float *mass_d, float *xForce_d, float *yForce_d);\n\nvoid launch() {\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n\n    int threadsPerBlock = prop.maxThreadsPerBlock;\n    int maxBlocks = (prop.maxThreadsPerMultiProcessor / threadsPerBlock) * prop.multiProcessorCount;\n    int requiredBlocks = NUM_PARTICLES / NUM_PARTICLES_PER_TILE;\n    int usedBlocks = maxBlocks < requiredBlocks? maxBlocks : requiredBlocks;\n    dim3 gridDim(usedBlocks, 1, 1);\n    dim3 blockDim(threadsPerBlock, 1, 1);\n\n    // Host buffer allocation.\n    float *xPosition_h = new float[NUM_PARTICLES];\n    float *yPosition_h = new float[NUM_PARTICLES];\n    float *mass_h = new float[NUM_PARTICLES];\n    float *xForce_h = new float[NUM_PARTICLES];\n    float *yForce_h = new float[NUM_PARTICLES];\n\n    // Device buffer allocation.\n    float *xPosition_d;\n    float *yPosition_d;\n    float *mass_d;\n    float *xForce_d;\n    float *yForce_d;\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    CUDA_CHECK(cudaMallocAsync(&xPosition_d, ALLOCATION_SIZE, stream));\n    CUDA_CHECK(cudaMallocAsync(&yPosition_d, ALLOCATION_SIZE, stream));\n    CUDA_CHECK(cudaMallocAsync(&mass_d, ALLOCATION_SIZE, stream));\n    CUDA_CHECK(cudaMallocAsync(&xForce_d, ALLOCATION_SIZE, stream));\n    CUDA_CHECK(cudaMallocAsync(&yForce_d, ALLOCATION_SIZE, stream));\n    auto hToD = cudaMemcpyHostToDevice;\n    auto dToH = cudaMemcpyDeviceToHost;\n    void *args[5] = { &xPosition_d, &yPosition_d, &mass_d, &xForce_d, &yForce_d };\n    \n    // Initializing all host buffers.\n    for(int i = 0; i < NUM_PARTICLES; i++) {\n        xPosition_h[i] = (i % 100) * 10;\n        yPosition_h[i] = (i / 100) * 10;\n        mass_h[i] = 1.0f;\n        xForce_h[i] = 0.0f;\n        yForce_h[i] = 0.0f;\n    }\n\n    // Initializing all device buffers.\n    CUDA_CHECK(cudaMemcpyAsync(xPosition_d, xPosition_h, ALLOCATION_SIZE, hToD, stream));\n    CUDA_CHECK(cudaMemcpyAsync(yPosition_d, yPosition_h, ALLOCATION_SIZE, hToD, stream));\n    CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, ALLOCATION_SIZE, hToD, stream));\n    CUDA_CHECK(cudaMemcpyAsync(xForce_d, xForce_h, ALLOCATION_SIZE, hToD, stream));\n    CUDA_CHECK(cudaMemcpyAsync(yForce_d, yForce_h, ALLOCATION_SIZE, hToD, stream));\n\n    // Test 1: All particles in the exact same position -> no force applied.\n    {\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            xPosition_h[i] = 500.0f;\n            yPosition_h[i] = 500.0f;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(xPosition_d, xPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yPosition_d, yPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, ALLOCATION_SIZE, hToD, stream));\n        \n        // Grid: (2, 1, 1)\n        // Block: (1024, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateNBodyForce, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    SHARED_MEM_ALLOCATION_SIZE, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(xForce_h, xForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yForce_h, yForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            assert(fabs(xForce_h[i]) < EPSILON);\n            assert(fabs(yForce_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 2: All particles are on same line with with equal distances -> end points have equal magnitude of force, inner points have equal magnitude of force, middle point has zero force due to symmetry of all forces acting on it.\n    {\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            xPosition_h[i] = 100.0f + i * 100.0f;\n            yPosition_h[i] = 100.0f + i * 100.0f;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(xPosition_d, xPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yPosition_d, yPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, ALLOCATION_SIZE, hToD, stream));\n        \n        // Grid: (2, 1, 1)\n        // Block: (1024, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateNBodyForce, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    SHARED_MEM_ALLOCATION_SIZE, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(xForce_h, xForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yForce_h, yForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_PARTICLES / 2; i++) {\n            assert(fabs(fabs(xForce_h[i]) - fabs(xForce_h[NUM_PARTICLES - i - 1])) < EPSILON);\n            assert(fabs(fabs(yForce_h[i]) - fabs(yForce_h[NUM_PARTICLES - i - 1])) < EPSILON);\n        }\n        \n        assert(fabs(xForce_h[NUM_PARTICLES / 2]) < EPSILON);\n        assert(fabs(yForce_h[NUM_PARTICLES / 2]) < EPSILON);        \n    }\n\n    // Test 3: Test 2: All particles are on the same line with equal distances -> the endpoints have equal magnitude of force, the inner points have equal magnitude of force, and the middle point has zero force due to the symmetry of all the forces acting on it.\n    {\n        float radius = 500.0f;\n        float centerX = 500.0f;\n        float centerY = 500.0f;\n\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            float pi = acos(-1);\n            float posX = sin( (i / (float) NUM_PARTICLES) * 2 * pi) * radius;\n            float posY = cos( (i / (float) NUM_PARTICLES) * 2 * pi) * radius;\n            xPosition_h[i] = posX + centerX;\n            yPosition_h[i] = posY + centerY;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(xPosition_d, xPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yPosition_d, yPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, ALLOCATION_SIZE, hToD, stream));\n        \n        // Grid: (2, 1, 1)\n        // Block: (1024, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateNBodyForce, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    SHARED_MEM_ALLOCATION_SIZE, \n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(xForce_h, xForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yForce_h, yForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_PARTICLES - 1; i++) {\n            float magnitude = sqrt(xForce_h[i] * xForce_h[i] + yForce_h[i] * yForce_h[i]);\n            float unitVectorX = xForce_h[i] / magnitude;\n            float unitVectorY = yForce_h[i] / magnitude;\n\n            for(int j = i + 1; j < NUM_PARTICLES; j++) {\n                float magnitude2 = sqrt(xForce_h[j] * xForce_h[j] + yForce_h[j] * yForce_h[j]);\n                float unitVector2X = xForce_h[j] / magnitude2;\n                float unitVector2Y = yForce_h[j] / magnitude2;\n                assert(fabs(magnitude - magnitude2) < EPSILON);\n            }\n\n            float targetVectorX = radius * unitVectorX;\n            float targetVectorY = radius * unitVectorY;\n            float targetPointX = targetVectorX + xPosition_h[i];\n            float targetPointY = targetVectorY + yPosition_h[i];\n\n            // Checking whether forces are directed towards the center of the circle.\n            assert(fabs(targetPointX - centerX) < EPSILON);\n            assert(fabs(targetPointY - centerY) < EPSILON);\n        }\n    }\n\n    // Test 4: Randomly scattered particles -> the result is compared with the host-side calculation.\n    {\n        std::mt19937 generator(static_cast<unsigned int>(std::time(nullptr)));\n        std::uniform_real_distribution<float> dist(0.0f, 1000.0f);\n\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            xPosition_h[i] = dist(generator);\n            yPosition_h[i] = dist(generator);\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(xPosition_d, xPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yPosition_d, yPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, ALLOCATION_SIZE, hToD, stream));\n\n        // Grid: (2, 1, 1)\n        // Block: (1024, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateNBodyForce,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    SHARED_MEM_ALLOCATION_SIZE,\n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(xForce_h, xForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yForce_h, yForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            float xForce = 0.0f;\n            float yForce = 0.0f;\n\n            for(int j = 0; j < NUM_PARTICLES; j++) {\n                float dx = xPosition_h[i] - xPosition_h[j];\n                float dy = yPosition_h[i] - yPosition_h[j];\n                float distanceSquared = dx * dx + dy * dy;\n                float inverseDistance = 1.0f / sqrtf(distanceSquared + SMOOTHING);\n                float forceMultiplier = distanceSquared < VERY_CLOSE ?\n                                        0.0f :\n                                        -GRAVITATIONAL_FORCE_CONSTANT * mass_h[i] * mass_h[j] *\n                                        inverseDistance * inverseDistance * inverseDistance;\n                xForce = fmaf(dx, forceMultiplier, xForce);\n                yForce = fmaf(dy, forceMultiplier, yForce);\n            }\n\n            assert(fabs(xForce - xForce_h[i]) < EPSILON);\n            assert(fabs(yForce - yForce_h[i]) < EPSILON);\n        }\n    }\n\n    // Test 5: Very close particles -> The smoothing variable prevents divide-by-zero errors, avoids denormal values, and keeps the force magnitude within acceptable limits.\n    {\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            xPosition_h[i] = i * 0.00001f;\n            yPosition_h[i] = i * 0.00001f;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(xPosition_d, xPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yPosition_d, yPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, ALLOCATION_SIZE, hToD, stream));\n\n        // Grid: (2, 1, 1)\n        // Block: (1024, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateNBodyForce,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    SHARED_MEM_ALLOCATION_SIZE,\n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(xForce_h, xForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yForce_h, yForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            float xForce = 0.0f;\n            float yForce = 0.0f;\n\n            for(int j = 0; j < NUM_PARTICLES; j++) {\n                float dx = xPosition_h[i] - xPosition_h[j];\n                float dy = yPosition_h[i] - yPosition_h[j];\n                float massMultiplied = mass_h[i] * mass_h[j];\n                float distanceSquared = dx * dx + dy * dy;\n                float inverseDistance = 1.0f / sqrtf(distanceSquared + SMOOTHING);\n                float forceMultiplier = distanceSquared < VERY_CLOSE ?\n                                        0.0f :\n                                        -GRAVITATIONAL_FORCE_CONSTANT * massMultiplied *\n                                        inverseDistance * inverseDistance * inverseDistance;\n                xForce = fmaf(dx, forceMultiplier, xForce);\n                yForce = fmaf(dy, forceMultiplier, yForce);\n            }\n\n            assert(fabs(xForce - xForce_h[i]) < EPSILON);\n            assert(fabs(yForce - yForce_h[i]) < EPSILON);\n            assert(!isnan(xForce));\n            assert(!isnan(yForce));\n            assert(!isinf(xForce));\n            assert(!isinf(yForce));\n            assert(fabs(xForce_h[i]) < ACCEPTABLE_MAGNITUDE);\n            assert(fabs(yForce_h[i]) < ACCEPTABLE_MAGNITUDE);\n        }\n    }\n\n    // Test 6: One of the particles is at infinity  -> all particles have nan force.\n    {\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            xPosition_h[i] = ((i == 0) ? INFINITY : i);\n            yPosition_h[i] = ((i == 0) ? INFINITY : i);\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(xPosition_d, xPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yPosition_d, yPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, ALLOCATION_SIZE, hToD, stream));\n\n        // Grid: (2, 1, 1)\n        // Block: (1024, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateNBodyForce,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    SHARED_MEM_ALLOCATION_SIZE,\n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(xForce_h, xForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yForce_h, yForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            assert(isnan(xForce_h[i]));\n            assert(isnan(yForce_h[i]));\n        }\n    }\n\n    // Test 7: Half of the particles are CLOSE_TO_INFINITE from the center, and the other half are CLOSE_TO_INFINITE from the center in the opposite direction -> only the x-component of the forces has nan value.\n    {\n        float CLOSE_TO_INFINITE = 3.0e+38f;\n\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            xPosition_h[i] = ((i < NUM_PARTICLES / 2) ? -CLOSE_TO_INFINITE : CLOSE_TO_INFINITE);\n            yPosition_h[i] = i;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(xPosition_d, xPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yPosition_d, yPosition_h, ALLOCATION_SIZE, hToD, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, ALLOCATION_SIZE, hToD, stream));\n\n        // Grid: (2, 1, 1)\n        // Block: (1024, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateNBodyForce,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    SHARED_MEM_ALLOCATION_SIZE,\n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync(xForce_h, xForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaMemcpyAsync(yForce_h, yForce_d, ALLOCATION_SIZE, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < NUM_PARTICLES; i++) {\n            assert(isnan(xForce_h[i]));\n            assert(!isnan(yForce_h[i]));\n        }\n    }\n\n    CUDA_CHECK(cudaFreeAsync(xPosition_d, stream));\n    CUDA_CHECK(cudaFreeAsync(yPosition_d, stream));\n    CUDA_CHECK(cudaFreeAsync(mass_d, stream));\n    CUDA_CHECK(cudaFreeAsync(xForce_d, stream));\n    CUDA_CHECK(cudaFreeAsync(yForce_d, stream));\n\n    delete [] xPosition_h;\n    delete [] yPosition_h;\n    delete [] mass_h;\n    delete [] xForce_h;\n    delete [] yForce_h;\n\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n// This kernel calculates the forces acting on each particle independently, utilizing shared-memory tiling to optimize the accesses by each thread.\n__global__ void k_calculateNBodyForce(float *xPosition_d, float *yPosition_d, float *mass_d, float *xForce_d, float *yForce_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/134", "date": "2025-07-30", "prompt": "Write a CUDA kernel that computes trajectory to update position, velocity, and mass for each time step, using thrust and gravitational forces. To ensure optimal performance kernel should derive execution parameters at run time using device properties and employ cluster configurations to enhance communication efficiency across multiple blocks.\n\nThe signature of the function is __global__ void k_computeTrajectory(float* position_d, float* velocity_d, float* mass_d, float* timeSteps_d, float thrust, float exhaustVelocity, int totalSteps) where position_d, velocity_d and mass_d are pointers to the output arrays containing updated positions, velocities, and mass of the object respectively, timeSteps_d is pointer to the array of time step values, specifying the interval for each update, thrust is the constant thrust force applied to the object, exhaustVelocity is the the exhaust velocity to calculate fuel burn rate, and totalSteps are the total number of time steps to process.\n\n>>> k_computeTrajectory(*position_d, *velocity_d, {500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0}, {0.05, 0.75, 1.45, 2.15, 2.85, 3.55, 4.25, 4.95, 5.65, 6.35}, 15000.0, 2000.0, 10) => position_d: {0.050, 11.357, 42.449, 93.328, 163.993, 254.444, 364.682, 494.705, 644.515, 814.111}, velocity_d: {1.010, 15.142, 29.276, 43.409, 57.542, 71.674, 85.808, 99.940, 114.074, 128.206}, mass_d: {499.625, 494.375, 489.125, 483.875, 478.625, 473.375, 468.125, 462.875, 457.625, 452.375}\n>>> k_computeTrajectory(*position_d, *velocity_d, {500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0}, {0.1, 0.7, 1.3, 1.9, 2.5, 3.1, 3.7, 4.3, 4.9, 5.5}, 15000.0, 2000.0, 10) => position_d:  {0.202, 9.893, 34.121, 72.886, 126.188, 194.026, 276.401, 373.313, 484.762, 610.748}, velocity_d: {2.019, 14.133, 26.247, 38.361, 50.475, 62.589, 74.703, 86.817, 98.931, 111.045}, mass_d: {499.250, 494.750, 490.250, 485.750, 481.250, 476.750, 472.250, 467.750, 463.250, 458.750}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cuda_runtime.h>\n#undef    NDEBUG\n#include <assert.h>\n\n// Maximum number of threads per block\n#define MAX_BLOCK_SIZE  (512)\n#define NUM_ATTRIBUTES  (3)\n#define TOLERANCE       (1e-3)\n#define CUDA_CHECK(call)                              \\\ndo {                                                  \\\n       cudaError_t error = call;                      \\\n       if (error != cudaSuccess) {                    \\\n           fprintf(stderr, \"CUDA error at %s:%d %s\\n\",\\\n                   __FILE__, __LINE__,                \\\n                   cudaGetErrorString(error));        \\\n           exit(EXIT_FAILURE);                        \\\n       }                                              \\\n} while(0)\n\n// Earth's gravity in meters/sec^2\nconst __device__ float GRAVITY = 9.81f;\n\n__global__ void k_computeTrajectory(float* position_d, float* velocity_d, float* mass_d, float* timeSteps_d, float thrust, float exhaustVelocity, int totalSteps);\n\nvoid launch() {\n\n    // Number of test cases\n    const int TEST_CASE_COUNT = 7;\n    \n    int timeLength[TEST_CASE_COUNT] = {10, 10, 15, 15, 20, 20, 35};\n\n    // Object paramters\n    float initialMass[TEST_CASE_COUNT] = {500.0, 500.0, 1000.0, 1000.0, 1500.0, 1500.0, 2000.0};\n    float exhaustVelocity[TEST_CASE_COUNT] = {2000.0, 2000.0, 2500.0, 2500, 3000, 3000, 3500};\n    float thrust[TEST_CASE_COUNT] = {15000.0, 15000.0, 20000.0, 20000.0, 25000.0, 25000.0, 30000.0};\n\n    // Maximum time step length\n    const int MAX_TIME_LENGTH = *std::max_element(timeLength, timeLength + TEST_CASE_COUNT);\n\n    // Predefined time steps\n    float timeSteps_h[TEST_CASE_COUNT][MAX_TIME_LENGTH] = {\n        {0.05, 0.75, 1.45, 2.15, 2.85, 3.55, 4.25, 4.95, 5.65, 6.35},\n        {0.1, 0.7, 1.3, 1.9, 2.5, 3.1, 3.7, 4.3, 4.9, 5.5},\n        {0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5},\n        {0.11, 0.51, 0.91, 1.31, 1.71, 2.11, 2.51, 2.91, 3.31, 3.71, 4.11, 4.51, 4.91, 5.31, 5.71},\n        {0.01, 0.31, 0.61, 0.91, 1.21, 1.51, 1.81, 2.11, 2.41, 2.71, 3.01, 3.31, 3.61, 3.91, 4.21, 4.51, 4.81, 5.11, 5.41, 5.71},\n        {0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9, 2.1, 2.3, 2.5, 2.7, 2.9, 3.1, 3.3, 3.5, 3.7, 3.9, 4.1, 4.3},\n        {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3, 3.1, 3.2, 3.3, 3.4, 3.5}\n    };\n\n    float expectedPosition[TEST_CASE_COUNT][MAX_TIME_LENGTH] = {\n        {0.050, 11.357, 42.449, 93.328, 163.993, 254.444, 364.682, 494.705, 644.515, 814.111},\n        {0.202, 9.893, 34.121, 72.886, 126.188, 194.026, 276.401, 373.313, 484.762, 610.748},\n        {2.547, 10.190, 22.927, 40.760, 63.687, 91.710, 124.827, 163.040, 206.348, 254.750, 308.247, 366.840, 430.527, 499.310, 573.187},\n        {0.123, 2.650, 8.438, 17.487, 29.797, 45.367, 64.198, 86.290, 111.643, 140.256, 172.131, 207.266, 245.662, 287.318, 332.236},\n        {0.001, 0.659, 2.551, 5.678, 10.039, 15.634, 22.463, 30.527, 39.824, 50.356, 62.122, 75.122, 89.357, 104.825, 121.528, 139.465, 158.637, 179.042, 200.682, 223.555},\n        {1.714, 3.360, 5.554, 8.297, 11.588, 15.427, 19.816, 24.753, 30.238, 36.272, 42.854, 49.985, 57.665, 65.893, 74.669, 83.994, 93.868, 104.290, 115.261, 126.780},\n        {0.052, 0.208, 0.467, 0.830, 1.297, 1.868, 2.543, 3.322, 4.204, 5.190, 6.280, 7.474, 8.771, 10.172, 11.677, 13.286, 14.999, 16.816, 18.736, 20.760, 22.888, 25.120, 27.455, 29.894, 32.437, 35.084, 37.835, 40.690, 43.648, 46.710, 49.876, 53.146, 56.519, 59.996, 63.577}\n    };\n\n    float expectedVelocity[TEST_CASE_COUNT][MAX_TIME_LENGTH] = {\n        {1.010, 15.142, 29.276, 43.409, 57.542, 71.674, 85.808, 99.940, 114.074, 128.206},\n        {2.019, 14.133, 26.247, 38.361, 50.475, 62.589, 74.703, 86.817, 98.931, 111.045},\n        {5.095, 10.190, 15.285, 20.380, 25.475, 30.570, 35.665, 40.760, 45.855, 50.950, 56.045, 61.140, 66.235, 71.330, 76.425},\n        {1.121, 5.197, 9.273, 13.349, 17.425, 21.501, 25.577, 29.653, 33.729, 37.805, 41.881, 45.957, 50.033, 54.109, 58.185},\n        {0.069, 2.126, 4.183, 6.240, 8.297, 10.354, 12.411, 14.468, 16.525, 18.582, 20.639, 22.696, 24.753, 26.810, 28.867, 30.924, 32.981, 35.038, 37.095, 39.152},\n        {3.428, 4.800, 6.171, 7.542, 8.914, 10.285, 11.656, 13.028, 14.399, 15.770, 17.142, 18.513, 19.884, 21.256, 22.627, 23.998, 25.370, 26.741, 28.112, 29.484},\n        {0.519, 1.038, 1.557, 2.076, 2.595, 3.114, 3.633, 4.152, 4.671, 5.190, 5.709, 6.228, 6.747, 7.266, 7.785, 8.304, 8.823, 9.342, 9.861, 10.380, 10.899, 11.418, 11.937, 12.456, 12.975, 13.494, 14.013, 14.532, 15.051, 15.570, 16.089, 16.608, 17.127, 17.646, 18.165}\n    };\n\n    float expectedMass[TEST_CASE_COUNT][MAX_TIME_LENGTH] = {\n        {499.625, 494.375, 489.125, 483.875, 478.625, 473.375, 468.125, 462.875, 457.625, 452.375},\n        {499.250, 494.750, 490.250, 485.750, 481.250, 476.750, 472.250, 467.750, 463.250, 458.750},\n        {996.000, 992.000, 988.000, 984.000, 980.000, 976.000, 972.000, 968.000, 964.000, 960.000, 956.000, 952.000, 948.000, 944.000, 940.000},\n        {999.120, 995.920, 992.720, 989.520, 986.320, 983.120, 979.920, 976.720, 973.520, 970.320, 967.120, 963.920, 960.720, 957.520, 954.320},\n        {1499.917, 1497.417, 1494.917, 1492.417, 1489.917, 1487.417, 1484.917, 1482.417, 1479.917, 1477.417, 1474.917, 1472.417, 1469.917, 1467.417, 1464.917, 1462.417, 1459.917, 1457.417, 1454.917, 1452.417},\n        {1495.833, 1494.167, 1492.500, 1490.833, 1489.167, 1487.500, 1485.833, 1484.167, 1482.500, 1480.833, 1479.167, 1477.500, 1475.833, 1474.167, 1472.500, 1470.833, 1469.167, 1467.500, 1465.833, 1464.167},\n        {1999.143, 1998.286, 1997.429, 1996.571, 1995.714, 1994.857, 1994.000, 1993.143, 1992.286, 1991.429, 1990.571, 1989.714, 1988.857, 1988.000, 1987.143, 1986.286, 1985.429, 1984.571, 1983.714, 1982.857, 1982.000, 1981.143, 1980.286, 1979.429, 1978.571, 1977.714, 1976.857, 1976.000, 1975.143, 1974.286, 1973.429, 1972.571, 1971.714, 1970.857, 1970.000}\n    };\n\n    // Host memory allocation\n    float* position_h = (float*) malloc(MAX_TIME_LENGTH * sizeof(float));\n    float* velocity_h = (float*) malloc(MAX_TIME_LENGTH * sizeof(float));\n    float* mass_h = (float*) malloc(MAX_TIME_LENGTH * sizeof(float));\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Device memory allocation\n    float* position_d, * velocity_d, * mass_d, * timeSteps_d;\n    CUDA_CHECK(cudaMallocAsync((void**)&position_d, MAX_TIME_LENGTH * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&velocity_d, MAX_TIME_LENGTH * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&mass_d, MAX_TIME_LENGTH * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&timeSteps_d, MAX_TIME_LENGTH * sizeof(float), stream));\n\n    // Array for attributes\n    cudaLaunchAttribute attributes[NUM_ATTRIBUTES];\n    // Use streaming for position_d\n    attributes[0].id = cudaLaunchAttributeAccessPolicyWindow;\n    attributes[0].val.accessPolicyWindow.base_ptr  = position_d;\n    attributes[0].val.accessPolicyWindow.num_bytes = MAX_TIME_LENGTH * sizeof(float);\n    attributes[0].val.accessPolicyWindow.hitRatio  = 0.0f;  \n    attributes[0].val.accessPolicyWindow.hitProp   = cudaAccessPropertyStreaming;\n    attributes[0].val.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;\n\n    // Use streaming for velocity_d\n    attributes[1].id = cudaLaunchAttributeAccessPolicyWindow;\n    attributes[1].val.accessPolicyWindow.base_ptr  = velocity_d;\n    attributes[1].val.accessPolicyWindow.num_bytes = MAX_TIME_LENGTH * sizeof(float);\n    attributes[1].val.accessPolicyWindow.hitRatio  = 0.0f;  \n    attributes[1].val.accessPolicyWindow.hitProp   = cudaAccessPropertyStreaming;\n    attributes[1].val.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;\n\n    // Use streaming for mass_d\n    attributes[2].id = cudaLaunchAttributeAccessPolicyWindow;\n    attributes[2].val.accessPolicyWindow.base_ptr  = mass_d;\n    attributes[2].val.accessPolicyWindow.num_bytes = MAX_TIME_LENGTH * sizeof(float);\n    attributes[2].val.accessPolicyWindow.hitRatio  = 0.0f;  \n    attributes[2].val.accessPolicyWindow.hitProp   = cudaAccessPropertyStreaming;\n    attributes[2].val.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;\n\n    // Get the optimal block size for k_computeTrajectory\n    int gridSize, blockSize;\n    cudaOccupancyMaxPotentialBlockSize(&gridSize, &blockSize, k_computeTrajectory, 0, MAX_BLOCK_SIZE);\n\n    // Setting up kernel launch configurations\n    cudaLaunchConfig_t config = {};\n    config.numAttrs     = 3;  \n    config.gridDim = gridSize;\n    config.blockDim = blockSize;\n    config.attrs = attributes;\n    config.stream = stream;\n    \n    unsigned iTestCase = 0;\n    //Test Case 1\n    {\n        // Initialize values on host\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            position_h[i] = 0.0f;\n            velocity_h[i] = 0.0f;\n            mass_h[i] = initialMass[iTestCase];\n        }\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(position_d, position_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_d, velocity_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(timeSteps_d, timeSteps_h[iTestCase], timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        void *args[] = {&position_d, &velocity_d, &mass_d, &timeSteps_d, (void*)&thrust[iTestCase], (void*)&exhaustVelocity[iTestCase], (void*)&timeLength[iTestCase]};\n        CUDA_CHECK(cudaLaunchKernelExC(&config, reinterpret_cast<const void*>(k_computeTrajectory), args));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(position_h, position_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_h, velocity_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_h, mass_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            assert(abs(position_h[i] - expectedPosition[iTestCase][i]) < TOLERANCE);\n            assert(abs(velocity_h[i] - expectedVelocity[iTestCase][i]) < TOLERANCE);\n            assert(abs(mass_h[i] - expectedMass[iTestCase][i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 2\n    {\n        iTestCase++;\n\n        // Initialize values on host\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            position_h[i] = 0.0f;\n            velocity_h[i] = 0.0f;\n            mass_h[i] = initialMass[iTestCase];\n        }\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(position_d, position_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_d, velocity_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(timeSteps_d, timeSteps_h[iTestCase], timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        void *args[] = {&position_d, &velocity_d, &mass_d, &timeSteps_d, (void*)&thrust[iTestCase], (void*)&exhaustVelocity[iTestCase], (void*)&timeLength[iTestCase]};\n        CUDA_CHECK(cudaLaunchKernelExC(&config, reinterpret_cast<const void*>(k_computeTrajectory), args));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(position_h, position_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_h, velocity_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_h, mass_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            assert(abs(position_h[i] - expectedPosition[iTestCase][i]) < TOLERANCE);\n            assert(abs(velocity_h[i] - expectedVelocity[iTestCase][i]) < TOLERANCE);\n            assert(abs(mass_h[i] - expectedMass[iTestCase][i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 3\n    {\n        iTestCase++;\n\n        // Initialize values on host\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            position_h[i] = 0.0f;\n            velocity_h[i] = 0.0f;\n            mass_h[i] = initialMass[iTestCase];\n        }\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(position_d, position_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_d, velocity_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(timeSteps_d, timeSteps_h[iTestCase], timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        void *args[] = {&position_d, &velocity_d, &mass_d, &timeSteps_d, (void*)&thrust[iTestCase], (void*)&exhaustVelocity[iTestCase], (void*)&timeLength[iTestCase]};\n        CUDA_CHECK(cudaLaunchKernelExC(&config, reinterpret_cast<const void*>(k_computeTrajectory), args));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(position_h, position_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_h, velocity_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_h, mass_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            assert(abs(position_h[i] - expectedPosition[iTestCase][i]) < TOLERANCE);\n            assert(abs(velocity_h[i] - expectedVelocity[iTestCase][i]) < TOLERANCE);\n            assert(abs(mass_h[i] - expectedMass[iTestCase][i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 4\n    {\n        iTestCase++;\n\n        // Initialize values on host\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            position_h[i] = 0.0f;\n            velocity_h[i] = 0.0f;\n            mass_h[i] = initialMass[iTestCase];\n        }\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(position_d, position_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_d, velocity_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(timeSteps_d, timeSteps_h[iTestCase], timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        void *args[] = {&position_d, &velocity_d, &mass_d, &timeSteps_d, (void*)&thrust[iTestCase], (void*)&exhaustVelocity[iTestCase], (void*)&timeLength[iTestCase]};\n        CUDA_CHECK(cudaLaunchKernelExC(&config, reinterpret_cast<const void*>(k_computeTrajectory), args));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(position_h, position_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_h, velocity_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_h, mass_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            assert(abs(position_h[i] - expectedPosition[iTestCase][i]) < TOLERANCE);\n            assert(abs(velocity_h[i] - expectedVelocity[iTestCase][i]) < TOLERANCE);\n            assert(abs(mass_h[i] - expectedMass[iTestCase][i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 5\n    {\n        iTestCase++;\n\n        // Initialize values on host\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            position_h[i] = 0.0f;\n            velocity_h[i] = 0.0f;\n            mass_h[i] = initialMass[iTestCase];\n        }\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(position_d, position_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_d, velocity_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(timeSteps_d, timeSteps_h[iTestCase], timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        void *args[] = {&position_d, &velocity_d, &mass_d, &timeSteps_d, (void*)&thrust[iTestCase], (void*)&exhaustVelocity[iTestCase], (void*)&timeLength[iTestCase]};\n        CUDA_CHECK(cudaLaunchKernelExC(&config, reinterpret_cast<const void*>(k_computeTrajectory), args));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(position_h, position_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_h, velocity_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_h, mass_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            assert(abs(position_h[i] - expectedPosition[iTestCase][i]) < TOLERANCE);\n            assert(abs(velocity_h[i] - expectedVelocity[iTestCase][i]) < TOLERANCE);\n            assert(abs(mass_h[i] - expectedMass[iTestCase][i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 6\n    {\n        iTestCase++;\n\n        // Initialize values on host\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            position_h[i] = 0.0f;\n            velocity_h[i] = 0.0f;\n            mass_h[i] = initialMass[iTestCase];\n        }\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(position_d, position_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_d, velocity_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(timeSteps_d, timeSteps_h[iTestCase], timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        void *args[] = {&position_d, &velocity_d, &mass_d, &timeSteps_d, (void*)&thrust[iTestCase], (void*)&exhaustVelocity[iTestCase], (void*)&timeLength[iTestCase]};\n        CUDA_CHECK(cudaLaunchKernelExC(&config, reinterpret_cast<const void*>(k_computeTrajectory), args));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(position_h, position_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_h, velocity_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_h, mass_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            assert(abs(position_h[i] - expectedPosition[iTestCase][i]) < TOLERANCE);\n            assert(abs(velocity_h[i] - expectedVelocity[iTestCase][i]) < TOLERANCE);\n            assert(abs(mass_h[i] - expectedMass[iTestCase][i]) < TOLERANCE);\n        }\n    }\n\n    //Test Case 7\n    {\n        iTestCase++;\n\n        // Initialize values on host\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            position_h[i] = 0.0f;\n            velocity_h[i] = 0.0f;\n            mass_h[i] = initialMass[iTestCase];\n        }\n\n        // Copy data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(position_d, position_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_d, velocity_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_d, mass_h, timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(timeSteps_d, timeSteps_h[iTestCase], timeLength[iTestCase] * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        void *args[] = {&position_d, &velocity_d, &mass_d, &timeSteps_d, (void*)&thrust[iTestCase], (void*)&exhaustVelocity[iTestCase], (void*)&timeLength[iTestCase]};\n        CUDA_CHECK(cudaLaunchKernelExC(&config, reinterpret_cast<const void*>(k_computeTrajectory), args));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(position_h, position_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocity_h, velocity_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(mass_h, mass_d, timeLength[iTestCase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        for (int i = 0; i < timeLength[iTestCase]; i++) {\n            assert(abs(position_h[i] - expectedPosition[iTestCase][i]) < TOLERANCE);\n            assert(abs(velocity_h[i] - expectedVelocity[iTestCase][i]) < TOLERANCE);\n            assert(abs(mass_h[i] - expectedMass[iTestCase][i]) < TOLERANCE);\n        }\n    }\n    // Free device memory\n    CUDA_CHECK(cudaFree(position_d));\n    CUDA_CHECK(cudaFree(velocity_d));\n    CUDA_CHECK(cudaFree(mass_d));\n    CUDA_CHECK(cudaFree(timeSteps_d));\n\n    // Free host memory\n    delete[] position_h;\n    delete[] velocity_h;\n    delete[] mass_h;\n}\n\n__global__ void k_computeTrajectory(float* position_d, float* velocity_d, float* mass_d, float* timeSteps_d, float thrust, float exhaustVelocity, int totalSteps) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/135", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform matrix multiplication by using tensor cores by using mma ptx instruction of dimension m16n8k16 for Ampere Architecture.\nConfigure it in row major by column major format, using uint8_t data type for input matrices A (row major) and B (column major), and s32 for accumulator matrix C(row major), all input\nmatrices will have m16n8k16 compatible dimensions.\n\n\n\n>>> k_mmaTensorMatMulM16N8k16Int8({1, 2, 3, 4}, {2, 4}, outputMatrix_d, 2, 1, 2)-> outputMatrix_d: ({10, 23})\n>>> k_mmaTensorMatMulM16N8k16Int8({3, 6, 17, 15}, {13, 15}, outputMatrix_d, 2, 1, 2)-> outputMatrix_d: ({294, 303})\n", "cc_flags": "-arch=sm_80", "ld_flags": "", "declaration": "\n#include <cstdio>\n#include <cassert>\n#include <cstdint>\n#include <random>\n\n#include <cuda_runtime.h>\n#include <mma.h>\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n#undef NDEBUG\n\n#define MMA_M 16\n#define MMA_N 8\n#define MMA_K 16\n\n__global__ void k_mmaTensorMatMulM16N8k16Int8(uint8_t *inputMatrixA_d, uint8_t *inputMatrixB_d, int *outputMatrix_d, int mDim, int nDim, int kDim);\n\n// Function to compute valid reference result\n\nvoid cpuMatMulReference(const uint8_t* A,\n                        const uint8_t* B,\n                        int* cpuRefC,\n                        int M,\n                        int N,\n                        int K) {\n    for (int i = 0; i < M; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = 0; k < K; k++) {\n                int a_val = static_cast<int>(A[i * K + k]);\n                int b_val = static_cast<int>(B[k * N + j]);\n                sum += a_val * b_val;\n            }\n            cpuRefC[i * N + j] = sum;\n        }\n    }\n}\n\nvoid launch() {\n    const int TEST_CASE_COUNT = 7;\n    //Test case dimensions {M, N, K}\n    const int TEST_CASES_DIMS[TEST_CASE_COUNT][3] = {{16,8,16}, {512,512,512}, {32,16,32}, {256, 256, 256}, {64, 64, 64} , {64, 32, 32}, {128, 128, 128}};\n\n    const int BLOCK_SIZE = 256;\n\n    //Set up random number generation\n    std::random_device randomSeedSource; // Automatically configure high quality seed using system info\n    std::mt19937 randEngine(randomSeedSource());\n\n    // Bounded random distribution for test case initialization\n    std::uniform_real_distribution<float> randDist(1.0f, 255.0f);\n\n    for (int i = 0; i < TEST_CASE_COUNT; i++) {\n        // Dimensions of the input and output layers\n        int M = TEST_CASES_DIMS[i][0]; //Number of Rows in Matrix A\n        int N = TEST_CASES_DIMS[i][1]; //Number of Columns in Matrix B\n        int K = TEST_CASES_DIMS[i][2]; //Number of Columns in Matrix A and Rows in Matrix B\n\n        //Pointers for Host Memory\n        uint8_t* A_h =(uint8_t*)malloc(M * K * sizeof(uint8_t));\n        uint8_t* B_h =(uint8_t*)malloc(K * N * sizeof(uint8_t));\n\n        int* cpuC_h =(int*)malloc(M * N * sizeof(int)); // Reference Matrix space allocation on host\n        int* gpuC_h = (int*)malloc(M * N * sizeof(int));// GPU result Matrix space allocation on host\n\n        //Pointers for device memory (GPU)\n        uint8_t* A_d;\n        uint8_t* B_d;\n        int* C_d;\n\n        //Populating input matrices with random values\n        for (int i = 0; i < M * K; i++) {\n            uint32_t val = randDist(randEngine);\n            A_h[i] = uint8_t(val);\n        }\n\n        for (int i = 0; i < K * N; i++) {\n            uint32_t val = randDist(randEngine);\n            B_h[i] = uint8_t(val);\n        }\n\n        // Use a CUDA stream for asynchronous operations\n        cudaStream_t stream;\n        CUDA_CHECK(cudaStreamCreate(&stream));\n\n        // Allocate the memory on the device\n        CUDA_CHECK(cudaMallocAsync(&A_d, M * K * sizeof(uint8_t), stream));\n        CUDA_CHECK(cudaMallocAsync(&B_d, K * N * sizeof(uint8_t), stream));\n        CUDA_CHECK(cudaMallocAsync(&C_d, M * N * sizeof(int), stream));\n\n        //Load Test Cases\n        CUDA_CHECK(cudaMemcpyAsync(A_d, A_h, M * K * sizeof(uint8_t), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(B_d, B_h, K * N * sizeof(uint8_t), cudaMemcpyHostToDevice, stream));\n\n        // Initialize the result on the device\n        CUDA_CHECK(cudaMemsetAsync(C_d, 0, M * N * sizeof(int), stream));\n\n        //Check if the dimensions are divisible by the block tile dimensions\n        assert(M % MMA_M == 0);\n        assert(N % MMA_N == 0);\n        assert(K % MMA_K == 0);\n\n        dim3 gridDim((N + MMA_N - 1) / MMA_N, (M + MMA_M - 1) / MMA_M);\n        dim3 blockDim(BLOCK_SIZE);\n        int shmemBytes = (MMA_M * MMA_K + MMA_K * MMA_N) * sizeof(uint8_t);\n\n        // Launch kernel\n        // Grid: ((N + MMA_N - 1/ MMA_N), (M + MMA_M - 1)/ MMA_M, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&A_d,\n                        &B_d,\n                        &C_d,\n                        (void*)&M,\n                        (void*)&N,\n                        (void*)&K};\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_mmaTensorMatMulM16N8k16Int8,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    shmemBytes,\n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        cpuMatMulReference(A_h, B_h, cpuC_h, M, N, K);\n\n        //Copying the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync(gpuC_h, C_d, M * N * sizeof(int), cudaMemcpyDeviceToHost, stream));\n\n        //Validate the result\n        for(int t = 0; t < M*N; ++t) {\n            assert(gpuC_h[t] == cpuC_h[t]);\n        }\n\n        //Free up resources\n        CUDA_CHECK(cudaFreeAsync(A_d, stream));\n        CUDA_CHECK(cudaFreeAsync(B_d, stream));\n        CUDA_CHECK(cudaFreeAsync(C_d, stream));\n        CUDA_CHECK(cudaStreamDestroy(stream));\n        free(A_h);\n        free(B_h);\n        free(cpuC_h);\n        free(gpuC_h);\n    }\n}\n\n__device__ __forceinline__ void manual_load_fragment_A(const uint8_t* src, uint32_t frag[2], int stride) {\n    uint32_t reg0 = uint32_t(src[0]) | (uint32_t(src[1]) << 8) | (uint32_t(src[2]) << 16) | (uint32_t(src[3]) << 24);\n    uint32_t reg1 = uint32_t(src[8*stride]) | (uint32_t(src[8*stride + 1]) << 8) |\n                    (uint32_t(src[8*stride + 2]) << 16) | (uint32_t(src[8*stride + 3]) << 24);\n    frag[0] = reg0;\n    frag[1] = reg1;\n}\n\n__device__ __forceinline__ uint32_t manual_load_fragment_B(const uint8_t* src) {\n    int lane = threadIdx.x % 32;\n    uint8_t v0 = src[0 + lane/4];\n    uint8_t v1 = src[8 + lane/4];\n    uint8_t v2 = src[16 + lane/4];\n    uint8_t v3 = src[24 + lane/4];\n    uint32_t frag = (uint32_t)v0 | ((uint32_t)v1 << 8) | ((uint32_t)v2 << 16) | ((uint32_t)v3 << 24);\n    return frag;\n}\n\n// Kernel: Multiply uint8_t matrices inputMatrixA_d (mDimx kDim) and inputMatrixB_d (kDim x nDim) to produce an output outputMatrix_d (mDim x nDim) in f32.\n// Each block (a single warp) computes one 168 output tile using MMA instructions.\n__global__ void k_mmaTensorMatMulM16N8k16Int8(uint8_t *inputMatrixA_d,\n                                              uint8_t *inputMatrixB_d,\n                                              int *outputMatrix_d,\n                                              int mDim,\n                                              int nDim,\n                                              int kDim) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/136", "date": "2025-07-30", "prompt": "Write a CUDA kernel to calculate the dot product between a common vector and many other vectors. Assume the vectors are transposed and use shared memory for the common vector and wider element types to load other vectors to maximize memory throughput.\n\nThe signature of the CUDA kernel is __global__ void k_runDotProducts(const float *const __restrict__ commonVector_d, const float *const __restrict__ otherVectors_d, float *const __restrict__ result_d, int numVectors, int numElementsPerVector), where commonVector_d is a pointer to the common vector to be used in every dot-product operation, otherVectors_d is a pointer to the transposed vectors to be used in dot-products with commonVector_c vector, result_d is pointer to the output vector, numVectors is the number of vectors to be used as second vector in dot-product with the common vector, numElementsPerVector is the number of elements for each vector.\n\n>>> k_runDotProducts({ 1.0f, 1.0f, 1.0f },\n{ \n    1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, \n    1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, \n    1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f\n}, result_d) -> result_d: { 3.0f, 3.0f, 3.0f, 3.0f, 3.0f, 3.0f, 3.0f, 3.0f }\n>>> k_runDotProducts({ 1.0f, 1.0f, 1.0f }, \n{ \n    0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, \n    1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, \n    2.0f, 2.0f, 2.0f, 2.0f, 2.0f, 2.0f, 2.0f, 2.0f\n}, result_d) -> result_d: { 3.0f, 3.0f, 3.0f, 3.0f, 3.0f, 3.0f, 3.0f, 3.0f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <cstdio>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                                                                                 \\\ndo {                                                                                                     \\\n        cudaError_t error = call;                                                                        \\\n        if (error != cudaSuccess) {                                                                      \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\", __FILE__, __LINE__, cudaGetErrorString(error));\\\n            exit(EXIT_FAILURE);                                                                          \\\n        }                                                                                                \\\n} while (0)\n\n// Algorithm-related constants.\nconstexpr int MAXIMUM_NUMBER_OF_VECTORS = 10000;\nconstexpr int MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR = 1000;\nconstexpr int MAXIMUM_TOTAL_ELEMENTS_IN_OTHER_VECTORS = MAXIMUM_NUMBER_OF_VECTORS * MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR;\nconstexpr int ALLOCATION_SIZE = MAXIMUM_TOTAL_ELEMENTS_IN_OTHER_VECTORS * sizeof(float);\nconstexpr float SMOOTHING_FOR_DIVIDE_BY_ZERO = 1e-10f;\nconstexpr float RELATIVE_ERROR = 1e-4f;\n\n// CUDA-related constants.\nconstexpr int NUM_THREADS_PER_BLOCK = 128;\nconstexpr int FLOAT4_ELEMENTS = 4;\nconstexpr int FLOAT2_ELEMENTS = 2;\n\n__global__ void k_runDotProducts(const float *const __restrict__ commonVector_d, const float *const __restrict__ otherVectors_d, float *const __restrict__ result_d, int numVectors, int numElementsPerVector);\n\nvoid launch() {\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n    int maxBlocks = (prop.maxThreadsPerMultiProcessor / NUM_THREADS_PER_BLOCK) * prop.multiProcessorCount;\n\n    float *commonVector_h = new float[MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR];\n    float *otherVectors_h = new float[MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR * MAXIMUM_NUMBER_OF_VECTORS];\n    float *result_h = new float[MAXIMUM_NUMBER_OF_VECTORS];\n    float *otherVectors_d;\n    float *commonVector_d;\n    float *result_d;\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    CUDA_CHECK(cudaMallocAsync(&otherVectors_d, ALLOCATION_SIZE, stream));\n    CUDA_CHECK(cudaMallocAsync(&commonVector_d, MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&result_d, MAXIMUM_NUMBER_OF_VECTORS * sizeof(float), stream));\n\n    auto hToD = cudaMemcpyHostToDevice;\n    auto dToH = cudaMemcpyDeviceToHost;\n\n    // Test 1: All elements are 1.0f.\n    {\n        int numVectors = 8;\n        int numElementsPerVector = 3;\n        assert(numVectors <= MAXIMUM_NUMBER_OF_VECTORS);\n        assert(numElementsPerVector <= MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR);\n        int numElementsPerWideAccess = (numVectors % FLOAT4_ELEMENTS == 0) ? FLOAT4_ELEMENTS : (numVectors % FLOAT2_ELEMENTS == 0 ? FLOAT2_ELEMENTS : 1);\n        int numRequiredBlocks = 1 + (numVectors - 1) / (NUM_THREADS_PER_BLOCK * numElementsPerWideAccess);\n        int usedBlocks = min(maxBlocks, numRequiredBlocks);\n        dim3 gridDim(usedBlocks, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        void * args[5] = { (void*)&commonVector_d, (void*)&otherVectors_d, (void*)&result_d, &numVectors, &numElementsPerVector };\n\n        for(int i = 0; i < numElementsPerVector; i++) {\n            commonVector_h[i] = 1.0f; \n        }\n\n        for(int i = 0; i < numElementsPerVector; i++) {\n            for(int j = 0; j < numVectors; j++) {\n                otherVectors_h[j + i * numVectors] = 1.0f;\n            }   \n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( commonVector_d, \n                                    commonVector_h, \n                                    numElementsPerVector * sizeof(float), \n                                    hToD, \n                                    stream));\n\n        CUDA_CHECK(cudaMemcpyAsync( otherVectors_d, \n                                    otherVectors_h, \n                                    ALLOCATION_SIZE, \n                                    hToD, \n                                    stream));\n        // Block: (128, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_runDotProducts, gridDim, blockDim, args, numElementsPerVector * sizeof(float), stream));\n        CUDA_CHECK(cudaMemcpyAsync(result_h, result_d, numVectors * sizeof(float), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int j = 0; j < numVectors; j++) {\n            float result = 0.0f;\n\n            for(int i = 0; i < numElementsPerVector; i++) {\n                result += otherVectors_h[j + i * numVectors] * commonVector_h[i];\n            }\n\n            assert(fabs(result - result_h[j]) / fabs(result) < RELATIVE_ERROR);\n        } \n    }\n\n    // Test 2: The common vector has all elements equal to 1.0f, while other vectors contain values that correspond to their element indices.\n    {\n        int numVectors = 8;\n        int numElementsPerVector = 3;\n        assert(numVectors <= MAXIMUM_NUMBER_OF_VECTORS);\n        assert(numElementsPerVector <= MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR);\n        int numElementsPerWideAccess = (numVectors % FLOAT4_ELEMENTS == 0) ? FLOAT4_ELEMENTS : (numVectors % FLOAT2_ELEMENTS == 0 ? FLOAT2_ELEMENTS : 1);\n        int numRequiredBlocks = 1 + (numVectors - 1) / (NUM_THREADS_PER_BLOCK * numElementsPerWideAccess);\n        int usedBlocks = min(maxBlocks, numRequiredBlocks);\n        dim3 gridDim(usedBlocks, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        void * args[5] = { (void*)&commonVector_d, (void*)&otherVectors_d, (void*)&result_d, &numVectors, &numElementsPerVector };\n\n        for(int i = 0; i < numElementsPerVector; i++) {\n            commonVector_h[i] = 1.0f; \n        }\n\n        for(int i = 0; i < numElementsPerVector; i++) {\n            for(int j = 0; j < numVectors; j++) {\n                otherVectors_h[j + i * numVectors] = i;\n            }   \n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( commonVector_d, \n                                    commonVector_h, \n                                    numElementsPerVector * sizeof(float), \n                                    hToD, \n                                    stream));\n\n        CUDA_CHECK(cudaMemcpyAsync( otherVectors_d, \n                                    otherVectors_h, \n                                    ALLOCATION_SIZE, \n                                    hToD, \n                                    stream));\n\n        // Block: (128, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_runDotProducts, gridDim, blockDim, args, numElementsPerVector * sizeof(float), stream));\n        CUDA_CHECK(cudaMemcpyAsync(result_h, result_d, numVectors * sizeof(float), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int j = 0; j < numVectors; j++) {\n            float result = 0.0f;\n\n            for(int i = 0; i < numElementsPerVector; i++) {\n                result += otherVectors_h[j + i * numVectors] * commonVector_h[i];\n            }\n\n            assert(fabs(result - result_h[j]) / fabs(result) < RELATIVE_ERROR);\n        } \n    }\n\n    // Test 3: The common vector has element values equal to their indices, while other vectors have values of 1.0.\n    {\n        int numVectors = 800;\n        int numElementsPerVector = 300;\n        assert(numVectors <= MAXIMUM_NUMBER_OF_VECTORS);\n        assert(numElementsPerVector <= MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR);\n        int numElementsPerWideAccess = (numVectors % FLOAT4_ELEMENTS == 0) ? FLOAT4_ELEMENTS : (numVectors % FLOAT2_ELEMENTS == 0 ? FLOAT2_ELEMENTS : 1);\n        int numRequiredBlocks = 1 + (numVectors - 1) / (NUM_THREADS_PER_BLOCK * numElementsPerWideAccess);\n        int usedBlocks = min(maxBlocks, numRequiredBlocks);\n        dim3 gridDim(usedBlocks, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        void * args[5] = { (void*)&commonVector_d, (void*)&otherVectors_d, (void*)&result_d, &numVectors, &numElementsPerVector };\n\n        for(int i = 0; i < numElementsPerVector; i++) {\n            commonVector_h[i] = i; \n        }\n\n        for(int i = 0; i < numElementsPerVector; i++) {\n            for(int j = 0; j < numVectors; j++) {\n                otherVectors_h[j + i * numVectors] = 1.0f;\n            }   \n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( commonVector_d, \n                                    commonVector_h, \n                                    numElementsPerVector * sizeof(float), \n                                    hToD, \n                                    stream));\n\n        CUDA_CHECK(cudaMemcpyAsync( otherVectors_d, \n                                    otherVectors_h, \n                                    ALLOCATION_SIZE, \n                                    hToD, \n                                    stream));\n\n        // Block: (128, 1, 1)\n        // Grid: (2, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_runDotProducts, gridDim, blockDim, args, numElementsPerVector * sizeof(float), stream));\n        CUDA_CHECK(cudaMemcpyAsync(result_h, result_d, numVectors * sizeof(float), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int j = 0; j < numVectors; j++) {\n            float result = 0.0f;\n            for(int i = 0; i < numElementsPerVector; i++) {\n                result += otherVectors_h[j + i * numVectors] * commonVector_h[i];\n            }\n\n            assert(fabs(result - result_h[j]) / fabs(result) < RELATIVE_ERROR);\n        } \n    }\n\n    // Test 4: All element values equal their indices, and each vector has an offset of j * numElementsPerVector in element values.\n    {\n        int numVectors = 10000;\n        int numElementsPerVector = 1000;\n        assert(numVectors <= MAXIMUM_NUMBER_OF_VECTORS);\n        assert(numElementsPerVector <= MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR);\n        int numElementsPerWideAccess = (numVectors % FLOAT4_ELEMENTS == 0) ? FLOAT4_ELEMENTS : (numVectors % FLOAT2_ELEMENTS == 0 ? FLOAT2_ELEMENTS : 1);\n        int numRequiredBlocks = 1 + (numVectors - 1) / (NUM_THREADS_PER_BLOCK * numElementsPerWideAccess);\n        int usedBlocks = min(maxBlocks, numRequiredBlocks);\n        dim3 gridDim(usedBlocks, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        void * args[5] = { (void*)&commonVector_d, (void*)&otherVectors_d, (void*)&result_d, &numVectors, &numElementsPerVector };\n        \n        for(int i = 0; i < numElementsPerVector; i++) {\n            commonVector_h[i] = i; \n        }\n        \n        for(int i = 0; i < numElementsPerVector; i++) {\n            for(int j = 0; j < numVectors; j++) {\n                otherVectors_h[j + i * numVectors] = i + j * numElementsPerVector;\n            }   \n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( commonVector_d, \n                                    commonVector_h, \n                                    numElementsPerVector * sizeof(float), \n                                    hToD, \n                                    stream));\n\n        CUDA_CHECK(cudaMemcpyAsync( otherVectors_d, \n                                    otherVectors_h, \n                                    ALLOCATION_SIZE, \n                                    hToD, \n                                    stream));\n\n        // Block: (128, 1, 1)\n        // Grid: (20, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_runDotProducts, gridDim, blockDim, args, numElementsPerVector * sizeof(float), stream));\n        CUDA_CHECK(cudaMemcpyAsync(result_h, result_d, numVectors * sizeof(float), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int j = 0; j < numVectors; j++) {\n            float result = 0.0f;\n            for(int i = 0; i < numElementsPerVector; i++) {\n                result += otherVectors_h[j + i * numVectors] * commonVector_h[i];\n            }\n\n            assert(fabs(result - result_h[j]) / fabs(result) < RELATIVE_ERROR);\n        } \n    }\n\n    // Test 5: Only one element in the common vector is non-zero.\n    {\n        int numVectors = 100;\n        int numElementsPerVector = 100;\n        assert(numVectors <= MAXIMUM_NUMBER_OF_VECTORS);\n        assert(numElementsPerVector <= MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR);\n        int numElementsPerWideAccess = (numVectors % FLOAT4_ELEMENTS == 0) ? FLOAT4_ELEMENTS : (numVectors % FLOAT2_ELEMENTS == 0 ? FLOAT2_ELEMENTS : 1);\n        int numRequiredBlocks = 1 + (numVectors - 1) / (NUM_THREADS_PER_BLOCK * numElementsPerWideAccess);\n        int usedBlocks = min(maxBlocks, numRequiredBlocks);\n        dim3 gridDim(usedBlocks, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        void * args[5] = { (void*)&commonVector_d, (void*)&otherVectors_d, (void*)&result_d, &numVectors, &numElementsPerVector };\n        \n        for(int i = 0; i < numElementsPerVector; i++) {\n            commonVector_h[i] = (i == numElementsPerVector / 2 ? 1.0f : 0.0f); \n        }\n        \n        for(int i = 0; i < numElementsPerVector; i++) {\n            for(int j = 0; j < numVectors; j++) {\n                otherVectors_h[j + i * numVectors] = i + j * numElementsPerVector;\n            }   \n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( commonVector_d, \n                                    commonVector_h, \n                                    numElementsPerVector * sizeof(float), \n                                    hToD, \n                                    stream));\n\n        CUDA_CHECK(cudaMemcpyAsync( otherVectors_d, \n                                    otherVectors_h, \n                                    ALLOCATION_SIZE, \n                                    hToD, \n                                    stream));\n        // Block: (128, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_runDotProducts, gridDim, blockDim, args, numElementsPerVector * sizeof(float), stream));\n        CUDA_CHECK(cudaMemcpyAsync(result_h, result_d, numVectors * sizeof(float), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int j = 0; j < numVectors; j++) {\n            float result = 0.0f;\n            for(int i = 0; i < numElementsPerVector; i++) {\n                result += otherVectors_h[j + i * numVectors] * commonVector_h[i];\n            }\n\n            assert(fabs(result - result_h[j]) / fabs(result + SMOOTHING_FOR_DIVIDE_BY_ZERO) < RELATIVE_ERROR);\n        } \n    }\n\n    // Test 6: The common vector has element values that increase with the index, while other vectors display a diagonal pattern of non-zero elements.\n    {\n        // Also testing the float (scalar) codepath in the kernel.\n        int numVectors = 107;\n        int numElementsPerVector = 107;\n        assert(numVectors <= MAXIMUM_NUMBER_OF_VECTORS);\n        assert(numElementsPerVector <= MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR);\n        int numElementsPerWideAccess = (numVectors % FLOAT4_ELEMENTS == 0) ? FLOAT4_ELEMENTS : (numVectors % FLOAT2_ELEMENTS == 0 ? FLOAT2_ELEMENTS : 1);\n        int numRequiredBlocks = 1 + (numVectors - 1) / (NUM_THREADS_PER_BLOCK * numElementsPerWideAccess);\n        int usedBlocks = min(maxBlocks, numRequiredBlocks);\n        dim3 gridDim(usedBlocks, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        void * args[5] = { (void*)&commonVector_d, (void*)&otherVectors_d, (void*)&result_d, &numVectors, &numElementsPerVector };\n        \n        for(int i = 0; i < numElementsPerVector; i++) {\n            commonVector_h[i] = i; \n        }\n        \n        for(int i = 0; i < numElementsPerVector; i++) {\n            for(int j = 0; j < numVectors; j++) {\n                otherVectors_h[j + i * numVectors] = ((i == j) ? 1.0f : 0.0f);\n            }\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( commonVector_d, \n                                    commonVector_h, \n                                    numElementsPerVector * sizeof(float), \n                                    hToD, \n                                    stream));\n\n        CUDA_CHECK(cudaMemcpyAsync( otherVectors_d, \n                                    otherVectors_h, \n                                    ALLOCATION_SIZE, \n                                    hToD, \n                                    stream));\n        // Block: (128, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_runDotProducts, gridDim, blockDim, args, numElementsPerVector * sizeof(float), stream));\n        CUDA_CHECK(cudaMemcpyAsync(result_h, result_d, numVectors * sizeof(float), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int j = 0; j < numVectors; j++) {\n            float result = 0.0f;\n            for(int i = 0; i < numElementsPerVector; i++) {\n                result += otherVectors_h[j + i * numVectors] * commonVector_h[i];\n            }\n\n            assert(fabs(result - result_h[j]) / fabs(result + SMOOTHING_FOR_DIVIDE_BY_ZERO) < RELATIVE_ERROR);\n        } \n    }\n\n    // Test 7: All elements are zero.\n    {\n        // Also testing the float2 codepath in the kernel.\n        int numVectors = 502;\n        int numElementsPerVector = 256;\n        assert(numVectors <= MAXIMUM_NUMBER_OF_VECTORS);\n        assert(numElementsPerVector <= MAXIMUM_NUMBER_OF_ELEMENTS_PER_VECTOR);\n        int numElementsPerWideAccess = (numVectors % FLOAT4_ELEMENTS == 0) ? FLOAT4_ELEMENTS : (numVectors % FLOAT2_ELEMENTS == 0 ? FLOAT2_ELEMENTS : 1);\n        int numRequiredBlocks = 1 + (numVectors - 1) / (NUM_THREADS_PER_BLOCK * numElementsPerWideAccess);\n        int usedBlocks = min(maxBlocks, numRequiredBlocks);\n        dim3 gridDim(usedBlocks, 1, 1);\n        dim3 blockDim(NUM_THREADS_PER_BLOCK, 1, 1);\n        void * args[5] = { (void*)&commonVector_d, (void*)&otherVectors_d, (void*)&result_d, &numVectors, &numElementsPerVector };\n        \n        for(int i = 0; i < numElementsPerVector; i++) {\n            commonVector_h[i] = 0.0f; \n        }\n\n        for(int i = 0; i < numElementsPerVector; i++) {\n            for(int j = 0; j < numVectors; j++) {\n                otherVectors_h[j + i * numVectors] = 0.0f;\n            }   \n        }\n\n        CUDA_CHECK(cudaMemcpyAsync( commonVector_d, \n                                    commonVector_h, \n                                    numElementsPerVector * sizeof(float), \n                                    hToD, \n                                    stream));\n\n        CUDA_CHECK(cudaMemcpyAsync( otherVectors_d, \n                                    otherVectors_h, \n                                    ALLOCATION_SIZE, \n                                    hToD, \n                                    stream));\n        // Block: (128, 1, 1)\n        // Grid: (2, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_runDotProducts, gridDim, blockDim, args, numElementsPerVector * sizeof(float), stream));\n        CUDA_CHECK(cudaMemcpyAsync(result_h, result_d, numVectors * sizeof(float), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int j = 0; j < numVectors; j++) {\n            assert(fabs(result_h[j]) < RELATIVE_ERROR);\n        } \n    }\n\n    CUDA_CHECK(cudaFreeAsync(otherVectors_d, stream));\n    CUDA_CHECK(cudaFreeAsync(commonVector_d, stream));\n    CUDA_CHECK(cudaFreeAsync(result_d, stream));\n    delete [] commonVector_h;\n    delete [] otherVectors_h;\n    delete [] result_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_runDotProducts(const float *const __restrict__ commonVector_d, const float *const __restrict__ otherVectors_d, float *const __restrict__ result_d, int numVectors, int numElementsPerVector) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/137", "date": "2025-07-30", "prompt": "Develop a CUDA program using the Thrust API to search for a set of elements within an input array and return their corresponding indices. If multiple occurrences of an element exist in the input array,the index of its first occurrence will be returned; if an element is not found, -1 will be returned.\n\nThe signature of the function is \nthrust::device_vector<int> findIndices(const thrust::device_vector<int>& inputArray_d, const thrust::device_vector<int>& testData_d), where inputArray_d represents the source/input array (unsorted), testData_d contains elements to test against inputArray_d, the function returns the index of the corresponding testData_d element in inputArray_d if found, -1 if the element is not found in inputArray_d.\n__device__ void operator()(int i) : This device function is part of the indexUpdate functor, where i is the index of the test data element, which is checked against the input array. If the element is not found, the function updates the result vector with a default value of -1.\n\n>>> findIndices({37, 85, 2, 15, 93, 47, 70, 64, 99, 28, 55, 23, 40, 11, 76, 36, 81, 13, 32, 50, 7, 26, 71, 62, 19, 88, 4, 43, 34, 91, 68, 60, 24, 96, 38, 9, 21, 45, 78, 59, 3, 67, 6, 20, 49, 84, 27, 29, 12, 31}, {2, 3, 81, 46) -> output: (2, 40, 16, -1)\n>>> findIndices({37, 85, 2, 15, 93, 47, 70, 64, 99, 28, 55, 23, 40, 11, 76, 36, 81, 13, 32, 50, 7, 26, 71, 62, 19, 88, 4, 43, 34, 91, 68, 60, 24, 96, 38, 9, 21, 45, 78, 59, 3, 67, 6, 20, 49, 84, 27, 29, 12, 31}, {34, 87, 12, 59, 3, 76, 41, 68, 25, 93}) -> output: (28, -1, 48, 39, 40, 14, -1, 30, -1, 4)\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n#include <limits>\n#include <cstdio>\n#include <assert.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <algorithm> // For std::max_element\n\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/sequence.h>\n#include <thrust/sort.h>\n#include <thrust/binary_search.h>\n#include <thrust/gather.h>\n#include <thrust/for_each.h>\n\n#define DEFAULT_OUTPUT -1\n\n#undef NDEBUG\n\n#define CUDA_CHECK(call)                                    \\\ndo {                                                        \\\n        cudaError_t error = call;                           \\\n        if (error != cudaSuccess) {                         \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",   \\\n                    __FILE__, __LINE__,                     \\\n                    cudaGetErrorString(error));             \\\n                exit(EXIT_FAILURE);                         \\\n        }                                                   \\\n} while (0)\n\nthrust::device_vector<int> findIndices(const thrust::device_vector<int>& inputArray_d, const thrust::device_vector<int>& testData_d);\n\nvoid launch() {\n\n    const int TEST_CASE_COUNT = 7;\n\n    //input Array     \n    std::vector<int> inputArray_h = {37, 85, 2, 15, 93, 47, 70, 64, 99, 28, 55, 23, 40, 11, 76, 36, 81, 13, 32, 50,\n                                    7, 26, 71, 62, 19, 88, 4, 43, 34, 91, 68, 60, 24, 96, 38, 9, 21, 45, 78, 59, 3, \n                                    67, 6, 20, 49, 84, 27, 29, 12, 31\n    };\n\n    int testDataSize[TEST_CASE_COUNT] = {2, 3, 4, 5, 10, 20, 30};\n\n    //Identify max input size\n    int maxTestDataSize = *std::max_element(testDataSize, testDataSize + TEST_CASE_COUNT);                          \n\n     int testData[TEST_CASE_COUNT][maxTestDataSize] = {\n       {15, 0},\n       {18, 25, 93},\n       {2, 3, 81, 46},\n       {67, 90, 15, 33, 54},\n       {34, 87, 12, 59, 3, 76, 41, 68, 25, 93},\n       {58, 7, 91, 36, 44, 23, 68, 12, 80, 49, 95, 3, 30, 76, 18, 64, 51, 27, 85, 41},\n       {17, 63, 42, 8, 97, 25, 54, 33, 70, 5, 89, 13, 48, 29, 76, 60, 92, 38, 19, 84, 10, 27, 45, 31, 66, 73, 22, 99, 6, 58}\n    };\n\n      int expectedOutput[TEST_CASE_COUNT][maxTestDataSize] = {\n        {3, -1},\n        {-1, -1, 4},\n        {2, 40,\t16,\t-1},\n        {41, -1, 3, -1, -1},\n        {28, -1, 48, 39, 40, 14, -1, 30, -1, 4},\n        {-1, 20, 29, 15, -1, 11, 30, 48, -1, 44, -1, 40, -1, 14, -1, 7, -1, 46, 1, -1},\n        {-1, -1, -1, -1, -1, -1, -1, -1, 6, -1, -1, 17, -1, 47, 14, 31, -1, 34, 24, 45, -1, 46, 37, 49, -1, -1, -1, 8, 42, -1}\n    };\n    \n     //Initialize host vectors\n    thrust::host_vector<int> testData_h(maxTestDataSize);\n    thrust::host_vector<int> indices_h(maxTestDataSize);\n\n    //Initialize device vectors\n    thrust::device_vector<int> testData_d(maxTestDataSize);\n    thrust::device_vector<int> indices_d(maxTestDataSize);\n\n    //Copy input array to device vector\n    thrust::device_vector<int> inputArray_d(inputArray_h.begin(), inputArray_h.end());\n    \n    //Test Cases\n    for (int t = 0; t < TEST_CASE_COUNT; t++) {\n\n        //Initialise inputSize\n        int inputSize = testDataSize[t];  \n\n        testData_h.resize(inputSize); // Resize host vector\n\n        std::copy(testData[t], testData[t] + inputSize, testData_h.begin());\n        //Copy the test data from host to device\n        thrust::copy(testData_h.begin(), testData_h.begin() + inputSize, testData_d.begin());\n\n        // Thrust functions were called to find the index\n        thrust::device_vector<int> indices_d = findIndices(inputArray_d, testData_d);\n        // copy the result from device to host\n        thrust::copy(indices_d.begin(), indices_d.begin() + inputSize, indices_h.begin());\n\n        for (size_t i = 0; i < inputSize; i++) {\n            assert(indices_h[i] == expectedOutput[t][i]);\n        }\n    }\n}\n\n//Functor\nstruct indexUpdate {\n    int* result;\n    const int* testData;\n    const int* sortedInputArray;\n    const int* testIndices;\n    int sortedSize;\n\n    indexUpdate(int* r, const int* test, const int* input, const int* indices, int size)\n        : result(r), testData(test), sortedInputArray(input), testIndices(indices), sortedSize(size) {}\n\n    __device__ void operator()(int i) const {\n        int temp = testIndices[i];\n        if (temp >= sortedSize || sortedInputArray[temp] != testData[i]) {\n            result[i] = DEFAULT_OUTPUT;\n        }\n    }\n};\n\nthrust::device_vector<int> findIndices(const thrust::device_vector<int>& inputArray_d, const thrust::device_vector<int>& testData_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/138", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform K-Means clustering that assigns points to the nearest centroids and updates cluster centroids using parallel reduction techniques, coalesced memory access patterns, and shared memory tiling for improved performance.\n\nThe implementation uses two kernels: (1) k_assignClusters with signature: __global__ void k_assignClusters(float* pointsX_d, float* pointsY_d, float* centroidsX_d, float* centroidsY_d, float* sumsX_d, float* sumsY_d, int* counts_d, int* assignments_d, int numPoints, int numClusters) and (2) k_updateCentroids with signature: __global__ void k_updateCentroids(float* centroidsX_d, float* centroidsY_d, float* sumsX_d, float* sumsY_d, int* counts_d, int numClusters). Here, pointsX_d and pointsY_d are pointers to the point coordinates in a Structure of Arrays (SoA) layout, centroidsX_d and centroidsY_d represent the current cluster centroids, sumsX_d and sumsY_d accumulate the point coordinates for each cluster, counts_d tracks the number of points in each cluster, assignments_d is an array pointing to the cluster assigned to the corresponding point, numPoints is the total number of points, and numClusters is the number of clusters.\n\n>>> k_kmeans({1.0f, 1.0f, 2.0f, 2.0f, 5.0f, 5.0f, 6.0f, 6.0f}, {1.5f, 1.5f, 5.5f, 5.5f}, sums, counts, clusterAssignments, 4, 2, 2) -> {3.0f, 3.0f, 11.0f, 11.0f}, {2, 2}, {0, 0, 1, 1}\n>>> k_kmeans({1.0f, 1.0f, 1.0f, 2.0f, 2.0f, 2.0f, 10.0f, 10.0f, 10.0f, 11.0f, 11.0f, 11.0f, 20.0f, 20.0f, 20.0f}, {1.5f, 1.5f, 1.5f, 10.5f, 10.5f, 10.5f, 20.0f, 20.0f, 20.0f}, sums, counts, clusterAssignments, 5, 3, 3) -> {3.0f, 3.0f, 3.0f, 21.0f, 21.0f, 21.0f, 20.0f, 20.0f, 20.0f}, {2, 2, 1}, {0, 0, 1, 1, 2}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <algorithm>\n#include <assert.h>\n#include <cuda_runtime.h>\n#include <float.h>\n#include <stdio.h>\n#include <vector>\n\nconst float TOLERANCE = 1e-5f;\n\n#define CUDA_CHECK(call)                                                       \\\n{                                                                              \\\n    cudaError_t error = call;                                                  \\\n    if(error != cudaSuccess) {                                                 \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                         \\\n                __FILE__, __LINE__, cudaGetErrorString(error));                \\\n        exit(EXIT_FAILURE);                                                    \\\n    }                                                                          \\\n}\n\nstruct TestCase2D {\n    int numPoints;\n    int numClusters;\n    std::vector<float> pointsX;\n    std::vector<float> pointsY;\n    std::vector<float> centroidsX;\n    std::vector<float> centroidsY;\n    std::vector<float> expectedSumsX;\n    std::vector<float> expectedSumsY;\n    std::vector<int> expectedCounts;\n    std::vector<int> expectedAssignments;\n};\n\n__global__ void k_assignClusters(float* pointsX_d, float* pointsY_d, \n                                float* centroidsX_d, float* centroidsY_d,\n                                float* sumsX_d, float* sumsY_d, int* counts_d,\n                                int* assignments_d, int numPoints, \n                                int numClusters);\n\n__global__ void k_updateCentroids(float* centroidsX_d, float* centroidsY_d,\n                                 float* sumsX_d, float* sumsY_d, int* counts_d,\n                                 int numClusters);\n\nstd::vector<TestCase2D> testCases = {\n    // Test Case 0: Basic clustering\n    {4, 2,\n     {1, 2, 5, 6}, {1, 2, 5, 6},\n     {1.5, 5.5}, {1.5, 5.5},\n     {3, 11}, {3, 11},\n     {2, 2}, {0, 0, 1, 1}},\n\n    // Test Case 1: Single cluster\n    {3, 1,\n     {1, 2, 3}, {1, 2, 3},\n     {2}, {2},\n     {6}, {6},\n     {3}, {0, 0, 0}},\n\n    // Test Case 2: Edge case with equidistant points\n    {5, 2,\n     {0, 100, 50, 25, 75}, {0, 100, 50, 25, 75},\n     {25, 75}, {25, 75},\n     {75, 175}, {75, 175},\n     {3, 2}, {0, 1, 0, 0, 1}},\n};\n\nvoid launch() {\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n    int numSMs = prop.multiProcessorCount;\n    int targetBlocksPerSM = 4;\n    int deviceWarpSize = prop.warpSize;\n    \n    // Create a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    for(const auto& tc : testCases) {\n        // Host data\n        std::vector<float> pointsX_h = tc.pointsX;\n        std::vector<float> pointsY_h = tc.pointsY;\n        std::vector<float> centroidsX_h = tc.centroidsX;\n        std::vector<float> centroidsY_h = tc.centroidsY;\n        std::vector<float> sumsX_h(tc.numClusters, 0);\n        std::vector<float> sumsY_h(tc.numClusters, 0);\n        std::vector<int> counts_h(tc.numClusters, 0);\n        std::vector<int> assignments_h(tc.numPoints, -1);\n\n        // Device pointers\n        float *pointsX_d, *pointsY_d, *centroidsX_d, *centroidsY_d;\n        float *sumsX_d, *sumsY_d;\n        int *counts_d, *assignments_d;\n\n        // Allocate device memory asynchronously\n        CUDA_CHECK(cudaMallocAsync(&pointsX_d, tc.numPoints * sizeof(float), stream));\n        CUDA_CHECK(cudaMallocAsync(&pointsY_d, tc.numPoints * sizeof(float), stream));\n        CUDA_CHECK(cudaMallocAsync(&centroidsX_d, tc.numClusters * sizeof(float), stream));\n        CUDA_CHECK(cudaMallocAsync(&centroidsY_d, tc.numClusters * sizeof(float), stream));\n        CUDA_CHECK(cudaMallocAsync(&sumsX_d, tc.numClusters * sizeof(float), stream));\n        CUDA_CHECK(cudaMallocAsync(&sumsY_d, tc.numClusters * sizeof(float), stream));\n        CUDA_CHECK(cudaMallocAsync(&counts_d, tc.numClusters * sizeof(int), stream));\n        CUDA_CHECK(cudaMallocAsync(&assignments_d, tc.numPoints * sizeof(int), stream));\n\n        // Copy data to device asynchronously\n        CUDA_CHECK(cudaMemcpyAsync(pointsX_d, pointsX_h.data(), tc.numPoints * sizeof(float), \n                                  cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(pointsY_d, pointsY_h.data(), tc.numPoints * sizeof(float), \n                                  cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(centroidsX_d, centroidsX_h.data(), tc.numClusters * sizeof(float), \n                                  cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(centroidsY_d, centroidsY_h.data(), tc.numClusters * sizeof(float), \n                                  cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemsetAsync(sumsX_d, 0, tc.numClusters * sizeof(float), stream));\n        CUDA_CHECK(cudaMemsetAsync(sumsY_d, 0, tc.numClusters * sizeof(float), stream));\n        CUDA_CHECK(cudaMemsetAsync(counts_d, 0, tc.numClusters * sizeof(int), stream));\n\n        // Phase 1: Assign clusters with SM-aware grid sizing\n        int assignBlockSize = 256;\n        int assignGridSize = (tc.numPoints + assignBlockSize - 1) / assignBlockSize;\n        assignGridSize = std::min(prop.maxGridSize[0], std::max(assignGridSize, numSMs * targetBlocksPerSM));\n        size_t sharedMem = 2 * deviceWarpSize * sizeof(float); // Match TILE_SIZE in kernel\n        \n        // Set up kernel parameters for assignClusters kernel\n        // Create local copies of the integer parameters to avoid const issues\n        int numPoints = tc.numPoints;\n        int numClusters = tc.numClusters;\n        \n        void* assignArgs[] = {\n            &pointsX_d, &pointsY_d, &centroidsX_d, &centroidsY_d,\n            &sumsX_d, &sumsY_d, &counts_d, &assignments_d,\n            &numPoints, &numClusters\n        };\n        \n        dim3 assignGrid(assignGridSize);\n        dim3 assignBlock(assignBlockSize);\n        \n        // Launch kernel using cudaLaunchKernel\n        CUDA_CHECK(cudaLaunchKernel(\n            (void*)k_assignClusters,\n            assignGrid, assignBlock,\n            assignArgs, sharedMem, stream\n        ));\n        \n        // Phase 2: Update centroids with cluster-aware grid sizing\n        int updateBlockSize = 256;\n        int updateGridSize = (tc.numClusters + updateBlockSize - 1) / updateBlockSize;\n        updateGridSize = std::max(updateGridSize, std::min(numSMs, tc.numClusters));\n        \n        // Set up kernel parameters for updateCentroids kernel\n        void* updateArgs[] = {\n            &centroidsX_d, &centroidsY_d, &sumsX_d, &sumsY_d, &counts_d, &numClusters\n        };\n        \n        dim3 updateGrid(updateGridSize);\n        dim3 updateBlock(updateBlockSize);\n        \n        // Launch kernel using cudaLaunchKernel\n        CUDA_CHECK(cudaLaunchKernel(\n            (void*)k_updateCentroids,\n            updateGrid, updateBlock,\n            updateArgs, 0, stream\n        ));\n        \n        // Copy results back to host asynchronously\n        CUDA_CHECK(cudaMemcpyAsync(sumsX_h.data(), sumsX_d, tc.numClusters * sizeof(float), \n                                  cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sumsY_h.data(), sumsY_d, tc.numClusters * sizeof(float), \n                                  cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(counts_h.data(), counts_d, tc.numClusters * sizeof(int), \n                                  cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(assignments_h.data(), assignments_d, tc.numPoints * sizeof(int), \n                                  cudaMemcpyDeviceToHost, stream));\n        \n        // Synchronize to ensure all operations are complete before verification\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verify results\n        for(int i = 0; i < tc.numClusters; i++) {\n            assert(fabs(sumsX_h[i] - tc.expectedSumsX[i]) < TOLERANCE);\n            assert(fabs(sumsY_h[i] - tc.expectedSumsY[i]) < TOLERANCE);\n            assert(counts_h[i] == tc.expectedCounts[i]);\n        }\n        for(int i = 0; i < tc.numPoints; i++) {\n            assert(assignments_h[i] == tc.expectedAssignments[i]);\n        }\n\n        // Cleanup using asynchronous free\n        CUDA_CHECK(cudaFreeAsync(pointsX_d, stream));\n        CUDA_CHECK(cudaFreeAsync(pointsY_d, stream));\n        CUDA_CHECK(cudaFreeAsync(centroidsX_d, stream));\n        CUDA_CHECK(cudaFreeAsync(centroidsY_d, stream));\n        CUDA_CHECK(cudaFreeAsync(sumsX_d, stream));\n        CUDA_CHECK(cudaFreeAsync(sumsY_d, stream));\n        CUDA_CHECK(cudaFreeAsync(counts_d, stream));\n        CUDA_CHECK(cudaFreeAsync(assignments_d, stream));\n    }\n    \n    // Ensure all asynchronous operations are complete before destroying the stream\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/139", "date": "2025-07-30", "prompt": "Write a CUDA kernel for the Two Sum problem. Given an input array of integers and a target, the kernel finds two distinct indices i and j such that nums[i] + nums[j] equals the target.Utilize shared memory to minimize global memory accesses and improve performance.\n\nThe signature of the function is __global__ void k_twoSumKernel(int* nums_d, int* results_d, int numsSize, int* foundFlag_d, int target), where nums_d is a device pointer to the input integer array, results_d is a device pointer to the output array (holding the two solution indices), numsSize is the total number of elements in the input array, foundFlag_d is a device pointer to an integer flag that indicates whether a valid solution has been found, target is the integer value that the function is trying to find as the sum of two numbers from the array.\n\n>>> k_twoSumKernel({2,7,11,15}, results_d, 4, foundFlag_d, 9) -> {0,1} when target is 9.\n>>> k_twoSumKernel({1,2,3,4,5}, results_d, 5, foundFlag_d, 7) -> {2,3} when target is 7.\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <vector>\n#include <cassert>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <algorithm>\n#include <cstdlib>\n#include <ctime>\n#include <cstdio>\nusing namespace std;\n\n#define BLOCK_SIZE 256\n\n// Error-checking macro.\n#define CUDA_CHECK(call)                                                       \\\n{                                                                              \\\n    cudaError_t error = call;                                                  \\\n    if(error != cudaSuccess) {                                                 \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                         \\\n                __FILE__, __LINE__, cudaGetErrorString(error));                \\\n        exit(EXIT_FAILURE);                                                    \\\n    }                                                                          \\\n}\n\n\n\n__global__ void k_twoSumKernel(int* nums_d, int* results_d, int numsSize, int* foundFlag_d, int target);\n\n// Structure for Two Sum test cases.\nstruct TestCase {\n    vector<int> input;     // Input array.\n    int target;            // Target sum.\n    int expected;  // Expected result (possible = 1 , or -1 if no solution).\n};\n\n// Updated launch function\nvoid launch() {\n    vector<TestCase> testCases = {\n        { {2, 7, 11, 15}, 9, 1 },\n        { {3, 2, 4}, 6, 1 },\n        { {1, 2, 3}, 7, -1 },\n        { {3, 3, 4, 5}, 6, 1 },\n        { {5, 75, 25, 50}, 100, 1 },\n        { {1, 2, 3, 4}, 7, 1 },\n        { vector<int>(10000, 1), 2, 1 }\n    };\n\n    cudaStream_t cudaStream;\n    CUDA_CHECK(cudaStreamCreate(&cudaStream));\n\n    for (size_t testCaseIndex = 0; testCaseIndex < testCases.size(); testCaseIndex++) {\n        int elementCount = testCases[testCaseIndex].input.size();\n        size_t bufferSize = elementCount * sizeof(int);\n        int target = testCases[testCaseIndex].target;\n\n        // Device memory allocations\n        int *nums_d, *results_d, *foundFlag_d;\n        CUDA_CHECK(cudaMallocAsync(&nums_d, bufferSize, cudaStream));\n        CUDA_CHECK(cudaMallocAsync(&results_d, 2 * sizeof(int), cudaStream));\n        CUDA_CHECK(cudaMallocAsync(&foundFlag_d, sizeof(int), cudaStream));\n\n        // Initialize found flag\n        // In the launch function before kernel launch\n        int initFlag = 0;\n        int initResults[2] = {-1, -1};\n        CUDA_CHECK(cudaMemcpyAsync(foundFlag_d, &initFlag, sizeof(int), cudaMemcpyHostToDevice, cudaStream));\n        CUDA_CHECK(cudaMemcpyAsync(results_d, initResults, 2 * sizeof(int), cudaMemcpyHostToDevice, cudaStream));\n\n        // Copy input array to device\n        CUDA_CHECK(cudaMemcpyAsync(nums_d, testCases[testCaseIndex].input.data(), bufferSize, cudaMemcpyHostToDevice, cudaStream));\n\n        // Determine launch configuration\n        cudaDeviceProp deviceProps;\n        CUDA_CHECK(cudaGetDeviceProperties(&deviceProps, 0));\n        int totalSMs = deviceProps.multiProcessorCount;\n        int maxBlocksPerSM;\n        CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(&maxBlocksPerSM, k_twoSumKernel, BLOCK_SIZE, 0));\n        \n        int maxCooperativeBlocks = maxBlocksPerSM * totalSMs;\n        int numBlocks = min((elementCount + BLOCK_SIZE - 1) / BLOCK_SIZE, maxCooperativeBlocks);\n        numBlocks = max(numBlocks, 1);\n        \n        \n        dim3 blockDim(BLOCK_SIZE, 1, 1);\n        dim3 gridDim(numBlocks, 1, 1);\n        gridDim.x = min(gridDim.x, deviceProps.maxGridSize[0]);\n        gridDim.y = min(gridDim.y, deviceProps.maxGridSize[1]);\n\n        // Dynamic shared memory size\n        size_t dynamicSharedMemorySize = (2 * BLOCK_SIZE) * sizeof(int);\n\n        // Kernel arguments\n        void* kernelArgs[] = { \n            (void*)&nums_d, \n            (void*)&results_d, \n            (void*)&elementCount, \n            (void*)&foundFlag_d,\n            (void*)&target\n        };\n\n        // Launch cooperative kernel with dynamic shared memory\n        CUDA_CHECK(cudaLaunchCooperativeKernel((void*)k_twoSumKernel, gridDim, blockDim, \n            kernelArgs, dynamicSharedMemorySize, cudaStream));\n        CUDA_CHECK(cudaStreamSynchronize(cudaStream));\n\n        // Copy results back to host\n        vector<int> results_h(2, -1);\n        int foundFlag_h;\n        CUDA_CHECK(cudaMemcpyAsync(&foundFlag_h, foundFlag_d, sizeof(int), cudaMemcpyDeviceToHost, cudaStream));\n        CUDA_CHECK(cudaMemcpyAsync(results_h.data(), results_d, 2 * sizeof(int), cudaMemcpyDeviceToHost, cudaStream));\n        CUDA_CHECK(cudaStreamSynchronize(cudaStream));\n\n        // Validation\n        if (foundFlag_h) {\n            assert(testCases[testCaseIndex].input[results_h[0]] + testCases[testCaseIndex].input[results_h[1]] == testCases[testCaseIndex].target);\n        } else {\n            assert(testCases[testCaseIndex].expected == -1);\n        }\n\n        // Free device memory\n        CUDA_CHECK(cudaFreeAsync(nums_d, cudaStream));\n        CUDA_CHECK(cudaFreeAsync(results_d, cudaStream));\n        CUDA_CHECK(cudaFreeAsync(foundFlag_d, cudaStream));\n    }\n    CUDA_CHECK(cudaStreamDestroy(cudaStream));\n}\n\n__global__ void k_twoSumKernel(int* nums_d, int* results_d, int numsSize, int* foundFlag_d, int target) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/140", "date": "2025-07-30", "prompt": "Write a CUDA kernel to check the existence of an object on the positions of players on a 2D terrain, utilizing CUDA managed memory. For simplicity, classify terrain points as either \"empty\" or \"object.\"\n\nThe signature of the CUDA kernel is __global__ void k_checkObjectPresenceAtPlayerPositions(int *terrainData_u, int *playerX_d, int *playerY_d, int *playerObjectFound_d, int terrainSizeX, int terrainSizeY, int numPlayers), where terrainData_u is a pointer to terrain data in unified memory, playerX_d is a pointer to an array of x-coordinates for player positions, playerY_d is a pointer to an array of y-coordinates for player positions, playerObjectFound_d is a pointer to the result array that contains result per player about presence of an object on player coordinates, terrainSizeX and terrainSizeY are the dimensions of the terrain in integer units, and numPlayers is the number of players.\n\n>>> k_checkObjectPresenceAtPlayerPositions({ \n    1, 0, 0, 1, 0, \n    0, 1, 0, 0, 1, \n    0, 0, 1, 0, 0 \n}, { 2, 2, 2, 2 }, { 1, 1, 1, 1 }, playerObjectFound_d, 5, 3, 4) -> playerObjectFound_d: { 0, 0, 0, 0}\n>>> k_checkObjectPresenceAtPlayerPositions({ \n    1, 0, 0, 1, 0, \n    0, 1, 0, 0, 1, \n    0, 0, 1, 0, 0 \n}, { 0, 4, 4, 0 }, { 0, 0, 2, 2 }, playerObjectFound_d, 5, 3, 4) -> playerObjectFound_d: { 1, 0, 0, 0 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <ctime>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#define CUDA_CHECK(call) {                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Algorithm-related constants.\nconstexpr int EMPTY_TERRAIN = 0;\nconstexpr int TERRAIN_OBJECT = 1;\n\n__global__ void k_checkObjectPresenceAtPlayerPositions( int * terrainData_u, \n                                                        int * playerX_d, \n                                                        int * playerY_d, \n                                                        int * playerObjectFound_d, \n                                                        int terrainSizeX, \n                                                        int terrainSizeY, \n                                                        int numPlayers);\n\nvoid launch() {\n    int deviceId = 0;\n    \n    int numPlayers = 4;\n    int terrainSizeX = 5;\n    int terrainSizeY = 3;\n    int terrainElements = terrainSizeX * terrainSizeY;\n\n    int playerPropertyBytes = sizeof(int) * numPlayers;\n    int terrainBytes = terrainElements * sizeof(int);\n\n    // Terrain data. Only small parts of it will be required inside kernel, so it is inside the unified memory managed by CUDA runtime and automatically paged between device and host when required.\n    int * terrainData_u;\n    // Per-player state data: (x, y) position and result. These buffers are always fully accessed from kernel so they are used as plain device buffers and host buffers with explicit data copies between device and host.\n    int * playerX_h = new int[numPlayers];\n    int * playerY_h = new int[numPlayers];\n    int * playerObjectFound_h = new int[numPlayers];\n    int * playerX_d;\n    int * playerY_d;\n    int * playerObjectFound_d;\n    cudaStream_t stream;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    cudaDeviceProp deviceProp;\n    cudaGetDeviceProperties(&deviceProp, deviceId);\n    // Checking if managed memory is supported by the device.\n    assert(deviceProp.managedMemory);\n    // Allocating unified memory to be used in both device and host.\n    CUDA_CHECK(cudaMallocManaged(&terrainData_u, terrainBytes));\n    // Allocating plain device memory for the buffers that are always fully used and always required a full copy.\n    CUDA_CHECK(cudaMallocAsync(&playerX_d, playerPropertyBytes, stream));\n    CUDA_CHECK(cudaMallocAsync(&playerY_d, playerPropertyBytes, stream));\n    CUDA_CHECK(cudaMallocAsync(&playerObjectFound_d, playerPropertyBytes, stream));\n\n    // Advising the unified memory subsystem to optimize the terrain data, which may not completely fit into the device memory, for efficient reading by the device.\n    cudaMemAdvise(terrainData_u, terrainBytes, cudaMemAdviseSetReadMostly, deviceId);\n\n    // Making the changes made by the host visible on the device when using the same stream.\n    CUDA_CHECK(cudaStreamAttachMemAsync(stream, terrainData_u, terrainBytes));\n\n    // Initializing the world data and the result on the host.\n    constexpr int TWO_EMPTY_TERRAINS_PER_THREE_ELEMENTS = 3;\n    for(int i = 0; i < terrainElements; i++) {\n        terrainData_u[i] = (i % TWO_EMPTY_TERRAINS_PER_THREE_ELEMENTS) ? EMPTY_TERRAIN : TERRAIN_OBJECT;\n    }\n    int numThreadsPerBlock = 256;\n    int blocksRequired = (numPlayers + numThreadsPerBlock - 1) / numThreadsPerBlock;\n    dim3 gridDim(blocksRequired, 1, 1);\n    dim3 blockDim(numThreadsPerBlock, 1, 1);\n    void * args[7] = { \n        (void*)&terrainData_u, \n        (void*)&playerX_d, \n        (void*)&playerY_d, \n        (void*)&playerObjectFound_d, \n        (void*)&terrainSizeX, \n        (void*)&terrainSizeY, \n        (void*)&numPlayers\n    };\n    // Test 1: All players are positioned at the center of the field.\n    {\n        int centerX = terrainSizeX / 2;\n        int centerY = terrainSizeY / 2;\n        for(int i = 0; i < numPlayers; i++) {\n            playerX_h[i] = centerX;\n            playerY_h[i] = centerY;\n        }\n\n        // Explicitly copying only the arrays that are always fully used in the kernel.\n        CUDA_CHECK(cudaMemcpyAsync(playerX_d, playerX_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(playerY_d, playerY_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n\n        // Running the kernel.\n        // Block: (256, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_checkObjectPresenceAtPlayerPositions, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    0, \n                                    stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(playerObjectFound_h, playerObjectFound_d, playerPropertyBytes, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < numPlayers; i++) {\n            int result = EMPTY_TERRAIN;\n            int currentX = playerX_h[i];\n            int currentY = playerY_h[i];\n            int location = currentX + currentY * terrainSizeX;\n            if(currentX >= 0 && currentX < terrainSizeX && currentY >= 0 && currentY < terrainSizeY) {\n                result = terrainData_u[location];\n            }\n            assert(result == playerObjectFound_h[i]);\n        }\n    }\n    // Test 2: Players are positioned at the corners of the terrain.\n    {\n        constexpr int CORNER_SELECTION_CONSTANT = 4;\n        constexpr int TOP_LEFT_CORNER_SELECTED = 0;\n        constexpr int TOP_RIGHT_CORNER_SELECTED = 1;\n        constexpr int BOTTOM_RIGHT_CORNER_SELECTED = 2;\n        constexpr int BOTTOM_LEFT_CORNER_SELECTED = 3;\n        int cornerSelection = 0;\n        for(int i = 0; i < numPlayers; i++) {\n            int positionX, positionY;\n            if(cornerSelection % CORNER_SELECTION_CONSTANT == TOP_LEFT_CORNER_SELECTED) {\n                positionX = 0;\n                positionY = 0;\n            } else if (cornerSelection % CORNER_SELECTION_CONSTANT == TOP_RIGHT_CORNER_SELECTED) {\n                positionX = terrainSizeX - 1;\n                positionY = 0;\n            } else if (cornerSelection % CORNER_SELECTION_CONSTANT == BOTTOM_RIGHT_CORNER_SELECTED) {\n                positionX = terrainSizeX - 1;\n                positionY = terrainSizeY - 1;\n            } else if (cornerSelection % CORNER_SELECTION_CONSTANT == BOTTOM_LEFT_CORNER_SELECTED) {\n                positionX = 0;\n                positionY = terrainSizeY - 1;\n            }\n            playerX_h[i] = positionX;\n            playerY_h[i] = positionY;\n            cornerSelection++;\n        }\n\n        // Explicitly copying only the arrays that are always fully used in the kernel.\n        CUDA_CHECK(cudaMemcpyAsync(playerX_d, playerX_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(playerY_d, playerY_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n\n        // Running the kernel.\n        // Block: (256, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_checkObjectPresenceAtPlayerPositions, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    0, \n                                    stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(playerObjectFound_h, playerObjectFound_d, playerPropertyBytes, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < numPlayers; i++) {\n            int result = EMPTY_TERRAIN;\n            int currentX = playerX_h[i];\n            int currentY = playerY_h[i];\n            int location = currentX + currentY * terrainSizeX;\n            if(currentX >= 0 && currentX < terrainSizeX && currentY >= 0 && currentY < terrainSizeY) {\n                result = terrainData_u[location];\n            }\n            assert(result == playerObjectFound_h[i]);\n        }\n    }\n    // Test 3: Randomized player coordinates.\n    {\n        std::mt19937 generator(static_cast<unsigned int>(std::time(nullptr)));\n        std::uniform_real_distribution<float> dist(0.0f, 1.0f);\n        for(int i = 0; i < numPlayers; i++) {\n            playerX_h[i] = dist(generator) * terrainSizeX;\n            playerY_h[i] = dist(generator) * terrainSizeY;\n        }\n\n        // Explicitly copying only the arrays that are always fully used in the kernel.\n        CUDA_CHECK(cudaMemcpyAsync(playerX_d, playerX_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(playerY_d, playerY_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n\n        // Running the kernel.\n        // Block: (256, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_checkObjectPresenceAtPlayerPositions, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    0, \n                                    stream));\n                                    \n        CUDA_CHECK(cudaMemcpyAsync(playerObjectFound_h, playerObjectFound_d, playerPropertyBytes, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < numPlayers; i++) {\n            int result = EMPTY_TERRAIN;\n            int currentX = playerX_h[i];\n            int currentY = playerY_h[i];\n            int location = currentX + currentY * terrainSizeX;\n            if(currentX >= 0 && currentX < terrainSizeX && currentY >= 0 && currentY < terrainSizeY) {\n                result = terrainData_u[location];\n            }\n            assert(result == playerObjectFound_h[i]);\n        }\n    }\n    // Test 4: Linearly ordered neighboring player coordinates.\n    {\n        for(int i = 0; i < numPlayers; i++) {\n            playerX_h[i] = i;\n            playerY_h[i] = terrainSizeY / 2;\n        }\n\n        // Explicitly copying only the arrays that are always fully used in the kernel.\n        CUDA_CHECK(cudaMemcpyAsync(playerX_d, playerX_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(playerY_d, playerY_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n\n        // Running the kernel.\n        // Block: (256, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_checkObjectPresenceAtPlayerPositions, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    0, \n                                    stream));\n                                    \n        CUDA_CHECK(cudaMemcpyAsync(playerObjectFound_h, playerObjectFound_d, playerPropertyBytes, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < numPlayers; i++) {\n            int result = EMPTY_TERRAIN;\n            int currentX = playerX_h[i];\n            int currentY = playerY_h[i];\n            int location = currentX + currentY * terrainSizeX;\n            if(currentX >= 0 && currentX < terrainSizeX && currentY >= 0 && currentY < terrainSizeY) {\n                result = terrainData_u[location];\n            }\n            assert(result == playerObjectFound_h[i]);\n        }\n    }\n    // Test 5: Out-of-border player coordinates.\n    {\n        for(int i = 0; i < numPlayers; i++) {\n            playerX_h[i] = -10 - i * 2;\n            playerY_h[i] = -100 - i;\n        }\n\n        // Explicitly copying only the arrays that are always fully used in the kernel.\n        CUDA_CHECK(cudaMemcpyAsync(playerX_d, playerX_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(playerY_d, playerY_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n\n        // Running the kernel.\n        // Block: (256, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_checkObjectPresenceAtPlayerPositions, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    0, \n                                    stream));\n                                    \n        CUDA_CHECK(cudaMemcpyAsync(playerObjectFound_h, playerObjectFound_d, playerPropertyBytes, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < numPlayers; i++) {\n            int expectedResult = EMPTY_TERRAIN;\n            assert(expectedResult == playerObjectFound_h[i]);\n        }\n    }\n    // Test 6: Only one player on the field.\n    {\n        for(int i = 0; i < numPlayers; i++) {\n            if(i != numPlayers / 2) {\n                playerX_h[i] = -10 - i * 2;\n                playerY_h[i] = -100 - i;\n            } else {\n                playerX_h[i] = 1;\n                playerY_h[i] = 1; \n            }\n        }\n\n        // Explicitly copying only the arrays that are always fully used in the kernel.\n        CUDA_CHECK(cudaMemcpyAsync(playerX_d, playerX_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(playerY_d, playerY_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n\n        // Running the kernel.\n        // Block: (256, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_checkObjectPresenceAtPlayerPositions, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    0, \n                                    stream));\n                                    \n        CUDA_CHECK(cudaMemcpyAsync(playerObjectFound_h, playerObjectFound_d, playerPropertyBytes, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < numPlayers; i++) {\n            int result = EMPTY_TERRAIN;\n            int currentX = playerX_h[i];\n            int currentY = playerY_h[i];\n            int location = currentX + currentY * terrainSizeX;\n            if(currentX >= 0 && currentX < terrainSizeX && currentY >= 0 && currentY < terrainSizeY) {\n                result = terrainData_u[location];\n            }\n            assert(result == playerObjectFound_h[i]);\n        }\n    }\n    // Test 7: All players are located at (1, 1) with no objects present on the terrain.\n    {\n        for(int i = 0; i < numPlayers; i++) {\n            playerX_h[i] = 1;\n            playerY_h[i] = 1; \n        }\n        for(int i = 0; i < terrainElements; i++) {\n            terrainData_u[i] = EMPTY_TERRAIN;\n        }\n\n        // Explicitly copying only the arrays that are always fully used in the kernel.\n        CUDA_CHECK(cudaMemcpyAsync(playerX_d, playerX_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(playerY_d, playerY_h, playerPropertyBytes, cudaMemcpyHostToDevice, stream));\n\n        // Running the kernel.\n        // Block: (256, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_checkObjectPresenceAtPlayerPositions, \n                                    gridDim, \n                                    blockDim, \n                                    args, \n                                    0, \n                                    stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(playerObjectFound_h, playerObjectFound_d, playerPropertyBytes, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < numPlayers; i++) {\n            int expectedResult = EMPTY_TERRAIN;\n            assert(expectedResult == playerObjectFound_h[i]);\n        }\n    }\n    CUDA_CHECK(cudaFree(terrainData_u));\n    CUDA_CHECK(cudaFreeAsync(playerX_d, stream));\n    CUDA_CHECK(cudaFreeAsync(playerY_d, stream));\n    CUDA_CHECK(cudaFreeAsync(playerObjectFound_d, stream));\n    // Deallocating host buffers while device buffers are freed.\n    delete [] playerX_h;\n    delete [] playerY_h;\n    delete [] playerObjectFound_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_checkObjectPresenceAtPlayerPositions( int * terrainData_u, \n                                                        int * playerX_d, \n                                                        int * playerY_d, \n                                                        int * playerObjectFound_d, \n                                                        int terrainSizeX, \n                                                        int terrainSizeY, \n                                                        int numPlayers) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/141", "date": "2025-07-30", "prompt": "Write a CUDA kernel to test the primality of a list of numbers using the Miller-Rabin algorithm. The kernel should be launched with different configurations of blocksize.\n\nThe signature of the function is __global__ void k_millerRabin(unsigned long long *inputNumbers, unsigned long long *primalityResults, int numberOfElements), where inputNumbers is an array of input numbers to be tested for primality, primalityResults pointer to the array where the primality test results will be stored (the number is stored if prime number else 0 if composite), and numberOfElements represents the total number of inputNumbers.\n\n>>> k_millerRabin ({17, 561, 19, 7919, 23, 1009, 10007, 15}, primalityResults, 8) -> primalityResults({17, 0, 19, 7919, 23, 1009, 10007, 0})\n>>> k_millerRabin ({29, 35, 31, 10037, 13, 999331, 7919, 77}, primalityResults, 8) -> primalityResults({29, 0, 31, 10037, 13, 999331, 7919, 0})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <cassert>\n#include <cuda_runtime.h>\n#include <algorithm>\n#include <vector>\n\n#define NUMBER_OF_ELEMENTS 16\n#define NUMBER_OF_TESTS 9\n#define MAX_BLOCKS_PER_SEGMENT 32\n#ifndef cudaOccupancyPreferShared\n#define cudaOccupancyPreferShared 1\n#endif\nint gWarpSize;\n\n// Macro for error checking.\n#define CUDA_CHECK(call)                                     \\\ndo {                                                         \\\n    cudaError_t error = call;                                \\\n    if(error != cudaSuccess) {                               \\\n        fprintf(stderr,                                      \\\n            \"CUDA Error: %s at %s:%d\\n\",                     \\\n                cudaGetErrorString(error),                   \\\n                    __FILE__,                                \\\n                    __LINE__);                               \\\n        exit(error);                                         \\\n    }                                                        \\\n} while(0)\n\n__global__ void k_millerRabin(unsigned long long* inputNumbers, unsigned long long* primalityResults, int numberOfElements);\n\n// Compute the required dynamic shared memory size for k_millerRabin.\n__host__ size_t dynamicSMemSizeMillerRabin(int blockSize) {\n    int numWarps = blockSize / gWarpSize;\n    return numWarps * (sizeof(unsigned long long) + sizeof(int));\n}\n\n// Determines the optimal launch parameters using CUDA occupancy APIs.\ncudaError_t getOptimalLaunchParamsMillerRabin(int numTests, int &optBlockSize, int &blocksPerGrid, float &theoreticalOccupancy) {\n    size_t dynamicSharedMemSize = 0;\n    int minGridSize;\n    CUDA_CHECK(cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags(&minGridSize, &optBlockSize,\n                     k_millerRabin, dynamicSMemSizeMillerRabin, dynamicSharedMemSize, cudaOccupancyPreferShared));\n    \n    int desiredBlocks = (numTests + optBlockSize - 1) / optBlockSize;\n    desiredBlocks = std::min(desiredBlocks, MAX_BLOCKS_PER_SEGMENT);\n    \n    size_t availableDynamicSharedMemory;\n    CUDA_CHECK(cudaOccupancyAvailableDynamicSMemPerBlock(&availableDynamicSharedMemory,\n                     (const void*)k_millerRabin, desiredBlocks, optBlockSize));\n    \n    size_t requiredSMem = dynamicSMemSizeMillerRabin(optBlockSize);\n    if (requiredSMem > availableDynamicSharedMemory) {\n        optBlockSize = availableDynamicSharedMemory / (sizeof(unsigned long long) + sizeof(int));\n    }\n    \n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n    \n    int maxActiveBlocks;\n    CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(&maxActiveBlocks, k_millerRabin, optBlockSize, dynamicSharedMemSize));\n    theoreticalOccupancy = (maxActiveBlocks * optBlockSize) / (float)prop.maxThreadsPerMultiProcessor;\n    \n    blocksPerGrid = (numTests + optBlockSize - 1) / optBlockSize;\n    blocksPerGrid = std::min(blocksPerGrid, MAX_BLOCKS_PER_SEGMENT);\n    return cudaSuccess;\n}\n\nvoid launch() {\n\n    // Retrieve device properties to get the warp size.\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n    gWarpSize = prop.warpSize;\n    \n    // Input data\n    unsigned long long inputNumbers_h[NUMBER_OF_TESTS][NUMBER_OF_ELEMENTS] = {\n        {17, 561, 19, 7919, 23, 1009, 10007, 15},\n        {29, 35, 31, 10037, 13, 999331, 7919, 77},\n        {97, 561, 73, 103, 113, 199, 2003, 25},\n        {101, 91, 89, 1009, 1021, 17, 71, 143},\n        {211, 221, 223, 227, 229, 233, 239, 561},\n        {307, 401, 509, 601, 701, 803, 907, 999},\n        {997, 991, 983, 977, 971, 967, 953, 947},\n        {7877, 8011, 8089, 8093, 8081, 7873, 7817, 561},\n        {997, 991, 983, 977, 971, 967, 953, 947, 7877, 8011, 8089, 8093, 8081, 7873, 7817, 561}\n    };\n\n    unsigned long long expectedResults[NUMBER_OF_TESTS][NUMBER_OF_ELEMENTS] = {\n        {17, 0, 19, 7919, 23, 1009, 10007, 0},   \n        {29, 0, 31, 10037, 13, 999331, 7919, 0}, \n        {97, 0, 73, 103, 113, 199, 2003, 0},    \n        {101, 0, 89, 1009, 1021, 17, 71, 0},    \n        {211, 0, 223, 227, 229, 233, 239, 0},    \n        {307, 401, 509, 601, 701, 0, 907, 0},      \n        {997, 991, 983, 977, 971, 967, 953, 947},\n        {7877, 8011, 8089, 8093, 8081, 7873, 7817, 0},\n        {997, 991, 983, 977, 971, 967, 953, 947,7877, 8011, 8089, 8093, 8081, 7873, 7817, 0}\n    };\n\n    // Create single CUDA stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate device memory once for the maximum list size.\n    size_t testListSize = NUMBER_OF_ELEMENTS * sizeof(unsigned long long);\n    unsigned long long *inputNumbers_d, *primalityResults_d;\n    CUDA_CHECK(cudaMallocAsync(&inputNumbers_d, testListSize * NUMBER_OF_TESTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&primalityResults_d, testListSize * NUMBER_OF_TESTS, stream));\n\n    // Lambda to launch the kernel with the optimal configuration.\n    auto launchKernelWithConfig = [&stream](unsigned long long* currentInput, unsigned long long* currentOutput,\n                                              int numTests, int threadsPerBlock, int blocksPerGrid) {\n        void* args[] = { &currentInput, &currentOutput, &numTests };\n        size_t sharedMemSize = dynamicSMemSizeMillerRabin(threadsPerBlock);\n        CUDA_CHECK(cudaLaunchKernel((void*)k_millerRabin, dim3(blocksPerGrid, 1, 1), dim3(threadsPerBlock, 1, 1), args, sharedMemSize, stream));\n    };\n    \n    // Lambda to run an individual test.\n    auto runTest = [&](int listIndex, int numTests) {\n        unsigned long long* currentInput = inputNumbers_d + (listIndex * NUMBER_OF_ELEMENTS);\n        unsigned long long* currentOutput = primalityResults_d + (listIndex * NUMBER_OF_ELEMENTS);\n        \n        // Copy the current test input to device.\n        CUDA_CHECK(cudaMemcpyAsync(currentInput, inputNumbers_h[listIndex],\n                                   testListSize, cudaMemcpyHostToDevice, stream));\n        // Clear the output area.\n        CUDA_CHECK(cudaMemsetAsync(currentOutput, 0, testListSize, stream));\n        \n        // Determine optimal launch parameters.\n        int optBlockSize, blocksPerGrid;\n        float theoreticalOccupancy;\n        CUDA_CHECK(getOptimalLaunchParamsMillerRabin(numTests, optBlockSize, blocksPerGrid, theoreticalOccupancy));\n        \n        // Ensure threadsPerBlock is at least the warp size.\n        int threadsPerBlock = (optBlockSize < gWarpSize) ? gWarpSize : optBlockSize;\n        \n        // Launch the kernel.\n        launchKernelWithConfig(currentInput, currentOutput, numTests, threadsPerBlock, blocksPerGrid);\n        \n        // Copy the results back to host.\n        std::vector<unsigned long long> primalityResults_h(numTests, 0);\n        CUDA_CHECK(cudaMemcpyAsync(primalityResults_h.data(), currentOutput, testListSize, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Verify the results.\n        for (int i = 0; i < numTests; i++) {\n            assert(primalityResults_h[i] == expectedResults[listIndex][i]);\n        }\n    };\n    \n    // Process each test list.\n    for (int listIndex = 0; listIndex < NUMBER_OF_TESTS; listIndex++) {\n        runTest(listIndex, NUMBER_OF_ELEMENTS);\n    }\n    \n    // Cleanup device resources.\n    CUDA_CHECK(cudaFreeAsync(inputNumbers_d, stream));\n    CUDA_CHECK(cudaFreeAsync(primalityResults_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_millerRabin(unsigned long long* inputNumbers, unsigned long long* primalityResults, int numberOfElements) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/142", "date": "2025-07-30", "prompt": "Write a CUDA function using the Thrust library to remove integers from an array if they are divisible by any integer in another array. Use the functor ThrustTransformFunctor to do the divisibility checks within the thrust::transform function.\n\nThe signature of the functor is struct ThrustTransformFunctor {  __device__ char operator()(TransformInputParameters parameters); } where TransformInputParameters is the struct made of parameters for the divisibility checks, and the return value is 1 or 0 depending on the divisibility of selected element.\n\nThe signature of the function is int removeDivisibleElements(thrust::device_vector<int> & dividend_d, thrust::device_vector<int> & divisor_d, int numDividends, int numDivisors, Context & context), where dividend_d is a reference to the array of dividends in the device, divisor_d is a reference to the array of divisors in the device, numDividends is the number of elements in the array of dividends, numDivisors is the number of elements in the array of divisors, context is a reference to the thrust::tuple object that contains references to the temporary data used in the removeDivisibleElements function, and the return value is the number of retained elements in the dividend_d after the removal operation.\n\n>>> removeDivisibleElements({ 1000, 1001, 1002, ..., 1049 }, { 10, 11, 12, 13, 14 }, 50, 5, context) -> dividend_d: { 1002, 1003, 1004, 1005, 1006, 1007, 1009, 1011, 1013, 1015, 1016, 1017, 1018, 1019, 1021, 1024, 1025, 1026, 1028, 1029, 1031, 1033, 1035, 1037, 1038, 1039, 1041, 1042, 1043, 1046, 1047, 1048, 1049 }\n>>> removeDivisibleElements({ 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114 }, { 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 }, 15, 15, context) -> dividend_d: { 101, 103, 107, 109, 113 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <map>\n#include <cuda.h>\n#include <thrust/remove.h>\n#include <thrust/device_vector.h>\n#include <thrust/host_vector.h>\n#include <thrust/execution_policy.h>\n#include <thrust/system/cuda/vector.h>\n#include <thrust/iterator/iterator_categories.h>\n#include <thrust/iterator/constant_iterator.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\nconstexpr int MAXIMUM_NUMBER_OF_DIVIDENDS = 10000;\nconstexpr int MAXIMUM_NUMBER_OF_DIVISORS = 10000;\nconstexpr int NUM_EXAMPLES = 7;\n// The input parameters for thrust::transform for each work item will be the dividend array element, a pointer to the divisor array, and the number of divisors, allowing each dividend element to be tested against all divisors.\n// The divisors are constants for each work item of the transform..\nusing TransformInputParameters = thrust::tuple<int, int*, int>;\nusing ItemZip = thrust::zip_iterator<\n    thrust::tuple<\n        thrust::device_vector<int>::iterator,\n        thrust::constant_iterator<int*>,\n        thrust::constant_iterator<int>\n    >\n>;\n\n// This functor is utilized in the thrust::transform function to evaluate the divisibility of each element in the dividend array against all elements in the divisor array, producing the output for the thrust::remove_if function (as the stencil parameter).\nstruct ThrustTransformFunctor {\n    __device__ char operator()(TransformInputParameters parameters) {\n        int dividend = thrust::get<0>(parameters);\n        int * divisorList = thrust::get<1>(parameters);\n        int numDivisors = thrust::get<2>(parameters);\n        for (int i = 0; i < numDivisors; i++) {\n            // return 0: divisible by at least one integer in the divisor list.\n            if(dividend % divisorList[i] == 0) {\n                return (char)0;\n            }\n        }\n        // return 1: not divisible by any integer in the divisor list.\n        return (char)1;\n    }\n};\n// Thrust functions require a temporary array, stream, and a functor to operate efficiently and correctly. A tuple of references is used as a context to hold all necessary data for each example test.\nusing Context = thrust::tuple<  thrust::device_vector<char>&, \n                                cudaStream_t&, \n                                ThrustTransformFunctor&>;\n// Specifying indices for tuple element selection in this context.\nenum ContextParameter{\n    STENCIL = 0,\n    STREAM = 1,\n    TRANSFORM_FUNCTOR = 2\n};\n\n// Function to remove any integers from the dividend_h array that are divisible by any integer in the divisor_h array. It returns the number of retained, compacted elements after the removal.\nint removeDivisibleElements(thrust::device_vector<int> & dividend_d, \n                            thrust::device_vector<int> & divisor_d, \n                            int numDividends, int numDivisors, \n                            Context & context);\n\nvoid launch() {\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    // Functor used in the thrust::transform function.\n    ThrustTransformFunctor transformFunctor;\n\n    // Allocating the host arrays.\n    thrust::host_vector<int> dividend_h(MAXIMUM_NUMBER_OF_DIVIDENDS);\n    thrust::host_vector<int> divisor_h(MAXIMUM_NUMBER_OF_DIVISORS);\n    // Allocating the device arrays.\n    thrust::device_vector<int> dividend_d(MAXIMUM_NUMBER_OF_DIVIDENDS);\n    thrust::device_vector<int> divisor_d(MAXIMUM_NUMBER_OF_DIVISORS);\n    thrust::device_vector<char> stencil_d(MAXIMUM_NUMBER_OF_DIVIDENDS);\n    // Establishing the context that includes references to the temporary data for the divisibility tests.\n    Context context = thrust::tie(stencil_d, stream, transformFunctor);\n    \n    // Initializing test data.\n    int * testDividends = new int[NUM_EXAMPLES * MAXIMUM_NUMBER_OF_DIVIDENDS];\n    int * testDivisors = new int[NUM_EXAMPLES * MAXIMUM_NUMBER_OF_DIVISORS];\n    int * testNumberOfDividends = new int[NUM_EXAMPLES];\n    int * testNumberOfDivisors = new int[NUM_EXAMPLES];\n    \n    int testIndex = 0;\n    // Test 1: Dividends ranging from 1000 to 1050, divisors ranging from 10 to 15.\n    int dividendStart = 1000;\n    int divisorStart = 10;\n    testNumberOfDividends[testIndex] = 50;\n    testNumberOfDivisors[testIndex] = 5;\n    for(int i = 0; i < testNumberOfDividends[testIndex]; i++) {\n        testDividends[testIndex * MAXIMUM_NUMBER_OF_DIVIDENDS + i] = dividendStart + i;\n    }\n    for(int i = 0; i < testNumberOfDivisors[testIndex]; i++) {\n        testDivisors[testIndex * MAXIMUM_NUMBER_OF_DIVISORS + i] = divisorStart + i;\n    }\n    testIndex++;\n    // Test 2: Dividends ranging from 100 to 115, divisors ranging from 2 to 17.\n    dividendStart = 100;\n    divisorStart = 2;\n    testNumberOfDividends[testIndex] = 15;\n    testNumberOfDivisors[testIndex] = 15;\n    for(int i = 0; i < testNumberOfDividends[testIndex]; i++) {\n        testDividends[testIndex * MAXIMUM_NUMBER_OF_DIVIDENDS + i] = dividendStart + i;\n    }\n    for(int i = 0; i < testNumberOfDivisors[testIndex]; i++) {\n        testDivisors[testIndex * MAXIMUM_NUMBER_OF_DIVISORS + i] = divisorStart + i;\n    }\n    testIndex++;\n    // Test 3: Dividends have duplicates of 10000; divisors are 3 and 7.\n    dividendStart = 10000;\n    divisorStart = 7;\n    testNumberOfDividends[testIndex] = 5;\n    testNumberOfDivisors[testIndex] = 2;\n    for(int i = 0; i < testNumberOfDividends[testIndex]; i++) {\n        testDividends[testIndex * MAXIMUM_NUMBER_OF_DIVIDENDS + i] = dividendStart;\n    }\n    testDivisors[testIndex * MAXIMUM_NUMBER_OF_DIVISORS] = 3;\n    testDivisors[testIndex * MAXIMUM_NUMBER_OF_DIVISORS + 1] = divisorStart;\n    testIndex++;\n    // Test 4: Dividends ranging from 100000 to 100100, divisors ranging from 1000 to 1100\n    dividendStart = 100000;\n    divisorStart = 1000;\n    testNumberOfDividends[testIndex] = 100;\n    testNumberOfDivisors[testIndex] = 100;\n    for(int i = 0; i < testNumberOfDividends[testIndex]; i++) {\n        testDividends[testIndex * MAXIMUM_NUMBER_OF_DIVIDENDS + i] = i + dividendStart;\n    }\n    for(int i = 0; i < testNumberOfDivisors[testIndex]; i++) {\n        testDivisors[testIndex * MAXIMUM_NUMBER_OF_DIVISORS + i] = i + divisorStart;\n    }\n    testIndex++;\n    // Test 5: Dividends ranging from 100000000 to 100010000, divisors ranging from 1000 to 1100\n    dividendStart = 100000000;\n    divisorStart = 1000;\n    testNumberOfDividends[testIndex] = 10000;\n    testNumberOfDivisors[testIndex] = 100;\n    for(int i = 0; i < testNumberOfDividends[testIndex]; i++) {\n        testDividends[testIndex * MAXIMUM_NUMBER_OF_DIVIDENDS + i] = i + dividendStart;\n    }\n    for(int i = 0; i < testNumberOfDivisors[testIndex]; i++) {\n        testDivisors[testIndex * MAXIMUM_NUMBER_OF_DIVISORS + i] = i + divisorStart;\n    }\n    testIndex++;\n    // Test 6: Dividends ranging from 1000000000 to 1000010000, divisors ranging from 35 to 45\n    dividendStart = 1000000000;\n    divisorStart = 35;\n    testNumberOfDividends[testIndex] = 10000;\n    testNumberOfDivisors[testIndex] = 10;\n    for(int i = 0; i < testNumberOfDividends[testIndex]; i++) {\n        testDividends[testIndex * MAXIMUM_NUMBER_OF_DIVIDENDS + i] = i + dividendStart;\n    }\n    for(int i = 0; i < testNumberOfDivisors[testIndex]; i++) {\n        testDivisors[testIndex * MAXIMUM_NUMBER_OF_DIVISORS + i] = i + divisorStart;\n    }\n    testIndex++;\n    // Test 7: Dividends ranging from 5000 to 6000, divisors ranging from 1000 to 2000.\n    dividendStart = 5000;\n    divisorStart = 1000;\n    testNumberOfDividends[testIndex] = 1000;\n    testNumberOfDivisors[testIndex] = 1000;\n    for(int i = 0; i < testNumberOfDividends[testIndex]; i++) {\n        testDividends[testIndex * MAXIMUM_NUMBER_OF_DIVIDENDS + i] = i + dividendStart;\n    }\n    for(int i = 0; i < testNumberOfDivisors[testIndex]; i++) {\n        testDivisors[testIndex * MAXIMUM_NUMBER_OF_DIVISORS + i] = i + divisorStart;\n    }\n    testIndex++;\n    \n    int numExamplesPrepared = testIndex;\n    // Processing all tests.\n    for(testIndex = 0; testIndex < numExamplesPrepared; testIndex++)\n    {\n        int numDividends = testNumberOfDividends[testIndex];\n        int numDivisors = testNumberOfDivisors[testIndex];\n        for(int i = 0; i < numDividends; i++) {\n            dividend_h[i] = testDividends[testIndex * MAXIMUM_NUMBER_OF_DIVIDENDS + i];\n        }\n        for(int i = 0; i < numDivisors; i++) {\n            divisor_h[i] = testDivisors[testIndex * MAXIMUM_NUMBER_OF_DIVISORS + i];\n        }\n        \n        // Transferring inputs to the device asynchronously.\n        CUDA_CHECK(cudaMemcpyAsync( thrust::raw_pointer_cast(dividend_d.data()), \n                                    thrust::raw_pointer_cast(dividend_h.data()), \n                                    sizeof(int) * numDividends,\n                                    cudaMemcpyHostToDevice,\n                                    stream));\n        CUDA_CHECK(cudaMemcpyAsync( thrust::raw_pointer_cast(divisor_d.data()), \n                                    thrust::raw_pointer_cast(divisor_h.data()), \n                                    sizeof(int) * numDivisors, \n                                    cudaMemcpyHostToDevice, \n                                    stream));\n        \n        // Eliminating all elements in dividend_h that are divisible by any element in divisor_h asynchronously.\n        int numRetainedElements = removeDivisibleElements(  dividend_d, \n                                                            divisor_d, \n                                                            numDividends, \n                                                            numDivisors, \n                                                            context);\n        \n        // Copying results to the host in an asynchronous manner.\n        CUDA_CHECK(cudaMemcpyAsync( thrust::raw_pointer_cast(dividend_h.data()), \n                                    thrust::raw_pointer_cast(dividend_d.data()), \n                                    sizeof(int) * numRetainedElements, \n                                    cudaMemcpyDeviceToHost,\n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Verifying that all dividend elements are not divisible by any of the divisors.\n        for(int i = 0; i < numRetainedElements; i++) {\n            for(int j = 0; j < numDivisors; j++) {\n                assert((dividend_h[i] / divisor_h[j]) * divisor_h[j] != dividend_h[i]);\n            }\n        }\n    }\n\n    delete [] testDividends;\n    delete [] testDivisors;\n    delete [] testNumberOfDividends;\n    delete [] testNumberOfDivisors;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n\nint removeDivisibleElements(thrust::device_vector<int> & dividend_d, \n                            thrust::device_vector<int> & divisor_d, \n                            int numDividends, int numDivisors, \n                            Context & context) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/143", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute Lucas Kanade Sparse Optical Flow vectors for two consecutive single channel grayscale image frames, given a CUDA kernel which computes X, Y and time gradients, the kernel should compute the U and V optical flow vectors using the computed gradients.\n\n__global__ void k_opticalFlowKernel(float* x_gradient, float* y_gradient, float* t_gradient, int frame_width, int frame_height, int filter_size, float* u_vector, float* v_vector);\nWhere curr_frame is the data pointer to current frame, prev_frame is the data pointer to previous frame, frame_width is the width of the current/previous frame, frame_height is the height of the current/previous frame, filter_size is the size of the sliding window, x_gradient is the pointer to output X-axis gradient data of current frame, y_gradient is the pointer to output Y-axis gradient data of current frame, t_gradient is the pointer to output of difference of current frame and previous frame (time-gradient), u_vector and v_vector are the pointers to output velocity vectors.\n\n>>> k_opticalFlowKernel(float* x_gradient, float* y_gradient, float* t_gradient, 7, 5, 3, float* u_vector, float* v_vector) -> u_vector: {0.10,0.19,0.25,0.59,0.55,0.14,-0.52,0.81,0.32,-0.48,0.01,0.56,0.04,-0.44,0.72,0.34,0.11,0.16,0.33,-0.39,-0.65,0.18,0.05,-0.03,-0.47,-0.52,-0.89,-0.69,0.27,0.20,0.38,-0.16,-0.67,-0.97,-0.47,}, v_vector: {1.01,0.94,0.93,0.97,0.46,0.68,0.61,0.21,0.47,0.12,0.52,0.55,0.41,0.14,-0.15,0.06,0.15,0.52,0.62,-0.16,-1.01,-1.10,-1.15,-0.43,-0.35,-0.37,-0.73,-1.37,-0.48,-0.84,-0.54,-0.46,-0.56,-0.67,-1.22}\n\n>>> k_opticalFlowKernel(float* x_gradient, float* y_gradient, float* t_gradient, 7, 5, 5, float* u_vector, float* v_vector) -> u_vector: {0.50,0.43,0.11,0.03,-0.44,-0.60,-0.62,0.55,0.46,0.14,0.05,-0.38,-0.57,-0.53,0.38,0.35,0.17,0.02,-0.38,-0.48,-0.53,0.40,0.31,0.16,0.05,-0.23,-0.46,-0.54,0.26,0.25,0.17,0.11,-0.13,-0.25,-0.37}, v_vector: {0.38,0.56,0.56,0.59,0.45,0.63,0.53,0.38,0.47,0.46,0.52,0.40,0.64,0.65,0.06,0.10,0.08,0.06,0.05,0.08,0.17,-0.47,-0.32,-0.47,-0.50,-0.28,-0.08,0.07,-0.54,-0.45,-0.57,-0.52,-0.40,-0.32,-0.16}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime_api.h>\n#include <math.h>\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#include <time.h>\n#undef NDEBUG\n#include <assert.h>\n\n#define NUM_IMAGES 4\n#define NUM_WINDOWS 3\n#define EPSILON 1e-3\n#define TOLERANCE 1e-4\n#define GRADIENT_FILTER_SIZE 3\n#define DEFAULT_BLOCK_SIZE 4\n#define DIVIDE_BY_TWO 2\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n__global__ void k_gradientComputeKernel(float* curr_frame, float* prev_frame, int frame_width, int frame_height, float* x_gradient, float* y_gradient, float* t_gradient);\n__global__ void k_opticalFlowKernel(float* x_gradient, float* y_gradient, float* t_gradient, int frame_width, int frame_height, int filter_size, float* u_vector, float* v_vector);\n\nvoid launch() {\n\n    // Image filter sizes \n    int input_img_size[NUM_IMAGES][2] = { {7,5},{64,48},{220,127},{320,240} };\n    int window_sizes[NUM_WINDOWS] = { 3,5,7 };\n\n    // Allocating memory for largest image and filter on CPU & GPU \n    int max_width = 320;\n    int max_height = 240;\n\n    // CPU data pointers \n    float* curr_frame_h = (float*)malloc(max_width * max_height * sizeof(float));\n    float* prev_frame_h = (float*)malloc(max_width * max_height * sizeof(float));\n    float* x_gradient_h = (float*)malloc(max_width * max_height * sizeof(float));\n    float* y_gradient_h = (float*)malloc(max_width * max_height * sizeof(float));\n    float* t_gradient_h = (float*)malloc(max_width * max_height * sizeof(float));\n    float* u_output = (float*)malloc(max_width * max_height * sizeof(float));\n    float* v_output = (float*)malloc(max_width * max_height * sizeof(float));\n\n    // Use a CUDA stream for asynchronous operations \n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // GPU data pointers \n    float* curr_frame_d = nullptr;\n    float* prev_frame_d = nullptr;\n    float* x_gradient_d = nullptr;\n    float* y_gradient_d = nullptr;\n    float* t_gradient_d = nullptr;\n    float* u_output_d = nullptr;\n    float* v_output_d = nullptr;\n    float* u_output_h = (float*)malloc(max_width * max_height * sizeof(float));\n    float* v_output_h = (float*)malloc(max_width * max_height * sizeof(float));\n    CUDA_CHECK(cudaMallocAsync((void**)&curr_frame_d, max_width * max_height * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&prev_frame_d, max_width * max_height * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&x_gradient_d, max_width * max_height * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&y_gradient_d, max_width * max_height * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&t_gradient_d, max_width * max_height * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&u_output_d, max_width * max_height * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&v_output_d, max_width * max_height * sizeof(float), stream));\n\n    // Run test cases \n    srand(time(NULL));\n    for (int img_iter = 0; img_iter < NUM_IMAGES; img_iter++) {\n        for (int f_iter = 0; f_iter < NUM_WINDOWS; f_iter++) {\n\n            // Input and filter dimensions \n            int img_width = input_img_size[img_iter][0];\n            int img_height = input_img_size[img_iter][1];\n            int win_size = window_sizes[f_iter];\n\n            // Image data initialization \n            int max_integer_pixel_value = 256;\n            float max_float_pixel_value = 1.f;\n            float random_noise = ((rand() % 10) / 100.f);\n            for (int i = 0; i < img_width * img_height; i++) {\n                prev_frame_h[i] = (i % max_integer_pixel_value) / (float)max_integer_pixel_value;\n                float val = ((i % (max_integer_pixel_value / DIVIDE_BY_TWO)) + (rand() % (max_integer_pixel_value / DIVIDE_BY_TWO))) / (float)max_integer_pixel_value;\n                curr_frame_h[i] = fminf(val, max_float_pixel_value);\n            }\n\n            // CPU Optical Flow implementation \n\n            // Loop for X, Y and Time Gradient Computations \n            for (int row = 0; row < img_height; row++) {\n                for (int col = 0; col < img_width; col++) {\n\n                    // X-Gradient Computation {-1, 0, +1} \n                    if (col + 1 >= img_width)\n                        x_gradient_h[row * img_width + col] = -curr_frame_h[row * img_width + col - 1];\n                    else if (col - 1 < 0)\n                        x_gradient_h[row * img_width + col] = curr_frame_h[row * img_width + col + 1];\n                    else\n                        x_gradient_h[row * img_width + col] = curr_frame_h[row * img_width + col + 1] - curr_frame_h[row * img_width + col - 1];\n\n                    // Y-Gradient Computation {-1; 0; +1} \n                    if (row + 1 >= img_height)\n                        y_gradient_h[row * img_width + col] = -curr_frame_h[(row - 1) * img_width + col];\n                    else if (row - 1 < 0)\n                        y_gradient_h[row * img_width + col] = curr_frame_h[(row + 1) * img_width + col];\n                    else\n                        y_gradient_h[row * img_width + col] = curr_frame_h[(row + 1) * img_width + col] - curr_frame_h[(row - 1) * img_width + col];\n\n                    // Time-derivative Computation I(t) - I(t-1) \n                    t_gradient_h[row * img_width + col] = curr_frame_h[row * img_width + col] - prev_frame_h[row * img_width + col];\n                }\n            }\n\n            // Computing the velocity vectors [u;v] = inv(A'A) * (A'b)\n            // where, A'A = [window_sum(Ixx), window_sum(I_x_y) ; window_sum(I_x_y), window_sum(Iyy)]\n            // A'b = [window_sum(I_x_t); window_sum(I_y_t)]\n\n            int f_bounds = (int)(win_size / 2);\n            for (int row = 0; row < img_height; row++) {\n                for (int col = 0; col < img_width; col++) {\n\n                    // Initializing hessian components for the window \n                    float Ix_sq = 0; float Iy_sq = 0; float I_x_y = 0;\n                    float I_x_t = 0; float I_y_t = 0;\n\n                    // Computing components of Hessian Matrix on the window\n                    for (int frow = -f_bounds; frow <= f_bounds; frow++) {\n                        for (int fcol = -f_bounds; fcol <= f_bounds; fcol++) {\n                            // Image Boundary checks \n                            if ((col + fcol < 0) || (col + fcol >= img_width)\n                                || (row + frow < 0) || (row + frow >= img_height)) {\n                                Ix_sq += 0; Iy_sq += 0; I_x_y += 0; I_x_t += 0; I_y_t += 0;\n                            }\n                            else {\n                                int linIdx = (row + frow) * img_width + (col + fcol);\n                                Ix_sq += x_gradient_h[linIdx] * x_gradient_h[linIdx];\n                                Iy_sq += y_gradient_h[linIdx] * y_gradient_h[linIdx];\n                                I_x_y += x_gradient_h[linIdx] * y_gradient_h[linIdx];\n                                I_x_t += x_gradient_h[linIdx] * t_gradient_h[linIdx];\n                                I_y_t += y_gradient_h[linIdx] * t_gradient_h[linIdx];\n                            }\n                        }\n                    }\n\n                    // Computing final velocity vectors\n                    float scaling_factor = 1 / ((Ix_sq * Iy_sq - I_x_y * I_x_y) + EPSILON);\n                    u_output[row * img_width + col] = (Iy_sq * I_x_t - I_x_y * I_y_t) * scaling_factor;\n                    v_output[row * img_width + col] = (Ix_sq * I_y_t - I_x_y * I_x_t) * scaling_factor;\n                }\n            }\n\n            // GPU Implementation of Lucas-Kanade Optical Flow \n\n            // Host to device copy of image frames \n            CUDA_CHECK(cudaMemcpyAsync(prev_frame_d, prev_frame_h, img_width * img_height * sizeof(float), cudaMemcpyHostToDevice, stream));\n            CUDA_CHECK(cudaMemcpyAsync(curr_frame_d, curr_frame_h, img_width * img_height * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n            // Setting GPU Launch parameters using device properties \n            cudaDeviceProp prop;\n            int device;\n            cudaGetDevice(&device);\n            cudaGetDeviceProperties(&prop, device);\n            int block_size = DEFAULT_BLOCK_SIZE;\n\n            // Setting block-size according to the available shared-memory \n            int num_input_gradient_images = 3;\n            int minSharedMemoryRequired = num_input_gradient_images * (block_size + win_size - 1) * (block_size + win_size - 1) * sizeof(float);\n            while (minSharedMemoryRequired > prop.sharedMemPerBlock) {\n                block_size /= DIVIDE_BY_TWO;\n                minSharedMemoryRequired = num_input_gradient_images * (block_size + win_size - 1) * (block_size + win_size - 1) * sizeof(float);\n            }\n\n            // Block-size \n            dim3 blockDim(block_size, block_size);\n            blockDim.x = min(blockDim.x, prop.maxThreadsDim[0]);\n            blockDim.y = min(blockDim.y, prop.maxThreadsDim[1]);\n\n            // Grid-size \n            dim3 gridDim((img_width + block_size - 1) / block_size, (img_height + block_size - 1) / block_size);\n            gridDim.x = min(gridDim.x, prop.maxGridSize[0]);\n            gridDim.y = min(gridDim.y, prop.maxGridSize[1]);\n\n            // Gradient computation kernel launch with shared memory allocated for curr_frame. This is same as convolving with {-1,0,+1} and it's transpose with each image \n            int sharedMemoryBytes_kernel1 = (block_size + GRADIENT_FILTER_SIZE - 1) * (block_size + GRADIENT_FILTER_SIZE - 1) * sizeof(float);\n            void* grad_args[] = { &curr_frame_d, &prev_frame_d, &img_width, &img_height, &x_gradient_d, &y_gradient_d, &t_gradient_d };\n            CUDA_CHECK(cudaLaunchKernel((void*)k_gradientComputeKernel, gridDim, blockDim, grad_args, sharedMemoryBytes_kernel1, stream));\n\n            // Hessian and velocity vector computation kernel launch with shared memory allocated for X, Y and Time gradient data together \n            int sharedMemoryBytes_kernel2 = minSharedMemoryRequired;\n            void* args[] = { &x_gradient_d, &y_gradient_d, &t_gradient_d , &img_width, &img_height, &win_size, &u_output_d, &v_output_d };\n            CUDA_CHECK(cudaLaunchKernel((void*)k_opticalFlowKernel, gridDim, blockDim, args, sharedMemoryBytes_kernel2, stream));\n\n            // Device to Host copy of velocity vectors \n            CUDA_CHECK(cudaMemcpyAsync(u_output_h, u_output_d, img_width * img_height * sizeof(float), cudaMemcpyDeviceToHost, stream));\n            CUDA_CHECK(cudaMemcpyAsync(v_output_h, v_output_d, img_width * img_height * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n            // Synchronize call, since the async-memcpy call might return before the memcpy operation is completed.\n            cudaStreamSynchronize(stream);\n\n            // output verification \n            for (int row = 0; row < img_height; row++) {\n                for (int col = 0; col < img_width; col++) {\n                    assert(fabsf(u_output[row * img_width + col] - u_output_h[row * img_width + col]) < TOLERANCE);\n                    assert(fabsf(v_output[row * img_width + col] - v_output_h[row * img_width + col]) < TOLERANCE);\n                }\n            }\n        }\n    }\n\n    // Free CPU Heap\n    free(curr_frame_h);\n    free(prev_frame_h);\n    free(x_gradient_h);\n    free(y_gradient_h);\n    free(t_gradient_h);\n    free(u_output);\n    free(v_output);\n    free(u_output_h);\n    free(v_output_h);\n\n    // Free GPU data\n    cudaFreeAsync(curr_frame_d, stream);\n    cudaFreeAsync(prev_frame_d, stream);\n    cudaFreeAsync(x_gradient_d, stream);\n    cudaFreeAsync(y_gradient_d, stream);\n    cudaFreeAsync(t_gradient_d, stream);\n    cudaFreeAsync(u_output_d, stream);\n    cudaFreeAsync(v_output_d, stream);\n    cudaStreamDestroy(stream);\n}\n\n__global__ void k_gradientComputeKernel(float* curr_frame, float* prev_frame, int frame_width, int frame_height, float* x_gradient, float* y_gradient, float* t_gradient) {\n\n    // Allocating shared memory of size (BLOCK_SIZE + (filter_size/2) - 1)*(BLOCK_SIZE + (filter_size/2) - 1), where gradient filter size is 3 {-1,0,+1} & {-1;0;+1} \n    int block_size = blockDim.x;\n    extern __shared__ float sharedMem[];\n    int ltx = threadIdx.x;\n    int lty = threadIdx.y;\n    int sharedMem_stride = block_size + GRADIENT_FILTER_SIZE - 1;\n\n    // Computing Gradients \n    int gtx = blockIdx.x * block_size + ltx;\n    int gty = blockIdx.y * block_size + lty;\n\n    // Computing the overall bounds inclusive of shared memory loading\n    int row_bound = max(frame_height + GRADIENT_FILTER_SIZE - 1, block_size);\n    int col_bound = max(frame_width + GRADIENT_FILTER_SIZE - 1, block_size);\n\n    for (int row = gty; row < row_bound; row += gridDim.y * blockDim.y) {\n        for (int col = gtx; col < col_bound; col += gridDim.x * blockDim.x) {\n\n            // Computing offsets needed to fill shared memory while iterating through grid-stride loops\n            int rowOffset = row - gty;\n            int colOffset = col - gtx;\n\n            // Loading current frame data (curr_frame) into shared memory as the data is used repeatedly by consecutive threads \n            for (int srow = lty; srow < sharedMem_stride; srow += block_size) {\n                for (int scol = ltx; scol < sharedMem_stride; scol += block_size) {\n\n                    int row_i = blockIdx.y * block_size + srow - 1 + rowOffset;\n                    int col_i = blockIdx.x * block_size + scol - 1 + colOffset;\n\n                    if (row_i >= 0 && row_i < frame_height && col_i >= 0 && col_i < frame_width) {\n                        sharedMem[srow * sharedMem_stride + scol] = curr_frame[row_i * frame_width + col_i];\n                    }\n                    else {\n                        sharedMem[srow * sharedMem_stride + scol] = 0.0f;\n                    }\n                }\n            }\n            __syncthreads();\n\n            // Only threads under the image boundaries are allowed to enter this conditional code.\n            if ((row < frame_height) && (col < frame_width)) {\n\n                // Re-Pointing the curr_frame data with an offset equivalent to floor(filter_size/2) for readable array indexes \n                int startIdx = 1;\n                float* curr_frame_sharedMem = &sharedMem[startIdx * sharedMem_stride + startIdx];\n                x_gradient[row * frame_width + col] = curr_frame_sharedMem[lty * sharedMem_stride + ltx + 1] - curr_frame_sharedMem[lty * sharedMem_stride + ltx - 1];\n                y_gradient[row * frame_width + col] = curr_frame_sharedMem[(lty + 1) * sharedMem_stride + ltx] - curr_frame_sharedMem[(lty - 1) * sharedMem_stride + ltx];\n                t_gradient[row * frame_width + col] = curr_frame_sharedMem[lty * sharedMem_stride + ltx] - prev_frame[row * frame_width + col];\n            }\n        }\n    }\n}\n\n\n__global__ void k_opticalFlowKernel(float* x_gradient, float* y_gradient, float* t_gradient, int frame_width, int frame_height, int filter_size, float* u_vector, float* v_vector) {\n\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/144", "date": "2025-07-30", "prompt": "Write a cublas function to solve linear equations such as Ax = B using the matrix conjugate gradient method. The function should perform iterative computations to find the optimal solution vector x. Assume that the maximum number of iterations to find the optimal solution is pre-defined as MAX_ITERATIONS equals 1000.  \n\nThe signature of the function is k_conjugateGradientKernel(cublasHandle_t cublasHandle, int matrixSize, double* residual_d, double* matrixA_d, double* searchDir_d, double* vectorX_d, double* productAdir_d). Where cublasHandle is the handle to cuBLAS library context,  matrixSize is the size of the square matrix A, \nresidual_d is the pointer to the residual vector in device memory, residual vector represents the difference between the current approximation of Ax and the vector B, matrixA_d is the pointer to square matrix A stored in device memory, searchDir_d is the pointer to the direction in which the algorithm searches for a minimum of the quadratic function,  vectorX_d is the pointer to output vector X stored in device memory, and productAdir_d is the pointer to the product of matrix A and the search direction vector. \n\n>>> k_conjugateGradientKernel(cublasHandle, 2, {1.0, 2.0}, {4.0, 1.0, 1.0, 3.0}, searchDir_d, vectorX_d, productAdir_d)-> vectorX_d: {0.09091, 0.63636} \n>>> k_conjugateGradientKernel(cublasHandle, 3, {10.0, 40.0, 90.0}, {10.0, 0.0, 0.0, 0.0, 20.0, 0.0, 0.0, 0.0, 30.0}, searchDir_d, vectorX_d, productAdir_d)-> vectorX_d: {1.0, 2.0, 3.0} \n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "-lcublas", "declaration": "#include <cuda_runtime.h>   \n#include <cublas_v2.h>\n#include <vector>\n#include <cstdio>\n#undef NDEBUG\n#include <assert.h>\n\n#define CUDA_CHECK(call) \\\ndo {                                                                                                         \\\n        cudaError_t error = call;                                                                            \\\n        if (error != cudaSuccess) {                                                                          \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\", __FILE__, __LINE__, cudaGetErrorString(error));    \\\n            exit(EXIT_FAILURE);                                                                              \\\n        }                                                                                                    \\\n} while (0)\n\n#define CUBLAS_CHECK(call)                                            \\\n    {                                                                 \\\n        cublasStatus_t status = call;                                 \\\n        if (status != CUBLAS_STATUS_SUCCESS)                          \\\n        {                                                             \\\n            fprintf(stderr, \"cuBLAS Error: %d, line %d\\n\", status, __LINE__); \\\n            exit(EXIT_FAILURE);                                       \\\n        }                                                             \\\n    }\n\n#define TOLERANCE 1e-3      // Tolerance for floating-point comparisons\n#define MAX_ITERATIONS 1000 // Maximum iterations for Conjugate Gradient\n\n// Structure to define a test case\nstruct TestCase {\n    int matrixSize;                      // Size of the square matrix (matrixSize x matrixSize)\n    std::vector<double> matrixA;         // Matrix matrixA in row-major order (matrixSize matrixSize x matrixSize)\n    std::vector<double> vectorB;         // Vector vectorB (matrixSize matrixSize)\n    std::vector<double> expectedVectorX; // Expected solution vector x (matrixSize matrixSize)\n};\n\nvoid k_conjugateGradientKernel(cublasHandle_t cublasHandle, int matrixSize, double* residual_d, double* matrixA_d, double* searchDir_d, double* vectorX_d, double* productAdir_d);\n\nvoid launch() {\n    // Initialize cuBLAS\n    cublasHandle_t cublasHandle;\n    CUBLAS_CHECK(cublasCreate(&cublasHandle));\n\n    // Define all test cases\n    std::vector<TestCase> testCases;\n\n    // Test Case 1: Simple 2x2 Matrix\n    testCases.push_back({\n        2,                 //matrix order\n        {4.0, 1.0,\n         1.0, 3.0},        // matrixA (Row-Major)\n        {1.0, 2.0},        // vectorB\n        {0.09091, 0.63636} // Expected x\n    });\n\n    // Test Case 2: Diagonal Matrix\n    testCases.push_back({\n        3,                 //matrix order\n        {10.0, 0.0, 0.0,\n         0.0, 20.0, 0.0,\n         0.0, 0.0, 30.0},   // matrixA (Row-Major)\n        {10.0, 40.0, 90.0}, // vectorB\n        {1.0, 2.0, 3.0}     // Expected x\n    });\n\n    // Test Case 3: Symmetric Positive-Definite Matrix\n    testCases.push_back({\n        3,                          //matrix order\n        {6.0, 2.0, 1.0,\n         2.0, 5.0, 2.0,\n         1.0, 2.0, 4.0},            // matrixA (Row-Major)\n        {14.0, 18.0, 17.0},         // vectorB\n        {1.19277, 1.92771, 2.98795} // Expected x (corrected)\n    });\n\n    // Test Case 4: Matrix with Negative Entries\n    testCases.push_back({\n        2,                 //matrix order\n        {4.0, -1.0,\n         -1.0, 3.0},       // matrixA (Row-Major)\n        {1.0, 2.0},        // vectorB\n        {0.45455, 0.81818} // Expected x (corrected)\n    });\n\n    // Test Case 5: Identity Matrix\n    testCases.push_back({\n        4,                      //matrix order\n        {1.0, 0.0, 0.0, 0.0,\n         0.0, 1.0, 0.0, 0.0,\n         0.0, 0.0, 1.0, 0.0,\n         0.0, 0.0, 0.0, 1.0},    // matrixA (Row-Major)\n        {5.0, 10.0, 15.0, 20.0}, // vectorB\n        {5.0, 10.0, 15.0, 20.0}  // Expected x\n    });\n\n    // Test Case 6: All Ones Matrix\n    testCases.push_back({\n        3,               //matrix order \n        {2.0, 1.0, 1.0,\n         1.0, 2.0, 1.0,\n         1.0, 1.0, 2.0}, // matrixA (Row-Major)\n        {6.0, 6.0, 6.0}, // vectorB\n        {1.5, 1.5, 1.5}  // Expected x (corrected)\n    });\n\n    // Test Case 7: Larger Symmetric Matrix\n    testCases.push_back({\n        5,                                           //matrix order\n        {6.0, 2.0, 1.0, 0.0, 0.0,\n         2.0, 5.0, 2.0, 0.0, 0.0,\n         1.0, 2.0, 4.0, 1.0, 0.0,\n         0.0, 0.0, 1.0, 3.0, 1.0,\n         0.0, 0.0, 0.0, 1.0, 2.0},                    // matrixA (Row-Major)\n        {14.0, 18.0, 17.0, 12.0, 8.0},                // vectorB\n        {1.22039, 2.20386, 2.26995, 2.29202, 2.85399} // Expected x (corrected)\n    });\n\n    // Test Case 8: Near-Zero Diagonal Matrix\n    testCases.push_back({\n        3,                  //matrix order  \n        {1e-6, 0.0, 0.0,\n         0.0, 1e-6, 0.0,\n         0.0, 0.0, 1e-6},   // matrixA (Row-Major)\n        {1e-6, 2e-6, 3e-6}, // vectorB\n        {1.0, 2.0, 3.0}     // Expected x\n    });\n\n    // Test Case 9: Matrix with Repeated Eigenvalues\n    testCases.push_back({\n        2,          //matrix order\n        {5.0, 4.0,\n         4.0, 5.0}, // matrixA (Row-Major)\n        {9.0, 9.0}, // vectorB\n        {1.0, 1.0}  // Expected x\n    });\n\n    // Test Case 10: Symmetric Positive-Definite Matrix\n    testCases.push_back({\n        2,          //matrix order\n        {2.0, 1.0,\n         1.0, 3.0}, // matrixA (Row-Major)\n        {1.0, 2.0}, // vectorB\n        {0.2, 0.6}  // Expected x (corrected)\n    });\n\n    // Create a CUDA stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    size_t maxMatrixSize = 5;    \n\n    // Allocate device memory\n    double *matrixA_d, *vectorX_d, *residual_d, *searchDir_d, *productAdir_d;\n    CUDA_CHECK(cudaMallocAsync((void **)&matrixA_d, maxMatrixSize * maxMatrixSize * sizeof(double), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&vectorX_d, maxMatrixSize * sizeof(double), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&residual_d, maxMatrixSize * sizeof(double), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&searchDir_d, maxMatrixSize * sizeof(double), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&productAdir_d, maxMatrixSize * sizeof(double), stream));\n        \n    // Run all test cases\n    for (const auto &currTestCase : testCases) {\n        int matrixSize = currTestCase.matrixSize;\n\n        // Initialize vectors\n        std::vector<double> vectorX(matrixSize, 0.0);         // Initial guess vector (all zeros)\n        std::vector<double> residualVector(matrixSize, 0.0);  // Residual vector\n        std::vector<double> searchDirection(matrixSize, 0.0); // Search direction\n        std::vector<double> productAdir(matrixSize, 0.0);     // matrixA * searchDirection\n\n        // Convert matrixA from row-major to column-major\n        std::vector<double> columnMajorA(matrixSize * matrixSize, 0.0);\n        for (int i = 0; i < matrixSize; i++) { // Iterate over rows\n            for (int j = 0; j < matrixSize; j++) { // Iterate over columns\n                columnMajorA[j * matrixSize + i] = currTestCase.matrixA[i * matrixSize + j];\n            }\n        }\n\n        // Copy matrix matrixA and vector vectorB to device\n        CUDA_CHECK(cudaMemcpyAsync(matrixA_d, columnMajorA.data(), matrixSize * matrixSize * sizeof(double), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(residual_d, currTestCase.vectorB.data(), matrixSize * sizeof(double), cudaMemcpyHostToDevice, stream));\n\n        // Initialize vectorX to zeros on both host and device\n        std::fill(vectorX.begin(), vectorX.end(), 0.0);\n        CUDA_CHECK(cudaMemcpyAsync(vectorX_d, vectorX.data(), matrixSize * sizeof(double), cudaMemcpyHostToDevice, stream));\n\n        // residualVector = vectorB - matrixA * vectorX = vectorB (since vectorX is initialized to zero)\n        // searchDirection = residualVector\n        CUDA_CHECK(cudaMemcpyAsync(searchDir_d, residual_d, matrixSize * sizeof(double), cudaMemcpyDeviceToDevice, stream));\n\n        // Bind the cuBLAS handle to stream\n        CUBLAS_CHECK(cublasSetStream(cublasHandle, stream));\n\n        k_conjugateGradientKernel(cublasHandle, matrixSize, residual_d, matrixA_d, searchDir_d, vectorX_d, productAdir_d);\n\n        // Copy solution vectorX back to host\n        CUDA_CHECK(cudaMemcpyAsync(vectorX.data(), vectorX_d, matrixSize * sizeof(double), cudaMemcpyDeviceToHost, stream));\n\n        // Compute CPU solution for verification using CPU Conjugate Gradient\n        std::vector<double> vectorXCpu(matrixSize, 0.0);     // Initial guess\n        std::vector<double> vectorRCpu = currTestCase.vectorB;         // residualVector = vectorB - matrixA * vectorX = vectorB\n        std::vector<double> searchDirCpu = vectorRCpu;       // searchDirection = residualVector\n        std::vector<double> productADirCpu(matrixSize, 0.0); // matrixA * searchDirection\n\n        double oldResidualCpu = 0.0;\n        for (int i = 0; i < matrixSize; i++) {\n            oldResidualCpu += vectorRCpu[i] * vectorRCpu[i];\n        }\n\n        double newResidualCpu, aphaCpu, betaCpu;\n\n        int iteratorCpu = 0;\n        while (iteratorCpu < MAX_ITERATIONS) {\n            // productADirCpu = matrixA * searchDirCpu\n            for (int i = 0; i < matrixSize; i++) {\n                productADirCpu[i] = 0.0;\n                for (int j = 0; j < matrixSize; j++) {\n                    productADirCpu[i] += currTestCase.matrixA[i * matrixSize + j] * searchDirCpu[j];\n                }\n            }\n\n            // Compute aphaCpu = oldResidualCpu / (searchDirCpu^T * productADirCpu)\n            double pAp_cpu = 0.0;\n            for (int i = 0; i < matrixSize; i++) {\n                pAp_cpu += searchDirCpu[i] * productADirCpu[i];\n            }\n\n            if (pAp_cpu == 0.0) {\n                break; // \"CPU: Encountered zero denominator in alpha computation.\"\n            }\n            aphaCpu = oldResidualCpu / pAp_cpu;\n\n            // Update vectorXCpu = vectorXCpu + aphaCpu * searchDirCpu\n            for (int i = 0; i < matrixSize; i++) {\n                vectorXCpu[i] += aphaCpu * searchDirCpu[i];\n            }\n\n            // Update vectorRCpu = vectorRCpu - aphaCpu * productADirCpu\n            for (int i = 0; i < matrixSize; i++) {\n                vectorRCpu[i] -= aphaCpu * productADirCpu[i];\n            }\n\n            // Compute newResidualCpu = vectorRCpu^T * vectorRCpu\n            newResidualCpu = 0.0;\n            for (int i = 0; i < matrixSize; i++) {\n                newResidualCpu += vectorRCpu[i] * vectorRCpu[i];\n            }\n\n            // Check for convergence\n            if (std::sqrt(newResidualCpu) < TOLERANCE) {\n                break;\n            }\n\n            // Compute betaCpu = newResidualCpu / oldResidualCpu\n            betaCpu = newResidualCpu / oldResidualCpu;\n\n            // Update searchDirCpu = vectorRCpu + betaCpu * searchDirCpu\n            for (int i = 0; i < matrixSize; i++) {\n                searchDirCpu[i] = vectorRCpu[i] + betaCpu * searchDirCpu[i];\n            }\n\n            // Update oldResidualCpu for next iteration\n            oldResidualCpu = newResidualCpu;\n            iteratorCpu++;\n        }\n\n        // Compare GPU solution with expected solution\n        bool solutionMatched = true;\n        for (int i = 0; i < matrixSize; i++) {\n            if (std::fabs(vectorX[i] - currTestCase.expectedVectorX[i]) > TOLERANCE) {\n                solutionMatched = false;\n            }\n        }\n\n        // Compare GPU solution with CPU solution\n        bool cpuGpuMatched = true;\n        for (int i = 0; i < matrixSize; i++) {\n            if (std::fabs(vectorX[i] - vectorXCpu[i]) > TOLERANCE) {\n                cpuGpuMatched = false;\n            }\n        }\n\n        // Handle the case where the solution is close but not exact due to floating-point precision\n        if (!solutionMatched) {\n            // Calculate relative error\n            bool relativeErrorOk = true;\n            for (int i = 0; i < matrixSize; i++) {\n                double expected = currTestCase.expectedVectorX[i];\n                double computed = vectorX[i];\n                double relativeError = (expected != 0.0) ? std::fabs((computed - expected) / expected) : std::fabs(computed);\n                if (relativeError > TOLERANCE) {\n                    relativeErrorOk = false;\n                }\n            }\n            assert(relativeErrorOk == true);\n        }\n\n        // Additionally, compare CPU and GPU solutions\n        if (cpuGpuMatched == false) {\n            assert(cpuGpuMatched);\n        }\n        \n    }\n\n    // Free device memory\n    CUDA_CHECK(cudaFreeAsync(matrixA_d, stream));\n    CUDA_CHECK(cudaFreeAsync(vectorX_d, stream));\n    CUDA_CHECK(cudaFreeAsync(residual_d, stream));\n    CUDA_CHECK(cudaFreeAsync(searchDir_d, stream));\n    CUDA_CHECK(cudaFreeAsync(productAdir_d, stream));\n\n    // Destroy the CUDA stream\n    CUDA_CHECK(cudaStreamDestroy(stream));\n\n    // Synchronize the device to ensure all operations are complete\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    // Destroy cuBLAS cublasHandle\n    CUBLAS_CHECK(cublasDestroy(cublasHandle));\n}\nvoid k_conjugateGradientKernel(cublasHandle_t cublasHandle, int matrixSize, double* residual_d, double* matrixA_d, double* searchDir_d, double* vectorX_d, double* productAdir_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/145", "date": "2025-07-30", "prompt": "Implement a CUDA-based linear regression function that calculates the intercept and slope for a simple linear model (y =  + x) using cuBLAS operations. The function should return return the coefficients through the provided device memory pointers.\n\nThe signature of the function is float* linearRegression(float* x_d, float* y_d, int n, cublasHandle_t cublasHandle), x_d: device pointer to x values, y_d: device pointer to y values, n: number of data points, cublasHandle: cuBLAS library handle.\n\n>>> linearRegression({1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f}, {2.0f, 4.0f, 6.0f, 8.0f, 10.0f, 12.0f, 14.0f, 16.0f, 18.0f, 20.0f}, 10, beta_d, XtX_d, Xty_d, ones_d) -> beta: {22.0f, -2.0f}\n>>> linearRegression({1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f},{3.0f, 5.0f, 7.0f, 9.0f, 11.0f, 13.0f, 15.0f, 17.0f, 19.0f, 21.0f}, 10, beta_d, XtX_d, Xty_d, ones_d) -> beta: {23.0f, -2.0f}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "-lcublas", "declaration": "\n#include <cstdio>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n\n#define TOLERANCE 1E-1\n\n#define MAX_VALUE 20\n\n// Macro for CUDA error checking\n#define CUDA_CHECK(call)                                                                 \\\n    {                                                                                    \\\n        cudaError_t err = call;                                                          \\\n        if (err != cudaSuccess) {                                                        \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\", __FILE__, __LINE__,            \\\n                    cudaGetErrorString(err));                                            \\\n            exit(err);                                                                   \\\n        }                                                                                \\\n    }\n\n#define CHECK_CUBLAS(call) { cublasStatus_t status = call; if(status != CUBLAS_STATUS_SUCCESS) { printf(\"cuBLAS Error: %d, line %d\\n\", status, __LINE__); } }\n\nfloat* linearRegression(float* x_d, float* y_d, int n, \n                     cublasHandle_t cublasHandle);\n\n#undef NDEBUG\n#include <assert.h>\n// Function to launch linear regression with all test cases\nint launch() {\n    const int MAX_N = 1000;\n    const int NUM_TESTS = 7;\n    \n    // Create CUDA stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Create cuBLAS handle\n    cublasHandle_t cublasHandle;\n    CHECK_CUBLAS(cublasCreate(&cublasHandle));\n    \n    // Set stream\n    CHECK_CUBLAS(cublasSetStream(cublasHandle, stream));\n    \n    // Allocate all device memory once\n    float *x_d = NULL, *y_d = NULL;\n    \n    // Allocate memory for input/output data\n    CUDA_CHECK(cudaMallocAsync(&x_d, MAX_N * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&y_d, MAX_N * sizeof(float), stream));    \n    \n    // Host memory for results\n    float beta[2] = {0.0f, 0.0f};\n    \n    struct TestCase {\n        int n;\n        float expected_intercept;\n        float expected_slope;\n        float x[MAX_VALUE]; \n        float y[MAX_VALUE]; \n    };\n    \n    \n    // Test case 1: Perfect linear relationship (y = 2x)\n    TestCase test1 = {\n        10,\n        22.0f, -2.0f,\n        {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f},\n        {2.0f, 4.0f, 6.0f, 8.0f, 10.0f, 12.0f, 14.0f, 16.0f, 18.0f, 20.0f}\n    };\n    \n    // Test case 2: Linear with intercept (y = 1 + 2x)\n    TestCase test2 = {\n        10,\n        23.0f, -2.0f,\n        {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f},\n        {3.0f, 5.0f, 7.0f, 9.0f, 11.0f, 13.0f, 15.0f, 17.0f, 19.0f, 21.0f}\n    };\n    \n    // Test case 3: Negative slope (y = 12 - 2x)\n    TestCase test3 = {\n        10,\n        -20.667f, 4.667f,\n        {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f},\n        {10.0f, 8.0f, 6.0f, 4.0f, 2.0f, 0.0f, -2.0f, -4.0f, -6.0f, -8.0f}\n    };\n    \n    // Test case 4: Larger dataset with slight noise (y  3 + 1.5x)\n    TestCase test4 = {\n        15,\n        27.0f, -1.5f,\n        {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f, 11.0f, 12.0f, 13.0f, 14.0f, 15.0f},\n        {4.6f, 5.9f, 7.4f, 9.1f, 10.5f, 12.2f, 13.4f, 15.1f, 16.7f, 18.0f, 19.4f, 21.1f, 22.6f, 23.9f, 25.3f}\n    };\n    \n    // Test case 5: Zero slope (y = 7)\n    TestCase test5 = {\n        10,\n        7.0f, 0.0f,\n        {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f},\n        {7.0f, 7.0f, 7.0f, 7.0f, 7.0f, 7.0f, 7.0f, 7.0f, 7.0f, 7.0f}\n    };\n    \n    // Test case 6: Quadratic relationship approximated by linear (y  0.5x)\n    TestCase test6 = {\n        10,\n        49.5f, -5.5f,\n        {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f},\n        {0.5f, 2.0f, 4.5f, 8.0f, 12.5f, 18.0f, 24.5f, 32.0f, 40.5f, 50.0f}\n    };\n    \n    // Test case 7: Linear relationship (y = 3x+2)\n    TestCase test7 = {\n        10,\n        35.0f, -3.0f,\n      {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f},\n      {5.0f, 8.0f, 11.0f, 14.0f, 17.0f, 20.0f, 23.0f, 26.0f, 29.0f, 32.0f}\n    };\n    \n    // Array of test cases\n    TestCase* testCases[NUM_TESTS] = {&test1, &test2, &test3, &test4, &test5, &test6, &test7};\n    \n    // Run all test cases\n    for (int t = 0; t < NUM_TESTS; t++) {\n        TestCase* test = testCases[t];\n        \n        // Reset beta\n        beta[0] = 0.0f;\n        beta[1] = 0.0f;\n        \n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(x_d, test->x, test->n * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(y_d, test->y, test->n * sizeof(float), cudaMemcpyHostToDevice, stream));\n        \n        // Perform regression\n        float* result = linearRegression(x_d, y_d, test->n,\n                                     cublasHandle);\n        // Copy the results to beta array\n        beta[0] = result[0];\n        beta[1] = result[1];\n        // Copy result back to host\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        assert(fabs(beta[0] - test->expected_intercept) < TOLERANCE && fabs(beta[1] - test->expected_slope) < TOLERANCE); \n    }\n    \n    // Free all device memory\n    CUDA_CHECK(cudaFreeAsync(x_d, stream));\n    CUDA_CHECK(cudaFreeAsync(y_d, stream));\n    \n    // Destroy handle and stream\n    CHECK_CUBLAS(cublasDestroy(cublasHandle));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    \n    return 0;\n}\n\nfloat* linearRegression(float* x_d, float* y_d, int n, cublasHandle_t cublasHandle) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/146", "date": "2025-07-30", "prompt": "Implement a CUDA function to find the kth smallest element in an array of floats. Use Thrust libraries sort functionality to find the kth smallest element. \n\nThe signature of the function is float findKthSmallest(thrust::device_vector<float>& data_d, int k, thrust::device_vector<float>& buffer_d, cudaStream_t& cudaStream), where data_d is the device vector containing the input data, k is the position of the element to find with 1-based indexing (meaning the count starts from 1), buffer_d is a device vector used as temporary working storage, cudaStream is the CUDA stream for executing operations asynchronously.\n\n>>> findKthSmallest({2.0f, 7.0f, 11.0f, 15.0f}, 3, buffer_d, cudaStream) -> 11.0f (3rd smallest element)\n>>> findKthSmallest({3.0f, 2.0f, 4.0f}, 2, buffer_d, cudaStream) -> 3.0f (2nd smallest element)\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <cstdio>\n#include <map>\n#include <random>\n#include <cuda.h>\n#include <algorithm>\n#include <thrust/partition.h>\n#include <thrust/sort.h>\n#include <thrust/device_vector.h>\n#include <thrust/host_vector.h>\n#include <thrust/execution_policy.h>\n#include <thrust/system/cuda/vector.h>\nusing namespace std;\n\n#define CUDA_CHECK(call) {                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\nfloat findKthSmallest(thrust::device_vector<float>& data_d, int k, \n                      thrust::device_vector<float>& buffer_d,\n                      cudaStream_t& cudaStream);\n\nvoid launch() {\n    cudaStream_t cudaStream;\n    CUDA_CHECK(cudaStreamCreate(&cudaStream));\n    \n    // Define test cases with input array and target k value\n    struct TestCase {\n        vector<float> input;\n        int k;\n    };\n    \n    vector<TestCase> testCases = {\n        { {2.0f, 7.0f, 11.0f, 15.0f}, 2 },\n        { {3.0f, 2.0f, 4.0f}, 1 },\n        { {1.0f, 2.0f, 3.0f}, 3 },\n        { {3.0f, 3.0f, 4.0f, 5.0f}, 4 },\n        { {5.0f, 75.0f, 25.0f, 50.0f}, 2 },\n        { {1.0f, 2.0f, 3.0f, 4.0f}, 1 }\n    };\n    \n    // Add one large test case with random values \n    vector<float> largeInput(10000);\n    std::mt19937 randomGenerator(static_cast<unsigned int>(std::time(nullptr))); \n    std::uniform_real_distribution<float> randomDist(1.0f, 10000.0f);\n        \n    for (int i = 0; i < 10000; i++) {\n        largeInput[i] = randomDist(randomGenerator); // Generate random values between 1.0f and 10000.0f\n    }\n    testCases.push_back({largeInput, 1999});\n    \n    for (int testIdx = 0; testIdx < testCases.size(); testIdx++) {\n        const auto& currentTest = testCases[testIdx];\n        const int arraySize = currentTest.input.size();\n        const int kValue = currentTest.k;\n        \n        // Skip test cases where k is out of bounds\n        if (kValue > arraySize) {\n            continue;\n        }\n        \n        // Allocate host array\n        thrust::host_vector<float> data_h(currentTest.input.begin(), currentTest.input.end());\n        \n        // Allocate device arrays\n        thrust::device_vector<float> data_d(arraySize);\n        thrust::device_vector<float> buffer_d(arraySize);\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(thrust::raw_pointer_cast(data_d.data()),\n                                   thrust::raw_pointer_cast(data_h.data()),\n                                   sizeof(float) * arraySize,\n                                   cudaMemcpyHostToDevice,\n                                   cudaStream));\n        // Find kth smallest by passing individual parameters\n        float kthSmallest = findKthSmallest(data_d, kValue, \n                                           buffer_d, cudaStream);\n        \n        // Create a sorted copy for verification\n        thrust::host_vector<float> sortedData = data_h;\n        std::sort(sortedData.begin(), sortedData.end());\n        \n        // Verify result\n        assert(fabs(kthSmallest - sortedData[kValue-1]) < 0.0001f);\n    }\n    \n    // Clean up stream\n    CUDA_CHECK(cudaStreamDestroy(cudaStream));\n}\n\nfloat findKthSmallest(thrust::device_vector<float>& data_d, int k,\n                    thrust::device_vector<float>& buffer_d,\n                    cudaStream_t& cudaStream) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/147", "date": "2025-07-30", "prompt": "Write a function to compute the cumulative distribution function (CDF) of the grayscale images using the CUDA Thrust library. The pixel values of the image range from (0-255). The function should sort the pixel values, calculate their histogram, derive the CDF, and then map each pixel to its CDF.\n\nThe signature of the function is void calculatePixelCdf(thrust::device_vector<int> &srcImage, int size, cudaStream_t stream, thrust::device_vector<float> &output), where srcImage is a reference to input image matrix of type thurst device vector, size is total pixels in the input image, the stream is the CUDA stream for asynchronous execution and output is reference to CDF of each pixel of type thurst device vector.\n\n>>>calculatePixelCdf({61, 112, 91, 176, 134, 154, 32, 68, 84, 163, 31, 58, 135, 162, 163, 52}, 16, stream, *output) -> output: ({0.3125, 0.5625, 0.5, 1, 0.625 , 0.75  , 0.125 , 0.375 , 0.4375, 0.9375, 0.0625, 0.25  , 0.6875, 0.8125, 0.9375, 0.1875})\n>>>calculatePixelCdf({18, 171, 45, 34, 239, 61, 73, 52, 82, 57, 245, 228, 102, 31, 205, 193}, 16, stream, *output) -> output: ({0.0625, 0.6875, 0.25  , 0.1875, 0.9375, 0.4375, 0.5   , 0.3125, 0.5625, 0.375 , 1, 0.875 , 0.625 , 0.125 , 0.8125, 0.75})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "--extended-lambda", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <thrust/device_vector.h>\n#include <thrust/sort.h>\n#include <cuda_runtime.h>\n#undef NDEBUG\n#include <assert.h>\n\n#define EPSILON   1e-4\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\nvoid calculatePixelCdf(thrust::device_vector<int> &srcImage, int size, cudaStream_t stream, thrust::device_vector<float> &output);\n\nvoid launch() {\n    // Number of test cases\n    constexpr int TEST_CASE_COUNT = 7;\n    int inputDataLength[TEST_CASE_COUNT] = {16, 16, 16, 16, 35, 42, 64};\n    const int MAX_VECTOR_SIZE = *std::max_element(inputDataLength, inputDataLength + TEST_CASE_COUNT);\n    // input image in row major layout\n    int srcData_h[TEST_CASE_COUNT][MAX_VECTOR_SIZE] = {\n        // 4x4 image\n        {   61, 112, 91, 176,\n            134, 154, 32, 68,\n            84, 163, 31, 58,\n            135, 162, 163, 52},\n        // 4x4 image  \n        {   18, 171, 45, 34,\n            239, 61, 73, 52,\n            82, 57, 245, 228,\n            102, 31, 205, 193},\n        // 4x4 image\n        {   211, 246, 150, 18,\n            117, 42, 152, 25,\n            199, 108, 163, 129,\n            254, 165, 234, 115},\n        // 4x4 image\n        {   208, 208, 251, 85,\n            229, 113, 75, 58,\n            253, 84, 149, 225,\n            69, 40, 187, 175},\n        // 7x5 image\n        {   125, 125, 86, 230, 94,\n            28, 199, 99, 61, 103,\n            24, 33, 241, 244, 147,\n            15, 60, 90, 210, 3,\n            11, 43, 166, 187, 165,\n            115, 140, 75, 190, 48,\n            175, 46, 94, 160, 199},\n        // 6x7 image\n        {   85, 174, 34, 184, 27, 167, 126,\n            199, 183, 231, 228, 85, 178, 50,\n            7, 190, 128, 122, 231, 156, 158,\n            220, 206, 147, 46, 61, 226, 7,\n            125, 42, 250, 182, 128, 120, 15,\n            174, 10, 18, 133, 24, 209, 209},\n        // 4x16 image\n        {192, 112, 211, 140, 193, 254, 185, 158, 222,  41, 218,  17,  54,  21, 230,  98,\n         174,  31, 248, 162, 149, 109,  23, 115, 215, 205, 219,  28, 121, 148,  70, 229,\n          50, 162,  51,  83, 238,  54,  24,  56, 93, 216, 124,  11, 247, 137,  80,  61,\n          25,  71,  75,   1, 203, 236, 128,  75, 123, 133,  62, 198, 144,  87, 140, 183\n         }\n    };\n\n    // expected output in row major layout\n    float expectedOutput[TEST_CASE_COUNT][MAX_VECTOR_SIZE] = {\n        // 4x4 image\n        {   0.3125, 0.5625, 0.5   , 1.   ,\n            0.625 , 0.75  , 0.125 , 0.375 ,\n            0.4375, 0.9375, 0.0625, 0.25  ,\n            0.6875, 0.8125, 0.9375, 0.1875},\n        // 4x4 image\n        {   0.0625, 0.6875, 0.25  , 0.1875,\n            0.9375, 0.4375, 0.5   , 0.3125,\n            0.5625, 0.375 , 1.    , 0.875 ,\n            0.625 , 0.125 , 0.8125, 0.75  },\n        // 4x4 image\n        {   0.8125, 0.9375, 0.5   , 0.0625,\n            0.375 , 0.1875, 0.5625, 0.125 ,\n            0.75  , 0.25  , 0.625 , 0.4375,\n            1.    , 0.6875, 0.875 , 0.3125},\n        // 4x4 image\n        {   0.75  , 0.75  , 0.9375, 0.375,\n            0.875 , 0.4375, 0.25  , 0.125 ,\n            1.    , 0.3125, 0.5   , 0.8125,\n            0.1875, 0.0625, 0.625 , 0.5625},\n        // 7x5 image\n        {   0.6000, 0.6000, 0.3714, 0.9429, 0.4571,\n            0.1429, 0.8857, 0.4857, 0.3143, 0.5143,\n            0.1143, 0.1714, 0.9714, 1.0000, 0.6571,\n            0.0857, 0.2857, 0.4000, 0.9143, 0.0286,\n            0.0571, 0.2000, 0.7429, 0.8000, 0.7143,\n            0.5429, 0.6286, 0.3429, 0.8286, 0.2571,\n            0.7714, 0.2286, 0.4571, 0.6857, 0.8857},\n        // 6x7 image\n        {   0.3333, 0.6429, 0.1905, 0.7381, 0.1667, 0.5952, 0.4286,\n            0.7857, 0.7143, 0.9762, 0.9286, 0.3333, 0.6667, 0.2619,\n            0.0476, 0.7619, 0.4762, 0.3810, 0.9762, 0.5476, 0.5714,\n            0.8810, 0.8095, 0.5238, 0.2381, 0.2857, 0.9048, 0.0476,\n            0.4048, 0.2143, 1.0000, 0.6905, 0.4762, 0.3571, 0.0952,\n            0.6429, 0.0714, 0.1190, 0.5000, 0.1429, 0.8571, 0.8571},\n        // 4x16 image\n        {   0.734375, 0.4375  , 0.8125  , 0.578125, 0.75    , 1.      , 0.71875 , 0.640625, 0.890625, 0.15625 , 0.859375, 0.046875, 0.21875 , 0.0625  , 0.921875, 0.40625 ,\n            0.6875  , 0.140625, 0.984375, 0.671875, 0.625   , 0.421875, 0.078125, 0.453125, 0.828125, 0.796875, 0.875   , 0.125   , 0.46875 , 0.609375, 0.28125 , 0.90625 ,\n            0.171875, 0.671875, 0.1875  , 0.359375, 0.953125, 0.21875 , 0.09375 , 0.234375, 0.390625, 0.84375 , 0.5     , 0.03125 , 0.96875 , 0.546875, 0.34375 , 0.25    ,\n            0.109375, 0.296875, 0.328125, 0.015625, 0.78125 , 0.9375  , 0.515625, 0.328125, 0.484375, 0.53125 , 0.265625, 0.765625, 0.59375 , 0.375   , 0.578125, 0.703125}\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    cudaStreamCreate(&stream);\n\n    // Initialize result on the host\n    std::vector<float> cdf_h(MAX_VECTOR_SIZE);\n\n    // Vectors for device memory\n    thrust::device_vector<float> cdf_d(MAX_VECTOR_SIZE);\n    thrust::device_vector<int> srcImage_d(MAX_VECTOR_SIZE);\n\n    // Loop to execute each test case\n    for (int iTestcase = 0; iTestcase < TEST_CASE_COUNT; iTestcase++) {\n        \n        // Copy host data into device memory\n        CUDA_CHECK(cudaMemcpyAsync(thrust::raw_pointer_cast(srcImage_d.data()), srcData_h[iTestcase], inputDataLength[iTestcase] * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        // Execute kernel\n        calculatePixelCdf(srcImage_d, inputDataLength[iTestcase], stream, cdf_d);\n\n        // Copy device data into host memory\n        CUDA_CHECK(cudaMemcpyAsync(cdf_h.data(), thrust::raw_pointer_cast(cdf_d.data()), inputDataLength[iTestcase] * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Synchronize the stream to ensure all operations are complete\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Verifying result with expected output\n        for (unsigned i=0; i < inputDataLength[iTestcase]; i++) {\n            assert(abs(cdf_h[i] - expectedOutput[iTestcase][i]) < EPSILON);\n        }\n    }\n}\n\nvoid calculatePixelCdf(thrust::device_vector<int> &srcImage_d, int size, cudaStream_t stream, thrust::device_vector<float> &output) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/148", "date": "2025-07-30", "prompt": "Write a CUDA Kernel to implement the Local Binary Pattern with dynamic number of neighbours. The implementation on the host side should find the optimal tiling segment dimension as per the available shared memory size and compute the optimal blocks and grid dimension required to launch the kernel.\n\nThe signature of the function is __global__ void k_localBinaryKernel(const unsigned char* input, unsigned char* output, int width, int segHeight, int numNeighbors, float radius), where input is the pointer to the input image data, output refers to computed image data, width refers to the image width, segHeight represents segment height of the image, numNeighbors represents the number of neighbor samples used for binary comparison, radius refers to the sampling radius used to determine the neighbor pixel positions.\n\nThe signature of the first helper function is size_t dynamicSMemSizeFunc(int blockSize), where blockSize represents the number of threads per block for which to compute the required dynamic sharedmemory size.\n\nThe Signature of the second helper function is cudaError_t getOptimalLaunchParams(int segmentSize, int &optBlockSize, int &blocksPerGrid, float &theoreticalOccupancy), where segmentSize is total number of pixels in the current image segment, optBlockSize defines the computed optimal number of threads per block, blocksPerGrid refers the computed number of blocks to launch in the grid, theoreticalOccupancy represents the estimated GPU occupancy for that configuration.\n\n\n>>> k_localBinaryKernel({10,10,10,10,10, 10,20,20,20,10, 10,20,30,20,10, 10,20,20,20,10, 10,10,10,10,10}, output, 5, 5, 8, 1.0)\n     --> output({255, 191, 191, 191, 255, 255, 131, 143, 14, 255, 255, 195, 2, 62, 255, 255, 128, 176, 48, 255, 255, 255, 255, 255, 255})\n>>> k_localBinaryKernel({5,5,5, 5,10,5, 5,5,5}, output, 9, 3, 3, 8, 1.0f) --> output({255, 191, 191, 255, 2, 191, 255, 191, 191}).\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <cstdio>\n#include <cstdlib>\n#include <cmath>\n#include <algorithm>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define CUDA_CHECK(call)                                                       \\\n    do {                                                                       \\\n        cudaError_t error = call;                                              \\\n        if(error != cudaSuccess) {                                             \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\",                       \\\n                    cudaGetErrorString(error), __FILE__, __LINE__);            \\\n            exit(error);                                                       \\\n        }                                                                      \\\n    } while(0)\n\n#define NUM_TEST_CASES 8\n#ifndef cudaOccupancyPreferShared\n#define cudaOccupancyPreferShared 1\n#endif\n\n// Maximum number of thread blocks per image segment for processing\nconst int MAX_BLOCKS_PER_SEGMENT = 32;\n\nstruct TestCase {\n    int widthOfImage;      \n    int heightOfImage;\n    std::vector<unsigned char> inputImage;         \n    std::vector<unsigned char> expectedOutputImage;\n    TestCase(int width, int height, const std::vector<unsigned char>& input, \n             const std::vector<unsigned char>& expected) : widthOfImage(width), heightOfImage(height), inputImage(input), expectedOutputImage(expected) {}\n};\n\n__global__ void k_localBinaryKernel(const unsigned char* input, unsigned char* output, int width, int segHeight, int numNeighbors, \n                                 float radius);\n                                 \nsize_t dynamicSMemSizeFunc(int blockSize);\n\ncudaError_t getOptimalLaunchParams(int segmentSize, int &optBlockSize, int &blocksPerGrid, float &theoreticalOccupancy);\n\nvoid launch(){\n    // Create a CUDA stream for asynchronous operations.\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    std::vector<TestCase> testCases = {\n        // 5x3 non-square image.\n        TestCase(\n            5, 3,\n            {10, 10, 10, 10, 10,\n             20, 20, 20, 20, 20,\n             30, 30, 30, 30, 30},\n            {131, 143, 143, 143, 14,\n             128, 128, 128, 128, 0,\n             128, 128, 128, 128, 0}\n        ),\n        \n        // 3x2 non-square image.\n        TestCase(\n            3, 2,\n            {50, 60, 70,\n             80, 90, 100},\n            {131, 135, 6, \n             128, 128, 0}\n        ),\n\n        // 2x5 non-square image.\n        TestCase(\n            2, 5,\n            {10, 20,\n             30, 40,\n             50, 60,\n             70, 80,\n             90, 100},\n            {131, 6,\n             131, 6,\n             128, 0, \n             131, 6, \n             128, 0}\n        ),\n\n        // 5x5 image.\n        TestCase(\n            5, 5,\n            {10, 10, 10, 10, 10,\n             10, 20, 20, 20, 10,\n             10, 20, 30, 20, 10,\n             10, 20, 20, 20, 10,\n             10, 10, 10, 10, 10},\n            {131, 143, 143, 143, 14, \n             195, 131, 143, 14, 62, \n             192, 224, 0, 56, 56, \n             131, 128, 136, 8, 14, \n             192, 240, 240, 240, 48}\n        ),\n\n        // 3x3 image.\n        TestCase(\n            3, 3,\n            {5, 5, 5,\n             5, 10, 5,\n             5, 5, 5},\n            {131, 143, 14,\n             192, 0, 56, \n             128, 128, 0}\n        ),\n\n        // 4x4 image.\n        TestCase(\n            4, 4,\n            {1, 2, 3, 4,\n             5, 6, 7, 8,\n             9, 10, 11, 12,\n             13, 14, 15, 16},\n            {135, 135, 135, 6,\n             128, 128, 128, 0,\n             131, 135, 135, 6,\n             128, 128, 128, 0}\n        ),\n        \n        // 6x6 constant image.\n        TestCase(\n            6, 6,\n            {50, 50, 50, 50, 50, 50,\n             50, 50, 50, 50, 50, 50,\n             50, 50, 50, 50, 50, 50,\n             50, 50, 50, 50, 50, 50,\n             50, 50, 50, 50, 50, 50,\n             50, 50, 50, 50, 50, 50},\n            {131, 143, 143, 143, 143, 14,\n             195, 255, 255, 255, 255, 62,\n             192, 240, 240, 240, 240, 48, \n             131, 143, 143, 143, 143, 14, \n             195, 255, 255, 255, 255, 62,\n             192, 240, 240, 240, 240, 48}\n        ),\n        \n        // 7x7 image.\n        TestCase(\n            7, 7,\n            {10, 10, 10, 10, 10, 10, 10,\n             20, 20, 20, 20, 20, 20, 20,\n             30, 30, 30, 30, 30, 30, 30,\n             40, 40, 40, 40, 40, 40, 40,\n             50, 50, 50, 50, 50, 50, 50,\n             60, 60, 60, 60, 60, 60, 60,\n             70, 70, 70, 70, 70, 70, 70},\n            {131, 143, 143, 143, 143, 143, 14,\n             131, 143, 143, 143, 143, 143, 14, \n             128, 128, 128, 128, 128, 128, 0,\n             131, 143, 143, 143, 143, 143, 14,\n             128, 128, 128, 128, 128, 128, 0,\n             131, 143, 143, 143, 143, 143, 14,\n             128, 128, 128, 128, 128, 128, 0}\n        )\n    };\n\n    int totalTestCases = testCases.size();\n\n    // Allocate device memory once based on maximum image size.\n    int maxWidth = 0, maxHeight = 0;\n    for (int i = 0; i < totalTestCases; i++) {\n         maxWidth = std::max(maxWidth, testCases[i].widthOfImage);\n         maxHeight = std::max(maxHeight, testCases[i].heightOfImage);\n    }\n    int maxNumPixels = maxWidth * maxHeight;\n    size_t maxImgSize = maxNumPixels * sizeof(unsigned char);\n    unsigned char *input_d, *output_d;\n    CUDA_CHECK(cudaMallocAsync(&input_d, maxImgSize, stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, maxImgSize, stream));\n\n    // Launch Kernel\n    auto launchKernelWithConfig = [&stream](\n        const unsigned char* segmentInput_d, unsigned char* segmentOutput_d,\n        int segmentSize, int width, int segHeight,\n        int numNeighbors, float radius,\n        int threadsPerBlock, int blocksPerGrid) {\n        \n        // Convert to thread blocks\n        int blockDimX = min(MAX_BLOCKS_PER_SEGMENT, threadsPerBlock);\n        int blockDimY = max(1, threadsPerBlock / blockDimX);\n\n        // Shared memory size calculation from dynamicSMemSizeFunc\n        size_t sharedMemSize = dynamicSMemSizeFunc(threadsPerBlock);\n        \n        // Kernel Launch\n        void* args[] = { \n            (void*)&segmentInput_d, \n            (void*)&segmentOutput_d, \n            (void*)&width, \n            (void*)&segHeight, \n            (void*)&numNeighbors, \n            (void*)&radius \n        };\n        CUDA_CHECK(cudaLaunchKernel((void*)k_localBinaryKernel,\n                                    dim3(blocksPerGrid, 1, 1),\n                                    dim3(blockDimX, blockDimY, 1),\n                                    args, sharedMemSize, stream));\n    };\n    \n    // Run test with multiple kernel launches for segmentation.\n    auto runTestWithMultipleLaunches = [&](\n        const unsigned char* input, const unsigned char* expected,\n\n        // Total number of pixels, image width, image height , neighbouring sample and radius.\n        int numPixels, int width, int height,\n        int numNeighbors, float radius) {\n\n        // Allocate device memory\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input, numPixels * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemsetAsync(output_d, 0, numPixels * sizeof(unsigned char), stream));\n        \n        // Divide the image into segments by rows.\n        int numRows = height;\n\n        // Choose number of segments based on row count.\n        int numSegments = (numRows <= 2) ? 1 : ((numRows <= 6) ? 2 : 3);\n        int baseRows = numRows / numSegments;\n        int remainderRows = numRows % numSegments;\n        \n        // Starting row index for the current segment.\n        int startRow = 0;\n        for (int i = 0; i < numSegments; i++) {\n\n            // Number of rows in this segment.\n            int segmentRows = baseRows + (i < remainderRows ? 1 : 0);\n            // Total pixels in this segment.\n            int segmentSize = segmentRows * width;\n            \n            const unsigned char* segmentInput_d = input_d + startRow * width;\n            unsigned char* segmentOutput_d = output_d + startRow * width;\n            \n            // Launch configuration for segment.\n            // Use occupancy API to determine optimal block size for segment.   \n            int optBlockSize, blocksPerGrid;\n            float theoreticalOccupancy;\n            CUDA_CHECK(getOptimalLaunchParams(segmentSize, optBlockSize, blocksPerGrid, theoreticalOccupancy));\n\n            // Set threads per block to optimal value, but not more than segment size.\n            int threadsPerBlock = (segmentSize < optBlockSize) ? segmentSize : optBlockSize;\n\n            // Ensure minimum thread count for very small images\n            threadsPerBlock = max(threadsPerBlock, MAX_BLOCKS_PER_SEGMENT);\n            \n            // Launch kernel on the current segment\n            launchKernelWithConfig(segmentInput_d, segmentOutput_d, segmentSize, width, segmentRows, numNeighbors, radius, threadsPerBlock, blocksPerGrid);\n\n            // Update starting row for next segment.\n            startRow += segmentRows;\n        }\n        \n        // Allocate host output buffer and copy the device output to host.\n        std::vector<unsigned char> output_h(numPixels);\n        CUDA_CHECK(cudaMemcpyAsync(output_h.data(), output_d, numPixels * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Verify results\n        for (int i = 0; i < numPixels; i++) {\n            assert(output_h[i] == expected[i]);\n        }\n    };\n    auto runTest = [&] (const TestCase &tc) {\n        int numPixels = tc.widthOfImage * tc.heightOfImage;\n        runTestWithMultipleLaunches(tc.inputImage.data(), tc.expectedOutputImage.data(), numPixels, tc.widthOfImage,                   tc.heightOfImage, 8, 1.0f);\n    };\n   \n    for (int i = 0; i < totalTestCases; i++) {\n        runTest(testCases[i]);\n    }\n        \n    // Cleanup\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream)); \n}\n\n__global__ void k_localBinaryKernel(const unsigned char* input, unsigned char* output, int width, int segHeight, int numNeighbors,     float radius) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/149", "date": "2025-07-30", "prompt": "Use CUDA cuBLAS to perform matrix-vector multiplication with single-precision floating-point values. The matrix is stored in column-major order and provided in transposed form.\n\nThe signature of the function is void calcMatchingSignal(cublasHandle_t handle, float* input_d, float* ref_d, int numCols, int numRows, float* output_d), where handle is the cuBLAS handle for managing cuBLAS operations, input_d is the pointer to the input signal array, ref_d is the pointer to matrix in column major layout, numCols is the number of columns of matrix, numRows is the number of rows of matrix, and output_d is the pointer to the resultant vector.\n\n>>> calcMatchingSignal( handle, {0.62, 0.31, 0.72}, {{0.62, 0.31, 0.72}, {0.64, 0.64, 0.43}, {0.24, 0.97, 0}}, 3, 3, *output_d) -> output : {0.9989, 0.9048, 0.4495}\n>>> calcMatchingSignal( handle, {0.67, 0.67, 0.33}, {{0.17, 0.85, 0.51}, {0.67, 0.67, 0.33}, {0.18, 0.55, 0.82}}, 3, 3, *output_d) -> output : {0.8517, 1.0067, 0.7597}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "-lcublas", "declaration": "#include <cstdio>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n#include <algorithm>\n#undef NDEBUG\n#include <assert.h>\n\n#define EPSILON   (1e-3)\n#define CUDA_CHECK(call)                              \\\ndo {                                                  \\\n       cudaError_t error = call;                      \\\n       if (error != cudaSuccess) {                    \\\n           fprintf(stderr, \"CUDA error at %s:%d %s\\n\",\\\n                   __FILE__, __LINE__,                \\\n                   cudaGetErrorString(error));        \\\n           exit(EXIT_FAILURE);                        \\\n       }                                              \\\n} while(0)\n#define CUBLAS_CHECK(call)                            \\\n{                                                     \\\n    cublasStatus_t status = call;                     \\\n    if(status != CUBLAS_STATUS_SUCCESS)               \\\n    {                                                 \\\n        printf(\"cuBLAS Error: %d, line %d\\n\",         \\\n         status, __LINE__);                           \\\n    }                                                 \\\n}\n\nvoid calcMatchingSignal(cublasHandle_t handle, float* input_d, float* ref_d, int numCols, int numRows, float* output_d);\n\nvoid launch() {\n    // Number of test cases\n    constexpr int TEST_CASE_COUNT = 8;\n    // Sizes of the input vectors in each test case\n    int inputDataLength[TEST_CASE_COUNT] = {3, 3, 8, 8, 8, 8, 8, 8};\n    // Total number of elements in matrix in each test case\n    int refDataLength[TEST_CASE_COUNT] = {9, 9, 40, 64, 56, 64, 64, 32};\n\n    // Calculating maximum sizes\n    const int MAX_INPUT_VECTOR_SIZE = *std::max_element(inputDataLength, inputDataLength + TEST_CASE_COUNT);\n    const int MIN_INPUT_VECTOR_SIZE = *std::min_element(inputDataLength, inputDataLength + TEST_CASE_COUNT);\n    const int MAX_REF_VECTOR_SIZE = *std::max_element(refDataLength, refDataLength + TEST_CASE_COUNT);\n    const int MAX_OUT_VECTOR_SIZE = MAX_REF_VECTOR_SIZE / MIN_INPUT_VECTOR_SIZE;\n\n    // Input vectors\n    float inputData_h[TEST_CASE_COUNT][MAX_INPUT_VECTOR_SIZE] = {\n        {0.62, 0.31, 0.72},\n        {0.67, 0.67, 0.33},\n        {0.07, 0.4, 0.6, 0.4, 0, 0.54, 0.07, 0.13},\n        {0.49, 0.19, 0.06, 0.56, 0.43, 0.19, 0.43, 0},\n        {0.34, 0, 0.25, 0.51, 0.08, 0.68, 0.25, 0.17},\n        {0.31, 0.57, 0.06, 0.5 , 0, 0.57, 0, 0},\n        {0.4, 0.2, 0.2, 0, 0.47, 0.47, 0.53, 0.2},\n        {0.47, 0.4, 0.07, 0.2, 0.53, 0.07, 0.27, 0.47}\n    };\n\n    // input matrices\n    const float referenceData_h[TEST_CASE_COUNT][MAX_REF_VECTOR_SIZE] = {\n        {0.62, 0.31, 0.72, 0.64, 0.64, 0.43, 0.24, 0.97, 0},    //3x3 matrix\n        {0.17, 0.85, 0.51, 0.67, 0.67, 0.33, 0.18, 0.55, 0.82}, //3x3 matrix\n        {0.54, 0.07, 0.47, 0.07, 0.34, 0.34, 0.14, 0.47,        //5x8 matrix\n          0, 0.51, 0.28, 0.34, 0.28, 0, 0.51, 0.45,\n          0.07, 0.4 , 0.6, 0.4, 0, 0.54, 0.07, 0.13,\n          0.3 , 0.42, 0.3, 0.42, 0.18, 0.36, 0.36, 0.42,\n          0.26, 0.32, 0.32, 0.45, 0.06, 0.39, 0.13, 0.58\n        },\n        {0.13, 0.57, 0.19, 0.44, 0.13, 0.25, 0.31, 0.5,         //8x8 matrix\n          0.57, 0, 0.31, 0.38, 0.31, 0.57, 0.13, 0,\n          0.14, 0, 0, 0.97, 0.14, 0, 0.14, 0,\n          0.49, 0.19, 0.06, 0.56, 0.43, 0.19, 0.43, 0,\n          0, 0.34, 0.39, 0.45, 0.28, 0.45, 0, 0.5,\n          0.41, 0.47, 0.1, 0.31, 0.31, 0.41, 0.26, 0.41,\n          0.23, 0.15, 0.38, 0.53, 0.46, 0.23, 0.15, 0.46,\n          0.53, 0.3, 0.3, 0.3, 0.06, 0.53, 0, 0.41\n        },\n        {0.35, 0.06, 0.23, 0.52, 0.23, 0.52, 0.41, 0.23,        //7x8 matrix\n          0.07, 0.07, 0.35, 0.21, 0.35, 0.49, 0.64, 0.21,\n          0.1 , 0.39, 0.39, 0.24, 0.44, 0.39, 0.44, 0.29,\n          0.42, 0.47, 0.05, 0.37, 0.47, 0.32, 0.05, 0.37,\n          0.34, 0, 0.25, 0.51, 0.08, 0.68, 0.25, 0.17,\n          0.64, 0.36, 0.36, 0.07, 0.28, 0.07, 0, 0.5,\n          0.17, 0.34, 0.08, 0, 0.08, 0.59, 0.68, 0.17\n        },\n        {0.12, 0, 0.18, 0.55, 0.49, 0.43, 0.31, 0.37,           //8x8 matrix\n          0.59, 0.46, 0.33, 0.26, 0, 0.33, 0.07, 0.39,\n          0, 0.46, 0, 0.26, 0, 0.33, 0.59, 0.52,\n          0.17, 0, 0.51, 0.51, 0, 0.51, 0.43, 0.09,\n          0, 0.12, 0.4, 0.52, 0.46, 0.4, 0.4, 0.12,\n          0.31, 0.57, 0.06, 0.5 , 0, 0.57, 0, 0,\n          0.54, 0.12, 0.36, 0.48, 0.36, 0.36, 0.3 , 0,\n          0.56, 0.5, 0.19, 0.5, 0.06, 0.31, 0, 0.25\n        },\n        {0.33, 0.27, 0.6, 0.13, 0, 0.47, 0.47, 0,               //8x8 matrix\n          0.34, 0.07, 0.2, 0.41, 0.27, 0.27, 0.54, 0.48,\n          0.54, 0.36, 0.12, 0.24, 0.36, 0.48, 0.18, 0.36,\n          0.22, 0.44, 0.15, 0.44, 0, 0.51, 0.44, 0.29,\n          0.44, 0.13, 0, 0.38, 0.5 , 0.38, 0, 0.5,\n          0.11, 0.45, 0.34, 0.06, 0.45, 0.51, 0, 0.45,\n          0.4 , 0.2 , 0.2 , 0, 0.47, 0.47, 0.53, 0.2,\n          0.34, 0.27, 0.54, 0.34, 0.07, 0.4, 0.47, 0.13\n        },\n        {0.07, 0.62, 0, 0.35, 0.62, 0.14, 0.28, 0.07,           //4x8 matrix\n          0.29, 0.44, 0, 0.65, 0.15, 0.15, 0.51, 0,\n          0.08, 0.69, 0.15, 0.08, 0.31, 0.31, 0.46, 0.31,\n          0.33, 0, 0.2 , 0.47, 0.4 , 0.27, 0.33, 0.53\n        }\n    };\n\n    // expected output vectors\n    float expectedOutput[TEST_CASE_COUNT][MAX_INPUT_VECTOR_SIZE] = {\n      {0.9989, 0.9048, 0.4495},\n      {0.8517, 1.0067, 0.7597},\n      {0.6303, 0.6022, 0.9983, 0.8112, 0.8133},\n      {0.6665, 0.8082, 0.7322, 0.9993, 0.5459, 0.7928, 0.7668, 0.6292},\n      {0.9553, 0.7753, 0.7136, 0.6746, 0.9984, 0.4983, 0.6843},\n      {0.5681, 0.783, 0.5803, 0.629, 0.5804, 0.9995, 0.7026, 0.8967},\n      {0.776, 0.826, 0.8742, 0.7369, 0.7156, 0.7432, 1.0027, 0.794},\n      {0.7978, 0.67, 0.796, 0.8322}\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Initialize cuBLAS\n    cublasHandle_t handle;\n    cublasCreate(&handle);\n    CUBLAS_CHECK(cublasSetStream(handle, stream));\n\n    // Initialize result on the host\n    float *outputData_h = (float*)malloc(sizeof(float) * MAX_OUT_VECTOR_SIZE);\n\n    // Allocate memory on device\n    float *inputData_d, *referenceData_d, *outputData_d;\n    CUDA_CHECK(cudaMallocAsync((void**)&referenceData_d, sizeof(float) * MAX_REF_VECTOR_SIZE, stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&inputData_d, sizeof(float) * MAX_INPUT_VECTOR_SIZE, stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&outputData_d, sizeof(float) * MAX_OUT_VECTOR_SIZE, stream));\n\n    CUDA_CHECK(cudaMemsetAsync(outputData_d, 0, sizeof(float) * MAX_OUT_VECTOR_SIZE, stream));\n    \n    // Loop to execute each test case\n    for (int iTestcase = 0; iTestcase < TEST_CASE_COUNT; iTestcase++) {\n        // Memory copying\n        CUDA_CHECK(cudaMemcpyAsync(inputData_d, inputData_h[iTestcase], sizeof(float) * inputDataLength[iTestcase], cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(referenceData_d, referenceData_h[iTestcase], sizeof(float) * refDataLength[iTestcase], cudaMemcpyHostToDevice, stream));\n\n        int numCols = inputDataLength[iTestcase];\n        int numRows = refDataLength[iTestcase]/numCols;\n        \n        calcMatchingSignal(handle, inputData_d, referenceData_d, numCols, numRows, outputData_d);\n\n        // Copying data back to host memory\n        CUDA_CHECK(cudaMemcpyAsync(outputData_h, outputData_d, sizeof(float) * inputDataLength[iTestcase], cudaMemcpyDeviceToHost, stream));\n\n        // Synchronize the stream to ensure all operations are complete\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int j = 0; j < numRows; j++) {\n          assert(fabs(outputData_h[j] - expectedOutput[iTestcase][j]) < EPSILON);\n        }\n    }\n    // Free device memory and host memory\n    CUDA_CHECK(cudaFreeAsync(inputData_d, stream));\n    CUDA_CHECK(cudaFreeAsync(referenceData_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputData_d, stream));\n    free(outputData_h);\n    // Free cuBLAS handle and stream\n    cublasDestroy(handle);\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\nvoid calcMatchingSignal(cublasHandle_t handle, float* input_d, float* ref_d, int numCols, int numRows, float* output_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/150", "date": "2025-07-30", "prompt": "Use CUB's device-wide primitives to identify all continuous sequences of identical boolean values in a given boolean array, and count the frequency of each sequence.\n\nThe signature of the function is void detectStreaks(char* input_d, char* streakTypes_d, int* streakLengths_d, int* numStreaks_d, int inputSize, cudaStream_t stream), where input_d is a device pointer to array of char values (0 or 1), streakTypes_d is a device pointer to an array of char values, which indicates the type of each streak (whether it's streak of 1s or 0s), streakLengths_d is a device pointer where the length of each streak will be stored, numStreaks_d is a device pointer to store the count of streaks found, inputSize is the size of the input array, and stream is the CUDA stream for executing operations asynchronously.\n\n>>> detectStreaks({1, 1, 0, 0, 0, 1}, streakTypes_d, streakLengths_d, numStreaks_d, 6, cudaStream) -> streakTypes_d contains {1, 0, 1}, streakLengths_d contains {2, 3, 1}, *numStreaks_d = 3\n>>> detectStreaks({0, 0, 1, 1, 1, 0, 0}, streakTypes_d, streakLengths_d, numStreaks_d, 7, cudaStream) -> streakTypes_d contains {0, 1, 0}, streakLengths_d contains {2, 3, 2}, *numStreaks_d = 3\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <vector>\n#include <utility>\n#include <cassert>\n#include <cuda_runtime.h>\n#include <cub/device/device_run_length_encode.cuh>\n\n#define CUDA_CHECK(call)                                                   \\\n{                                                                          \\\n    cudaError_t error = call;                                              \\\n    if(error != cudaSuccess) {                                             \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                      \\\n                __FILE__, __LINE__, cudaGetErrorString(error));            \\\n        exit(EXIT_FAILURE);                                                \\\n    }                                                                      \\\n}\n\nvoid detectStreaks(char* input_d, char* streakTypes_d, int* streakLengths_d, int* numStreaks_d, int inputSize, cudaStream_t stream);\n\nstruct StreakTestCase {\n    std::vector<char> inputArray;            // Input boolean array using char \n    std::vector<std::pair<bool, int>> expectedStreaks;  // Expected pairs of (boolean type, count)\n};\n\nvoid launch() {\n    std::vector<StreakTestCase> testCases = {\n        // Test case 1: Random values 1/0\n        {\n            {1, 1, 0, 0, 0, 1},\n            {{true, 2}, {false, 3}, {true, 1}}\n        },\n        // Test case 2: Start with 0\n        {\n            {0, 0, 1, 1, 1, 0, 0},\n            {{false, 2}, {true, 3}, {false, 2}}\n        },\n        // Test case 3: All 1s\n        {\n            {1, 1, 1, 1},\n            {{true, 4}}\n        },\n        // Test case 4: All 0s\n        {\n            {0, 0, 0},\n            {{false, 3}}\n        },\n        // Test case 5: Basic alternating 1/0\n        {\n            {1, 0, 1, 0, 1},\n            {{true, 1}, {false, 1}, {true, 1}, {false, 1}, {true, 1}}\n        },\n        // Test case 6: Empty array\n        {\n            {},\n            {}\n        },\n        // Test case 7: Complex pattern\n        {\n            {1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1},\n            {{true, 3}, {false, 2}, {true, 1}, {false, 4}, {true, 1}}\n        }\n    };\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Find the maximum input size to allocate memory once\n    size_t maxInputSize = 0;\n    for (const auto& testCase : testCases) {\n        maxInputSize = std::max(maxInputSize, testCase.inputArray.size());\n    }\n    \n    // Skip if all test cases have empty arrays\n    if (maxInputSize == 0) {\n        CUDA_CHECK(cudaStreamDestroy(stream));\n        return;\n    }\n    \n    // Device memory allocations \n    char *input_d = nullptr, *streakTypes_d = nullptr;\n    int *streakLengths_d = nullptr, *numStreaks_d = nullptr;\n    \n    // Allocate device memory for maximum size\n    CUDA_CHECK(cudaMallocAsync(&input_d, maxInputSize * sizeof(char), stream));\n    CUDA_CHECK(cudaMallocAsync(&streakTypes_d, maxInputSize * sizeof(char), stream));\n    CUDA_CHECK(cudaMallocAsync(&streakLengths_d, maxInputSize * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&numStreaks_d, sizeof(int), stream));\n\n    for (size_t testCaseIndex = 0; testCaseIndex < testCases.size(); testCaseIndex++) {\n        const std::vector<char>& input_h = testCases[testCaseIndex].inputArray;\n        const std::vector<std::pair<bool, int>>& expectedStreaks = testCases[testCaseIndex].expectedStreaks;\n        \n        int inputSize = input_h.size();\n        \n        // Skip if input array is empty but verify empty result\n        if (inputSize == 0) {\n            assert(expectedStreaks.size() == 0);\n            continue;\n        }\n        \n        // Initialize counter to 0\n        CUDA_CHECK(cudaMemsetAsync(numStreaks_d, 0, sizeof(int), stream));\n        \n        // Copy input directly to device \n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h.data(), \n                                  inputSize * sizeof(char), cudaMemcpyHostToDevice, stream));\n        \n        // Call the function to detect streaks\n        detectStreaks(input_d, streakTypes_d, streakLengths_d, numStreaks_d, inputSize, stream);\n        \n        // Copy results back to host for verification\n        int numStreaks_h;\n        CUDA_CHECK(cudaMemcpyAsync(&numStreaks_h, numStreaks_d, \n                                  sizeof(int), cudaMemcpyDeviceToHost, stream));\n        \n        // Use vector<char> for streak types\n        std::vector<char> streakTypes_h(numStreaks_h);\n        std::vector<int> streakLengths_h(numStreaks_h);\n        \n        if (numStreaks_h > 0) {\n            CUDA_CHECK(cudaMemcpyAsync(streakTypes_h.data(), streakTypes_d, \n                                      numStreaks_h * sizeof(char), cudaMemcpyDeviceToHost, stream));\n            CUDA_CHECK(cudaMemcpyAsync(streakLengths_h.data(), streakLengths_d, \n                                      numStreaks_h * sizeof(int), cudaMemcpyDeviceToHost, stream));\n        }\n        \n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Verify results using assertions\n        assert(numStreaks_h == expectedStreaks.size());\n\n        for (int i = 0; i < numStreaks_h; i++) {            \n            assert((bool)streakTypes_h[i] == expectedStreaks[i].first);\n            assert(streakLengths_h[i] == expectedStreaks[i].second);\n        }\n    }\n    \n    // Free device memory once outside the loop\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(streakTypes_d, stream));\n    CUDA_CHECK(cudaFreeAsync(streakLengths_d, stream));\n    CUDA_CHECK(cudaFreeAsync(numStreaks_d, stream));\n    \n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n// Host function to detect streaks of boolean values in an array\nvoid detectStreaks(char* input_d, char* streakTypes_d, int* streakLengths_d, int* numStreaks_d, int inputSize, cudaStream_t stream) {\n\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/151", "date": "2025-07-30", "prompt": "Write a CUDA function using a cuBLAS device-wide routine to compute the operation C = AB + C, where A is a Hermitian matrix (stored in upper triangular form), and B and C are column-major double-precision complex matrices with dimensions m  n.\n\nThe signauture of the function is void hermitianMultiply(cublasHandle_t handle, int rows, int cols, cuDoubleComplex alpha, cuDoubleComplex beta, cuDoubleComplex* matrixA_d, cuDoubleComplex* matrixB_d, cuDoubleComplex* matrixC_d), where handle represent cuBLAS library context that carries all internal state and configuration, rows define the number of rows in matrix, cols refers to number of coloumns in a matrix, alpha represents a complex scalar that multiplies the matrix product, beta defines a complex scalar that multiplies the existing contents of C before adding the AB, matrixA_d represents the Hermitian matrix A in columnmajor layout, matrixB_d represents general matrixB in columnmajor layout, matrixC_d output matxix.\n\n>>> hermitianMultiply(handle, 2, 2,{{2.0,0.0}, {0.0,0.0}, {0.0,0.0}, {3.0,0.0}}, {{1.0,0.0}, {3.0,0.0}, {2.0,0.0}, {4.0,0.0}}, matrixC_d) --> matrixC_d: ({2.5, 7.5, 7.5, 15.0 })\n>>> hermitianMultiply(handle, 3, 1, {{1.0,0.0}, {0.0,0.0}, {0.0,0.0}, {0.0,0.0}, {2.0,0.0}, {0.0,0.0}, {0.0,0.0}, {0.0,0.0}, {3.0,0.0}}, {{1.0,0.0}, {1.0,0.0}, {1.0,0.0}}, matrixC_d) --> matrixC_d: ({1.25, 2.50, 3.75})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "-lcublas", "declaration": "#include <cuda_runtime.h>\n#include <cublas_v2.h>\n#include <cuComplex.h>\n#include <vector>\n#include <cstdio>\n#undef NDEBUG\n#include <assert.h>\n\n#define CUDA_CHECK(call)                                                       \\\ndo {                                                                           \\\n    cudaError_t err = call;                                                    \\\n    if (err != cudaSuccess) {                                                  \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                          \\\n                __FILE__, __LINE__, cudaGetErrorString(err));                  \\\n        exit(EXIT_FAILURE);                                                    \\\n    }                                                                          \\\n} while (0)\n\n#define CUBLAS_CHECK(call)                                            \\\n{                                                                 \\\n    cublasStatus_t status = call;                                 \\\n    if (status != CUBLAS_STATUS_SUCCESS)                          \\\n    {                                                             \\\n        fprintf(stderr, \"cuBLAS Error: %d, line %d\\n\", status, __LINE__); \\\n        exit(EXIT_FAILURE);                                       \\\n    }                                                             \\\n}\n#define TOLERANCE 1e-6  \n\n\nstruct TestCase {\n    int rows;                                   // rows of A\n    int cols;                                   // cols of B\n    std::vector<cuDoubleComplex> matrixA;    // A in row-major\n    std::vector<cuDoubleComplex> matrixB;    // B in row-major\n    std::vector<cuDoubleComplex> expectedMatrixC;  // expected C in row-major\n};\n\n// Prototype for the cuBLAS HEMM wrapper\nvoid hermitianMultiply(cublasHandle_t handle, int rows, int cols, cuDoubleComplex alpha, cuDoubleComplex beta,                          cuDoubleComplex* matrixA_d, cuDoubleComplex* matrixB_d, cuDoubleComplex* matrixC_d);\n\nvoid launch() {\n    // Initialize cuBLAS\n    cublasHandle_t cublasHandle;\n    CUBLAS_CHECK(cublasCreate(&cublasHandle));\n\n    cuDoubleComplex alpha = {1.25, 0.0};\n    cuDoubleComplex beta  = {0.0,  0.0};\n\n    // Define all test cases\n    std::vector<TestCase> testCases;\n\n    // 11\n    testCases.push_back({\n        1, 1,\n        {{5.0,0.0}},            \n        {{2.0,0.0}},            \n        {{12.50,0.0}}           \n    });\n\n    // 22 diagonal\n    testCases.push_back({\n        2, 2,\n        {{2.0,0.0},{0.0,0.0},\n         {0.0,0.0},{3.0,0.0}},  \n        {{1.0,0.0},{3.0,0.0},\n         {2.0,0.0},{4.0,0.0}},  \n        {{2.50,0.0},{7.50,0.0},\n         {7.50,0.0},{15.00,0.0}}\n    });\n\n    // Test 3: 31\n    testCases.push_back({\n        3, 1,\n        {{1.0,0.0},{0.0,0.0},{0.0,0.0},\n         {0.0,0.0},{2.0,0.0},{0.0,0.0},\n         {0.0,0.0},{0.0,0.0},{3.0,0.0}}, \n        {{1.0,0.0},{1.0,0.0},{1.0,0.0}}, \n        {{1.25,0.0},{2.50,0.0},{3.75,0.0}} \n    });\n\n    // Test 4: 23\n    testCases.push_back({\n        2, 3,\n        {{1.0,0.0},{2.0,0.0},\n         {2.0,0.0},{3.0,0.0}},  \n        {{1.0,0.0},{4.0,0.0},\n         {2.0,0.0},{5.0,0.0},\n         {3.0,0.0},{6.0,0.0}},  \n        {{13.75,0.0},{12.50,0.0},{17.50,0.0},\n         {21.25,0.0},{21.25,0.0},{27.50,0.0}} \n    });\n\n    //  41, A = 2I\n    testCases.push_back({\n        4, 1,\n        {{2.0,0.0},{0.0,0.0},{0.0,0.0},{0.0,0.0},\n         {0.0,0.0},{2.0,0.0},{0.0,0.0},{0.0,0.0},\n         {0.0,0.0},{0.0,0.0},{2.0,0.0},{0.0,0.0},\n         {0.0,0.0},{0.0,0.0},{0.0,0.0},{2.0,0.0}}, \n        {{1.0,0.0},{2.0,0.0},{3.0,0.0},{4.0,0.0}}, \n        {{2.50,0.0},{5.00,0.0},{7.50,0.0},{10.00,0.0}} \n    });\n\n    // 33 Identity\n    testCases.push_back({\n        3, 3,\n        {{1.0,0.0},{0.0,0.0},{0.0,0.0},\n         {0.0,0.0},{1.0,0.0},{0.0,0.0},\n         {0.0,0.0},{0.0,0.0},{1.0,0.0}}, \n        {{1.0,0.0},{4.0,0.0},{7.0,0.0},\n         {2.0,0.0},{5.0,0.0},{8.0,0.0},\n         {3.0,0.0},{6.0,0.0},{9.0,0.0}}, \n        {{1.25,0.0},{5.00,0.0},{8.75,0.0},\n         {2.50,0.0},{6.25,0.0},{10.00,0.0},\n         {3.75,0.0},{7.50,0.0},{11.25,0.0}} \n    });\n\n    // 14\n    testCases.push_back({\n        1, 4,\n        {{7.0,0.0}},                        \n        {{1.0,0.0},{2.0,0.0},{3.0,0.0},{4.0,0.0}}, \n        {{8.75,0.0},{17.50,0.0},{26.25,0.0},{35.00,0.0}} \n    });\n\n    // 21 swap\n    testCases.push_back({\n        2, 1,\n        {{0.0,0.0},{1.0,0.0},\n         {1.0,0.0},{0.0,0.0}},            \n        {{1.0,0.0},{0.0,0.0}},            \n        {{0.0,0.0},{1.25,0.0}}             \n    });\n\n    // Create CUDA stream and allocate device buffers\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    int maxN = 4, maxM = 4;\n    cuDoubleComplex *matrixA_d, *matrixB_d, *matrixC_d;\n    CUDA_CHECK(cudaMallocAsync(&matrixA_d, maxN*maxN*sizeof(cuDoubleComplex), stream));\n    CUDA_CHECK(cudaMallocAsync(&matrixB_d, maxN*maxM*sizeof(cuDoubleComplex), stream));\n    CUDA_CHECK(cudaMallocAsync(&matrixC_d, maxN*maxM*sizeof(cuDoubleComplex), stream));\n\n    // Run all test cases\n    for (size_t ti = 0; ti < testCases.size(); ++ti) {\n        const auto &tc = testCases[ti];\n        int rows = tc.rows, cols = tc.cols;\n        size_t szA = rows*rows, szB = rows*cols;\n\n        // Convert rowmajor  colmajor\n        std::vector<cuDoubleComplex> colA(szA), colB(szB);\n        for (int i = 0; i < rows; ++i)\n            for (int j = 0; j < rows; ++j)\n                colA[j*rows + i] = tc.matrixA[i*rows + j];\n        for (int i = 0; i < rows; ++i)\n            for (int j = 0; j < cols; ++j)\n                colB[j*rows + i] = tc.matrixB[i*cols + j];\n\n        CUDA_CHECK(cudaMemcpyAsync(matrixA_d, colA.data(), szA*sizeof(cuDoubleComplex),\n                                   cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matrixB_d, colB.data(), szB*sizeof(cuDoubleComplex),\n                                   cudaMemcpyHostToDevice, stream));\n        \n        // Bind the cuBLAS handle to stream\n        CUBLAS_CHECK(cublasSetStream(cublasHandle, stream));\n\n        // Call cuBLAS ZHEMM wrapper\n        hermitianMultiply(cublasHandle, rows, cols, alpha, beta, matrixA_d, matrixB_d, matrixC_d);\n\n        // Copy back and convert colmajor  rowmajor\n        std::vector<cuDoubleComplex> colC(szB), rowC(szB);\n        CUDA_CHECK(cudaMemcpyAsync(colC.data(), matrixC_d, szB*sizeof(cuDoubleComplex),\n                                   cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < rows; ++i)\n            for (int j = 0; j < cols; ++j)\n                rowC[i*cols + j] = colC[j*rows + i];\n\n        // Verify and print actual vs expected\n        for (size_t idx = 0; idx < szB; ++idx) {\n            double cr = cuCreal(rowC[idx]), ci = cuCimag(rowC[idx]);\n            double er = cuCreal(tc.expectedMatrixC[idx]), ei = cuCimag(tc.expectedMatrixC[idx]);\n            assert(fabs(cr - er) < TOLERANCE && fabs(ci - ei) < TOLERANCE);\n        }\n    }\n\n    // Cleanup\n    CUDA_CHECK(cudaFreeAsync(matrixA_d, stream));\n    CUDA_CHECK(cudaFreeAsync(matrixB_d, stream));\n    CUDA_CHECK(cudaFreeAsync(matrixC_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    CUDA_CHECK(cudaDeviceSynchronize());\n    CUBLAS_CHECK(cublasDestroy(cublasHandle));\n}\n\n// Wrapper for cuBLAS ZHEMM\nvoid hermitianMultiply(cublasHandle_t handle, int rows, int cols, cuDoubleComplex alpha, cuDoubleComplex beta,             cuDoubleComplex* matrixA_d, cuDoubleComplex* matrixB_d, cuDoubleComplex* matrixC_d)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/152", "date": "2025-07-30", "prompt": "Perform a variation of the symmetric rank-k update on a column-major matrices A and B of complex data type with dimension n x k, storing only the lower triangular matrix of complex data type with dimension n x n using cublas.\n\nThe signature of the function is void symmetricRank(cublasHandle_t handle, int n, int k, const cuComplex* alpha, const cuComplex* A, const cuComplex* B, const cuComplex* beta, cuComplex* C), where handle is the CUBLAS library context handle, n is the number of rows and columns of matrix C, k is the number of columns in matrices A and B, alpha is a pointer to the scaling factor for the AB^T operation, A is a pointer to input matrix A, B is a pointer to input matrix B, beta is a pointer to the scaling factor for matrix C, C is a pointer to output matrix C.\n\n>>> symmetricRank(2, 2, {2.0f, 0.0f}, {{1.5f, 0.0f}, {2.5f, 0.0f}, {3.5f, 0.0f}, {4.5f, 0.0f}}, {{1.5f, 0.0f}, {2.5f, 0.0f},{3.5f, 0.0f}, {4.5f, 0.0f}}, 2, {0.0f, 0.0f}, C) -> C:{{29.0f, 0.0f}, * ,{1.0f, 0.5f}, {53.0f, 0.0f}}\n>>> symmetricRank(3, 2, {1.0f, 1.0f}, {{1.0f, 1.0f}, {2.0f, 1.0f}, {3.0f, 1.0f}, {4.0f, 1.0f}, {5.0f, 1.0f}, {6.0f, 1.0f}}, {{1.0f, 1.0f}, {2.0f, 1.0f},{3.0f, 1.0f}, {4.0f, 1.0f},{5.0f, 1.0f}, {6.0f, 1.0f}}, {0.0f, 0.0f}, C) -> C:{{1.0f, 0.5f}, *, {1.0f, 0.5f}, {1.0f, 0.5f}, *, {1.0f, 0.5f}, {1.0f, 0.5f}, {1.0f, 0.5f}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "-l cublas", "declaration": "#include <cstdio>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n#include <cuComplex.h>\n#include <cublas_v2.h>\n#include <assert.h>\n\n// Error checking macros\n#define CHECK_CUDA(call) do {                                              \\\n    cudaError_t status = call;                                            \\\n    if (status != cudaSuccess) {                                          \\\n        fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\",                      \\\n                cudaGetErrorString(status), __FILE__, __LINE__);          \\\n        exit(status);                                                     \\\n    }                                                                     \\\n} while(0)\n\n#define CHECK_CUBLAS(call) do {                                           \\\n    cublasStatus_t status = call;                                        \\\n    if (status != CUBLAS_STATUS_SUCCESS) {                               \\\n        fprintf(stderr, \"cuBLAS Error: %d at %s:%d\\n\",                   \\\n                status, __FILE__, __LINE__);                             \\\n        exit(status);                                                    \\\n    }                                                                    \\\n} while(0)\n\n#define TOLERANCE 1E-3\n#define MAX_SIZE 4\n\n// Wrapper for cuBLAS csyrkx API\nvoid symmetricRank(cublasHandle_t handle,\n           int n,\n           int k,\n           const cuComplex* alpha,\n           const cuComplex* A,\n           const cuComplex* B,\n           const cuComplex* beta,\n           cuComplex* C);\n\nvoid launch() {\n    // Test configuration array [test_id][param_id]\n    const int testConfig[7][2] = {\n        {2, 2},  // Test 1: 2x2\n        {3, 2},  // Test 2: 3x2\n        {2, 3},  // Test 3: 2x3\n        {4, 2},  // Test 4: 4x2\n        {2, 2},  // Test 5: 2x2 with non-zero beta\n        {3, 3},  // Test 6: 3x3 with complex alpha\n        {2, 4}   // Test 7: 2x4 with complex values\n    };\n\n    // Matrix data array [test_id][row][col][2] with strictly positive values\n    const float matrixData[7][4][4][2] = {\n        // Test 1: 2x2 with positive real values\n        {{{1.5f, 0.0f}, {2.5f, 0.0f}},\n         {{3.5f, 0.0f}, {4.5f, 0.0f}}},\n\n        // Test 2: 3x2 with positive complex values\n        {{{1.0f, 1.0f}, {2.0f, 1.0f}},\n         {{3.0f, 1.0f}, {4.0f, 1.0f}},\n         {{5.0f, 1.0f}, {6.0f, 1.0f}}},\n\n        // Test 3: 2x3 with positive mixed values\n        {{{1.0f, 2.0f}, {2.0f, 3.0f}, {3.0f, 4.0f}},\n         {{4.0f, 5.0f}, {5.0f, 6.0f}, {6.0f, 7.0f}}},\n\n        // Test 4: 4x2 with larger positive values\n        {{{5.0f, 0.0f}, {6.0f, 0.0f}},\n         {{7.0f, 0.0f}, {8.0f, 0.0f}},\n         {{9.0f, 0.0f}, {10.0f, 0.0f}},\n         {{11.0f, 0.0f}, {12.0f, 0.0f}}},\n\n        // Test 5: 2x2 with positive complex values for non-zero beta\n        {{{1.5f, 1.0f}, {2.5f, 1.0f}},\n         {{3.5f, 1.0f}, {4.5f, 1.0f}}},\n\n        // Test 6: 3x3 with positive complex values\n        {{{1.0f, 1.0f}, {2.0f, 2.0f}, {3.0f, 3.0f}},\n         {{4.0f, 4.0f}, {5.0f, 5.0f}, {6.0f, 6.0f}},\n         {{7.0f, 7.0f}, {8.0f, 8.0f}, {9.0f, 9.0f}}},\n\n        // Test 7: 2x4 with all positive complex values\n        {{{1.0f, 1.0f}, {2.0f, 2.0f}, {3.0f, 3.0f}, {4.0f, 4.0f}},\n         {{5.0f, 5.0f}, {6.0f, 6.0f}, {7.0f, 7.0f}, {8.0f, 8.0f}}}\n    };\n\n    // Alpha and Beta values [test_id][param][real/imag]\n    const float scalarData[7][2][2] = {\n        {{2.0f, 0.0f}, {0.0f, 0.0f}},    // Test 1: real alpha=2\n        {{1.0f, 1.0f}, {0.0f, 0.0f}},    // Test 2: complex alpha=1+i\n        {{2.0f, -1.0f}, {0.0f, 0.0f}},   // Test 3: complex alpha=2-i\n        {{1.5f, 0.0f}, {0.0f, 0.0f}},    // Test 4: real alpha=1.5\n        {{1.0f, 0.0f}, {1.5f, 0.5f}},    // Test 5: real alpha=1, complex beta=1.5+0.5i\n        {{2.0f, 1.0f}, {0.0f, 0.0f}},    // Test 6: complex alpha=2+i\n        {{1.5f, -0.5f}, {0.5f, 1.0f}}    // Test 7: complex alpha=1.5-0.5i, complex beta=0.5+i\n    };\n\n    float expectedResult_h[7][4][4][2] = {\n        {{{29.0f, 0.0f},                },\n         {{1.0f, 0.5f}, {53.0f, 0.0f}}},\n\n        // Test 2: 3x2 with positive complex values\n        {{{5.0f, 25.0f}                             },\n         {{1.0f, 0.5f}, {13.0f, 41.0f}                },\n         {{1.0f, 0.5f}, {1.0f, 0.5f}, {25.0f, 61.0f}}},\n\n        // Test 3: 2x3 with positive mixed values\n        {{{46.0f, 197.0f}              },\n         {{1.0f, 0.5f} , {82.0f, 299.0f}}},\n\n        // Test 4: 4x2 with larger positive values\n        {{{159.0f, 0.0f}                                         },\n         {{1.0f, 0.5f}, {204.0f, 0.0f}                           },\n         {{1.0f, 0.5f}, {1.0f, 0.5f}, {255.0f, 0.0f}             },\n         {{1.0f, 0.5f}, {1.0f, 0.5f}, {1.0f, 0.5f}, {312.0f, 0.0f}}},\n\n        // Test 5: 2x2 with positive complex values for non-zero beta\n        {{{13.75f, 11.25f},               },\n         {{1.0f, 0.5f}, {25.75f, 15.25f}}},\n\n        // Test 6: 3x3 with positive complex values\n        {{{-132.0f, 264.0f}                             },\n         {{1.0f, 0.5f},{-186.0f, 372.0f}               },\n         {{1.0f, 0.5f}, {1.0f, 0.5f}, {-252.0f, 504.0f}}},\n\n        // Test 7: 2x4 with all positive complex values\n        {{{84.0f, 253.25f}             },\n         {{1.0f, 0.5f}, {120.0f, 361.25f}}}\n    };\n\n    cudaStream_t stream;\n    CHECK_CUDA(cudaStreamCreate(&stream));\n\n    cublasHandle_t handle;\n    CHECK_CUBLAS(cublasCreate(&handle));\n    CHECK_CUBLAS(cublasSetStream(handle, stream));\n\n    cuComplex *A_h, *B_h, *C_h;\n    A_h = (cuComplex*)malloc(MAX_SIZE * MAX_SIZE * sizeof(cuComplex));\n    B_h = (cuComplex*)malloc(MAX_SIZE * MAX_SIZE * sizeof(cuComplex));\n    C_h = (cuComplex*)malloc(MAX_SIZE * MAX_SIZE * sizeof(cuComplex));\n\n    cuComplex *A_d, *B_d, *C_d;\n    CHECK_CUDA(cudaMallocAsync(&A_d, MAX_SIZE * MAX_SIZE * sizeof(cuComplex), stream));\n    CHECK_CUDA(cudaMallocAsync(&B_d, MAX_SIZE * MAX_SIZE * sizeof(cuComplex), stream));\n    CHECK_CUDA(cudaMallocAsync(&C_d, MAX_SIZE * MAX_SIZE * sizeof(cuComplex), stream));\n\n    for(int test = 0; test < 7; test++) {\n        int n = testConfig[test][0];\n        int k = testConfig[test][1];\n        cuComplex alpha = make_cuComplex(scalarData[test][0][0], scalarData[test][0][1]);\n        cuComplex beta = make_cuComplex(scalarData[test][1][0], scalarData[test][1][1]);\n\n        // Initialize with strictly positive values\n        for(int i = 0; i < n; i++) {\n            for(int j = 0; j < k; j++) {\n                A_h[i*k + j] = make_cuComplex(matrixData[test][i][j][0],\n                                            matrixData[test][i][j][1]);\n                B_h[i*k + j] = A_h[i*k + j];\n            }\n        }\n\n        // Initialize C with strictly positive values for tests with non-zero beta\n        for(int i = 0; i < n * n; i++) {\n            C_h[i] = make_cuComplex(1.0f, 0.5f);  // Non-zero initial values\n        }\n\n        CHECK_CUDA(cudaMemcpyAsync(A_d, A_h, n * k * sizeof(cuComplex),\n                                 cudaMemcpyHostToDevice, stream));\n        CHECK_CUDA(cudaMemcpyAsync(B_d, B_h, n * k * sizeof(cuComplex),\n                                 cudaMemcpyHostToDevice, stream));\n        CHECK_CUDA(cudaMemcpyAsync(C_d, C_h, n * n * sizeof(cuComplex),\n                                 cudaMemcpyHostToDevice, stream));\n\n        // Call the API wrapper\n        symmetricRank(handle, n, k, &alpha, A_d, B_d, &beta, C_d);\n\n        CHECK_CUDA(cudaMemcpyAsync(C_h, C_d, n * n * sizeof(cuComplex),\n                                 cudaMemcpyDeviceToHost, stream));\n\n        CHECK_CUDA(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < n; i++) {\n            for(int j = 0; j < n; j++) {\n                if(i >= j) {\n                  assert(fabs(expectedResult_h[test][i][j][0] - C_h[i*n + j].x) < TOLERANCE);\n                  assert(fabs(expectedResult_h[test][i][j][1] - C_h[i*n + j].y) < TOLERANCE);\n                }\n            }\n        }\n    }\n\n    // Cleanup\n    CHECK_CUDA(cudaFreeAsync(A_d, stream));\n    CHECK_CUDA(cudaFreeAsync(B_d, stream));\n    CHECK_CUDA(cudaFreeAsync(C_d, stream));\n    free(A_h);\n    free(B_h);\n    free(C_h);\n\n    CHECK_CUBLAS(cublasDestroy(handle));\n    CHECK_CUDA(cudaStreamDestroy(stream));\n}\n\nvoid symmetricRank(cublasHandle_t handle,\n           int n,\n           int k,\n           const cuComplex* alpha,\n           const cuComplex* A,\n           const cuComplex* B,\n           const cuComplex* beta,\n           cuComplex* C) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/153", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute linear congruential generator algorithm for 100 steps on elements of an integer array. Utilize coalesced, asynchronous memory accesses to hide memory latency to its maximum extent. For simplicity, zero-pad the array.\n\nThe signature of the CUDA kernel is __global__ void k_calculateLCG(uint32_t * __restrict__ data_d, int numElements), where data_d is a pointer to the array for in-place linear-congruential-generation on elements, and numElements is the number of elements to compute.\n\n>>> k_calculateLCG({ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }, 10) -> data_d: { 1148175737, 4135983138, 2828823248, 1521663358, 214503468, 3202310869, 1895150979, 587991089, 3575798490, 2268638600 }\n>>> k_calculateLCG({ 150, 149, 148, 147, 146, 145, 144, 143, 142 }, 9) -> data_d: { 2642687623, 3949847513, 962040112, 2269200002, 3576359892, 588552491, 1895712381, 3202872271, 215064870 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_pipeline.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Algorithm-related constants.\n#define MAXIMUM_ARRAY_LENGTH 100000\n// Number of 32bit elements per access per thread. The data array will be zero-padded accordingly with this number.\n#define MEMORY_ACCESS_WIDTH (sizeof(uint4) / sizeof(uint32_t))\n#define MAXIMUM_ARRAY_LENGTH_WITH_PADDING_FOR_WIDE_ACCESS ((1 + (MAXIMUM_ARRAY_LENGTH - 1) / MEMORY_ACCESS_WIDTH) * MEMORY_ACCESS_WIDTH)\n#define MAXIMUM_BUFFER_BYTES (sizeof(uint32_t) * MAXIMUM_ARRAY_LENGTH_WITH_PADDING_FOR_WIDE_ACCESS)\n// Constants for the linear congruential generator algorithm.\n#define LCG_MULTIPLIER (134775813ull)\n#define LCG_OFFSET (1ull)\n#define LCG_MODULUS (4294967291ull)\n#define LCG_STEPS 100\n\n// CUDA-related constants.\n#define NUMBER_OF_SHARED_MEMORY_ARRAYS_PER_BLOCK 2\n#define MAXIMUM_NUMBER_OF_THREADS_PER_BLOCK_ALLOWED 1024\n\n\n__global__ void k_calculateLCG(uint32_t * __restrict__ data_d, int numElements);\n\nvoid launch() {\n    int deviceId = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, deviceId));   \n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    // Allocating host buffers.\n    uint32_t * data_h = new uint32_t[MAXIMUM_ARRAY_LENGTH];\n    uint32_t * expectedData_h = new uint32_t[MAXIMUM_ARRAY_LENGTH];\n    // Allocating device buffer.\n    uint32_t * data_d;\n    CUDA_CHECK(cudaMallocAsync(&data_d, MAXIMUM_BUFFER_BYTES, stream));\n    // Initializing all elements including padding to zero.\n    CUDA_CHECK(cudaMemsetAsync(data_d, 0, MAXIMUM_BUFFER_BYTES, stream));\n    auto hToD = cudaMemcpyHostToDevice;\n    auto dToH = cudaMemcpyDeviceToHost;\n\n\n\n    // Test 1: 10 elements with sequentially increasing values.\n    {\n        int numElements = 10;\n        assert(numElements <= MAXIMUM_ARRAY_LENGTH);\n        for(int i = 0; i < numElements; i++) {\n            data_h[i] = i;\n            uint64_t data = data_h[i];\n            for(int step = 0; step < LCG_STEPS; step++) {\n                data = (data * LCG_MULTIPLIER + LCG_OFFSET) % LCG_MODULUS;\n            }\n            expectedData_h[i] = data;\n        }\n        CUDA_CHECK(cudaMemcpyAsync(data_d, data_h, numElements * sizeof(uint32_t), hToD, stream));\n\n        void * args[2] = { &data_d, &numElements };\n        // Dynamically calculating the shared memory size used to not surpassed the limitations of the hardware.\n        int sizeOfSharedMemoryRequiredPerThread = sizeof(uint4) * NUMBER_OF_SHARED_MEMORY_ARRAYS_PER_BLOCK;\n        int maximumSharedMemoryPerBlockAvailable = prop.sharedMemPerBlock;\n        int maximumThreadsPerBlockAvailable = maximumSharedMemoryPerBlockAvailable / sizeOfSharedMemoryRequiredPerThread;\n        int threadsPerBlockUsed = std::min(maximumThreadsPerBlockAvailable, MAXIMUM_NUMBER_OF_THREADS_PER_BLOCK_ALLOWED);\n        int sizeOfSharedMemoryRequired = sizeOfSharedMemoryRequiredPerThread * threadsPerBlockUsed;\n        // Dynamically calculating the number of blocks of the grid.\n        int maxBlocks = (prop.maxThreadsPerMultiProcessor * prop.multiProcessorCount) / threadsPerBlockUsed;\n        int elementsComputedPerBlockPerIteration = threadsPerBlockUsed * MEMORY_ACCESS_WIDTH;\n        int blocksRequired = 1 + (numElements - 1) / elementsComputedPerBlockPerIteration;\n        int blocksUsed = (blocksRequired <= maxBlocks ? blocksRequired : maxBlocks);\n\n        // Grid: (1, 1, 1)\n        // Block: (1024, 1, 1)\n        dim3 gridDim(blocksUsed, 1, 1);\n        dim3 blockDim(threadsPerBlockUsed, 1, 1);\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLCG, gridDim, blockDim, args, sizeOfSharedMemoryRequired, stream));\n        CUDA_CHECK(cudaMemcpyAsync(data_h, data_d, numElements * sizeof(uint32_t), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int i = 0; i < numElements; i++) {\n            assert(data_h[i] == expectedData_h[i]);\n        }\n    }\n    // Test 2: 9 elements with sequentially decreasing values.\n    {\n        int numElements = 9;\n        assert(numElements <= MAXIMUM_ARRAY_LENGTH);\n        for(int i = 0; i < numElements; i++) {\n            data_h[i] = 150 - i;\n            uint64_t data = data_h[i];\n            for(int step = 0; step < LCG_STEPS; step++) {\n                data = (data * LCG_MULTIPLIER + LCG_OFFSET) % LCG_MODULUS;\n            }\n            expectedData_h[i] = data;\n        }\n        CUDA_CHECK(cudaMemcpyAsync(data_d, data_h, numElements * sizeof(uint32_t), hToD, stream));\n\n        void * args[2] = { &data_d, &numElements };\n        // Dynamically calculating the shared memory size used to not surpassed the limitations of the hardware.\n        int sizeOfSharedMemoryRequiredPerThread = sizeof(uint4) * NUMBER_OF_SHARED_MEMORY_ARRAYS_PER_BLOCK;\n        int maximumSharedMemoryPerBlockAvailable = prop.sharedMemPerBlock;\n        int maximumThreadsPerBlockAvailable = maximumSharedMemoryPerBlockAvailable / sizeOfSharedMemoryRequiredPerThread;\n        int threadsPerBlockUsed = std::min(maximumThreadsPerBlockAvailable, MAXIMUM_NUMBER_OF_THREADS_PER_BLOCK_ALLOWED);\n        int sizeOfSharedMemoryRequired = sizeOfSharedMemoryRequiredPerThread * threadsPerBlockUsed;\n        // Dynamically calculating the number of blocks of the grid.\n        int maxBlocks = (prop.maxThreadsPerMultiProcessor * prop.multiProcessorCount) / threadsPerBlockUsed;\n        int elementsComputedPerBlockPerIteration = threadsPerBlockUsed * MEMORY_ACCESS_WIDTH;\n        int blocksRequired = 1 + (numElements - 1) / elementsComputedPerBlockPerIteration;\n        int blocksUsed = (blocksRequired <= maxBlocks ? blocksRequired : maxBlocks);\n\n        // Grid: (1, 1, 1)\n        // Block: (1024, 1, 1)\n        dim3 gridDim(blocksUsed, 1, 1);\n        dim3 blockDim(threadsPerBlockUsed, 1, 1);\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLCG, gridDim, blockDim, args, sizeOfSharedMemoryRequired, stream));\n        CUDA_CHECK(cudaMemcpyAsync(data_h, data_d, numElements * sizeof(uint32_t), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int i = 0; i < numElements; i++) {\n            assert(data_h[i] == expectedData_h[i]);\n        }\n    }\n    // Test 3: 10000 elements with only 5th bit set.\n    {\n        int numElements = 10000;\n        assert(numElements <= MAXIMUM_ARRAY_LENGTH);\n        for(int i = 0; i < numElements; i++) {\n            data_h[i] = (1 << 4);\n            uint64_t data = data_h[i];\n            for(int step = 0; step < LCG_STEPS; step++) {\n                data = (data * LCG_MULTIPLIER + LCG_OFFSET) % LCG_MODULUS;\n            }\n            expectedData_h[i] = data;\n        }\n        CUDA_CHECK(cudaMemcpyAsync(data_d, data_h, numElements * sizeof(uint32_t), hToD, stream));\n        \n        void * args[2] = { &data_d, &numElements };\n        // Dynamically calculating the shared memory size used to not surpassed the limitations of the hardware.\n        int sizeOfSharedMemoryRequiredPerThread = sizeof(uint4) * NUMBER_OF_SHARED_MEMORY_ARRAYS_PER_BLOCK;\n        int maximumSharedMemoryPerBlockAvailable = prop.sharedMemPerBlock;\n        int maximumThreadsPerBlockAvailable = maximumSharedMemoryPerBlockAvailable / sizeOfSharedMemoryRequiredPerThread;\n        int threadsPerBlockUsed = std::min(maximumThreadsPerBlockAvailable, MAXIMUM_NUMBER_OF_THREADS_PER_BLOCK_ALLOWED);\n        int sizeOfSharedMemoryRequired = sizeOfSharedMemoryRequiredPerThread * threadsPerBlockUsed;\n        // Dynamically calculating the number of blocks of the grid.\n        int maxBlocks = (prop.maxThreadsPerMultiProcessor * prop.multiProcessorCount) / threadsPerBlockUsed;\n        int elementsComputedPerBlockPerIteration = threadsPerBlockUsed * MEMORY_ACCESS_WIDTH;\n        int blocksRequired = 1 + (numElements - 1) / elementsComputedPerBlockPerIteration;\n        int blocksUsed = (blocksRequired <= maxBlocks ? blocksRequired : maxBlocks);\n\n        // Grid: (3, 1, 1)\n        // Block: (1024, 1, 1)\n        dim3 gridDim(blocksUsed, 1, 1);\n        dim3 blockDim(threadsPerBlockUsed, 1, 1);\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLCG, gridDim, blockDim, args, sizeOfSharedMemoryRequired, stream));\n        CUDA_CHECK(cudaMemcpyAsync(data_h, data_d, numElements * sizeof(uint32_t), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int i = 0; i < numElements; i++) {\n            assert(data_h[i] == expectedData_h[i]);\n        }\n    }\n    // Test 4: 100 elements with sequentially changing bits set.\n    {\n        int numElements = 100;\n        assert(numElements <= MAXIMUM_ARRAY_LENGTH);\n        for(int i = 0; i < numElements; i++) {\n            data_h[i] = (1 << (i % 32));\n            uint64_t data = data_h[i];\n            for(int step = 0; step < LCG_STEPS; step++) {\n                data = (data * LCG_MULTIPLIER + LCG_OFFSET) % LCG_MODULUS;\n            }\n            expectedData_h[i] = data;\n        }\n        CUDA_CHECK(cudaMemcpyAsync(data_d, data_h, numElements * sizeof(uint32_t), hToD, stream));\n        \n        void * args[2] = { &data_d, &numElements };\n        // Dynamically calculating the shared memory size used to not surpassed the limitations of the hardware.\n        int sizeOfSharedMemoryRequiredPerThread = sizeof(uint4) * NUMBER_OF_SHARED_MEMORY_ARRAYS_PER_BLOCK;\n        int maximumSharedMemoryPerBlockAvailable = prop.sharedMemPerBlock;\n        int maximumThreadsPerBlockAvailable = maximumSharedMemoryPerBlockAvailable / sizeOfSharedMemoryRequiredPerThread;\n        int threadsPerBlockUsed = std::min(maximumThreadsPerBlockAvailable, MAXIMUM_NUMBER_OF_THREADS_PER_BLOCK_ALLOWED);\n        int sizeOfSharedMemoryRequired = sizeOfSharedMemoryRequiredPerThread * threadsPerBlockUsed;\n        // Dynamically calculating the number of blocks of the grid.\n        int maxBlocks = (prop.maxThreadsPerMultiProcessor * prop.multiProcessorCount) / threadsPerBlockUsed;\n        int elementsComputedPerBlockPerIteration = threadsPerBlockUsed * MEMORY_ACCESS_WIDTH;\n        int blocksRequired = 1 + (numElements - 1) / elementsComputedPerBlockPerIteration;\n        int blocksUsed = (blocksRequired <= maxBlocks ? blocksRequired : maxBlocks);\n\n        // Grid: (1, 1, 1)\n        // Block: (1024, 1, 1)\n        dim3 gridDim(blocksUsed, 1, 1);\n        dim3 blockDim(threadsPerBlockUsed, 1, 1);\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLCG, gridDim, blockDim, args, sizeOfSharedMemoryRequired, stream));\n        CUDA_CHECK(cudaMemcpyAsync(data_h, data_d, numElements * sizeof(uint32_t), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int i = 0; i < numElements; i++) {\n            assert(data_h[i] == expectedData_h[i]);\n        }\n    }\n    // Test 5: MAXIMUM_ARRAY_LENGTH elements with index % 1000 value.\n    {\n        int numElements = MAXIMUM_ARRAY_LENGTH;\n        assert(numElements <= MAXIMUM_ARRAY_LENGTH);\n        for(int i = 0; i < numElements; i++) {\n            data_h[i] = i % 1000;\n            uint64_t data = data_h[i];\n            for(int step = 0; step < LCG_STEPS; step++) {\n                data = (data * LCG_MULTIPLIER + LCG_OFFSET) % LCG_MODULUS;\n            }\n            expectedData_h[i] = data;\n        }\n        CUDA_CHECK(cudaMemcpyAsync(data_d, data_h, numElements * sizeof(uint32_t), hToD, stream));\n        \n        void * args[2] = { &data_d, &numElements };\n        // Dynamically calculating the shared memory size used to not surpassed the limitations of the hardware.\n        int sizeOfSharedMemoryRequiredPerThread = sizeof(uint4) * NUMBER_OF_SHARED_MEMORY_ARRAYS_PER_BLOCK;\n        int maximumSharedMemoryPerBlockAvailable = prop.sharedMemPerBlock;\n        int maximumThreadsPerBlockAvailable = maximumSharedMemoryPerBlockAvailable / sizeOfSharedMemoryRequiredPerThread;\n        int threadsPerBlockUsed = std::min(maximumThreadsPerBlockAvailable, MAXIMUM_NUMBER_OF_THREADS_PER_BLOCK_ALLOWED);\n        int sizeOfSharedMemoryRequired = sizeOfSharedMemoryRequiredPerThread * threadsPerBlockUsed;\n        // Dynamically calculating the number of blocks of the grid.\n        int maxBlocks = (prop.maxThreadsPerMultiProcessor * prop.multiProcessorCount) / threadsPerBlockUsed;\n        int elementsComputedPerBlockPerIteration = threadsPerBlockUsed * MEMORY_ACCESS_WIDTH;\n        int blocksRequired = 1 + (numElements - 1) / elementsComputedPerBlockPerIteration;\n        int blocksUsed = (blocksRequired <= maxBlocks ? blocksRequired : maxBlocks);\n\n        // Grid: (25, 1, 1)\n        // Block: (1024, 1, 1)\n        dim3 gridDim(blocksUsed, 1, 1);\n        dim3 blockDim(threadsPerBlockUsed, 1, 1);\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLCG, gridDim, blockDim, args, sizeOfSharedMemoryRequired, stream));\n        CUDA_CHECK(cudaMemcpyAsync(data_h, data_d, numElements * sizeof(uint32_t), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int i = 0; i < numElements; i++) {\n            assert(data_h[i] == expectedData_h[i]);\n        }\n    }\n    // Test 6: MAXIMUM_ARRAY_LENGTH elements with 0xFFFFFFFF value.\n    {\n        int numElements = MAXIMUM_ARRAY_LENGTH;\n        assert(numElements <= MAXIMUM_ARRAY_LENGTH);\n        for(int i = 0; i < numElements; i++) {\n            data_h[i] = 0xFFFFFFFF;\n            uint64_t data = data_h[i];\n            for(int step = 0; step < LCG_STEPS; step++) {\n                data = (data * LCG_MULTIPLIER + LCG_OFFSET) % LCG_MODULUS;\n            }\n            expectedData_h[i] = data;\n        }\n        CUDA_CHECK(cudaMemcpyAsync(data_d, data_h, numElements * sizeof(uint32_t), hToD, stream));\n        \n        void * args[2] = { &data_d, &numElements };\n        // Dynamically calculating the shared memory size used to not surpassed the limitations of the hardware.\n        int sizeOfSharedMemoryRequiredPerThread = sizeof(uint4) * NUMBER_OF_SHARED_MEMORY_ARRAYS_PER_BLOCK;\n        int maximumSharedMemoryPerBlockAvailable = prop.sharedMemPerBlock;\n        int maximumThreadsPerBlockAvailable = maximumSharedMemoryPerBlockAvailable / sizeOfSharedMemoryRequiredPerThread;\n        int threadsPerBlockUsed = std::min(maximumThreadsPerBlockAvailable, MAXIMUM_NUMBER_OF_THREADS_PER_BLOCK_ALLOWED);\n        int sizeOfSharedMemoryRequired = sizeOfSharedMemoryRequiredPerThread * threadsPerBlockUsed;\n        // Dynamically calculating the number of blocks of the grid.\n        int maxBlocks = (prop.maxThreadsPerMultiProcessor * prop.multiProcessorCount) / threadsPerBlockUsed;\n        int elementsComputedPerBlockPerIteration = threadsPerBlockUsed * MEMORY_ACCESS_WIDTH;\n        int blocksRequired = 1 + (numElements - 1) / elementsComputedPerBlockPerIteration;\n        int blocksUsed = (blocksRequired <= maxBlocks ? blocksRequired : maxBlocks);\n\n        // Grid: (25, 1, 1)\n        // Block: (1024, 1, 1)\n        dim3 gridDim(blocksUsed, 1, 1);\n        dim3 blockDim(threadsPerBlockUsed, 1, 1);\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLCG, gridDim, blockDim, args, sizeOfSharedMemoryRequired, stream));\n        CUDA_CHECK(cudaMemcpyAsync(data_h, data_d, numElements * sizeof(uint32_t), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int i = 0; i < numElements; i++) {\n            assert(data_h[i] == expectedData_h[i]);\n        }\n    }\n    // Test 7: 55 elements with 0 value.\n    {\n        int numElements = 55;\n        assert(numElements <= MAXIMUM_ARRAY_LENGTH);\n        for(int i = 0; i < numElements; i++) {\n            data_h[i] = 0;\n            uint64_t data = data_h[i];\n            for(int step = 0; step < LCG_STEPS; step++) {\n                data = (data * LCG_MULTIPLIER + LCG_OFFSET) % LCG_MODULUS;\n            }\n            expectedData_h[i] = data;\n        }\n        CUDA_CHECK(cudaMemcpyAsync(data_d, data_h, numElements * sizeof(uint32_t), hToD, stream));\n        \n        void * args[2] = { &data_d, &numElements };\n        // Dynamically calculating the shared memory size used to not surpassed the limitations of the hardware.\n        int sizeOfSharedMemoryRequiredPerThread = sizeof(uint4) * NUMBER_OF_SHARED_MEMORY_ARRAYS_PER_BLOCK;\n        int maximumSharedMemoryPerBlockAvailable = prop.sharedMemPerBlock;\n        int maximumThreadsPerBlockAvailable = maximumSharedMemoryPerBlockAvailable / sizeOfSharedMemoryRequiredPerThread;\n        int threadsPerBlockUsed = std::min(maximumThreadsPerBlockAvailable, MAXIMUM_NUMBER_OF_THREADS_PER_BLOCK_ALLOWED);\n        int sizeOfSharedMemoryRequired = sizeOfSharedMemoryRequiredPerThread * threadsPerBlockUsed;\n        // Dynamically calculating the number of blocks of the grid.\n        int maxBlocks = (prop.maxThreadsPerMultiProcessor * prop.multiProcessorCount) / threadsPerBlockUsed;\n        int elementsComputedPerBlockPerIteration = threadsPerBlockUsed * MEMORY_ACCESS_WIDTH;\n        int blocksRequired = 1 + (numElements - 1) / elementsComputedPerBlockPerIteration;\n        int blocksUsed = (blocksRequired <= maxBlocks ? blocksRequired : maxBlocks);\n\n        // Grid: (1, 1, 1)\n        // Block: (1024, 1, 1)\n        dim3 gridDim(blocksUsed, 1, 1);\n        dim3 blockDim(threadsPerBlockUsed, 1, 1);\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLCG, gridDim, blockDim, args, sizeOfSharedMemoryRequired, stream));\n        CUDA_CHECK(cudaMemcpyAsync(data_h, data_d, numElements * sizeof(uint32_t), dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for(int i = 0; i < numElements; i++) {\n            assert(data_h[i] == expectedData_h[i]);\n        }\n    }\n    CUDA_CHECK(cudaFreeAsync(data_d, stream));\n    delete [] data_h;\n    delete [] expectedData_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_calculateLCG(uint32_t * __restrict__ data_d, int numElements) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/154", "date": "2025-07-30", "prompt": "Implement a function using Thrust that filters out elements from the input array wherever the corresponding value in the predicate array is one.\n\nThe function signature is void predicateBasedFilter(thrust::device_vector<int>& inpVector, thrust::device_vector<int>& predicateVector, thrust::device_vector<int>& outVector), where inpVector is the input vector, predicateVector is the vector used to filter input vector, and outVector is the output filtered vector. All vectors are Thrust device-vectors.\n\n>>> predicateBasedFilter({ 8, 7, -7, -6, 2 }, { 0, 1, 1, 0, 1 }, thrust::device_vector<int>& outVector) -> { 7, -7, 2 }\n\n>>> predicateBasedFilter({ 8, 7, -7, -6, 2, 6, 0, -13, 4, -19 }, { 0, 1, 1, 0, 1, 1, 0, 0, 0, 1 }, thrust::device_vector<int>& outVector) -> { 7, -7, 2, 6, -19 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#define _USE_MATH_DEFINES\n\n#include <cuda_runtime_api.h>\n#include <thrust/partition.h>\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/unique.h>\n#include <thrust/reduce.h>\n#include <thrust/random.h>\n#include <thrust/sort.h>\n#include <thrust/tabulate.h>\n#include <thrust/execution_policy.h>\n#include <iostream>\n#include <random>\n#undef NDEBUG\n#include <assert.h>\n\n#define TEST_CASES 10\n#define RAND_SEED 571149\n#define UNIFORM_DIST_LOWER_BOUND -20\n#define UNIFORM_DIST_UPPER_BOUND 10\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\nvoid predicateBasedFilter(thrust::device_vector<int>& inpArray, thrust::device_vector<int>& predicateVector, thrust::device_vector<int>& outVector);\nvoid cpuVerificationFunction(thrust::host_vector<int>& inputVector, thrust::host_vector<int>& predicateVector, thrust::host_vector<int>& outVector);\n\nvoid launch() {\n\n\t// Setting input lengths\n\tint input_sizes[TEST_CASES] = { 5,10,20,38,53,64,73,85,91,100 };\n\n\t// Initiating random number generators\n\tstd::minstd_rand randObj(RAND_SEED);\n\tstd::uniform_int_distribution<int> inpDistObj(UNIFORM_DIST_LOWER_BOUND, UNIFORM_DIST_UPPER_BOUND);\n\tstd::uniform_int_distribution<int> predicateDistObj(-50, 50);\n\n\t// Lambda functions to generate random numbers with different distribution parameters for input and predicate vectors\n\tauto getRandNum = [&](int x) {\n\t\trandObj.discard(x);\n\t\treturn (int)inpDistObj(randObj);\n\t\t};\n\n\tauto getRandPred = [&](int x) {\n\t\trandObj.discard(x);\n\t\treturn (int)(predicateDistObj(randObj) > 0);\n\t\t};\n\n\tfor (int i = 0; i < TEST_CASES; i++) {\n\n\t\tint inp_length = input_sizes[i];\n\n\t\t// Host vectors\n\t\tthrust::host_vector<int> inputVector(inp_length);\n\t\tthrust::host_vector<int> predicateVector(inp_length);\n\n\t\t// Generate random input sequence and fill input and predicate vectors with random numbers\n\t\tthrust::tabulate(inputVector.begin(), inputVector.end(), getRandNum);\n\t\tthrust::tabulate(predicateVector.begin(), predicateVector.end(), getRandPred);\n\n\t\t// Device vectors\n\t\tthrust::device_vector<int> inputVector_d = inputVector;\n\t\tthrust::device_vector<int> predicateVector_d = predicateVector;\n\n\t\t// Calling implementation on GPU device\n\t\tthrust::device_vector<int> outArray_d;\n\t\tpredicateBasedFilter(inputVector_d, predicateVector_d, outArray_d);\n\n\t\t// Copy output vector\n\t\tthrust::host_vector<int> outArray_device = outArray_d;\n\t\t\t\t\n\t\t// Calling implementation on CPU for verification\n\t\tthrust::host_vector<int> outArray_host;\n\t\tcpuVerificationFunction(inputVector, predicateVector, outArray_host);\n\n\t\tassert(outArray_device.size() == outArray_host.size());\n\t\tfor (int i = 0; i < outArray_device.size(); i++)\n\t\t\tassert(outArray_device[i] == outArray_host[i]);\n\t}\n}\n\nvoid cpuVerificationFunction(thrust::host_vector<int>& inputVector, thrust::host_vector<int>& predicateVector, thrust::host_vector<int>& outVector) {\n\n\t// Compute output length using the predicate vector\n\tint outLength = thrust::reduce(thrust::host, predicateVector.begin(), predicateVector.end());\n\tthrust::stable_partition(thrust::host, inputVector.begin(), inputVector.end(), predicateVector.begin(), ::cuda::std::identity{});\n\n\t// Copy subarray to output\n\toutVector.resize(outLength);\n\tthrust::copy(thrust::host, inputVector.begin(), inputVector.begin() + outLength, outVector.begin());\n}\n\nvoid predicateBasedFilter(thrust::device_vector<int>& inOutVector, thrust::device_vector<int>& predicateVector, thrust::device_vector<int>& outVector) {\n", "test": "int main() {\n\tlaunch();\n}\n", "example_test": "\t@example_test\n", "cuda_toolkit": "12.8"}
{"task_id": "CUDA/155", "date": "2025-07-30", "prompt": "Write a CUDA kernel to rotating a vector by one position towards left. Each thread shifts an element in the vector ensuring communication among all the threads inside the block.\n\nThe signature of the kernel is __global__ void k_rotateVector(float *input_d, float *output_d, int size, int blockSize), where  input_d is the pointer to input vector stored in device memory, output_d is the pointer to output vector stored in device memory, size is the length of the vectors, blockSize is the number of threads in the block.\n\n>>> k_rotateVector({1.0f, 2.0f, 4.0f, 12.0f}, output_d, 4, 32) -> output_d: {12.0f, 1.0f, 2.0f, 4.0f}\n>>> k_rotateVector({11.0f, 23.0f, 41.0f, 52.0f}, output_d, 4, 32) -> output_d: {52.0f, 11.0f, 23.0f, 41.0f}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cstdio>\n#include <cassert>\n#include <cmath>\n#include <algorithm>\n#include <cstdlib>\n\n#define CUDA_CHECK(call)                                                     \\\n    do {                                                                     \\\n        cudaError_t err = call;                                              \\\n        if (err != cudaSuccess) {                                            \\\n            fprintf(stderr, \"CUDA error %s at %s:%d\\n\",                   \\\n                    cudaGetErrorString(err), __FILE__, __LINE__);            \\\n            exit(EXIT_FAILURE);                                              \\\n        }                                                                    \\\n    } while (0)\n\n#undef NDEBUG\n#include <assert.h>\n\nconstexpr int MAX_BLOCK_SIZE = 256;\n__global__ void k_rotateVector(float *input_d, float *output_d, int size, int blockSize);\n\nvoid launch() {\n    // Determine blockSize and required shared memory based on device properties\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n    int maxShared = prop.sharedMemPerBlock;\n    int blockSize = std::min(MAX_BLOCK_SIZE, maxShared / (int)sizeof(float));\n    size_t sharedBytes = blockSize * sizeof(float);\n\n    int testSizes[10] = {1, 2, 32, 63, 256, 1000, 1024, 2047, 5000, 10000};\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    for (int t = 0; t < 10; ++t) {\n        int size = testSizes[t];\n        size_t bytes = size_t(size) * sizeof(float);\n        float *input_h  = (float*)malloc(bytes);\n        float *output_h = (float*)malloc(bytes);\n        for (int i = 0; i < size; ++i) input_h[i] = float(i + 1);\n\n        float *input_d, *output_d;\n        CUDA_CHECK(cudaMallocAsync(&input_d,  bytes, stream));\n        CUDA_CHECK(cudaMallocAsync(&output_d, bytes, stream));\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, bytes, cudaMemcpyHostToDevice, stream));\n\n        int numBlocks = (size + blockSize - 1) / blockSize;\n\n        void* args[] = { &input_d, &output_d, &size, &blockSize };\n        CUDA_CHECK(cudaLaunchKernel((void*)k_rotateVector, dim3(numBlocks), dim3(blockSize), args, sharedBytes, stream));\n        CUDA_CHECK(cudaGetLastError());\n\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, bytes, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // verify rotation per segment\n        for (int i = 0; i < size; ++i) {\n            int segStart = (i / blockSize) * blockSize;\n            int segLen   = std::min(blockSize, size - segStart);\n            int idx      = i - segStart;\n            int src      = (idx == 0 ? segLen - 1 : idx - 1);\n            float expected = input_h[segStart + src];\n            assert(fabs(output_h[i] - expected) < 1e-5f);\n        }\n\n        CUDA_CHECK(cudaFreeAsync(input_d, stream));\n        CUDA_CHECK(cudaFreeAsync(output_d, stream));\n        free(input_h); \n        free(output_h);\n    }\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_rotateVector(float *input_d, float *output_d, int size, int blockSize) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/156", "date": "2025-07-30", "prompt": "Write a CUDA kernel to identify number with highest bit count within a batch of four numbers, then compute the operation: {input - powf(a, n)}, where a is the average of four numbers, n is the highest bit count. Use coalesced loads and stores to improve memory throughput.\n\nThe signature of the CUDA kernel is __global__ void k_calculateDifferencesFromPowerOfAveragePerChunk(int32_t * input_d, float * output_d, int numChunks), where input_d is a pointer to the input integer array, output_d is a pointer to the output float array, and numChunks is the number of chunks to compute where each chunk is four elements.\n\n>>> k_calculateDifferencesFromPowerOfAveragePerChunk({ 0, 0, 0, 1, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0 }, output_d, 5) -> output_d: { -0.25f, -0.25f, -0.25f, 0.75f, -0.5f, 1.5f, -0.5f, -0.5f, 3.0f - 0.5625f, -0.5625f, -0.5625f, -0.5625f, -1.0f, -1.0f, 4.0f - 1.0f, -1.0f, -1.0f, -1.0f, -1.0f, -1.0f }\n>>> k_calculateDifferencesFromPowerOfAveragePerChunk({ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 }, output_d, 4) -> output_d: { -2.2500f, -1.2500f, -0.2500f, 0.7500f, -162.3750f, -161.3750f, -160.3750f, -159.3750f, -849.3750f, -848.3750f, -847.3750f, -846.3750f, -33203.0625f, -33202.0625f, -33201.0625f, -33200.0625f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Algorithm settings.\n// Type of data to use for coalesced load.\nusing INPUT_CHUNK_TYPE = int4;\n// Number of elements to be used in same calculation.\nconstexpr int CHUNK_ELEMENTS = sizeof(INPUT_CHUNK_TYPE) / sizeof(int32_t);\n// Maximum number of chunks allocated.\nconstexpr int MAX_CHUNKS = 1000;\n\n// CUDA settings.\nconstexpr int NUM_THREADS_PER_BLOCK = 256;\n\n// Test settings.\nconstexpr int NUM_TESTS = 7;\n// Two samples have constant literal expected results ready, not needing computation on host.\nconstexpr int NUM_SAMPLES = 2;\nconstexpr float ERROR_TOLERANCE = 1e-5f;\n\n__global__ void k_calculateDifferencesFromPowerOfAveragePerChunk(   int32_t * input_d, \n                                                                    float * output_d, \n                                                                    int numChunks);\n\nvoid launch() {\n    // Getting device properties.\n    int deviceId = 0;\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    // Allocating stream.\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    // Allocating host memory.\n    int32_t * input_h = new int32_t[MAX_CHUNKS * CHUNK_ELEMENTS];\n    float * output_h = new float[MAX_CHUNKS * CHUNK_ELEMENTS];\n    // Allocating device memory.\n    int32_t * input_d;\n    float * output_d;\n    CUDA_CHECK(cudaMallocAsync(&input_d, CHUNK_ELEMENTS * MAX_CHUNKS * sizeof(int32_t), stream));\n    CUDA_CHECK(cudaMemsetAsync(input_d, 0, CHUNK_ELEMENTS * MAX_CHUNKS * sizeof(int32_t), stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, CHUNK_ELEMENTS * MAX_CHUNKS * sizeof(float), stream));\n    CUDA_CHECK(cudaMemsetAsync(output_d, 0, CHUNK_ELEMENTS * MAX_CHUNKS * sizeof(float), stream));\n    \n    int * testNumChunks = new int[NUM_TESTS];\n    int32_t * testInputs_h = new int32_t[NUM_TESTS * MAX_CHUNKS * CHUNK_ELEMENTS];\n    float * testExpectedOutputs_h = new float[NUM_TESTS * MAX_CHUNKS * CHUNK_ELEMENTS];\n    // Test 1: Sample 5 chunks with only 1 non-zero item per chunk, 1 chunk with all zero items.\n    {\n        int testIndex = 0;\n        testNumChunks[testIndex] = 5;\n        std::initializer_list<int32_t> inputs = { 0, 0, 0, 1, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0 };\n        std::copy(inputs.begin(), inputs.end(), &testInputs_h[testIndex * MAX_CHUNKS * CHUNK_ELEMENTS]);\n        std::initializer_list<float> expectedOutputs = { \n            -0.25f, -0.25f, -0.25f, 0.75f, \n            -0.5f, 1.5f, -0.5f, -0.5f, \n            3.0f - 0.5625f, -0.5625f, -0.5625f, -0.5625f, \n            -1.0f, -1.0f, 4.0f - 1.0f, -1.0f, \n            -1.0f, -1.0f, -1.0f, -1.0f \n        };\n        std::copy(expectedOutputs.begin(), expectedOutputs.end(), &testExpectedOutputs_h[testIndex * MAX_CHUNKS * CHUNK_ELEMENTS]);\n    }\n    // Test 2: Sample 4 chunks with increasing elements.\n    {\n        int testIndex = 1;\n        testNumChunks[testIndex] = 4;\n        std::initializer_list<int32_t> inputs = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 };\n        std::copy(inputs.begin(), inputs.end(), &testInputs_h[testIndex * MAX_CHUNKS * CHUNK_ELEMENTS]);\n        std::initializer_list<float> expectedOutputs = { \n            -2.2500f, -1.2500f, -0.2500f, 0.7500f, \n            -162.3750f, -161.3750f, -160.3750f, -159.3750f, \n            -849.3750f, -848.3750f, -847.3750f, -846.3750f, \n            -33203.0625f, -33202.0625f, -33201.0625f, -33200.0625f \n        };\n        std::copy(expectedOutputs.begin(), expectedOutputs.end(), &testExpectedOutputs_h[testIndex * MAX_CHUNKS * CHUNK_ELEMENTS]);\n    }\n    // Test 3: randomized inputs in range (0, 10000)\n    {\n        int testIndex = 2;\n        testNumChunks[testIndex] = MAX_CHUNKS;\n        std::random_device randomDevice;\n        std::mt19937 generator(randomDevice());\n        std::uniform_int_distribution<int32_t> distribution(0, 10000);\n        // Initializing the inputs.\n        for(int i = 0; i < testNumChunks[testIndex] * CHUNK_ELEMENTS; i++) {\n            testInputs_h[i + testIndex * MAX_CHUNKS * CHUNK_ELEMENTS] = distribution(generator);\n        }\n    }\n    // Test 4: randomized inputs in range (10000, 100000)\n    {\n        int testIndex = 3;\n        testNumChunks[testIndex] = MAX_CHUNKS;\n        std::random_device randomDevice;\n        std::mt19937 generator(randomDevice());\n        std::uniform_int_distribution<int32_t> distribution(10000, 100000);\n        // Initializing the inputs.\n        for(int i = 0; i < testNumChunks[testIndex] * CHUNK_ELEMENTS; i++) {\n            testInputs_h[i + testIndex * MAX_CHUNKS * CHUNK_ELEMENTS] = distribution(generator);\n        }\n    }\n    // Test 5: randomized inputs in range (100000000, 1000000000)\n    {\n        int testIndex = 4;\n        testNumChunks[testIndex] = MAX_CHUNKS;\n        std::random_device randomDevice;\n        std::mt19937 generator(randomDevice());\n        std::uniform_int_distribution<int32_t> distribution(100000000, 1000000000);\n        // Initializing the inputs.\n        for(int i = 0; i < testNumChunks[testIndex] * CHUNK_ELEMENTS; i++) {\n            testInputs_h[i + testIndex * MAX_CHUNKS * CHUNK_ELEMENTS] = distribution(generator);\n        }\n    }\n    // Test 6: all elements are zero\n    {\n        int testIndex = 5;\n        testNumChunks[testIndex] = MAX_CHUNKS;\n        // Initializing the inputs.\n        for(int i = 0; i < testNumChunks[testIndex] * CHUNK_ELEMENTS; i++) {\n            testInputs_h[i + testIndex * MAX_CHUNKS * CHUNK_ELEMENTS] = 0;\n        }\n    }\n    // Test 7: Negative elements.\n    {\n        int testIndex = 6;\n        testNumChunks[testIndex] = MAX_CHUNKS;\n        // Initializing the inputs.\n        for(int i = 0; i < testNumChunks[testIndex] * CHUNK_ELEMENTS; i++) {\n            testInputs_h[i + testIndex * MAX_CHUNKS * CHUNK_ELEMENTS] = -i;\n        }\n    }\n    // Calculating the expected results for the non-sample tests.\n    for(int test = NUM_SAMPLES; test < NUM_TESTS; test++) {\n        int numChunks = testNumChunks[test];\n        for(int i = 0; i < numChunks; i++) {\n            int offset = test * MAX_CHUNKS * CHUNK_ELEMENTS;\n            int32_t data1 = testInputs_h[offset + i * CHUNK_ELEMENTS];\n            int32_t data2 = testInputs_h[offset + i * CHUNK_ELEMENTS + 1];\n            int32_t data3 = testInputs_h[offset + i * CHUNK_ELEMENTS + 2];\n            int32_t data4 = testInputs_h[offset + i * CHUNK_ELEMENTS + 3];\n            int bitCounts[4] = { 0, 0, 0, 0};\n            float average = data1 * 0.25f + data2 * 0.25f + data3 * 0.25f + data4 * 0.25f;\n            for(int j = 0; j < 32; j++) {\n                bitCounts[0] += ((data1 >> j) & 1u);\n                bitCounts[1] += ((data2 >> j) & 1u);\n                bitCounts[2] += ((data3 >> j) & 1u);\n                bitCounts[3] += ((data4 >> j) & 1u);\n            }\n            int maxBits = (bitCounts[0] > bitCounts[1] ? bitCounts[0] : bitCounts[1]);\n            maxBits = (maxBits > bitCounts[2] ? maxBits : bitCounts[2]);\n            maxBits = (maxBits > bitCounts[3] ? maxBits : bitCounts[3]);\n            testExpectedOutputs_h[offset + i * CHUNK_ELEMENTS] = data1 - powf(average, (float)maxBits);\n            testExpectedOutputs_h[offset + i * CHUNK_ELEMENTS + 1] = data2 - powf(average, (float)maxBits);\n            testExpectedOutputs_h[offset + i * CHUNK_ELEMENTS + 2] = data3 - powf(average, (float)maxBits);\n            testExpectedOutputs_h[offset + i * CHUNK_ELEMENTS + 3] = data4 - powf(average, (float)maxBits);\n        }\n    }\n    // Running the tests.\n    for(int test = 0; test < NUM_TESTS; test++)\n    {\n        int offset = test * MAX_CHUNKS * CHUNK_ELEMENTS;\n        int numChunks = testNumChunks[test];\n        for(int i = 0; i < numChunks * CHUNK_ELEMENTS; i++) {\n            input_h[i] = testInputs_h[i + offset];\n        }\n        // Copying the inputs to the device.\n        size_t sizeOfInput = numChunks * CHUNK_ELEMENTS * sizeof(int32_t);\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, sizeOfInput, cudaMemcpyHostToDevice, stream));\n        // Calculating the result.\n        void * args[3] = { &input_d, &output_d, &numChunks };\n        int smCount = deviceProperties.multiProcessorCount;\n        int smThreads = deviceProperties.maxThreadsPerMultiProcessor;\n        int maxThreads = smCount * smThreads;\n        int maxBlocks = maxThreads / NUM_THREADS_PER_BLOCK;\n         int requiredThreads = (numChunks + CHUNK_ELEMENTS - 1) / CHUNK_ELEMENTS;\n        int requiredBlocks = (requiredThreads + NUM_THREADS_PER_BLOCK - 1) / NUM_THREADS_PER_BLOCK;\n        int usedBlocks = requiredBlocks < maxBlocks ? requiredBlocks : maxBlocks;\n        // Grid: (usedBlocks, 1, 1)\n        // Block: (NUM_THREADS_PER_BLOCK, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateDifferencesFromPowerOfAveragePerChunk, \n                                    dim3(usedBlocks, 1, 1), \n                                    dim3(NUM_THREADS_PER_BLOCK, 1, 1), \n                                    args, \n                                    0, \n                                    stream));\n        // Copying the outputs to the host.\n        size_t sizeOfOutput = numChunks * CHUNK_ELEMENTS * sizeof(float);\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeOfOutput, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        for(int i = 0; i < numChunks; i++) {\n            for(int j = 0; j < 4; j++) {\n                float expected = testExpectedOutputs_h[offset + i * CHUNK_ELEMENTS + j];\n                if(!isinf(expected) && !isinf(output_h[i * CHUNK_ELEMENTS + j])) {\n                    if(fabs(expected) > 0.0f) {\n                        assert(fabs((expected - output_h[i * CHUNK_ELEMENTS + j]) / expected) < ERROR_TOLERANCE);\n                    } else {\n                        assert(fabsf(expected - output_h[i * CHUNK_ELEMENTS + j]) < ERROR_TOLERANCE);\n                    }\n                }\n            }\n        }\n    }\n    \n    // Deallocating device memory.\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    // Deallocating host memory.\n    delete [] input_h;\n    delete [] output_h;\n    delete [] testInputs_h;\n    delete [] testNumChunks;\n    delete [] testExpectedOutputs_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_calculateDifferencesFromPowerOfAveragePerChunk(   int32_t * input_d, \n                                                                    float * output_d, \n                                                                    int numChunks) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/157", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform matrix multiplication utilizing Tensor Cores by using inline PTX mma instruction of dimension m16n8k8.\nThe input matrix A is stored using column major indexing, and matrix B is stored using row major indexing.\nThe mma instruction with it's modifiers will be \"mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 \" where inputs A & B will be of data type __nv_bfloat16 (b16) with accumulator being of data type float (f32).\nLoad data into shared memory tiles while ensuring correct indexing order for A & B,\nbefore loading them into register via 'ldmatrix' instruction.\nAll the input test cases will have dimensions allowable as per the mma instruction dimension.\n\nThe kernel should be broken using device functions, the signatures and their respectives functions are as follows:\n__device__ __forceinline__ uint32_t d_cvtaToSharedU32(const void* ptr); To transform pointer into shared memory state space.\n__device__ __forceinline__ void d_storeMatrixTile16x8_f32(float* dst, float reg[4], int n); To store 16x8 matrix output of mma instruction.\n\nThe sinature for the kernel k_mmaM16N8K8AcolBrow is\n__global__ void k_mmaM16N8K8AcolBrow(__nv_bfloat16 *colMajorA_d, __nv_bfloat16 *rowMajorA_d, float *resultMatrixC_d, int mDim, int nDim, int kDim) where colMajorA_d is input Matrix A stored in column major order, rowMajorA_d is Matrix B stored in row major order, resultMatrixC_d is the resultant matrix multiply output,\nmDim, nDim, kDim are leading dimensions of the problem (A -> mDim x kDim), (B -> kDim x nDim), (C -> mDim x nDim).\n\n>>> k_mmaM16N8K8AcolBrow({3, 6, 7, 5}, {3, 5}, resultMatrixC_d, 2, 1, 2)-> resultMatrixC_d: ({44, 43})\n>>> k_mmaM16N8K8AcolBrow({3, 6, 17, 15}, {13, 15}, resultMatrixC_d, 2, 1, 2)-> resultMatrixC_d: ({294, 303})\n", "cc_flags": "-arch=sm_80", "ld_flags": "", "declaration": "\n#include <cstdio>\n#include <cassert>\n#include <cstdint>\n#include <random>\n\n#include <cuda_runtime.h>\n#include <mma.h>\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n#undef NDEBUG\n\n#define MMA_M 16\n#define MMA_N 8\n#define MMA_K 8\n\n__global__ void k_mmaM16N8K8AcolBrow(__nv_bfloat16 *colMajorA_d, __nv_bfloat16 *rowMajorB_d, float *resultMatrixC_d, int mDim, int nDim, int kDim);\n\n// Function to compute valid reference result\n// Inputs => ( Column Major -> A, Row Major -> B)\nvoid cpuMatMulReference(const __nv_bfloat16* A,\n                        const __nv_bfloat16* B,\n                        float* cpuRefC,\n                        int M,\n                        int N,\n                        int K) {\n    for (int i = 0; i < M; i++) {\n        for (int j = 0; j < N; j++) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; k++) {\n                float a_val = static_cast<float>(A[k*M + i]);\n                float b_val = static_cast<float>(B[k*N + j]);\n                sum += a_val * b_val;\n            }\n            cpuRefC[i*N + j] = sum;\n        }\n    }\n}\n\nvoid launch() {\n\n    const int TEST_CASE_COUNT = 7;\n    const int PROBLEM_DIMS = 3;\n    //Test case dimensions {M, N, K}\n    const int MAX_M = 512;\n    const int MAX_N = 512;\n    const int MAX_K = 512;\n\n    const int TEST_CASES_DIMS[TEST_CASE_COUNT][PROBLEM_DIMS] = {{16,8,8}, {512,512,512}, {32,16,32}, {256, 256, 256}, {64, 64, 64} , {64, 32, 32}, {128, 128, 128}};\n\n    //Tolerance for validation, set to 1% due to nature of half precision operations\n    const float TOLERANCE  = 0.01;\n    const int BLOCK_SIZE = 256;\n\n\n    //Set up random number generation\n    std::mt19937 randEngine(time(nullptr));\n\n    // Bounded random distribution for test case initialization\n    std::uniform_real_distribution<float> randDist(1.0f, 100.0f);\n\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    //Pointers for Host Memory\n    __nv_bfloat16* A_h =(__nv_bfloat16*)malloc(MAX_M * MAX_K * sizeof(__nv_bfloat16));\n    __nv_bfloat16* B_h =(__nv_bfloat16*)malloc(MAX_K * MAX_N * sizeof(__nv_bfloat16));\n\n    float* cpuC_h =(float*)malloc(MAX_M * MAX_N * sizeof(float)); // Reference Matrix space allocation on host\n    float* gpuC_h = (float*)malloc(MAX_M * MAX_N * sizeof(float));// GPU result Matrix space allocation on host\n\n    //Pointers for device memory (GPU)\n    __nv_bfloat16* A_d;\n    __nv_bfloat16* B_d;\n    float* C_d;\n\n    // Allocate the memory on the device\n    CUDA_CHECK(cudaMallocAsync(&A_d, MAX_M * MAX_K * sizeof(__nv_bfloat16), stream));\n    CUDA_CHECK(cudaMallocAsync(&B_d, MAX_K * MAX_N * sizeof(__nv_bfloat16), stream));\n    CUDA_CHECK(cudaMallocAsync(&C_d, MAX_M * MAX_N * sizeof(float), stream));\n\n\n\n\n    for (int i = 0; i < TEST_CASE_COUNT; i++) {\n        // Dimensions of the input and output layers\n        int M = TEST_CASES_DIMS[i][0]; //Number of Rows in Matrix A\n        int N = TEST_CASES_DIMS[i][1]; //Number of Columns in Matrix B\n        int K = TEST_CASES_DIMS[i][2]; //Number of Columns in Matrix A and Rows in Matrix B\n\n        //Populating input matrices with random values\n        for (int c = 0; c < K ; ++c) {\n            for (int r = 0; r < M; ++r) {\n                float val = randDist(randEngine);\n                A_h[c * M + r] = __nv_bfloat16(val); // Filling A Matrix in Column Major Way\n            }\n       }\n\n        for (int r = 0; r < K; ++r) {\n            for (int c = 0; c < N; ++c) {\n                float  val = randDist(randEngine);\n                B_h[r * N + c] = __nv_bfloat16(val);\n            }\n        }\n\n        // Initialize the result on the device\n        CUDA_CHECK(cudaMemsetAsync(C_d, 0, M * N * sizeof(float), stream));\n\n        // Load Test Cases\n        CUDA_CHECK(cudaMemcpyAsync(A_d, A_h, M * K * sizeof(__nv_bfloat16), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(B_d, B_h, K * N * sizeof(__nv_bfloat16), cudaMemcpyHostToDevice, stream));\n\n        // Check if the dimensions are divisible by the block tile dimensions\n        assert(M % MMA_M == 0);\n        assert(N % MMA_N == 0);\n        assert(K % MMA_K == 0);\n\n        // dim3 gridDim((N + MMA_N - 1) / MMA_N, (M + MMA_M - 1) / MMA_M);\n        dim3 gridDim((N + MMA_N - 1) / MMA_N, (M + MMA_M - 1) / MMA_M);\n        dim3 blockDim(BLOCK_SIZE);  // one warp per block\n        int shmemBytes = (MMA_M * MMA_K + MMA_K * MMA_N) * sizeof(__nv_bfloat16);\n\n        // Launch kernel\n        // Grid: ((N + MMA_N - 1/ MMA_N), (M + MMA_M - 1)/ MMA_M, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&A_d,\n                        &B_d,\n                        &C_d,\n                        (void*)&M,\n                        (void*)&N,\n                        (void*)&K};\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_mmaM16N8K8AcolBrow,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    shmemBytes,\n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        cpuMatMulReference(A_h, B_h, cpuC_h, M, N, K);\n\n        //Copying the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync(gpuC_h, C_d, M * N * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        //Validate the result, with in 1% tolerance\n        for(int t = 0; t < M*N; ++t) {\n            assert(std::fabs((gpuC_h[t] - cpuC_h[t]) / std::fabs(cpuC_h[t])) <= TOLERANCE);\n        }\n\n   }\n    //Free up resources\n    CUDA_CHECK(cudaFreeAsync(A_d, stream));\n    CUDA_CHECK(cudaFreeAsync(B_d, stream));\n    CUDA_CHECK(cudaFreeAsync(C_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    free(A_h);\n    free(B_h);\n    free(cpuC_h);\n    free(gpuC_h);\n}\n\n\n\n// Convert generic pointer to to shared memory state space\n__device__ __forceinline__ uint32_t d_cvtaToSharedU32(const void* ptr) {\n    unsigned long long address;\n    asm volatile(\"cvta.to.shared.u64 %0, %1;\" : \"=l\"(address) : \"l\"(ptr));\n    return static_cast<uint32_t>(address);\n}\n\n// Store 16x8 mma operation output tile\n__device__ __forceinline__ void d_storeMatrixTile16x8_f32(float* dst, float reg[4], int n) {\n    int lane = threadIdx.x % 32;  // 0..31\n    int r = lane / 4;        // 0..7 for the top half\n    int c = (lane % 4) * 2;    // columns: 0,2,4,6\n    dst[r * n + c] = reg[0];\n    dst[r * n + c + 1] = reg[1];\n    dst[(r + 8) * n + c] = reg[2];\n    dst[(r + 8) * n + c + 1] = reg[3];\n}\n\n// Kernel: Multiply bf16 matrices colMajorA_d (mDimx kDim) and rowMajorB_d (kDim x nDim) to produce an output resultMatrixC_d (mDim x nDim) in f32.\n// Each block (a single warp) computes one 168 output tile using MMA instructions.\n__global__ void k_mmaM16N8K8AcolBrow(__nv_bfloat16 *colMajorA_d, __nv_bfloat16 *rowMajorB_d, float *resultMatrixC_d, int mDim, int nDim, int kDim) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/158", "date": "2025-07-30", "prompt": "Write a CUDA kernel to multiply multiple matrices with dimensions of 32 x 32 each (row major format). Ensure minimal bank collisions in shared memory accesses and coalesced access to global memory to maximize memory throughput.\n\nThe signature of the CUDA kernel is __global__ void k_multiplyManyMatrices(int *matricesA_d, int *matricesB_d, int *matricesC_d, int numMatrices), where matricesA_d is a pointer to the array of A matrices in order 32 x 32, matricesB_d is a pointer to the array of B matrices in order 32 x 32, matricesC_d is a pointer to the array of C matrices in order 32 x 32, and numMatrices represents the number of matrices in matricesA_d, matricesB_d, and matricesC_d to compute C = A * B for each pair of A and B.\n\n>>> k_multiplyManyMatrices({ element(i, j) = (i + j) % 2 }, { element(i, j) = 1 }, matricesC_d, 5) -> matricesC_d: { element(i, j) = 16 }\n>>> k_multiplyManyMatrices({ element(i, j) = ((i == j) ? 1 : 0)  }, { element(i, j) = k }, matricesC_d, 10) -> matricesC_d: { element(i, j) = k }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call){                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess){                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Algorithm settings.\n// This must stay as 32.\nconstexpr int MATRIX_WIDTH = 32;\n// This must stay as 32.\nconstexpr int MATRIX_HEIGHT = 32;\n// Padding amount added to the row stride to reduce shared-memory bank collisions during storing transposed matrix data to the shared memory.\nconstexpr int PADDING_SIZE_PER_ROW = 1;\nconstexpr int MATRIX_ROW_STRIDE_IN_SHARED_MEMORY = MATRIX_WIDTH + PADDING_SIZE_PER_ROW;\nconstexpr int NUM_MATRICES_IN_SHARED_MEMORY = 2;\n\n// CUDA settings.\nconstexpr int NUM_THREADS_PER_BLOCK_X = MATRIX_WIDTH;\nconstexpr int NUM_THREADS_PER_BLOCK_Y = MATRIX_HEIGHT;\n\n// Test settings.\nconstexpr int NUM_MATRICES_MAXIMUM = 1000;\nconstexpr int ALLOCATED_ELEMENTS_MAXIMUM = NUM_MATRICES_MAXIMUM * MATRIX_WIDTH * MATRIX_HEIGHT;\n\n__global__ void k_multiplyManyMatrices( int * matricesA_d, \n                                        int * matricesB_d, \n                                        int * matricesC_d, \n                                        int numMatrices);\n\nint launch() {\n    int deviceId = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    // Allocating host arrays.\n    int * matricesA_h = new int[ALLOCATED_ELEMENTS_MAXIMUM];\n    int * matricesB_h = new int[ALLOCATED_ELEMENTS_MAXIMUM];\n    int * matricesC_h = new int[ALLOCATED_ELEMENTS_MAXIMUM];\n    // Allocating device arrays.\n    int * matricesA_d;\n    int * matricesB_d;\n    int * matricesC_d;\n    \n    CUDA_CHECK(cudaMallocAsync(&matricesA_d, ALLOCATED_ELEMENTS_MAXIMUM * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&matricesB_d, ALLOCATED_ELEMENTS_MAXIMUM * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&matricesC_d, ALLOCATED_ELEMENTS_MAXIMUM * sizeof(int), stream));\n    // Zero-initializing device buffers.\n    CUDA_CHECK(cudaMemsetAsync(matricesA_d, 0, ALLOCATED_ELEMENTS_MAXIMUM * sizeof(int), stream));\n    CUDA_CHECK(cudaMemsetAsync(matricesB_d, 0, ALLOCATED_ELEMENTS_MAXIMUM * sizeof(int), stream));\n    CUDA_CHECK(cudaMemsetAsync(matricesC_d, 0, ALLOCATED_ELEMENTS_MAXIMUM * sizeof(int), stream));\n    // Test 1: A = [checkerboard pattern] matrix, B = [1] matrix  C = [16] matrix.\n    {\n        int numMatrices = 5;\n        int size = numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT * sizeof(int);\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            int row = (i / MATRIX_WIDTH) % MATRIX_HEIGHT;\n            int column = i % MATRIX_WIDTH;\n            matricesA_h[i] = ((row + column) % 2);\n            matricesB_h[i] = 1;\n        }\n        // Copying inputs to device.\n        CUDA_CHECK(cudaMemcpyAsync(matricesA_d, matricesA_h, size, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matricesB_d, matricesB_h, size, cudaMemcpyHostToDevice, stream));\n        // Calculating.\n        int sharedMemorySize = MATRIX_ROW_STRIDE_IN_SHARED_MEMORY * MATRIX_HEIGHT * NUM_MATRICES_IN_SHARED_MEMORY * sizeof(int);\n        assert(sharedMemorySize <= deviceProperties.sharedMemPerBlock);\n        void * args[4] = { &matricesA_d, &matricesB_d, &matricesC_d, &numMatrices };\n        int sm = deviceProperties.multiProcessorCount;\n        int threadsPerSm = deviceProperties.maxThreadsPerMultiProcessor;\n        int maxBlocks = (sm * threadsPerSm) / (NUM_THREADS_PER_BLOCK_X * NUM_THREADS_PER_BLOCK_Y);\n        int usedBlocksX = (maxBlocks < numMatrices ? maxBlocks : numMatrices);\n        int usedBlocksY = 1;\n        int usedBlocksZ = 1;\n        // Grid: (usedBlocksX, usedBlocksY, usedBlocksZ)\n        // Block: (NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_multiplyManyMatrices, dim3(usedBlocksX, usedBlocksY, usedBlocksZ), dim3(NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1), args, sharedMemorySize, stream));\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(matricesC_h, matricesC_d, size, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            assert(MATRIX_WIDTH / 2 == matricesC_h[i]);\n        }\n    }\n    // Test 2: A = Identity matrix, B = [matrix index] matrix  C = B.\n    {\n        int numMatrices = 10;\n        int size = numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT * sizeof(int);\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            int matrixId = i / (MATRIX_WIDTH * MATRIX_HEIGHT);\n            int row = (i / MATRIX_WIDTH) % MATRIX_HEIGHT;\n            int column = i % MATRIX_WIDTH;\n            matricesA_h[i] = (row == column ? 1 : 0);\n            matricesB_h[i] = matrixId;\n        }\n        // Copying inputs to device.\n        CUDA_CHECK(cudaMemcpyAsync(matricesA_d, matricesA_h, size, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matricesB_d, matricesB_h, size, cudaMemcpyHostToDevice, stream));\n        // Calculating.\n        int sharedMemorySize = MATRIX_ROW_STRIDE_IN_SHARED_MEMORY * MATRIX_HEIGHT * NUM_MATRICES_IN_SHARED_MEMORY * sizeof(int);\n        assert(sharedMemorySize <= deviceProperties.sharedMemPerBlock);\n        void * args[4] = { &matricesA_d, &matricesB_d, &matricesC_d, &numMatrices };\n        int sm = deviceProperties.multiProcessorCount;\n        int threadsPerSm = deviceProperties.maxThreadsPerMultiProcessor;\n        int maxBlocks = (sm * threadsPerSm) / (NUM_THREADS_PER_BLOCK_X * NUM_THREADS_PER_BLOCK_Y);\n        int usedBlocksX = (maxBlocks < numMatrices ? maxBlocks : numMatrices);\n        int usedBlocksY = 1;\n        int usedBlocksZ = 1;\n        // Grid: (usedBlocksX, usedBlocksY, usedBlocksZ)\n        // Block: (NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_multiplyManyMatrices, dim3(usedBlocksX, usedBlocksY, usedBlocksZ), dim3(NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1), args, sharedMemorySize, stream));\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(matricesC_h, matricesC_d, size, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            int matrixId = i / (MATRIX_WIDTH * MATRIX_HEIGHT);\n            assert(matrixId == matricesC_h[i]);\n        }\n    }\n    // Test 3: A is a randomized matrix, B is the identity matrix, resulting in C = A.\n    {\n        std::random_device randomizationDevice;\n        std::mt19937 mersenneTwister(randomizationDevice());\n        std::uniform_int_distribution<int> distribution(0, 1000000);\n        int numMatrices = NUM_MATRICES_MAXIMUM;\n        int size = numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT * sizeof(int);\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            matricesA_h[i] = distribution(mersenneTwister);\n            int column = i % MATRIX_WIDTH;\n            int row = (i / MATRIX_WIDTH) % MATRIX_HEIGHT;\n            matricesB_h[i] = (column == row ? 1 : 0);\n        }\n        // Copying inputs to device.\n        CUDA_CHECK(cudaMemcpyAsync(matricesA_d, matricesA_h, size, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matricesB_d, matricesB_h, size, cudaMemcpyHostToDevice, stream));\n        // Calculating.\n        int sharedMemorySize = MATRIX_ROW_STRIDE_IN_SHARED_MEMORY * MATRIX_HEIGHT * NUM_MATRICES_IN_SHARED_MEMORY * sizeof(int);\n        assert(sharedMemorySize <= deviceProperties.sharedMemPerBlock);\n        void * args[4] = { &matricesA_d, &matricesB_d, &matricesC_d, &numMatrices };\n        int sm = deviceProperties.multiProcessorCount;\n        int threadsPerSm = deviceProperties.maxThreadsPerMultiProcessor;\n        int maxBlocks = (sm * threadsPerSm) / (NUM_THREADS_PER_BLOCK_X * NUM_THREADS_PER_BLOCK_Y);\n        int usedBlocksX = (maxBlocks < numMatrices ? maxBlocks : numMatrices);\n        int usedBlocksY = 1;\n        int usedBlocksZ = 1;\n        // Grid: (usedBlocksX, usedBlocksY, usedBlocksZ)\n        // Block: (NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_multiplyManyMatrices, dim3(usedBlocksX, usedBlocksY, usedBlocksZ), dim3(NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1), args, sharedMemorySize, stream));\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(matricesC_h, matricesC_d, size, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            assert(matricesA_h[i] == matricesC_h[i]);\n        }\n    }\n    // Test 4: A = randomized matrix, B = randomized matrix  C = A * B.\n    {\n        std::random_device randomizationDevice;\n        std::mt19937 mersenneTwister(randomizationDevice());\n        std::uniform_int_distribution<int> distribution(0, 1000000);\n        int numMatrices = NUM_MATRICES_MAXIMUM;\n        int size = numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT * sizeof(int);\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            matricesA_h[i] = distribution(mersenneTwister);\n            matricesB_h[i] = distribution(mersenneTwister);\n        }\n        // Copying inputs to device.\n        CUDA_CHECK(cudaMemcpyAsync(matricesA_d, matricesA_h, size, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matricesB_d, matricesB_h, size, cudaMemcpyHostToDevice, stream));\n        // Calculating.\n        int sharedMemorySize = MATRIX_ROW_STRIDE_IN_SHARED_MEMORY * MATRIX_HEIGHT * NUM_MATRICES_IN_SHARED_MEMORY * sizeof(int);\n        assert(sharedMemorySize <= deviceProperties.sharedMemPerBlock);\n        void * args[4] = { &matricesA_d, &matricesB_d, &matricesC_d, &numMatrices };\n        int sm = deviceProperties.multiProcessorCount;\n        int threadsPerSm = deviceProperties.maxThreadsPerMultiProcessor;\n        int maxBlocks = (sm * threadsPerSm) / (NUM_THREADS_PER_BLOCK_X * NUM_THREADS_PER_BLOCK_Y);\n        int usedBlocksX = (maxBlocks < numMatrices ? maxBlocks : numMatrices);\n        int usedBlocksY = 1;\n        int usedBlocksZ = 1;\n        // Grid: (usedBlocksX, usedBlocksY, usedBlocksZ)\n        // Block: (NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_multiplyManyMatrices, dim3(usedBlocksX, usedBlocksY, usedBlocksZ), dim3(NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1), args, sharedMemorySize, stream));\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(matricesC_h, matricesC_d, size, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        for(int i = 0; i < numMatrices; i++) {\n            int globalOffset = i * MATRIX_WIDTH * MATRIX_HEIGHT;\n            for(int y = 0; y < MATRIX_HEIGHT; y++) {\n                for(int x = 0; x < MATRIX_WIDTH; x++) {\n                    int c = 0;\n                    for(int k = 0; k < MATRIX_WIDTH; k++) {\n                        int a = matricesA_h[globalOffset + y * MATRIX_WIDTH + k];\n                        int b = matricesB_h[globalOffset + k * MATRIX_WIDTH + x];\n                        c += a * b;\n                    }\n                    assert(c == matricesC_h[globalOffset + x + y * MATRIX_WIDTH]);\n                }\n            }\n        }\n    }\n    // Test 5: A = [0] matrix, B = randomized matrix  C = [0] matrix.\n    {\n        std::random_device randomizationDevice;\n        std::mt19937 mersenneTwister(randomizationDevice());\n        std::uniform_int_distribution<int> distribution(0, 1000000);\n        int numMatrices = NUM_MATRICES_MAXIMUM;\n        int size = numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT * sizeof(int);\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            matricesA_h[i] = 0;\n            matricesB_h[i] = distribution(mersenneTwister);\n        }\n        // Copying inputs to device.\n        CUDA_CHECK(cudaMemcpyAsync(matricesA_d, matricesA_h, size, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matricesB_d, matricesB_h, size, cudaMemcpyHostToDevice, stream));\n        // Calculating.\n        int sharedMemorySize = MATRIX_ROW_STRIDE_IN_SHARED_MEMORY * MATRIX_HEIGHT * NUM_MATRICES_IN_SHARED_MEMORY * sizeof(int);\n        assert(sharedMemorySize <= deviceProperties.sharedMemPerBlock);\n        void * args[4] = { &matricesA_d, &matricesB_d, &matricesC_d, &numMatrices };\n        int sm = deviceProperties.multiProcessorCount;\n        int threadsPerSm = deviceProperties.maxThreadsPerMultiProcessor;\n        int maxBlocks = (sm * threadsPerSm) / (NUM_THREADS_PER_BLOCK_X * NUM_THREADS_PER_BLOCK_Y);\n        int usedBlocksX = (maxBlocks < numMatrices ? maxBlocks : numMatrices);\n        int usedBlocksY = 1;\n        int usedBlocksZ = 1;\n        // Grid: (usedBlocksX, usedBlocksY, usedBlocksZ)\n        // Block: (NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_multiplyManyMatrices, dim3(usedBlocksX, usedBlocksY, usedBlocksZ), dim3(NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1), args, sharedMemorySize, stream));\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(matricesC_h, matricesC_d, size, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            assert(0 == matricesC_h[i]);\n        }\n    }\n    // Test 6: A = Identity matrix, B = Identity matrix  C = Identity matrix.\n    {\n        int numMatrices = NUM_MATRICES_MAXIMUM;\n        int size = numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT * sizeof(int);\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            int column = i % MATRIX_WIDTH;\n            int row = (i / MATRIX_WIDTH) % MATRIX_HEIGHT;\n            matricesA_h[i] = (column == row ? 1 : 0);\n            matricesB_h[i] = (column == row ? 1 : 0);\n        }\n        // Copying inputs to device.\n        CUDA_CHECK(cudaMemcpyAsync(matricesA_d, matricesA_h, size, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matricesB_d, matricesB_h, size, cudaMemcpyHostToDevice, stream));\n        // Calculating.\n        int sharedMemorySize = MATRIX_ROW_STRIDE_IN_SHARED_MEMORY * MATRIX_HEIGHT * NUM_MATRICES_IN_SHARED_MEMORY * sizeof(int);\n        assert(sharedMemorySize <= deviceProperties.sharedMemPerBlock);\n        void * args[4] = { &matricesA_d, &matricesB_d, &matricesC_d, &numMatrices };\n        int sm = deviceProperties.multiProcessorCount;\n        int threadsPerSm = deviceProperties.maxThreadsPerMultiProcessor;\n        int maxBlocks = (sm * threadsPerSm) / (NUM_THREADS_PER_BLOCK_X * NUM_THREADS_PER_BLOCK_Y);\n        int usedBlocksX = (maxBlocks < numMatrices ? maxBlocks : numMatrices);\n        int usedBlocksY = 1;\n        int usedBlocksZ = 1;\n        // Grid: (usedBlocksX, usedBlocksY, usedBlocksZ)\n        // Block: (NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_multiplyManyMatrices, dim3(usedBlocksX, usedBlocksY, usedBlocksZ), dim3(NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1), args, sharedMemorySize, stream));\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(matricesC_h, matricesC_d, size, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            int row = (i / MATRIX_WIDTH) % MATRIX_HEIGHT;\n            int column = i % MATRIX_WIDTH;\n            assert((column == row ? 1 : 0) == matricesC_h[i]);\n        }\n    }\n    // Test 7: A = [1] matrix, B = [alternating 1 and -1 values for increasing values of i + j, where i is the column and j is the row of the matrix]  C = [0] matrix.\n    {\n        int numMatrices = NUM_MATRICES_MAXIMUM;\n        int size = numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT * sizeof(int);\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            matricesA_h[i] = 1;\n            int column = i % MATRIX_WIDTH;\n            int row = (i / MATRIX_WIDTH) % MATRIX_HEIGHT;\n            matricesB_h[i] = (((column + row) % 2) ? -1 : 1);\n        }\n        // Copying inputs to device.\n        CUDA_CHECK(cudaMemcpyAsync(matricesA_d, matricesA_h, size, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matricesB_d, matricesB_h, size, cudaMemcpyHostToDevice, stream));\n        // Calculating.\n        int sharedMemorySize = MATRIX_ROW_STRIDE_IN_SHARED_MEMORY * MATRIX_HEIGHT * NUM_MATRICES_IN_SHARED_MEMORY * sizeof(int);\n        assert(sharedMemorySize <= deviceProperties.sharedMemPerBlock);\n        void * args[4] = { &matricesA_d, &matricesB_d, &matricesC_d, &numMatrices };\n        int sm = deviceProperties.multiProcessorCount;\n        int threadsPerSm = deviceProperties.maxThreadsPerMultiProcessor;\n        int maxBlocks = (sm * threadsPerSm) / (NUM_THREADS_PER_BLOCK_X * NUM_THREADS_PER_BLOCK_Y);\n        int usedBlocksX = (maxBlocks < numMatrices ? maxBlocks : numMatrices);\n        int usedBlocksY = 1;\n        int usedBlocksZ = 1;\n        // Grid: (usedBlocksX, usedBlocksY, usedBlocksZ)\n        // Block: (NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_multiplyManyMatrices, dim3(usedBlocksX, usedBlocksY, usedBlocksZ), dim3(NUM_THREADS_PER_BLOCK_X, NUM_THREADS_PER_BLOCK_Y, 1), args, sharedMemorySize, stream));\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(matricesC_h, matricesC_d, size, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        for(int i = 0; i < numMatrices * MATRIX_WIDTH * MATRIX_HEIGHT; i++) {\n            assert(0 == matricesC_h[i]);\n        }\n    }\n    CUDA_CHECK(cudaFreeAsync(matricesA_d, stream));\n    CUDA_CHECK(cudaFreeAsync(matricesB_d, stream));\n    CUDA_CHECK(cudaFreeAsync(matricesC_d, stream));\n    delete [] matricesA_h;\n    delete [] matricesB_h;\n    delete [] matricesC_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    return 0;\n}\n\n__global__ void k_multiplyManyMatrices( int * matricesA_d, \n                                        int * matricesB_d, \n                                        int * matricesC_d, \n                                        int numMatrices) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/159", "date": "2025-07-30", "prompt": "Write a CUDA kernel to implement a particle swarm optimization algorithm to update the particle's best position. Use shared memory with coalesced memory access to improve the performance of the implementation.\n\nThe signature of the function is __global__ void k_ParticleSwarmOptimisation(float *position, float *velocity, float *pBestPosition, float *pBestFitness, int numParticles, float *randomValue) where positions, velocities store current positions and velocities of all particles, pBestPositions, and pBestFitness stores the personal best positions and personal fitness value for each particle, numParticles is the total number of particles used in the algorithm, randomValue is used to generate random numbers for velocity updates.\n\n>>> k_ParticleSwarmOptimisation(position, velocity, pBestPosition, pBestFitness, 5, randomValue): pBestPosition -> {{-64.7809, 23.5435}, {-0.490625, 59.9253}, {25.6428, 56.9856}, {66.7461, 58.7949}, {-13.0772, 75.7407}}\n>>> k_ParticleSwarmOptimisation(position, velocity, pBestPosition, pBestFitness, 10, randomValue): pBestPosition -> {{-47.7556,-29.8584}, {-73.3662, -100}, {-46.9425, 0.208972}, {-63.3111, -2.76497}, {63.3932, 38.5272}, {66.1842, -35.4868}, {96.2093, 44.6674}, {-71.6488, 63.5543}, {78.9458, -27.2473}, {-80.6111, -84.3671}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <limits>\n#include <cstdio>\n#include <iostream>\n#include <cfloat>\n#include <cmath>\n#include <cuda.h>\n#include <curand_kernel.h>\n#include <cuda_runtime.h>\n#include <algorithm>\n#include <cassert>\n#include <random>\n\n#define THREADS_PER_BLOCK 256\n#define NUM_DIMENSIONS 2\n#define TOLERANCE 1E-4\n\nconst float SEARCH_MIN = -100.0f;\nconst float SEARCH_MAX = 100.0f;\nconst float VELOCITY_MAX = 10.0f;\nconst float VELOCITY_MIN = -10.0f;\nconst float inertia = 0.5f;\nconst float cognitive = 1.5f;\n\n#define CUDA_CHECK(call)                                  \\\ndo {                                                     \\\n    cudaError_t error = call;                            \\\n    if (error != cudaSuccess) {                          \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",    \\\n                __FILE__, __LINE__, cudaGetErrorString(error)); \\\n        exit(EXIT_FAILURE);                              \\\n    }                                                    \\\n} while (0)\n\n__global__ void k_ParticleSwarmOptimisation(float *position, float *velocity, float *pBestPosition, float *pBestFitness, int numParticles, float *randomValue);\n\n#undef NDEBUG\n\nvoid launch() {\n    const int TEST_CASE_COUNT = 7; // Number of test cases\n\n    int numberOfParticles[TEST_CASE_COUNT] = {5, 10, 15, 20, 25, 30, 35};\n\n    int maxParticles = *std::max_element(numberOfParticles, numberOfParticles + TEST_CASE_COUNT);\n\n    float expectedValues[TEST_CASE_COUNT][maxParticles][NUM_DIMENSIONS] = {\n        {{-64.7809, 23.5435}, {-0.490625, 59.9253}, {25.6428, 56.9856}, {66.7461, 58.7949}, {-13.0772, 75.7407}},\n\n        {{-47.7556,-29.8584}, {-73.3662, -100}, {-46.9425, 0.208972}, {-63.3111, -2.76497}, {63.3932, 38.5272},\n         {66.1842, -35.4868}, {96.2093, 44.6674}, {-71.6488, 63.5543}, {78.9458, -27.2473}, {-80.6111, -84.3671}},\n\n        {{12.8512, 100}, {-93.2414, -27.547}, {0.647462, -80.3369}, {-12.0408, 90.9594}, {-100, -22.3057}, {-100, 31.7894}, \n        {57.2936, -9.92673}, {-43.947, -21.5875}, {80.3547, 8.01104}, {-53.2572, 60.6333}, {-28.3739, 61.0306}, {50.0402, -38.4644}, {24.2332, -7.17437}, {81.7375, 14.3007}, {-89.1699, -13.5802}},\n\n        {{77.5168, -92.4401}, {14.5905, 100}, {-13.4036, -66.2997}, {98.5444, 39.7395}, {63.451, -95.0554}, {-96.4832, 90.2047}, {-74.809, 36.725}, \n        {10.1051, 87.8045}, {42.8948, 19.8712}, {-7.85891, 99.8134}, {42.9621, 6.99513}, {-25.9408, -21.6296}, {-59.0537, -95.9019}, \n        {-36.1385, -49.8485}, {89.2222, 12.9009}, {65.8411, -14.6682}, {-12.1504, -35.7696}, {30.776, 55.5515}, {85.9564, 0.623029}, {-95.0404, 20.9435}},\n        \n        {{-81.5022, -99.579}, {-88.0011, -63.9082}, {22.5107, 85.3715}, {42.9543, -24.7133}, {13.8484, 60.1477}, {-70.5572, -93.6865}, \n        {-100, 25.0562}, {-98.2066, -10.1538}, {24.6628, -60.5663}, {-36.0884, 100}, {86.5458, -43.6162}, {-5.4894, -78.972}, {61.0101, 33.9244}, {-90.7022, -79.93}, {100, 81.1148}, {-24.6308, 50.0899}, \n        {96.3484, 35.9471}, {-80.1036, 18.3338}, {61.3125, -35.767}, {83.229, -5.95717}, {-45.0973, -61.454}, {55.5048, -82.5034}, {26.2325, -100}, {-26.165, -56.8641}, {-4.6003, -89.9759}},\n\n        {{83.9927, -72.9597}, {48.3117, -68.9917}, {-17.4818, 14.366}, {100, -77.3153}, {7.52885, 5.9271}, {39.9034, -18.4993}, {-100, 94.8}, \n        {100, -99.7802}, {-41.8652, -4.12458}, {40.0122, 13.3478}, {-13.2524, 0.537505}, {94.1059, 59.1944}, {23.5513, 7.74441}, {17.5654, -48.9174}, {87.8216, 67.0324}, {6.62619, -60.0065}, \n        {26.4047, -93.0057}, {-69.582, -67.713}, {43.2595, 35.5786}, {-71.6504, -14.8542}, {-73.5349, 56.0945}, \n        {91.9727, 99.1438}, {51.6733, 43.7042}, {45.8356, -95.2284}, {69.5115, 62.3421}, {-4.92604, 87.6444}, {28.0824, 12.1298}, {80.5672, -71.6247}, {-12.9549, 97.8257}, {-20.1175, 93.1104}},\n\n        {{-74.097, -88.3883}, {55.2639, 20.8225}, {-98.7741, -26.6602}, {-93.6856, -43.8637}, {19.7176, 91.0636}, {-51.6429, -73.207}, \n        {-81.0462, -17.0678}, {89.8743, -34.6133}, {94.6701, -55.8156}, {71.0454, 33.0859}, {-36.6014, 15.2218},\n        {-72.0459, -89.5981}, {-64.3401, 15.1433}, {40.0629, -93.4487}, {-8.85955, -17.4525}, {-27.9413, -37.1291}, \n        {88.2845, 26.4355}, {91.8693, 19.0012}, {79.6045, -58.7239}, {23.8066, 74.3187}, {-51.9507, 25.6324}, {41.8635, -44.7169}, \n        {-31.9182, -4.68896}, {0.618711, -68.6559}, {-66.6564, 99.6118}, {44.3333, -76.8555}, {84.3772, 57.3268}, \n        {-76.779, 100}, {43.3709, -99.3009}, {69.0757, -67.3055}, {47.5983, -58.5853}, {50.9976, -38.3152}, {84.0184, -79.7311}, {-9.02291, 14.3412}, {58.6244, -68.1925}}\n    };\n \n    float *positions_h = (float *)malloc(NUM_DIMENSIONS * maxParticles * sizeof(float));\n    float *pBestPositions_h = (float *)malloc(NUM_DIMENSIONS * maxParticles * sizeof(float));\n    float *velocity_h = (float *)malloc(NUM_DIMENSIONS * maxParticles * sizeof(float));\n    float *pBestFitness_h = (float *)malloc(maxParticles * sizeof(float));\n    float *randomNumber_h = (float *)malloc(NUM_DIMENSIONS * maxParticles * sizeof(float));\n\n    float *positions_d, *velocities_d, *pBestPositions_d, *pBestFitness_d, *randNumber_d;\n    \n    unsigned int seed = 1234; \n    \n    // Create a random number generator with the seed\n    std::mt19937 randGen(seed);  // Mersenne Twister PRNG\n\n    // Define a uniform distribution between 0 and 1\n    std::uniform_real_distribution<float> UnifDist(0.0, 1.0);\n   \n    // Fetch GPU properties\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, 0);\n    int maxBlocks = (prop.maxThreadsPerMultiProcessor / THREADS_PER_BLOCK) * prop.multiProcessorCount;\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    CUDA_CHECK(cudaMallocAsync(&positions_d, NUM_DIMENSIONS * maxParticles * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&pBestPositions_d, NUM_DIMENSIONS * maxParticles * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&velocities_d, NUM_DIMENSIONS * maxParticles * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&pBestFitness_d, maxParticles * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&randNumber_d, NUM_DIMENSIONS * maxParticles * sizeof(float), stream));\n\n    for (int t = 0; t < TEST_CASE_COUNT; t++) {  \n    \n        int numParticles = numberOfParticles[t];\n\n        // Initialize the particles position and value\n        for (int d = 0; d < NUM_DIMENSIONS; d++) {\n            for (int i = 0; i < numParticles; i++) {\n                randomNumber_h[d * numParticles + i] = UnifDist(randGen);\n            }\n        }\n\n        for (int d = 0; d < NUM_DIMENSIONS; d++ ) {\n            for (int i = 0; i < numParticles; i++ ) {\n                positions_h[d * numParticles + i] = randomNumber_h[d * numParticles + i] * (SEARCH_MAX - SEARCH_MIN) + SEARCH_MIN;\n                velocity_h[d * numParticles + i] = randomNumber_h[d * numParticles + i] * (VELOCITY_MAX - VELOCITY_MIN) + VELOCITY_MIN;\n                pBestPositions_h[d * numParticles + i] = positions_h[d * numParticles + i];\n            }\n        }\n\n        for (int i = 0; i < numParticles; i++) {\n            pBestFitness_h[i] = FLT_MAX;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(positions_d, positions_h, NUM_DIMENSIONS * numParticles * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(pBestPositions_d, pBestPositions_h, NUM_DIMENSIONS * numParticles * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(velocities_d, velocity_h, NUM_DIMENSIONS * numParticles * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(pBestFitness_d, pBestFitness_h, numParticles * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(randNumber_d, randomNumber_h, NUM_DIMENSIONS * numParticles * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Set the Grid dimension\n        int blocksPerGrid = std::min(maxBlocks, (numParticles + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK);\n        int numSharedMemblocks = 4;\n        // Particle Swarm Optimisation Kernel Launch \n        void *psoArgs[] = {&positions_d, &velocities_d, &pBestPositions_d, &pBestFitness_d, &numParticles, &randNumber_d};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_ParticleSwarmOptimisation, blocksPerGrid, THREADS_PER_BLOCK, psoArgs, numSharedMemblocks * THREADS_PER_BLOCK * NUM_DIMENSIONS * sizeof(float), stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Copy best particle position and fitness value back to host\n        CUDA_CHECK(cudaMemcpyAsync(pBestPositions_h, pBestPositions_d, numParticles* NUM_DIMENSIONS * sizeof(float), cudaMemcpyDeviceToHost, stream));        \n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < numParticles; i++) {\n            for (int d = 0; d < NUM_DIMENSIONS; d++) {\n                float actual = pBestPositions_h[d * numParticles + i];\n                float expected = expectedValues[t][i][d];\n                assert(std::fabs(actual - expected) < TOLERANCE);\n            }\n        }\n        \n    }\n\n    free(positions_h);\n    free(pBestPositions_h);\n    free(velocity_h);\n    free(pBestFitness_h);\n    CUDA_CHECK(cudaFreeAsync(positions_d, stream));\n    CUDA_CHECK(cudaFreeAsync(velocities_d, stream));\n    CUDA_CHECK(cudaFreeAsync(pBestPositions_d, stream));\n    CUDA_CHECK(cudaFreeAsync(pBestFitness_d, stream));\n    CUDA_CHECK(cudaFreeAsync(randNumber_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_ParticleSwarmOptimisation(float *position, float *velocity, float *pBestPosition, float *pBestFitness, int numParticles, float *randomValue) \n{  \n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/160", "date": "2025-07-30", "prompt": "Use CUB's device-wide primitives to compute inclusive prefix maximum on a device array.\n\nThe signature of the function is void computePrefixMax(int* input_d, int* output_d, int dataSize, cudaStream_t stream), where input_d: pointer to the input array in device memory, output_d: pointer to the output array in device memory, dataSize: number of elements to process in the input/output arrays, and stream: CUDA stream for asynchronous execution of the operation.\n>>> computePrefixMax({23, 17, 45, 31, 89, 56, 72, 94, 38, 61},output_d,10)->output_d:({23, 23, 45, 45, 89, 89, 89, 94, 94, 94})\n>>> computePrefixMax({105, 267, 143, 398, 215, 476, 182, 354, 429, 167},output_d,10)->output_d:({105, 267, 267, 398, 398, 476, 476, 476, 476, 476})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cub/device/device_scan.cuh>\n#include <cstdio>\n\n#define CHECK_CUDA(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            std::cerr << \"CUDA error at \" << __FILE__ << \":\" << __LINE__ << \": \" \\\n                      << cudaGetErrorString(err) << std::endl; \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while (0)\n\nvoid computePrefixMax(int* input_d, int* output_d, int dataSize, cudaStream_t stream);\n\nvoid launch() {\n    const int ARRAY_SIZE = 10;\n    const int NUM_TEST_CASES = 7;\n\n    int testInputs[NUM_TEST_CASES][ARRAY_SIZE] = {\n        {23, 17, 45, 31, 89, 56, 72, 94, 38, 61},\n        {105, 267, 143, 398, 215, 476, 182, 354, 429, 167},\n        {42, 87, 13, 95, 28, 64, 159, 73, 126, 35},\n        {543, 218, 892, 147, 635, 479, 761, 324, 958, 186},\n        {1253, 3867, 2491, 4716, 1875, 3142, 5283, 2768, 4395, 1624},\n        {82, 157, 43, 196, 124, 238, 67, 175, 93, 142},\n        {317, 582, 164, 843, 275, 649, 431, 796, 528, 937}\n    };\n\n    // Expected results for each test case\n    int expectedResults[NUM_TEST_CASES][ARRAY_SIZE] = {\n        {23, 23, 45, 45, 89, 89, 89, 94, 94, 94},\n        {105, 267, 267, 398, 398, 476, 476, 476, 476, 476},\n        {42, 87, 87, 95, 95, 95, 159, 159, 159, 159},\n        {543, 543, 892, 892, 892, 892, 892, 892, 958, 958},\n        {1253, 3867, 3867, 4716, 4716, 4716, 5283, 5283, 5283, 5283},\n        {82, 157, 157, 196, 196, 238, 238, 238, 238, 238},\n        {317, 582, 582, 843, 843, 843, 843, 843, 843, 937}\n    };\n\n    cudaStream_t stream;\n    CHECK_CUDA(cudaStreamCreate(&stream));\n\n    int *input_d, *output_d;\n\n    CHECK_CUDA(cudaMallocAsync(&input_d, ARRAY_SIZE * sizeof(int), stream));\n    CHECK_CUDA(cudaMallocAsync(&output_d, ARRAY_SIZE * sizeof(int), stream));\n\n    CHECK_CUDA(cudaStreamSynchronize(stream));\n\n    int outputArray[ARRAY_SIZE];\n\n    for (int testIdx = 0; testIdx < NUM_TEST_CASES; testIdx++) {\n\n        CHECK_CUDA(cudaMemcpyAsync(\n            input_d, testInputs[testIdx],\n            ARRAY_SIZE * sizeof(int),\n            cudaMemcpyHostToDevice, stream));\n\n        computePrefixMax(input_d, output_d, ARRAY_SIZE, stream);\n\n        CHECK_CUDA(cudaMemcpyAsync(\n            outputArray, output_d,\n            ARRAY_SIZE * sizeof(int),\n            cudaMemcpyDeviceToHost, stream));\n\n        CHECK_CUDA(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < ARRAY_SIZE; i++) {\n          assert(outputArray[i] == expectedResults[testIdx][i]);\n        }\n    }\n\n    CHECK_CUDA(cudaFreeAsync(input_d, stream));\n    CHECK_CUDA(cudaFreeAsync(output_d, stream));\n    CHECK_CUDA(cudaStreamSynchronize(stream));\n    CHECK_CUDA(cudaStreamDestroy(stream));\n}\n\nvoid computePrefixMax(int* input_d, int* output_d, int dataSize, cudaStream_t stream) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/161", "date": "2025-07-30", "prompt": "Write a function which uses cuda streams and events to run k_convertStringToFloat kernel. Use distinct streams for multiple independent operations to have three stages of the three-way pipeline with overlapping execution. Streams like mainStream, pipelineStreams[0], pipelineStreams[1], and pipelineStreams[2] can be used together with the events eventHostToDevice, eventCompute, eventDeviceToHost, and eventIterationComplete. mainStream should serve as the stream for host-device buffer allocations, deallocations, and synchronizations. The mainStream stream waits for events eventHostToDevice, eventCompute, and eventDeviceToHost before recording the eventIterationComplete for each pipeline iteration. pipelineStreams[0] stream should serve as the stream for host-to-device copy commands, waiting for the eventIterationComplete after each iteration loop before performing the copy and recording the eventHostToDevice. pipelineStreams[1] stream should serve as the stream for kernel executions, waiting for the eventIterationComplete after each iteration loop and recording the eventCompute. pipelineStreams[2] should be dedicated to device-to-host copy commands, waiting for the eventIterationComplete after each iteration loop and recording the eventDeviceToHost.\nThe kernel k_convertStringToFloat should convert substrings containing floating-point number texts into an array of floating-point variables. For simplicity, limit the processing to a fixed number of input characters per CUDA thread using the \"%+.6e\" scientific format zero-padded into \"%+.9e\" format.\n\nThe signature of the function is void run(unsigned char * input_h, float * output_h, dim3 blockDim, dim3 gridDim), where input_h is a pointer to the input unsigned char array on host, output_h is a pointer to the output float array on host, blockDim is the block dimensions for the CUDA kernel, gridDim is the grid dimensions for the CUDA kernel.\n\nThe signature of the CUDA kernel is __global__ void k_convertStringToFloat(unsigned char * input_d, float * output_d, int numElements), where input_d is a pointer to the input string that contains substrings of floating-point numbers formatted as \"%+.6e   \" using exactly 16 characters for each number. output_d is a pointer to the output array of floating-point values resulting from the conversion of 16-length substrings from the input_d parameter, and numElements represents the number of output elements, which is 1/16 of the length of the string pointed to by input_d.\n\n>>> run(\"+3.141593000e+00+2.997925000e+08+6.626070000e-34\", output_h, blockDim, gridDim) -> output_h: { 3.141593e+00f, 2.997925e+08f, 6.626070e-34f }\n>>> run(\"+1.602177000e-19+6.674301000e-11+9.806650000e+00\", output_h, blockDim, gridDim) -> output_h: { 1.602177e-19f, 6.674301e-11f, 9.806650e+00f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <cstdio>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Algorithm-related constants.\n// Each chunk is processed independently.\nconstexpr int NUM_ELEMENTS_PER_CHUNK = 3;\nconstexpr int NUM_TOTAL_ELEMENTS = 10;\nconstexpr int NUM_CHUNKS = 1 + (NUM_TOTAL_ELEMENTS - 1) / NUM_ELEMENTS_PER_CHUNK;\n// The 3-way pipeline is designed for copying inputs, executing the kernel, and transferring results.\nconstexpr int NUM_PIPELINE_STAGES = 3;\n// 1 chunk requires 3 iterations.\n// 5 chunks require 7 iterations\n// The more chunks used, the higher the efficiency.\nconstexpr int NUM_PIPELINE_ITERATIONS = NUM_CHUNKS + (NUM_PIPELINE_STAGES - 1);\n\n// The length of the substrings that are in scientific format \"%+.6e\" which are zero-padded into \"%+.9e\".\nconstexpr int NUM_CHARACTERS_PER_ELEMENT = 16;\n\n// CUDA-related constants.\nconstexpr int MINIMUM_ALLOWED_NUMBER_OF_THREADS_PER_BLOCK = 4;\nconstexpr int MINIMUM_ALLOWED_NUMBER_OF_BLOCKS_PER_GRID = 1;\n\n// Test-related constants.\nconstexpr int NUM_TESTS = 7;\nconstexpr float ERROR_TOLERANCE = 0.000001f;\nconstexpr float NEARLY_ZERO = 1e-30;\n\n// input_d: input string with 16-characters per number.\n// output_d: array of output values converted from the string.\n// numElements: number of elements to process inside current chunk. Chunk can have a maximum NUM_ELEMENTS_PER_CHUNK elements.\n__global__ void k_convertStringToFloat(unsigned char * input_d, float * output_d, int numElements) {\n    int numBlocks = gridDim.x;\n    int numThreads = blockDim.x;\n    int numGridThreads = numBlocks * numThreads;\n    int numGridStrideLoopIterations = 1 + (numElements - 1) / numGridThreads;\n    int id = threadIdx.x + blockIdx.x * numThreads;\n    for(int stride = 0; stride < numGridStrideLoopIterations; stride++) {\n        int itemId = id + stride * numGridThreads;\n        if(itemId < numElements) {\n            // Reading 16 characters.\n            uint4 data = *reinterpret_cast<uint4*>(&input_d[itemId * NUM_CHARACTERS_PER_ELEMENT]);\n            unsigned char * ptr = reinterpret_cast<unsigned char*>(&data);\n            // Transforming the characters into a floating-point value.\n            float sign = (ptr[0] == '-') ? -1.0f : 1.0f;\n            float wholeNumberPart = (ptr[1] - '0');\n            float fractionalPart1 = (ptr[3] - '0') * 0.1f;\n            float fractionalPart2 = (ptr[4] - '0') * 0.01f;\n            float fractionalPart3 = (ptr[5] - '0') * 0.001f;\n            float fractionalPart4 = (ptr[6] - '0') * 0.0001f;\n            float fractionalPart5 = (ptr[7] - '0') * 0.00001f;\n            float fractionalPart6 = (ptr[8] - '0') * 0.000001f;\n            // Skipping three '0' padding characters in the fraction part that are ptr[9], ptr[10], ptr[11] and the 'e' character which is ptr[12].\n            float exponentionalSign = (ptr[13] == '-') ? -1.0f : 1.0f;\n            float exponentialTen = (ptr[14] - '0') * 10.0f;\n            float exponentialOne = (ptr[15] - '0');\n            float result = fractionalPart6 + fractionalPart5;\n            result += fractionalPart4;\n            result += fractionalPart3;\n            result += fractionalPart2;\n            result += fractionalPart1;\n            result += wholeNumberPart;\n            result *= powf(10.0f, exponentionalSign * (exponentialTen + exponentialOne));\n            result *= sign;\n            output_d[itemId] = result;\n        }\n    }\n}\n\n// ---- Three-Way Pipeline Utilizing CUDA Streams and Events ----\n// 3-way pipeline to overlap input, output, and kernel execution in the same time window. The first stream only copies data to the device. The second stream only computes data. The third stream only copies data to the host. These streams receive new work simultaneously.\n// Example work flow for 4 chunks in 6 iterations from time points 0 through 5.\n// Time..................................:   0        1        2        3        4        5        6        7 \n// Stream 1 host to device copy, chunk id:   1        2        3        4        -        -        -        -\n// Stream 2 kernel execution, chunk id...:   -        1        2        3        4        -        -        - \n// Stream 3 device to host copy, chunk id:   -        -        1        2        3        4        -        - \n// Event dependency graph:                   \n// Stream 1 eventHostToDevice event........: o==>|===>o==>|===>o==>|===>o==>|        |        \n// Stream 2 eventCompute event.............:     |===>o==>|===>o==>|===>o==>|===>o==>|        \n// Stream 3 eventDeviceToHost event........:     |        |===>o==>|===>o==>|===>o==>|===>o==>|\n// Main stream eventIterationComplete event:     M        M        M        M        M        M==> sync with host once\nvoid run(unsigned char * input_h, float * output_h, dim3 blockDim, dim3 gridDim);\n\nvoid launch() {\n    cudaDeviceProp props;\n    CUDA_CHECK(cudaGetDeviceProperties(&props, 0));\n    \n    // Limiting the number of threads per block after dynamically scaling it to the size of the workload.\n    int minimumRequiredThreadsPerBlock = NUM_ELEMENTS_PER_CHUNK / props.multiProcessorCount;\n    minimumRequiredThreadsPerBlock = (minimumRequiredThreadsPerBlock / 32) * 32;\n    if(minimumRequiredThreadsPerBlock > props.maxThreadsPerBlock) {\n        minimumRequiredThreadsPerBlock = props.maxThreadsPerBlock;\n    }\n    if(minimumRequiredThreadsPerBlock < MINIMUM_ALLOWED_NUMBER_OF_THREADS_PER_BLOCK) {\n        minimumRequiredThreadsPerBlock = MINIMUM_ALLOWED_NUMBER_OF_THREADS_PER_BLOCK;\n    }\n    // Limiting the number of blocks per grid after dynamically scaling it to the size of the workload.\n    int numThreadsPerBlock = minimumRequiredThreadsPerBlock;\n    int numBlocksPerGrid = NUM_ELEMENTS_PER_CHUNK / numThreadsPerBlock;\n    if(numBlocksPerGrid > props.maxBlocksPerMultiProcessor * props.multiProcessorCount) {\n        numBlocksPerGrid = props.maxBlocksPerMultiProcessor * props.multiProcessorCount;\n    }\n    if(numBlocksPerGrid < MINIMUM_ALLOWED_NUMBER_OF_BLOCKS_PER_GRID) {\n        numBlocksPerGrid = MINIMUM_ALLOWED_NUMBER_OF_BLOCKS_PER_GRID;\n    }\n    \n    // Allocating pinned memory for host arrays enables non-blocking behavior for asynchronous copies and facilitates the overlapping of copies and kernels with events and streams.\n    unsigned char * input_h;\n    CUDA_CHECK(cudaMallocHost(  &input_h, \n                                NUM_CHARACTERS_PER_ELEMENT * NUM_TOTAL_ELEMENTS, \n                                cudaHostAllocWriteCombined));\n    float * output_h;\n    CUDA_CHECK(cudaMallocHost(&output_h, sizeof(float) * NUM_TOTAL_ELEMENTS));\n\n    \n    dim3 gridDim(numBlocksPerGrid, 1, 1);\n    dim3 blockDim(numThreadsPerBlock, 1, 1);\n    \n    const char * testInput[NUM_TESTS] = {\n        // Testing edge cases.\n        \"-0.000000000e-00+3.402823000e+38-3.402823000e+38+1.175494000e-38-1.175494000e-38+0.000000000e+00-0.000000000e+00+0.000000000e-00+9.000001000e+00-9.000001000e-00\",\n        // Testing changes in digits and exponent.\n        \"+1.000001000e+00+1.000011000e+00+1.000111000e+00+1.001111000e+00+1.011111000e+00+1.111111000e+00+1.111111000e+10+1.111111000e+11+1.111111000e+01+1.111110000e+00\",\n        // Testing constants that are frequently used in science.\n        \"+3.141593000e+00+2.997925000e+08+6.626070000e-34+1.602177000e-19+6.674301000e-11+9.806650000e+00+8.987551000e+16+1.380649000e-23+6.022140000e+23+5.670374000e-08\",\n        // Testing division of 1 by powers of 2\n        \"+5.000000000e-01+2.500000000e-01+1.250000000e-01+6.250000000e-02+3.125000000e-02+1.562500000e-02+7.812500000e-03+3.906250000e-03+1.953125000e-03+9.765625000e-04\",\n        // Testing not exactly representable numbers in floating-point format.\n        \"+1.000000000e-01+3.333333000e-01+1.666667000e-01+5.000000000e-03+1.000000000e-03+7.000000000e-01+9.000000000e-01+1.100000000e-01+1.300000000e-01+2.200000000e-01\",\n        // Testing square roots of non-perfect square numbers.\n        \"+1.414214000e+00+1.732051000e+00+2.236068000e+00+2.645751000e+00+3.162278000e+00+3.605551000e+00+4.242641000e+00+4.582576000e+00+5.385165000e+00+6.082763000e+00\",\n        // Testing fractions 1/3, 2/3, 1/6, 1/5, 2/5, 4/5, 1/7, 2/7, 4/7, and 5/7.\n        \"+3.333333000e-01+6.666667000e-01+1.666667000e-01+2.000000000e-01+4.000000000e-01+8.000000000e-01+1.428571000e-01+2.857143000e-01+5.714286000e-01+7.142857000e-01\"\n    };\n    float testOutput[NUM_TESTS][NUM_TOTAL_ELEMENTS] = {\n        // Testing edge cases.\n        -0.000000e-00f, +3.402823e+38f, -3.402823e+38f, +1.175494e-38f, -1.175494e-38f, +0.000000e+00f, -0.000000e+00f, +0.000000e-00f, +9.000001e+00f, -9.000001e-00f, \n        // Testing changes in digits and exponent.\n        +1.000001e+00f, +1.000011e+00f, +1.000111e+00f, +1.001111e+00f, +1.011111e+00f, +1.111111e+00f, +1.111111e+10f, +1.111111e+11f, +1.111111e+01f, +1.111110e+00, \n        // Testing constants that are frequently used in science.\n        +3.141593e+00f, +2.997925e+08f, +6.626070e-34f, +1.602177e-19f, +6.674301e-11f, +9.806650e+00f, +8.987551e+16f, +1.380649e-23f, +6.022140e+23f, +5.670374e-08, \n        // Testing division of 1 by powers of 2\n        +5.000000e-01f, +2.500000e-01f, +1.250000e-01f, +6.250000e-02f, +3.125000e-02f, +1.562500e-02f, +7.812500e-03f, +3.906250e-03f, +1.953125e-03f, +9.765625e-04, \n        // Testing not exactly representable numbers in floating-point format.\n        +1.000000e-01f, +3.333333e-01f, +1.666667e-01f, +5.000000e-03f, +1.000000e-03f, +7.000000e-01f, +9.000000e-01f, +1.100000e-01f, +1.300000e-01f, +2.200000e-01, \n        // Testing square roots of non-perfect square numbers.\n        +1.414214e+00f, +1.732051e+00f, +2.236068e+00f, +2.645751e+00f, +3.162278e+00f, +3.605551e+00f, +4.242641e+00f, +4.582576e+00f, +5.385165e+00f, +6.082763e+00, \n        // Testing fractions 1/3, 2/3, 1/6, 1/5, 2/5, 4/5, 1/7, 2/7, 4/7, and 5/7.\n        +3.333333e-01f, +6.666667e-01f, +1.666667e-01f, +2.000000e-01f, +4.000000e-01f, +8.000000e-01f, +1.428571e-01f, +2.857143e-01f, +5.714286e-01f, +7.142857e-01\n    };\n\n    for(int testId = 0; testId < NUM_TESTS; testId++)\n    {\n        // Copying test data to the host input buffer.\n        memcpy(input_h, testInput[testId], NUM_TOTAL_ELEMENTS * NUM_CHARACTERS_PER_ELEMENT);\n        \n        run(input_h, output_h, blockDim, gridDim);\n        \n        // Comparing the results with the expected outputs.\n        for(int i = 0; i < NUM_TOTAL_ELEMENTS; i++) {\n            double expectedValue = testOutput[testId][i];\n            double difference = abs(expectedValue - output_h[i]);\n            // To avoid division by zero or denormal values, only a specific range of values is checked for accuracy since 1e-38f cannot be measured for its error to be less than the rounding error of 1e-45.\n            if(abs(expectedValue) > NEARLY_ZERO) {\n                double relativeError = abs(difference / expectedValue);\n                assert(relativeError < ERROR_TOLERANCE);\n            } else {\n                // Checking whether the output is at least on the same scale as the expected value (both being zero or close to zero) when the expected value is nearly zero.\n                assert(!(output_h[i] > NEARLY_ZERO));\n            }\n        }\n    }\n    // Freeing the host buffers.\n    CUDA_CHECK(cudaFreeHost(input_h));\n    CUDA_CHECK(cudaFreeHost(output_h));\n}\n\nvoid run(unsigned char * input_h, float * output_h, dim3 blockDim, dim3 gridDim) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/162", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute the distance matrix for objects in 1D space. The distance matrix is a two-dimensional matrix that contains the distances between i-indexed and j-indexed objects, represented as matrix element (i, j) where i < j from an array of objects. Aim for high thread occupancy to fully utilize GPU resources by balancing block size, register usage, and shared memory.\n\nThe signature of the CUDA kernel is __global__ void k_calculateDistances(float * position1D_d, float * distance_d, int numObjects), where position1D_d is a pointer to the array of one-dimensional positions of objects, distance_d is a pointer to the array of distances that is a matrix of size numObjects x numObjects with only upper-triangular region filled with results to avoid duplicate work, and numObjects is the number of objects.\n\n>>> k_calculateDistances({ 1.0f, 10.0f, 100.0f, 1000.0f }, distance_d, 4) -> distance_d: {\n    000.00f, 009.00f, 099.00f, 999.00f, \n    000.00f, 000.00f, 090.00f, 990.00f, \n    000.00f, 000.00f, 000.00f, 900.00f, \n    000.00f, 000.00f, 000.00f, 000.00f \n}\n>>> k_calculateDistances(f{ 45.0f, -80.2f, 0.0f, 0.0f, 103.1f }, distance_d, 5) -> distance_d: {\n    000.00f, 125.20f, 045.00f, 045.00f, 058.10f, \n    000.00f, 000.00f, 080.20f, 080.20f, 183.30f, \n    000.00f, 000.00f, 000.00f, 000.00f, 103.10f, \n    000.00f, 000.00f, 000.00f, 000.00f, 103.10f, \n    000.00f, 000.00f, 000.00f, 000.00f, 000.00f\n}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call){                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess){                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n// Algorithm settings.  \n// The algorithm does not support a higher number of objects in the calculation of matrix element indices due to rounding errors and other issues with 32-bit floating point math.  \n// If the -use_fast_math flag is enabled, the maximum must be 2000.\n// If the -use_fast_math flag is not enabled, the maximum is 4600.\n// This limit must not be changed unless -use_fast_math is used.\nconstexpr int MAXIMUM_OBJECTS_32BIT = 4600;\n// This must not be changed.\nconstexpr int MAXIMUM_OBJECTS_64BIT = 32768;\n// This can be adjusted to optimize the kernel for low precision with low register pressure to utilize the GPU more efficiently, or for high precision when processing a higher number of objects correctly (requires A100 or similar high FP64 performance GPU).\n// A higher limit than MAXIMUM_OBJECTS_32BIT necessitates double-precision calculations, which require a high ratio of FP64 to FP32 performance with a suitable Tesla GPU such as A100.\nconstexpr int MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS = 4000;\n// Size of allocations for device and host buffers of the distance matrix.\nconstexpr int MAXIMUM_SUPPORTED_DISTANCE_MATRIX_ELEMENTS = MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS;\nconstexpr float CONST1 = 2.0f;\nconstexpr float CONST2 = 4.0f;\nconstexpr float CONST3 = 8.0f;\nconstexpr float CONST4 = 7.0f;\nconstexpr float CONST5 = 0.5f;\nconstexpr float CONST6 = 0.5f;\nconstexpr int CONST7 = 1;\nconstexpr int CONST8 = 1;\nconstexpr int CONST9 = 1;\nconstexpr double CONST1_64BIT = 2.0;\nconstexpr double CONST2_64BIT = 4.0;\nconstexpr double CONST3_64BIT = 8.0;\nconstexpr double CONST4_64BIT = 7.0;\nconstexpr double CONST5_64BIT = 0.5;\nconstexpr double CONST6_64BIT = 0.5;\nconstexpr int64_t CONST7_64BIT = 1;\nconstexpr int64_t CONST8_64BIT = 1;\nconstexpr int64_t CONST9_64BIT = 1;\n\n// Test settings.\nconstexpr float ERROR_TOLERANCE = 1e-5f;\nconstexpr int NUM_TESTS = 7;\n\n// Kernel to calculate distances between object[i] and object[j] in 1D space. \n// To enhance occupancy: \n//     Maps CUDA threads onto the upper-triangular matrix elements (excluding the diagonal) of the distance matrix instead of mapping onto N objects. This enables N times more occupancy at the GPU level until the GPU is fully utilized with a sufficiently high N value. \n//     The upper-triangular matrix elements have a one-to-one mapping to the linear thread index, which avoids duplicated work when i <= j, where i is the first object index and j is the second object index used to calculate a distance. \n//     This approach eliminates unnecessary global memory accesses, improving work balance and making the achieved occupancy closer to the theoretical occupancy. \n//     It minimizes the number of branch operations to reduce register pressure. \n//     It avoids shared memory, which could limit the number of active blocks per SM due to constrained shared memory capacity and increased register pressure during shared memory access. \n//     Calculations are executed using a minimal number of registers. \n//     Handles a branch at compile-time to further reduce register pressure and increase active warps.\n__global__ void k_calculateDistances(float * position1D_d, float * distance_d, int numObjects);\n\nvoid launch() {\n    int deviceId = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    // Calculating the maximum achievable parallelism while ensuring high occupancy.\n    int optimalGridSize = 0;\n    int optimalBlockSize = 0;\n    // Minimizing the use of shared memory to increase the number of available active blocks per SM.\n    size_t dynamicSmemSizeForDataSink = 0;\n    CUDA_CHECK(cudaOccupancyMaxPotentialBlockSize(&optimalGridSize, &optimalBlockSize, (void*)k_calculateDistances, dynamicSmemSizeForDataSink));\n    \n    // Allocating stream.\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Allocating host memory.\n    float * position1D_h = new float[MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS];\n    float * distances_h = new float[MAXIMUM_SUPPORTED_DISTANCE_MATRIX_ELEMENTS];\n    \n    // Allocating device memory and initializing outputs to zero.\n    float * position1D_d;\n    float * distances_d;\n    CUDA_CHECK(cudaMallocAsync(&position1D_d, sizeof(float) * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&distances_d, sizeof(float) * MAXIMUM_SUPPORTED_DISTANCE_MATRIX_ELEMENTS, stream));\n    CUDA_CHECK(cudaMemsetAsync(distances_d, 0, sizeof(float) * MAXIMUM_SUPPORTED_DISTANCE_MATRIX_ELEMENTS, stream));\n    \n    int * testObjectPopulations_h = new int[NUM_TESTS];\n    float * testObjectPositions_h = new float[NUM_TESTS * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS];\n    int testIndex = 0;\n    // Test 1 input.\n    {\n        testObjectPopulations_h[testIndex] = 4;\n        std::initializer_list<float> positions = { 1.0f, 10.0f, 100.0f, 1000.0f };\n        std::copy(positions.begin(), positions.end(), &testObjectPositions_h[testIndex * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS]);\n        testIndex++;\n    }\n    // Test 2 input.\n    {\n        testObjectPopulations_h[testIndex] = 5;\n        std::initializer_list<float> positions = { 45.0f, -80.2f, 0.0f, 0.0f, 103.1f };\n        std::copy(positions.begin(), positions.end(), &testObjectPositions_h[testIndex * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS]);\n        testIndex++;\n    }\n    // Test 3 input.\n    {\n        testObjectPopulations_h[testIndex] = MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS;\n        std::mt19937 gen(42);\n        std::uniform_real_distribution<float> dist(0.0f, 1000000.0f);\n        \n        int numObjects = MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS;\n        for(int j = 0; j < numObjects; j++) {\n            testObjectPositions_h[j + testIndex * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS] = dist(gen);\n        }\n        testIndex++;\n    }\n    // Test 4 input.\n    {\n        testObjectPopulations_h[testIndex] = MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS;\n        int numObjects = MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS;\n        for(int j = 0; j < numObjects; j++) {\n            testObjectPositions_h[j + testIndex * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS] = (float) j;\n        }\n        testIndex++;\n    }\n    // Test 5 input.\n    {\n        testObjectPopulations_h[testIndex] = 103;\n        int numObjects = MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS;\n        for(int j = 0; j < numObjects; j++) {\n            testObjectPositions_h[j + testIndex * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS] = 0.0f;\n        }\n        testIndex++;\n    }\n    // Test 6 input.\n    {\n        testObjectPopulations_h[testIndex] = MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS;\n        int numObjects = MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS;\n        for(int j = 0; j < numObjects; j++) {\n            testObjectPositions_h[j + testIndex * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS] = sin(j);\n        }\n        testIndex++;\n    }\n    // Test 7 input.\n    {\n        testObjectPopulations_h[testIndex] = MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS;\n        int numObjects = MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS;\n        for(int j = 0; j < numObjects; j++) {\n            testObjectPositions_h[j + testIndex * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS] = j * 0.0001f;\n        }\n        testIndex++;\n    }\n    // Iterating through the test inputs.\n    for(int i = 0; i < testIndex; i++)\n    {\n        // Preparing the inputs.\n        int numObjects = testObjectPopulations_h[i];\n        // Checking that the number of objects does not exceed the maximum supported.\n        assert(numObjects <= MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS);\n        if(MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS > MAXIMUM_OBJECTS_32BIT) {\n            assert(MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS <= MAXIMUM_OBJECTS_64BIT);\n        }\n        for(int j = 0; j < numObjects; j++) {\n            position1D_h[j] = testObjectPositions_h[j + i * MAXIMUM_SUPPORTED_NUMBER_OF_OBJECTS];\n        }\n        \n        int totalWorkRequired = (numObjects * (numObjects - 1)) / 2;\n        // Copying object positions to the device.\n        CUDA_CHECK(cudaMemcpyAsync(position1D_d, position1D_h, sizeof(float) * numObjects, cudaMemcpyHostToDevice, stream));\n        void * args[3] = { &position1D_d, &distances_d, &numObjects };\n        // If the work required is less than the optimal amount of threads, keep the optimal block size and scale the grid size to reduce the total number of registers used. \n        // For example, choose 2 blocks of 768 threads instead of 768 blocks of 2 threads to utilize the memory subsystem efficiently and maintain the memory-compute work balance in the kernel.\n        int usedGridSize = optimalGridSize;\n        if(optimalGridSize * optimalBlockSize > totalWorkRequired) {\n            usedGridSize = (totalWorkRequired + optimalBlockSize - 1) / optimalBlockSize;\n        }\n        // Grid: (usedGridSize, 1, 1)\n        // Block: (optimalBlockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateDistances, dim3(usedGridSize, 1, 1), dim3(optimalBlockSize, 1, 1), args, dynamicSmemSizeForDataSink, stream));\n        // Copying the distances to the host.\n        CUDA_CHECK(cudaMemcpyAsync(distances_h, distances_d, sizeof(float) * numObjects * numObjects, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < numObjects - 1; i++) {\n            for(int j = i + 1; j < numObjects; j++) {\n                float expected = fabsf(position1D_h[i] - position1D_h[j]);\n                assert(fabsf(expected - distances_h[i * numObjects + j]) < ERROR_TOLERANCE);\n            } \n        }\n    }\n    \n    // Deallocating device memory.\n    CUDA_CHECK(cudaFreeAsync(position1D_d, stream));\n    CUDA_CHECK(cudaFreeAsync(distances_d, stream));\n    \n    // Deallocating host memory.\n    delete [] position1D_h;\n    delete [] distances_h;\n    delete [] testObjectPopulations_h;\n    delete [] testObjectPositions_h;\n    // Deallocating stream.\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_calculateDistances(float * position1D_d, float * distance_d, int numObjects) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/163", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform a convolution operation using Tensor Cores. Achieve this by writing a matrix multiplication kernel based on the inline mma PTX assembly instruction of shape m16n8k8, with the input data types for matrix A and B being bf16, with their resepctive layout being row major and column major for mma instruction, the final result matrix will be of f32 datatype.\nThe signature of the mma kernel is\n__global__ void k_mmaTensorConvMatMul(__nv_bfloat16 *inputMatrixA_d, __nv_bfloat16 *inputMatrixB_d, float *outputMatrix_d, int mDim, int nDim, int kDim),\nwhere `inputMatrixA_d` is the first input matrix with dimensions mDim  kDim, `inputMatrixB_d` is the second input matrix with dimensions kDim  nDim, and `outputMatrix_d` is the output matrix with dimensions mDim  nDim. The kernel logic needs to be divided using the following device functions, which have these signatures:\n\nFollowing are some helper device functions that have to be used for the mma kernel for various steps in the logic:\nTo convert an address pointer to shared memory state space for usage with ldmatrix instrucion, use the provided device function `d_cvtaToSharedU32`, which has the following signature:\n__device__ __forceinline__ uint32_t d_cvtaToSharedU32(const void* ptr)\nTo store the 16x8 wmma output tile, use the device function `d_storeMatrixTile16x8_f32`, which has the following signature:\n__device__ __forceinline__ void d_storeMatrixTile16x8_f32(float* dst, float reg, int n)\n\nThe solution must use an already available kernel `k_im2ColTransfom(...)` to perform the image to column (im2col) transformation on the input image. It will be used to represent the convolution operation as simple matrix multiplication.\nIt will ensure proper padding of the input to make it compatible with the mma kernels allowable dimensions which means, the number of filters (M) will be a multiple of 16, the image size (height  width) will be a multiple of 8, and K will be padded to the next multiple of 16 (divisible by 8).\n\nThe name of helper image to column transformation kernel is `k_im2ColTransform`, it's output (im2ColPad) will be fed into the mma matrix multiply kernel, following is it's signature:\n__global__ void k_im2ColTransform(__nv_bfloat16 *input, __nv_bfloat16 *weights, __nv_bfloat16 *im2ColPad, __nv_bfloat16 *weightPad, int channels, int inputHeight, int inputWidth, int kernelHeight, int kernelWidth, int pad, int stride, int outHeight, int outWidth, int numFilters, int padK)\nThe test cases will include images with a varying number of channels (e.g., 1 for grayscale, 3 for RGB), and the value for each channel within a given pixel at location (x, y) will be between 0 and 1.\n\n>>> k_mmaTensorConvMatMul({0.23, 0.34, 0.11, 0.58}, {0.77, 0.88}, outputMatrix_d, 2, 1, 2)  `outputMatrix_d`: {0.4763, 0.5951}\n>>> k_mmaTensorConvMatMul({0.62, 0.71, 0.41, 0.38}, {0.22, 0.12}, outputMatrix_d, 2, 1, 2)  `outputMatrix_d`: {0.2216, 0.1358}\n", "cc_flags": "-arch=sm_80", "ld_flags": "", "declaration": "\n#include <cstdio>\n#include <cassert>\n#include <cstdint>\n#include <random>\n\n#include <cuda_runtime.h>\n#include <mma.h>\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n#undef NDEBUG\n\n#define MMA_M 16\n#define MMA_N 8\n#define MMA_K 8\n\n\n// Test case struct to store the test case parameters\nstruct TestCase {\n    int channels;\n    int inputHeight;\n    int inWidth;\n    int kernelHeight;\n    int kernelWidth;\n    int pad;\n    int stride;\n    int numFilters;\n};\n\n\n// PTX MMA instruction based Matrix Multiplication Kernel\n__global__ void k_mmaTensorConvMatMul(__nv_bfloat16 *inputMatrixA_d,\n                                      __nv_bfloat16 *inputMatrixB_d,\n                                      float *outputMatrix_d,\n                                      int mDim,\n                                      int nDim,\n                                      int kDim);\n\n\n// Device function to convert generic address to shared memory state space, to make it compatible with `ldmatrix` instruction\n__device__ __forceinline__ uint32_t d_cvtaToSharedU32(const void* ptr) {\n    unsigned long long address;\n    asm volatile(\"cvta.to.shared.u64 %0, %1;\" : \"=l\"(address) : \"l\"(ptr));\n    return static_cast<uint32_t>(address);\n}\n\n// Device function to store 16x8 output of mma instruction\n__device__ __forceinline__ void d_storeMatrixTile16x8_f32(float* dst, float reg[4], int n) {\n    int lane = threadIdx.x % 32;  // 0..31\n    int r = lane / 4;        // 0..7 for the top half\n    int c = (lane % 4) * 2;    // columns: 0,2,4,6\n    dst[r * n + c] = reg[0];\n    dst[r * n + c + 1] = reg[1];\n    dst[(r + 8) * n + c] = reg[2];\n    dst[(r + 8) * n + c + 1] = reg[3];\n}\n\n// CUDA kernel to perform im2Col transformation to allow convolution to be performed as a simple GEMM operation\n__global__ void k_im2ColTransform( __nv_bfloat16 *input,\n                                  __nv_bfloat16 *weights,\n                                  __nv_bfloat16 *im2ColPad,\n                                  __nv_bfloat16 *weightPad,\n                                  int channels,\n                                  int inputHeight,\n                                  int inputWidth,\n                                  int kernelHeight,\n                                  int kernelWidth,\n                                  int pad,\n                                  int stride,\n                                  int outHeight,\n                                  int outWidth,\n                                  int numFilters,\n                                  int padK) {\n    // Derived dimensions.\n    int weightK = channels * kernelHeight * kernelWidth;   // Unpadded reduction dimension.\n    int im2colRows = weightK;                                // Each row corresponds to (c, kh, kw).\n    int im2colCols = outHeight * outWidth;                   // Each column corresponds to an output spatial location.\n\n    // Total iterations for weight processing and im2col processing.\n    int totalWeight = numFilters * padK;         // Process padded weights.\n    int totalIm2col = padK * im2colCols;           // Process padded im2col.\n    int total = totalWeight + totalIm2col;         // Combined total.\n\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= total)\n        return;\n\n    //Weight Processing -> Flatten & Pad\n    if (tid < totalWeight) {\n        int filter = tid / padK;      // Which filter.\n        int j = tid % padK;           // Index in padded reduction dimension.\n        if (j < weightK) {\n            // Copy original weight element.\n            weightPad[tid] = weights[filter * weightK + j];\n        } else {\n            // Pad with zero.\n            weightPad[tid] = __nv_bfloat16(0.0f);\n        }\n    }\n    //im2col transformation and Padding\n    else {\n        int index = tid - totalWeight;  // Index into im2col padded space.\n        int row = index / im2colCols;     // Row in the padded im2col matrix.\n        int col = index % im2colCols;     // Column index.\n        if (row < im2colRows) {\n            // Determine (channel, kh, kw) indices.\n            int c = row / (kernelHeight * kernelWidth);\n            int rem = row % (kernelHeight * kernelWidth);\n            int kh = rem / kernelWidth;\n            int kw = rem % kernelWidth;\n            int outY = col / outWidth;\n            int outX = col % outWidth;\n            int inputY = outY * stride - pad + kh;\n            int inputX = outX * stride - pad + kw;\n            __nv_bfloat16 val = __nv_bfloat16(0.0f);\n            if (inputY >= 0 && inputY < inputHeight && inputX >= 0 && inputX < inputWidth) {\n                int inputIndex = c * (inputHeight * inputWidth) + inputY * inputWidth + inputX;\n                val = input[inputIndex];\n            }\n            im2ColPad[index] = val;\n        } else {\n            // Pad rows beyond the original im2col rows.\n            im2ColPad[index] = __nv_bfloat16(0.0f);\n        }\n    }\n}\n\n\n// Host-side reference convolution.\n// Computes N filters of an HW image with C channels.\n//  input : [CHW] row-major\n//  weights: [NCKHKW] row-major\n//  output : [NOHOW] row-major\nvoid refConv2dHost(const float* input,\n                   const float* weights,\n                   float* output,\n                   int channels,\n                   int inputHeight,\n                   int inputWidth,\n                   int numFilters,\n                   int kernelHeight,\n                   int kernelWidth,\n                   int pad,\n                   int stride,\n                   int outHeight,\n                   int outWidth) {\n    int outSize = outHeight * outWidth;\n    for (int f = 0; f < numFilters; ++f) {\n        for (int oy = 0; oy < outHeight; ++oy) {\n            for (int ox = 0; ox < outWidth; ++ox) {\n                float sum = 0.0f;\n                for (int c = 0; c < channels; ++c) {\n                    for (int kh = 0; kh < kernelHeight; ++kh) {\n                        for (int kw = 0; kw < kernelWidth; ++kw) {\n                            int iy = oy*stride - pad + kh;\n                            int ix = ox*stride - pad + kw;\n                            float v = 0.0f;\n                            if (iy >= 0 && iy < inputHeight && ix >= 0 && ix < inputWidth)\n                                v = input[c*inputHeight*inputWidth + iy*inputWidth + ix];\n                            float w = weights[\n                                f*(channels*kernelHeight*kernelWidth)\n                              + c*(kernelHeight*kernelWidth)\n                              + kh*kernelWidth + kw\n                            ];\n                            sum += v * w;\n                        }\n                    }\n                }\n                output[f*outSize + oy*outWidth + ox] = sum;\n            }\n        }\n    }\n}\n\nvoid launch() {\n    const int TEST_CASE_COUNT = 8;\n    const TestCase TEST_CASES[] = {\n        {3, 16, 16, 3, 3, 1, 1, 16},     // RGB image, 3x3 kernel, 16 filters.\n        {1, 8, 8, 3, 3, 1, 1, 16},       // Single channel 8x8 image, 3x3 kernel, 16 filters.\n        {3, 32, 32, 5, 5, 2, 1, 16},     // RGB image, 5x5 kernel, pad=2, stride=1, 16 filters.\n        {3, 64, 64, 3, 3, 1, 1, 32},     // Larger RGB image (64x64) with 3x3 kernel and 32 filters.\n        {1, 128, 128, 5, 5, 2, 1, 64},   // Grayscale image (128x128) with 5x5 kernel, pad=2, stride=1, 64 filters.\n        {3, 224, 224, 7, 7, 3, 2, 64},   // Standard RGB image (224x224) with 7x7 kernel, pad=3, stride=2, 64 filters.\n        {3, 256, 256, 3, 3, 1, 1, 128},  // Large RGB image (256x256) with 3x3 kernel and 128 filters.\n        {3, 40, 72, 3, 3, 1, 1, 32},     // Rectangular test: 4072 RGB image, 33 kernel, pad=1, stride=1, 32 filters\n        {1, 24, 48, 5, 5, 2, 1, 16}      // Rectangular Grayscale 2448 image, 55 kernel, pad=2, stride=1, 16 filters\n\n    };\n\n    const float TOLERANCE = 0.01f;\n    const int BLOCK_SIZE = 256;\n\n    // Create a CUDA stream.\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Setup random number generation.\n    std::mt19937 randEngine(static_cast<unsigned int>(time(nullptr)));\n    std::uniform_real_distribution<float> randDist(0.0f, 1.0f);\n\n    for (int tcIdx = 0; tcIdx < TEST_CASE_COUNT; tcIdx++) {\n        TestCase testCase = TEST_CASES[tcIdx];\n        int outHeight = (testCase.inputHeight + 2 * testCase.pad - testCase.kernelHeight) / testCase.stride + 1;\n        int outWidth = (testCase.inWidth + 2 * testCase.pad - testCase.kernelWidth) / testCase.stride + 1;\n        int im2colCols = outHeight * outWidth;\n        int weightK = testCase.channels * testCase.kernelHeight * testCase.kernelWidth;\n\n        // Choose padK as the next multiple of 16.\n        int padK = ((weightK + 16 - 1) / 16) * 16;\n        int convOutSize = testCase.numFilters * outHeight * outWidth;\n        int inputSize = testCase.channels * testCase.inputHeight * testCase.inWidth;\n        int weightSize = testCase.numFilters * weightK;\n\n        // Host arrays.\n        float *input_h = new float[inputSize];\n        float *weights_h = new float[weightSize];\n        float *convStd_h = new float[convOutSize];\n\n        // Random input (image) data\n        for (int j = 0; j < inputSize; j++) {\n            float val = randDist(randEngine);\n            input_h[j] = __nv_bfloat16(val);\n        }\n\n        // Random weight kernel\n        for (int j = 0; j < weightSize; j++) {\n            float val = randDist(randEngine);\n            weights_h[j] = __nv_bfloat16(val);\n        }\n\n        // Device arrays (using asynchronous allocations).\n        float *input_d;\n        float *weights_d;\n        CUDA_CHECK(cudaMallocAsync(&input_d, inputSize * sizeof(float), stream));\n        CUDA_CHECK(cudaMallocAsync(&weights_d, weightSize * sizeof(float), stream));\n\n        refConv2dHost(\n            input_h, weights_h, convStd_h,\n            testCase.channels,\n            testCase.inputHeight, testCase.inWidth,\n            testCase.numFilters,\n            testCase.kernelHeight, testCase.kernelWidth,\n            testCase.pad, testCase.stride,\n            outHeight, outWidth\n        );\n\n        // Convert input and weights to BF16.\n        __nv_bfloat16 *inputBf16_h = new __nv_bfloat16[inputSize];\n        __nv_bfloat16 *weightsBf16_h = new __nv_bfloat16[weightSize];\n        for (int j = 0; j < inputSize; j++) {\n            inputBf16_h[j] = __nv_bfloat16(input_h[j]);\n        }\n        for (int j = 0; j < weightSize; j++) {\n            weightsBf16_h[j] = __nv_bfloat16(weights_h[j]);\n        }\n\n        __nv_bfloat16 *inputBf16_d;\n        __nv_bfloat16 *weightsBf16_d;\n        CUDA_CHECK(cudaMallocAsync(&inputBf16_d, inputSize * sizeof(__nv_bfloat16), stream));\n        CUDA_CHECK(cudaMallocAsync(&weightsBf16_d, weightSize * sizeof(__nv_bfloat16), stream));\n        CUDA_CHECK(cudaMemcpyAsync(inputBf16_d, inputBf16_h, inputSize * sizeof(__nv_bfloat16), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(weightsBf16_d, weightsBf16_h, weightSize * sizeof(__nv_bfloat16), cudaMemcpyHostToDevice, stream));\n\n        // Allocate output arrays for preprocessing.\n        int im2colPadSize = padK * im2colCols;       // For im2col.\n        int weightPadSize = testCase.numFilters * padK;  // For weights.\n        __nv_bfloat16 *im2colPad_d;\n        __nv_bfloat16 *weightPad_d;\n\n        CUDA_CHECK(cudaMallocAsync(&im2colPad_d, im2colPadSize * sizeof(__nv_bfloat16), stream));\n        CUDA_CHECK(cudaMallocAsync(&weightPad_d, weightPadSize * sizeof(__nv_bfloat16), stream));\n\n        // Launch the im2col preprocessing kernel.\n        int totalPreprocess = weightPadSize + im2colPadSize;\n        int gridSize = (totalPreprocess + BLOCK_SIZE - 1) / BLOCK_SIZE;\n        k_im2ColTransform<<<gridSize, BLOCK_SIZE, 0, stream>>>(inputBf16_d, weightsBf16_d,\n                                                               im2colPad_d, weightPad_d,\n                                                               testCase.channels, testCase.inputHeight, testCase.inWidth,\n                                                               testCase.kernelHeight, testCase.kernelWidth,\n                                                               testCase.pad, testCase.stride,\n                                                               outHeight, outWidth,\n                                                               testCase.numFilters, padK);\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Allocate output buffer for the MMA kernel.\n        float *convMma_d;\n        CUDA_CHECK(cudaMallocAsync(&convMma_d, convOutSize * sizeof(float), stream));\n        CUDA_CHECK(cudaMemsetAsync(convMma_d, 0, convOutSize * sizeof(float), stream));\n\n        //The MMA kernel expects dimensions:\n        //mDim = numFilters, nDim = im2colCols, kDim = padK.\n        int M = testCase.numFilters;\n        int N = im2colCols;\n        int K = padK;\n\n        assert(M % MMA_M == 0);\n        assert(N % MMA_N == 0);\n        assert(K % MMA_K == 0);\n\n        // Kernel Launch\n        // blockDim -> (256,1,1)\n        // gridDim -> ((N + MMA_N -1)/MMA_N, (M + MMA_M - 1)/MMA_M))\n        dim3 gridDimMma((N + MMA_N - 1) / MMA_N, (M + MMA_M - 1) / MMA_M);\n        dim3 blockDimMma(256);\n        int shmemBytes = (MMA_M * MMA_K + MMA_K * MMA_N) * sizeof(__nv_bfloat16);\n\n        // Launch the MMA GEMM kernel: weightPad_d (mDimkDim) multiplied by im2colPad_d (kDimnDim)\n        void *kernelArgs[] = { &weightPad_d, &im2colPad_d, &convMma_d,\n                               (void*)&M, &N, &K };\n        CUDA_CHECK(cudaLaunchKernel((void*)k_mmaTensorConvMatMul,\n                                    gridDimMma,\n                                    blockDimMma,\n                                    kernelArgs,\n                                    shmemBytes,\n                                    stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        //Copy MMA result and reference convolution result back to host.\n        float *convMma_h  = new float[convOutSize];\n        CUDA_CHECK(cudaMemcpyAsync(convMma_h , convMma_d, convOutSize * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        // CUDA_CHECK(cudaMemcpyAsync(convStd_h, convStd_d, convOutSize * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        //Verify results.\n        for (int f = 0; f < testCase.numFilters; f++) {\n            for (int j = 0; j < outHeight * outWidth; j++) {\n                float stdVal = convStd_h[f * (outHeight * outWidth) + j];\n                float mmaVal = convMma_h[f * (outHeight * outWidth) + j];\n                float diff = fabs(stdVal - mmaVal);\n                assert(fabs(stdVal) > 1e-5f && diff / fabs(stdVal) < TOLERANCE);\n            }\n        }\n\n        //Memory Clean up\n        CUDA_CHECK(cudaFreeAsync(input_d, stream));\n        CUDA_CHECK(cudaFreeAsync(weights_d, stream));\n        CUDA_CHECK(cudaFreeAsync(inputBf16_d, stream));\n        CUDA_CHECK(cudaFreeAsync(weightsBf16_d, stream));\n        CUDA_CHECK(cudaFreeAsync(im2colPad_d, stream));\n        CUDA_CHECK(cudaFreeAsync(weightPad_d, stream));\n        CUDA_CHECK(cudaFreeAsync(convMma_d, stream));\n\n        // Free host memory.\n        delete[] input_h;\n        delete[] weights_h;\n        delete[] inputBf16_h;\n        delete[] weightsBf16_h;\n        delete[] convStd_h;\n        delete[] convMma_h ;\n    }\n\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n// Kernel: Multiply bf16 matrices inputMatrixA_d (mDimx kDim) and inputMatrixB_d (kDim x nDim) to produce an output outputMatrix_d (mDim x nDim) in f32.\n// Each block computes one 168 output tile using MMA instructions.\n__global__ void k_mmaTensorConvMatMul(__nv_bfloat16 *inputMatrixA_d,\n                                      __nv_bfloat16 *inputMatrixB_d,\n                                      float *outputMatrix_d,\n                                      int mDim,\n                                      int nDim,\n                                      int kDim) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/164", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform matrix multiplication through Tensor Cores using inline WMMA PTX instructions (m16n16k16) on Ampere+ GPUs.\nUse half-precision (__half, fp16) inputs in row-major (A, .alayout) and column-major (B, .blayout) layouts, single-precision accumulators (float, f32), and validate against a host reference.\nImplement explicit PTX with wmma.load.a.sync.aligned..., wmma.load.b.sync.aligned..., wmma.mma.sync.aligned..., and wmma.store.d.sync.aligned...\nAll input matrices will comply with valid dimensions allowable with m16n16k16 shape.\n\nTo load tile of Matrix A, use `loadMatrixA` device function, that has \"wmma.load.a.sync.aligned.m16n16k16.row.f16 {%0,%1,%2,%3,%4,%5,%6,%7}, [%8], %9\" as the PTX instruction, the signature is:\n__device__ __forceinline__ void loadMatrixA(RegF16 regs, const __half* ptr, int stride).\nTo load tile of Matrix B, use `loadMatrixB` device function, that has \"wmma.load.b.sync.aligned.m16n16k16.col.f16 {v0..v7}, [addr], stride\" as the PTX instruction, the signature is:\n__device__ __forceinline__ void loadMatrixB(RegF16 regs, const __half* ptr, int stride).\nTo perform Warp Matrix Multiply of A and B, through \"wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32\" PTX instruction, use the function `mmaSync`, the signature is as follows:\n__device__ __forceinline__ void mmaSync(RegF32 accum, const RegF16 a, const RegF16 b)\nTo store the resultant output, use storeMatriC, based on PTX instruction \"wmma.store.d.sync.aligned.m16n16k16.row.f32 \", with function signature as follows:\n__device__ __forceinline__ void storeMatrixC(const RegF32 regs, float* ptr, int stride), function to store the generated tile output of wmma instruction\n\n\nThe signature of the kernel is __global__ void k_wmmaMatMulM16N16K16Fp16(__half* A, __half* B, float* C,\n                                           int M, int N, int K, int warpsPerBlock);\n>>> k_wmmaMatMulM16N16k16Fp16({... 1, 1, 1, 1, ...}, {... 2, 2, ...}, C, 16, 16, 16, 1)-> C: ({... 32, 32, ...})\n>>> k_wmmaMatMulM16N16k16Fp16({... 1, 1, 1, 1, ...}, {... 3, 3, ...}, C, 16, 16, 16, 1)-> C: ({... 48, 48, ...})\n", "cc_flags": "-arch=sm_80", "ld_flags": "", "declaration": "\n\n#include <cstdio>\n#include <cassert>\n#include <random>\n#include <cmath>\n#include <cstdlib>\n\n#include <cuda_fp16.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                                                        \\\ndo {                                                                            \\\n        cudaError_t error = call;                                               \\\n        if (error != cudaSuccess) {                                             \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __func__, __LINE__);                                        \\\n            exit(error);                                                        \\\n        }                                                                       \\\n} while (0)\n#undef NDEBUG\n\n#define WMMA_M 16\n#define WMMA_N 16\n#define WMMA_K 16\n\n// WMMA m16n16k16 .f16 inputs take 8 registers (each holding 4 halfs)\ntypedef uint32_t RegF16[8];\n// WMMA m16n16 .f32 accumulators/stores take 8 registers (each holding 2 floats)\ntypedef float    RegF32[8];\n\n__global__ void k_wmmaMatMulM16N16K16Fp16(__half* A, __half* B, float* C,\n                                           int M, int N, int K, int warpsPerBlock);\n\n// Helper functions for the main kernel\n// Load 16x16 A tile (row-major __half) into registers\n__device__ __forceinline__ void loadMatrixA(RegF16 regs, const __half* ptr, int stride) {\n    unsigned long long gptr;\n    asm volatile(\"cvta.to.global.u64 %0, %1;\" : \"=l\"(gptr) : \"l\"(ptr));\n    asm volatile(\n        \"wmma.load.a.sync.aligned.m16n16k16.row.f16 {%0,%1,%2,%3,%4,%5,%6,%7}, [%8], %9;\"\n        : \"=r\"(regs[0]), \"=r\"(regs[1]), \"=r\"(regs[2]), \"=r\"(regs[3]),\n          \"=r\"(regs[4]), \"=r\"(regs[5]), \"=r\"(regs[6]), \"=r\"(regs[7])\n        : \"l\"(gptr), \"r\"(stride)\n        : \"memory\"\n    );\n}\n\n// Load 16x16 B tile (col-major __half) into registers\n__device__ __forceinline__ void loadMatrixB(RegF16 regs, const __half* ptr, int stride) {\n    unsigned long long gptr;\n    asm volatile(\"cvta.to.global.u64 %0, %1;\" : \"=l\"(gptr) : \"l\"(ptr));\n    // wmma.load.b.sync.aligned.m16n16k16.col.f16 {v0..v7}, [addr], stride;\n    asm volatile(\n        \"wmma.load.b.sync.aligned.m16n16k16.col.f16 {%0,%1,%2,%3,%4,%5,%6,%7}, [%8], %9;\"\n        : \"=r\"(regs[0]), \"=r\"(regs[1]), \"=r\"(regs[2]), \"=r\"(regs[3]),\n          \"=r\"(regs[4]), \"=r\"(regs[5]), \"=r\"(regs[6]), \"=r\"(regs[7])\n        : \"l\"(gptr), \"r\"(stride)\n        : \"memory\" // Added memory clobber\n    );\n}\n\n// Perform MMA: C += A * B\n__device__ __forceinline__ void mmaSync(RegF32 accum, const RegF16 a, const RegF16 b) {\n    // wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32 {d0..d7}, {a0..a7}, {b0..b7}, {c0..c7};\n    asm volatile(\n        \"wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32 \"\n        \"{%0,%1,%2,%3,%4,%5,%6,%7},\"     // d (c) (output/input accumulator)\n        \"{%8,%9,%10,%11,%12,%13,%14,%15},\" // a (input)\n        \"{%16,%17,%18,%19,%20,%21,%22,%23},\" // b (input)\n        \"{%0,%1,%2,%3,%4,%5,%6,%7};\"     // c (input accumulator)\n        : \"+f\"(accum[0]), \"+f\"(accum[1]), \"+f\"(accum[2]), \"+f\"(accum[3]),\n          \"+f\"(accum[4]), \"+f\"(accum[5]), \"+f\"(accum[6]), \"+f\"(accum[7])\n        : \"r\"(a[0]), \"r\"(a[1]), \"r\"(a[2]), \"r\"(a[3]),\n          \"r\"(a[4]), \"r\"(a[5]), \"r\"(a[6]), \"r\"(a[7]),\n          \"r\"(b[0]), \"r\"(b[1]), \"r\"(b[2]), \"r\"(b[3]),\n          \"r\"(b[4]), \"r\"(b[5]), \"r\"(b[6]), \"r\"(b[7])\n    );\n}\n\n// Store 16x16 C tile (row-major float) from registers\n__device__ __forceinline__ void storeMatrixC(const RegF32 regs, float* ptr, int stride) {\n    unsigned long long gptr;\n    asm volatile(\"cvta.to.global.u64 %0, %1;\" : \"=l\"(gptr) : \"l\"(ptr));\n    // wmma.store.d.sync.aligned.m16n16k16.row.f32 [addr], {v0..v7}, stride;\n    asm volatile(\n      \"wmma.store.d.sync.aligned.m16n16k16.row.f32 \"\n      \"[%0], {%1,%2,%3,%4,%5,%6,%7,%8}, %9;\"\n      : // No output operands\n      : \"l\"(gptr), // Global pointer\n        \"f\"(regs[0]), \"f\"(regs[1]), \"f\"(regs[2]), \"f\"(regs[3]),\n        \"f\"(regs[4]), \"f\"(regs[5]), \"f\"(regs[6]), \"f\"(regs[7]),\n        \"r\"(stride)\n      : \"memory\" // Added memory clobber\n    );\n}\n\n// Function to compute valid reference result\nvoid cpuMatMulReference(__half* A_h, // C-style pointer\n                        __half* B_h, // C-style pointer\n                        float* cpuRefC_h,     // C-style pointer\n                        int M,\n                        int N,\n                        int K) {\n    for (int i = 0; i < M; i++) {\n        for (int j = 0; j < N; j++) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; k++) {\n                float aVal = static_cast<float>(A_h[i*K + k]);\n                float bVal = static_cast<float>(B_h[j*K + k]);\n\n                sum += aVal * bVal;\n            }\n            cpuRefC_h[i*N + j] = sum;\n        }\n    }\n}\n\nvoid launch() {\n    const int TEST_CASE_COUNT = 5;\n    const int TEST_CASE_DIMS[TEST_CASE_COUNT][3] = {{16,16,16}, {32,16,48}, {64,64,64}, {128,32,64}, {512,512,512}};\n\n    int dev = 0;\n    CUDA_CHECK(cudaGetDevice(&dev));\n    cudaDeviceProp devProp;\n    CUDA_CHECK(cudaGetDeviceProperties(&devProp, dev));\n\n    //Extracting warpSize from deviceProp\n    int warpSize = devProp.warpSize;\n    const int BLOCK_SIZE         = 256;              // threads per block\n    const int WARPS_PER_BLOCK    = BLOCK_SIZE / warpSize;    // 8 warps per block\n\n    const float TOLERANCE = 1e-2f;\n\n\n    std::mt19937 randEngine(time(nullptr));\n    std::uniform_real_distribution<float> randDist(-1.0f, 1.0f);\n\n    // Create a CUDA stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    for (int t = 0; t < TEST_CASE_COUNT; ++t) {\n        int M = TEST_CASE_DIMS[t][0];\n        int N = TEST_CASE_DIMS[t][1];\n        int K = TEST_CASE_DIMS[t][2];\n\n        int sizeA = M*K;\n        int sizeB = K*N;\n        int sizeC = M*N;\n\n        // Use C-style arrays allocated with cudaHostAlloc\n        __half* A_h = nullptr;\n        __half* B_h = nullptr;\n        float* gpuC_h = nullptr;\n        float* cpuRefC_h = nullptr;\n\n        CUDA_CHECK(cudaHostAlloc(&A_h,      sizeA * sizeof(__half), cudaHostAllocDefault));\n        CUDA_CHECK(cudaHostAlloc(&B_h,      sizeB * sizeof(__half), cudaHostAllocDefault));\n        CUDA_CHECK(cudaHostAlloc(&gpuC_h,    sizeC * sizeof(float),  cudaHostAllocDefault));\n        CUDA_CHECK(cudaHostAlloc(&cpuRefC_h, sizeC * sizeof(float),  cudaHostAllocDefault));\n\n        for (int i = 0; i < sizeA; ++i) {\n            A_h[i]=__float2half(randDist(randEngine));\n        }\n\n        // Fill B col-major\n        for (int col = 0;col < N; ++col) {\n            for (int row=0;row<K; ++row) {\n                 B_h[col*K + row] = __float2half(randDist(randEngine)); // Correct column-major fill\n            }\n        }\n\n        // Initialize gpuC_h to 0.0f using a loop\n        for (size_t i = 0; i < sizeC; ++i) {\n            gpuC_h[i] = 0.0f;\n            cpuRefC_h[i] = 0.0f; // Also initialize cpuRefC_h\n        }\n\n\n        __half *A_d, *B_d; float *C_d;\n        // Use cudaMallocAsync with the stream\n        CUDA_CHECK(cudaMallocAsync(&A_d, sizeA*sizeof(__half), stream));\n        CUDA_CHECK(cudaMallocAsync(&B_d, sizeB*sizeof(__half), stream));\n        CUDA_CHECK(cudaMallocAsync(&C_d, sizeC*sizeof(float), stream));\n\n        // Use cudaMemcpyAsync with the stream\n        CUDA_CHECK(cudaMemcpyAsync(A_d,A_h,sizeA*sizeof(__half),cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(B_d,B_h,sizeB*sizeof(__half),cudaMemcpyHostToDevice, stream));\n        // cudaMemsetAsync is also available, but cudaMemset is fine for initialization before kernel launch\n        CUDA_CHECK(cudaMemsetAsync(C_d,0,sizeC*sizeof(float), stream));\n\n        assert(M%WMMA_M==0);\n        assert(N%WMMA_N==0);\n        assert(K%WMMA_K==0);\n\n        dim3 blockDim(BLOCK_SIZE);\n        int tilesN_total = N/WMMA_N;\n        dim3 gridDim(\n            (tilesN_total + WARPS_PER_BLOCK - 1)/WARPS_PER_BLOCK, // Number of blocks along N\n            M/WMMA_M // Number of blocks along M\n        );\n\n        void* args[] = {\n            (void*)&A_d,\n            (void*)&B_d,\n            (void*)&C_d,\n            (void*)&M,\n            (void*)&N,\n            (void*)&K,\n            (void*)&WARPS_PER_BLOCK\n        };\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_wmmaMatMulM16N16K16Fp16,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    0,\n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(gpuC_h,C_d,sizeC*sizeof(float),cudaMemcpyDeviceToHost, stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        cpuMatMulReference(A_h,B_h,cpuRefC_h,M,N,K);\n\n        // Validate\n        for (int i = 0;i < sizeC ; ++i) {\n            float diff = fabs(gpuC_h[i]-cpuRefC_h[i]);\n            assert(diff < TOLERANCE);\n        }\n\n        // Use cudaFreeAsync with the stream for device memory\n        CUDA_CHECK(cudaFreeAsync(A_d, stream));\n        CUDA_CHECK(cudaFreeAsync(B_d, stream));\n        CUDA_CHECK(cudaFreeAsync(C_d, stream));\n        CUDA_CHECK(cudaFreeHost(A_h));\n        CUDA_CHECK(cudaFreeHost(B_h));\n        CUDA_CHECK(cudaFreeHost(gpuC_h));\n        CUDA_CHECK(cudaFreeHost(cpuRefC_h));\n    }\n\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n\n// Main MMA kernel\n__global__ void k_wmmaMatMulM16N16K16Fp16(__half* A,\n                                          __half* B,\n                                          float* C,\n                                          int M,\n                                          int N,\n                                          int K,\n                                          int warpsPerBlock) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/165", "date": "2025-07-30", "prompt": "Write a CUDA function using CUB device-wide API to transform the input array into its logarithmic form using a given custom functor.\n\nThe signature of the CUDA kernel function is:\nvoid cubNormalizeFunction(int* input_d, float* output_d, int inputSize, cudaStream_t stream) where input_d is the pointer to the input array, output_d is pointer to the output array where the logarithmic results will be stored, inputSize is the number of elements in input array and stream is used for asynchronous operations.\n__host__ __device__ void operator()(int i) : This device function is defined within the logScalingnormalization functor , which computes logarithmic scaling for normalization. The parameter i represents the index of an element in the input array.\n\n>>> cubNormalizeFunction({85, 27, 49, 75, 63, 97, 0, 92, 17, 38}, output_d,  10, stream) : output_d -> {4.454347, 3.332205, 3.912023, 4.330733, 4.158883, 4.584968, 0.000000, 4.532599, 2.890372, 3.663562}        \n>>> cubNormalizeFunction({24, 96, 20, 64, 11, 52, 88, 43, 35, 28, 2, 59, 90, 78, 8}, output_d, 15, stream) : output -> {3.218876, 4.574711, 3.044523, 4.174387, 2.484907, 3.970292, 4.488636, 3.784190, 3.583519, 3.367296, 1.098612, 4.094345, 4.510859, 4.369448, 2.197225}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80 ", "ld_flags": "-Icccl/thrust -Icccl/libcudacxx/include -Icccl/cub --extended-lambda ", "declaration": "#include <stdio.h>\n#include <iostream>\n#include <assert.h>\n#include <algorithm>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#include <cub/cub.cuh>\n#include <cub/device/device_transform.cuh>\n\n#define TOLERANCE 1e-4\n#define CUDA_CHECK(call){                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess){                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n#undef NDEBUG\n\n#ifndef CUB_DEBUG\n#  define CUB_DEBUG(e) CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__)\n#endif\n\nstruct logScalingnormalization {\n\n    __device__ float operator()(int input) {\n    \n        return (log1pf(fmaxf(0.0f, (float)input)));  // natural logarithm(1 + input)\n    }   \n};\n\nvoid cubNormalizeFunction(int* input_d, float* output_d, int inputSize, cudaStream_t stream);\n\nvoid launch() {\n\n    const int TEST_CASE_COUNT = 7; // Total test cases\n\n    int numElements[TEST_CASE_COUNT] = {10, 15, 20, 30, 40, 50, 60};\n    // Maximum input elements\n    int maxNumElements = *std::max_element(numElements, numElements + TEST_CASE_COUNT);\n    //input Array\n    int input[TEST_CASE_COUNT][maxNumElements] = {\n        {85, 27, 49, 75, 63, 97, 0, 92, 17, 38},  \n\n        {24, 96, 20, 64, 11, 52, 88, 43, 35, 28, 2, 59, 90, 78, 8}, \n\n        {31, 79, 55, 17, 98, 94, 15, 62, 29, 45, 53, 33, 72, 81, 3, 38, 46, 6, 100, 0},  \n\n        {21, 8, 25, 67, 54, 84, 92, 2, 10, 36, 11, 99, 5, 97, 77, 58, 46, 70, 35, 17, 89, 9, 74, 65, 19, 27, 59, 100, 6, 22}, \n\n        {34, 92, 99, 58, 61, 50, 88, 78, 66, 85, 71, 97, 14, 10, 43, 26, 2, 75, 36, 69, 64, 3, 13, 5, 45, 63, 95, 55, 80, 22, 19, 33, 40, 7, 8, 16, 27, 20, 83, 49}, \n\n        {32, 23, 72, 56, 70, 93, 18, 30, 50, 45, 77, 67, 97, 26, 84, 91, 60, 34, 14, 59, 90, 25, 8, 42, 11, 73, 29, 12, 38, 41, 94, 55, 13, 7, 87, 48, 40, 100, 86, 66, 2, 61, 65, 10, 33, 16, 9, 28, 3, 21}, \n\n        {35, 94, 23, 13, 77, 56, 28, 14, 99, 15, 100, 75, 86, 3, 31, 91, 26, 83, 1, 25, 84, 11, 59, 48, 24, 41, 44, 98, 21, 6, 67, 96, 93, 50, 34, 7, 17, 39, 9, 10, 69, 43, 29, 38, 85, 66, 8, 36, 61, 5, 95, 73, 63, 19, 64, 60, 20, 87, 12, 2}\n    };\n\n    float expectedOutput[TEST_CASE_COUNT][maxNumElements] = {\n        \n        {4.454347, 3.332205, 3.912023, 4.330733, 4.158883, 4.584968, 0.000000, 4.532599, 2.890372, 3.663562},\n\n        {3.218876, 4.574711, 3.044523, 4.174387, 2.484907, 3.970292, 4.488636, 3.784190, 3.583519, 3.367296, 1.098612, 4.094345, 4.510859, 4.369448, 2.197225},\n        \n        {3.465736, 4.382027, 4.025352, 2.890372, 4.595120, 4.553877, 2.772589, 4.143135, 3.401197, 3.828641, 3.988984, 3.526361, 4.290460, 4.406719, 1.386294, 3.663562, 3.850148, 1.945910, 4.615120, 0.000000},\n        \n        {3.091043, 2.197225, 3.258096, 4.219508, 4.007333, 4.442651, 4.532599, 1.098612, 2.397895, 3.610918, 2.484907, 4.605170, 1.791759, 4.584968, 4.356709, 4.077538, 3.850148, 4.262680, 3.583519, 2.890372, 4.499810, 2.302585, 4.317488, 4.189655, 2.995732, 3.332205, 4.094345, 4.615120, 1.945910, 3.135494},\n        \n        {3.555348, 4.532599, 4.605170, 4.077538, 4.127134, 3.931826, 4.488636, 4.369448, 4.204693, 4.454347, 4.276666, 4.584968, 2.708050, 2.397895, 3.784190, 3.295837, 1.098612, 4.330733, 3.610918, 4.248495, 4.174387, 1.386294, 2.639057, 1.791759, 3.828641, 4.158883, 4.564348, 4.025352, 4.394449, 3.135494, 2.995732, 3.526361, 3.713572, 2.079442, 2.197225, 2.833213, 3.332205, 3.044523, 4.430817, 3.912023},\n        \n        {3.496508, 3.178054, 4.290460, 4.043051, 4.262680, 4.543295, 2.944439, 3.433987, 3.931826, 3.828641, 4.356709, 4.219508, 4.584968, 3.295837, 4.442651, 4.521789, 4.110874, 3.555348, 2.708050, 4.094345, 4.510859, 3.258096, 2.197225, 3.761200, 2.484907, 4.304065, 3.401197, 2.564949, 3.663562, 3.737670, 4.553877, 4.025352, 2.639057, 2.079442, 4.477337, 3.891820, 3.713572, 4.615120, 4.465908, 4.204693, 1.098612, 4.127134, 4.189655, 2.397895, 3.526361, 2.833213, 2.302585, 3.367296, 1.386294, 3.091043},\n        \n        {3.583519, 4.553877, 3.178054, 2.639057, 4.356709, 4.043051, 3.367296, 2.708050, 4.605170, 2.772589, 4.615120, 4.330733, 4.465908, 1.386294, 3.465736, 4.521789, 3.295837, 4.430817, 0.693147, 3.258096, 4.442651, 2.484907, 4.094345, 3.891820, 3.218876, 3.737670, 3.806663, 4.595120, 3.091043, 1.945910, 4.219508, 4.574711, 4.543295, 3.931826, 3.555348, 2.079442, 2.890372, 3.688879, 2.302585, 2.397895, 4.248495, 3.784190, 3.401197, 3.663562, 4.454347, 4.204693, 2.197225, 3.610918, 4.127134, 1.791759, 4.564348, 4.304065, 4.158883, 2.995732, 4.174387, 4.110874, 3.044523, 4.477337, 2.564949, 1.098612}\n    };\n    \n    // Use CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    float output_h[maxNumElements];\n    //Declare device pointers\n    int *input_d; float *output_d; \n    \n    // Allocate memory on device\n    CUDA_CHECK(cudaMallocAsync(&input_d, maxNumElements * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, maxNumElements * sizeof(float), stream));\n   \n    for (int t = 0; t < TEST_CASE_COUNT; t++){ \n    \n        int inputSize = numElements[t];\n        //Copy input array to device\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input[t], inputSize * sizeof(int), cudaMemcpyHostToDevice, stream));\n        //Host function\n        cubNormalizeFunction(input_d, output_d, inputSize, stream);\n        // Copy the result back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, inputSize * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        \n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        //Validate the result\n        for (int i = 0; i < inputSize; ++i) {\n            assert(fabs(output_h[i] - expectedOutput[t][i]) < TOLERANCE);\n        }  \n    }\n    // Free device memory\n    CUDA_CHECK(cudaFreeAsync(input_d,stream));\n    CUDA_CHECK(cudaFreeAsync(output_d,stream));  \n}\n\n// Host function to compute the normalized value\nvoid cubNormalizeFunction(int* input_d, float* output_d, int inputSize, cudaStream_t stream){\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/166", "date": "2025-07-30", "prompt": "Perform filtering of perfect squares from an integer array using the CUB library with a custom predicate functor which validates the elements, compacting the sequence to retain only qualifying elements and resulting the total count of selected values.\n\nThe signature of the function is void selectPerfectSquares(int* input_d, int* output_d, int* numSelected_d, int inputSize, cudaStream_t stream), where input_d is a device pointer to the input array, output_d is a device pointer where the selected perfect squares will be stored, numSelected_d is a device pointer to store the count of perfect squares found, inputSize is the size of the input array, and stream is the CUDA stream for executing operations asynchronously.\n\n>>> selectPerfectSquares({1, 4, 9, 3, 7, 16, 25}, output_d, numSelected_d, 7, cudaStream) -> output_d contains {4, 9, 16, 25}, *numSelected_d = 4\n>>> selectPerfectSquares({0, 1, 4, 7, 9, 36}, output_d, numSelected_d, 6, cudaStream) -> output_d contains {0, 1, 4, 9, 36}, *numSelected_d = 5\n", "cc_flags": "-arch=sm_80", "ld_flags": "", "declaration": "\n#include <vector>\n#include <cassert>\n#include <cuda_runtime.h>\n#include <cub/device/device_select.cuh>\n#include <cmath>\n#include <cstdlib>\nusing namespace std;\n\n// Error-checking macro\n#define CUDA_CHECK(call)                                                   \\\n{                                                                          \\\n    cudaError_t error = call;                                              \\\n    if(error != cudaSuccess) {                                             \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                      \\\n                __FILE__, __LINE__, cudaGetErrorString(error));            \\\n        exit(EXIT_FAILURE);                                                \\\n    }                                                                      \\\n}\n\n#define CUB_CHECK(call)                                                    \\\n{                                                                          \\\n    cudaError_t error = call;                                              \\\n    if(error != cudaSuccess) {                                             \\\n        fprintf(stderr, \"CUB error at %s:%d - %s\\n\",                       \\\n                __FILE__, __LINE__, cudaGetErrorString(error));            \\\n        exit(EXIT_FAILURE);                                                \\\n    }                                                                      \\\n}\n\nvoid selectPerfectSquares(int* input_d, int* output_d, int* numSelected_d, int inputSize, cudaStream_t stream);\n\n// Simplified structure for test cases\nstruct PerfectSquareTestCase {\n    vector<int> inputArray;      // Input array \n    vector<int> expectedSquares; // Expected perfect squares\n};\n\n// Launch function\nvoid launch() {\n    vector<PerfectSquareTestCase> testCases = {\n        { {1, 2, 3, 4, 5, 6, 7, 8, 9}, {1, 4, 9} },                           // Basic test with single-digit numbers\n        { {10, 16, 25, 30, 36, 40, 49, 50, 64, 70, 81, 100}, {16, 25, 36, 49, 64, 81, 100} }, // Two-digit perfect squares\n        { {-4, -1, 0, 1, 4, 9, 16}, {0, 1, 4, 9, 16} },                      // Test with negative values and zero\n        { {121, 144, 150, 169, 180, 196, 200, 225}, {121, 144, 169, 196, 225} }, // Larger perfect squares\n        { {}, {} },                                                           // Empty array\n        { {2, 3, 5, 7, 11, 13, 17, 19}, {} },                                // No perfect squares (prime numbers)\n        { {1, 4, 9, 16, 25, 36, 49, 64, 81, 100}, {1, 4, 9, 16, 25, 36, 49, 64, 81, 100} } // All perfect squares\n    };\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    for (size_t testCaseIndex = 0; testCaseIndex < testCases.size(); testCaseIndex++) {\n        const vector<int>& input_h = testCases[testCaseIndex].inputArray;\n        const vector<int>& expected_h = testCases[testCaseIndex].expectedSquares;\n        \n        int inputSize = input_h.size();\n        \n        // Skip if input array is empty but verify empty result\n        if (inputSize == 0) {\n            assert(expected_h.size() == 0);\n            continue;\n        }\n\n        // Device memory allocations\n        int *input_d = nullptr, *output_d = nullptr;\n        int *numSelected_d = nullptr;\n        \n        // Allocate device memory\n        CUDA_CHECK(cudaMallocAsync(&input_d, inputSize * sizeof(int), stream));\n        CUDA_CHECK(cudaMallocAsync(&output_d, inputSize * sizeof(int), stream)); // At most all elements could be selected\n        CUDA_CHECK(cudaMallocAsync(&numSelected_d, sizeof(int), stream));\n        \n        // Initialize counter to 0\n        CUDA_CHECK(cudaMemsetAsync(numSelected_d, 0, sizeof(int), stream));\n        \n        // Copy input to device\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h.data(), \n                                  inputSize * sizeof(int), cudaMemcpyHostToDevice, stream));\n        \n        // Call the function to select perfect squares\n        selectPerfectSquares(input_d, output_d, numSelected_d, inputSize, stream);\n        \n        // Copy results back to host for verification\n        int numSelected_h;\n        CUDA_CHECK(cudaMemcpyAsync(&numSelected_h, numSelected_d, \n                                  sizeof(int), cudaMemcpyDeviceToHost, stream));\n        \n        vector<int> result_h(numSelected_h);\n        if (numSelected_h > 0) {\n            CUDA_CHECK(cudaMemcpyAsync(result_h.data(), output_d, \n                                      numSelected_h * sizeof(int), cudaMemcpyDeviceToHost, stream));\n        }\n        \n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Verify results using assertions\n        assert(numSelected_h == expected_h.size());\n\n        \n        // Sort both results and expected for comparison\n        vector<int> sortedResult_h = result_h;\n        vector<int> sortedExpected_h = expected_h;\n        std::sort(sortedResult_h.begin(), sortedResult_h.end());\n        std::sort(sortedExpected_h.begin(), sortedExpected_h.end());\n        \n        // Verify content matches using assertions\n        bool contentMatches = true;\n        if (sortedResult_h.size() == sortedExpected_h.size()) {\n            for (size_t i = 0; i < sortedResult_h.size(); i++) {\n                if (sortedResult_h[i] != sortedExpected_h[i]) {\n                    contentMatches = false;\n                    break;\n                }\n            }\n        } else {\n            contentMatches = false;\n        }\n        \n        assert(contentMatches);\n        \n        // Free device memory\n        CUDA_CHECK(cudaFreeAsync(input_d, stream));\n        CUDA_CHECK(cudaFreeAsync(output_d, stream));\n        CUDA_CHECK(cudaFreeAsync(numSelected_d, stream));\n    }\n    \n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/167", "date": "2025-07-30", "prompt": "Write a function that uses CUDA cub library compute histograms for the R, G, and B channels from an RGBA image with equally sized bins. Image is stored as an interleaved array of 32-bit pixels, each with four 8-bit unsigned char channels.\n\nThe signature of the function is void calcMultiHistogram(uint8_t* inputData_d, int** histogram_d, int* numLevels_h, unsigned int* lowerLevel_h, unsigned int* upperLevel_h, int numPixels, cudaStream_t stream), where inputData_d is the pointer to the input sequence of data samples, histogram_d is the pointer to the histogram counter output array, numLevels is the pointer to array containing number of boundaries for histogram samples, lowerLevel is the pointer to the array containing lower sample value bound for the lowest histogram bin, upperLevel is the pointer to the array containing upper sample value bound for the highest histogram bin, numPixels is the number of pixels in input image, and stream is the handle for asynchronous operations.\n\n>>> calcMultiHistogram({{2, 6, 7, 5}, {3, 0, 2, 1}, {7, 0, 6, 2}, {0, 6, 7, 5}, {3, 0, 2, 6}}, **histogram_d, \n        {5, 5, 5}, {0, 0, 0}, {256, 256, 256}, 5, stream) =>\n    histogram_d : ({{ {5, 0, 0, 0}, {5, 0, 0, 0}, {5, 0, 0, 0} }})\n>>> calcMultiHistogram({{108, 80, 41, 45}, {108, 24, 153, 120}, {178, 179, 163, 8}, {17, 81, 135, 167}, {104, 209, 183, 247},\n        {136, 83, 27, 156}, {199, 108, 23, 68}}, **histogram_d, {10, 10, 10}, {0, 0, 0}, {256, 256, 256}, 7, stream) =>\n    histogram_d :   ({ {1, 0, 0, 3, 1, 0, 2, 0, 0}, {1, 0, 3, 1, 0, 0, 1, 1, 0}, {2, 1, 0, 0, 1, 2, 1, 0, 0} })\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n#undef NDEBUG\n#include <assert.h>\n\n#define NUM_CHANNELS        (4)\n#define NUM_ACTIVE_CHANNELS (3)\n#define LOWER_LEVEL         (0)\n#define UPPER_LEVEL         (256)\n#define MAX_NUM_LEVELS      (12)\n#define MAX_NUM_BINS        (MAX_NUM_LEVELS - 1)\n#define CUDA_CHECK(call)                              \\\ndo {                                                  \\\n       cudaError_t error = call;                      \\\n       if (error != cudaSuccess) {                    \\\n           fprintf(stderr, \"CUDA error at %s:%d %s\\n\",\\\n                   __FILE__, __LINE__,                \\\n                   cudaGetErrorString(error));        \\\n           exit(EXIT_FAILURE);                        \\\n       }                                              \\\n} while(0)\n\n#define CUB_CHECK(call)                                                 \\\ndo {                                                                    \\\n    call;                                                               \\\n    cudaError_t err = cudaGetLastError();                               \\\n    if (err != cudaSuccess) {                                           \\\n        fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,\\\n                cudaGetErrorString(err));                               \\\n        exit(EXIT_FAILURE);                                             \\\n    }                                                                   \\\n} while (0)\n\nvoid calcMultiHistogram(  uint8_t* inputData_d, int** histogram_d,\n                          int* numLevels_h, unsigned int* lowerLevel_h,\n                          unsigned int* upperLevel_h, int numPixels,\n                          cudaStream_t stream);\nvoid launch() {\n    // Number of test cases\n    constexpr int TEST_CASE_COUNT = 8;\n    // Number of pixels in each test case\n    int inputDataLength[TEST_CASE_COUNT] = {6, 8, 12, 12, 15, 8, 9, 16};\n    //  Number of boundaries in each test case for each channel\n    int numLevels[TEST_CASE_COUNT] = {5, 10, 8, 7, 12, 6, 5, 5};\n\n    // Calculating maximum sizes\n    const int MAX_NUM_PIXELS = *std::max_element(inputDataLength, inputDataLength + TEST_CASE_COUNT);\n    const int maxNumLevels = *std::max_element(numLevels, numLevels + TEST_CASE_COUNT);\n\n    if(MAX_NUM_LEVELS != maxNumLevels) {\n      assert(false && \"maxNumLevels is not equal to MAX_NUM_LEVELS\");\n    }\n\n    // Input vectors\n    uint8_t inputData_h[TEST_CASE_COUNT][MAX_NUM_PIXELS][NUM_CHANNELS] = {\n\n        // 2 x 3 image size \n        {{2, 6, 7, 5}, {3, 0, 2, 1}, {7, 0, 6, 2}, \n        {0, 6, 7, 5}, {3, 0, 2, 6}, {167, 9, 217, 239}},\n\n        // 2 x 4 image size\n        {{108, 80, 41, 45}, {108, 24, 153, 120}, {178, 179, 163, 8}, {17, 81, 135, 167},\n         {104, 209, 183, 247}, {136, 83, 27, 156}, {199, 108, 23, 68}, {167, 41, 30, 127}},\n\n        // 3 x 4 image size\n        {{39, 71, 112, 134}, {117, 224, 132, 241}, {163, 245, 61, 173}, {74, 171, 177, 17},\n        {65, 57, 170, 216}, {88, 199, 172, 1}, {154, 99, 234, 0}, {118, 108, 117, 197}, \n        {82, 200, 120, 9}, {45, 184, 121, 39}, {102, 19, 61, 31}, {47, 61, 106, 12}},\n\n        // 3 x 4 image size\n        {{87, 155, 49, 189}, {62, 234, 68, 195}, {48, 73, 23, 147}, {174, 139, 108, 164},\n         {165, 173, 162, 241}, {53, 181, 60, 30}, {155, 115, 117, 169}, {197, 89, 169, 106}, \n         {215, 213, 65, 157}, {149, 138, 222, 67}, {81, 30, 240, 165}, {122, 163, 139, 165}},\n\n        // 3 x 5 image size\n        {{139, 184, 133, 254}, {55, 27, 28, 16}, {103, 114, 93, 195}, {160, 197, 238, 249}, {49, 35, 178, 24},\n         {134, 135, 220, 124}, {100, 171, 189, 133}, {89, 38, 150, 67}, {11, 193, 62, 113}, {176, 91, 188, 101},\n         {174, 180, 113, 5}, {84, 108, 69, 50}, {210, 110, 227, 100}, {196, 101, 206, 193}, {96, 55, 202, 243}},\n\n        // 2 x 4 image size\n        { {83, 171, 112, 213}, {196, 42, 220, 253}, {131, 226, 150, 39}, {51, 104, 191, 211}, \n          {202, 81, 136, 23}, {28, 34, 173, 126}, {48, 126, 37, 14}, {217, 143, 237, 178}},\n\n        // 3 x 3 image size\n        { {149, 208, 225, 253}, {0, 221, 156, 253}, {135, 122, 205, 58}, \n          {127, 230, 147, 216}, {189, 150, 63, 170}, {21, 160, 169, 186}, \n          {228, 251, 196, 148}, {126, 199, 183, 231}, {228, 85, 178, 50}},\n\n        // 4 x 4 image size\n        { {237, 148, 4, 30}, {220, 123, 216, 53}, {141, 161, 8, 157}, {92, 12, 125, 49}, \n          {31, 52, 37, 48}, {10, 162, 72, 137}, {177, 127, 137, 113}, {31, 125, 218, 223}, \n          {69, 53, 144, 163}, {106, 52, 242, 21}, {44, 100, 212, 205}, {15, 102, 134, 106}, \n          {168, 160, 74, 110}, {3, 251, 42, 27}, {95, 50, 125, 86}, {243, 235, 13, 188}\n         }\n      };\n\n    // Expected histogram outputs\n    int expectedOutput[TEST_CASE_COUNT][NUM_ACTIVE_CHANNELS][MAX_NUM_BINS] = {\n      { {5, 0, 1, 0}, {6, 0, 0, 0}, {5, 0, 0, 1} },\n      { {1, 0, 0, 3, 1, 1, 2, 0, 0}, {1, 1, 3, 1, 0, 0, 1, 1, 0}, {2, 2, 0, 0, 1, 2, 1, 0, 0} },\n      { {0, 4, 4, 2, 2, 0, 0}, {1, 3, 2, 0, 1, 3, 2}, {0, 2, 1, 5, 3, 0, 1} },\n      { {0, 4, 2, 3, 2, 1}, {1, 1, 2, 4, 3, 1}, {1, 4, 2, 3, 0, 2} },\n      { {1, 0, 2, 2, 3, 2, 1, 2, 1, 1, 0}, {0, 3, 1, 1, 4, 1, 0, 3, 2, 0, 0}, {0, 1, 2, 1, 1, 1, 1, 1, 4, 2, 1} },\n      { {3, 1, 1, 2, 1}, {2, 1, 3, 1, 1}, {1, 0, 3, 2, 2} },\n      { {2, 2, 3, 2}, {0, 2, 2, 5}, {1, 0, 5, 3} },\n      { {6, 4, 3, 3}, {5, 5, 4, 2}, {5, 4, 3, 4} }\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Initialize host memories\n    unsigned int lowerLevel_h[NUM_ACTIVE_CHANNELS] = {LOWER_LEVEL, LOWER_LEVEL, LOWER_LEVEL};\n    unsigned int upperLevel_h[NUM_ACTIVE_CHANNELS] = {UPPER_LEVEL, UPPER_LEVEL, UPPER_LEVEL};\n    int* histogram_h[NUM_ACTIVE_CHANNELS];\n    // For each channel, allocate memory for the histogram\n    for (int i = 0; i < NUM_ACTIVE_CHANNELS; ++i) {\n        histogram_h[i] = (int*)malloc(MAX_NUM_BINS * sizeof(int));\n    }\n\n    // Allocating device memories for RGBA\n    uint8_t* inputData_d;\n    cudaMallocAsync(&inputData_d, MAX_NUM_PIXELS * NUM_CHANNELS * sizeof(uint8_t), stream);\n    // Declare an array of device pointers to store histograms for each channel\n    int* histogram_d[NUM_ACTIVE_CHANNELS];\n    // Allocate memory for histograms for each channel R, G, B\n    for (int i = 0; i < NUM_ACTIVE_CHANNELS; ++i) {\n        cudaMallocAsync(&histogram_d[i], MAX_NUM_BINS * sizeof(int), stream);\n    }\n\n    // Loop to execute each test case\n    for (int iTestcase = 0; iTestcase < TEST_CASE_COUNT; iTestcase++) {\n        // Memory copying to device\n        CUDA_CHECK(cudaMemcpyAsync(inputData_d, &inputData_h[iTestcase][0][0], sizeof(uint8_t) * inputDataLength[iTestcase] * NUM_CHANNELS, cudaMemcpyHostToDevice, stream));\n\n        // Preparing numLevel for histogram\n        int numLevelEachChannel = numLevels[iTestcase];\n        int numLevelArray_h[NUM_ACTIVE_CHANNELS] = {numLevelEachChannel, numLevelEachChannel, numLevelEachChannel};\n\n        calcMultiHistogram(inputData_d, histogram_d, numLevelArray_h, \n        lowerLevel_h, upperLevel_h, inputDataLength[iTestcase], stream);\n\n        // Copy each channel's histogram data from device to host\n        int numBins = numLevels[iTestcase] - 1;\n        for (int i = 0; i < NUM_ACTIVE_CHANNELS; ++i) {\n            cudaMemcpyAsync(histogram_h[i], histogram_d[i], numBins * sizeof(int), cudaMemcpyDeviceToHost, stream);\n        }\n\n        // Check tasks in the stream has completed\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Results verification\n        for(int i = 0; i < numBins; i++) {\n            assert(histogram_h[0][i] == expectedOutput[iTestcase][0][i]);\n            assert(histogram_h[1][i] == expectedOutput[iTestcase][1][i]);\n            assert(histogram_h[2][i] == expectedOutput[iTestcase][2][i]);\n        }\n    }\n\n    for (int i = 0; i < NUM_ACTIVE_CHANNELS; ++i) {\n        free(histogram_h[i]);\n        CUDA_CHECK(cudaFreeAsync(histogram_d[i], stream));\n    }\n    CUDA_CHECK(cudaFreeAsync(inputData_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\nvoid calcMultiHistogram(uint8_t* inputImage_d, int** histogram_d,\n                          int* numLevels_h, unsigned int* lowerLevel_h,\n                          unsigned int* upperLevel_h, int numPixels,\n                          cudaStream_t stream) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/168", "date": "2025-07-30", "prompt": "Use CUB's device-wide primitives to compute the minimum and maximum values of each segment of a row major matrix of dimension m x n. Consider each row as an individual segment.\n\nThe signature of the CUDA function is:\nvoid minMaxFunction(int* input_d, int* outputMin_d, int* outputMax_d, int rows, int cols, cudaStream_t stream) where input_d is the input matrix data (row-major order), outputMin_d stores the minimum value of each row, outputMax_d stores the maximum value of each row, rows specifies the number of rows in the matrix which also represents the number of segments, cols represents the number of columns in the matrix and stream is used for asynchronous operations.\n\n>>> minMaxFunction({{10, 6, 8}, {9, 5, 12}}, output Min_d, outputMax_d, 2, 3, stream): OutputMin_d,outputMax_d -> {{6, 10}, {5, 12}}\n>>> minMaxFunction({{18, 16, 14, 15}, {19, 21, 20, 17}, {22, 24, 28, 23}}, output Min_d, outputMax_d, 3, 4, stream): OutputMin_d, outputMax_d -> {{14, 18}, {17, 21}, {22, 28}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <stdio.h>\n#include <assert.h>\n#include <algorithm>\n#include <climits>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#include <cub/cub.cuh>\n\n#define CUDA_CHECK(call){                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess){                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n#undef NDEBUG\n\n#ifndef CUB_DEBUG\n#  define CUB_DEBUG(e) CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__)\n#endif\n\nvoid minMaxFunction(int* input_d, int* outputMin_d, int* outputMax_d, int rows, int cols, cudaStream_t stream);\n\nvoid launch() {\n\n    const int TEST_CASE_COUNT = 7;\n    int rows[TEST_CASE_COUNT] = {2, 2, 3, 5, 6, 5, 8};\n    int cols[TEST_CASE_COUNT] = {2, 3, 4, 5, 4, 7, 8};\n    int maxrows = *std::max_element(rows, rows + TEST_CASE_COUNT);\n    int maxcols = *std::max_element(cols, cols + TEST_CASE_COUNT);\n\n    int input_h[TEST_CASE_COUNT][maxrows][maxcols] = {\n        // Test Case 0 (2x2)\n        {\n            {3, 1}, \n            {4, 2}\n        },\n    \n        // Test Case 1 (2x3) \n        {\n            {10, 6, 8},\n            {9, 5, 12}\n        },\n    \n        // Test Case 2 (3x4)\n        {\n            {18, 16, 14, 15},\n            {19, 21, 20, 17},\n            {22, 24, 28, 23}\n        },\n    \n        // Test Case 3 (5x5)\n        {\n            {32, 30, 34, 31, 33},\n            {37, 35, 39, 36, 38},\n            {40, 42, 41, 44, 43},\n            {48, 46, 45, 47, 49},\n            {54, 52, 51, 50, 53}\n        },\n    \n        // Test Case 4 (6x4)\n        {\n            {55, 59, 57, 58},\n            {63, 65, 62, 61},\n            {68, 67, 70, 69},\n            {74, 76, 75, 73},\n            {80, 83, 79, 82},\n            {86, 90, 87, 88}\n        },\n    \n        // Test Case 5 (5x7)\n        {\n            {93, 91, 96, 94, 97, 92, 95},\n            {99, 104, 102, 101, 103, 98, 100},\n            {105, 107, 110, 106, 109, 108, 111},\n            {114, 112, 116, 113, 115, 118, 117},\n            {120, 119, 122, 121, 124, 123, 125} \n        },\n    \n        // Test Case 6 (8x8)\n        {\n            {144, 140, 145, 141, 146, 143, 142, 147},\n            {150, 148, 153, 149, 152, 151, 155, 154},\n            {158, 156, 160, 157, 159, 162, 161, 163},\n            {165, 167, 164, 170, 166, 168, 169, 171},\n            {175, 172, 173, 174, 178, 177, 176, 179},\n            {180, 183, 181, 182, 185, 184, 186, 187},\n            {193, 188, 189, 190, 192, 191, 195, 194},\n            {200, 196, 198, 197, 202, 199, 201, 203}\n        }\n    };\n\n    int expectedOutput[TEST_CASE_COUNT][maxrows][maxcols] = {\n        {\n            {1, 3},\n            {2, 4}\n        },\n        {\n            {6, 10},\n            {5, 12}\n        },\n        {\n            {14, 18},\n            {17, 21},\n            {22, 28}\n        },\n        {\n            {30, 34},\n            {35, 39},\n            {40, 44},\n            {45, 49},\n            {50, 54}\n        },\n        {\n            {55, 59},\n            {61, 65},\n            {67, 70},\n            {73, 76},\n            {79, 83},\n            {86, 90}\n        },\n        {\n            {91, 97},\n            {98, 104},\n            {105, 111},\n            {112, 118},\n            {119, 125}\n        },\n        {\n            {140, 147},\n            {148, 155},\n            {156, 163},\n            {164, 171},\n            {172, 179},\n            {180, 187},\n            {188, 195},\n            {196, 203}\n        }\n    };\n    \n    // Flatten matrix to 1D for device transfer\n    int numElements = maxrows * maxcols;\n    int inputFlat[numElements];\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Declare host and device pointers\n    int* input_d;\n    int *outputMin_d, *outputMax_d;  \n       \n    // Allocate memory on device\n    CUDA_CHECK(cudaMallocAsync(&input_d, sizeof(int) * numElements, stream));\n    CUDA_CHECK(cudaMallocAsync(&outputMin_d, sizeof(int) * maxrows, stream));\n    CUDA_CHECK(cudaMallocAsync(&outputMax_d, sizeof(int) * maxrows, stream));   \n    \n    // Loop to execute each test case\n    for (int t = 0; t < TEST_CASE_COUNT; ++t) {\n        int r = rows[t];\n        int c = cols[t];\n        for (int i = 0; i < r; ++i)\n            for (int j = 0; j < c; ++j)\n                inputFlat[i * c + j] = input_h[t][i][j];   //Flatten the input matrix into 1D Array          \n\n        CUDA_CHECK(cudaMemcpyAsync(input_d, inputFlat, sizeof(int) * r * c, cudaMemcpyHostToDevice, stream));\n              \n        //Host Function\n        minMaxFunction(input_d, outputMin_d, outputMax_d, r, c, stream);\n     \n        int outputMin_h[r];\n        int outputMax_h[r];\n\n        // Copy results back\n        CUDA_CHECK(cudaMemcpyAsync(outputMin_h, outputMin_d, sizeof(int) * r, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(outputMax_h, outputMax_d, sizeof(int) * r, cudaMemcpyDeviceToHost, stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        //validate the results\n        for(int i = 0; i < r; i++) {\n            assert(outputMin_h[i] == expectedOutput[t][i][0] && outputMax_h[i] == expectedOutput[t][i][1]);\n        }\n    }\n\n    // Free device memory\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputMin_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputMax_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n//Host Function\nvoid minMaxFunction(int* input_d, int* outputMin_d, int* outputMax_d, int rows, int cols, cudaStream_t stream) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/169", "date": "2025-07-30", "prompt": "Use the Thrust API to perform a parallel element-wise transformation on the array with a custom predicate that uses a flag array to mark peaks (elements greater than both neighbors) with 1 and valleys (elements smaller than both neighbors) with -1.\n\nThe signature of the function is void detectPeaksAndValleys(thrust::device_vector<int>& data_d, thrust::device_vector<int>& flags_d, cudaStream_t& cudaStream), where data_d is a device vector containing the input data points, flags_d is an output device vector that will be populated with flags indicating peaks (1), valleys (-1), or neither (0) for each corresponding position in data_d, cudaStream is the CUDA stream for executing operations asynchronously.\n\nThe signature of the functor is struct d_PeakValleyFunctor { __device__ int operator()(int idx) const; } where idx is the index of the element to check in the data array, and the return value is 1 for peaks, -1 for valleys, or 0 for neither.\n\n>>> detectPeaksAndValleys([5,2,6,2,7,3], flags_d, cudaStream) -> flags_d = [0,-1,1,-1,1,0]  (valleys at positions 1 and 3, peaks at positions 2 and 4)\n>>>  detectPeaksAndValleys([1,2,3,4,5], flags_d, cudaStream) -> flags_d = [0,0,0,0,0]  (no peaks or valleys in a monotonically increasing sequence)\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <vector>\n#include <cuda.h>\n#include <thrust/device_vector.h>\n#include <thrust/host_vector.h>\n#include <thrust/transform.h>\n#include <thrust/functional.h>\n#include <thrust/execution_policy.h>\n#include <thrust/iterator/counting_iterator.h>\nusing namespace std;\n\n#define CUDA_CHECK(call) {                                      \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                  \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\nvoid detectPeaksAndValleys(thrust::device_vector<int>& data_d, \n    thrust::device_vector<int>& flags_d,\n    cudaStream_t& cudaStream);\n\nvoid launch() {\n    cudaStream_t cudaStream;\n    CUDA_CHECK(cudaStreamCreate(&cudaStream));\n    \n    // Define test cases with input arrays\n    struct TestCase {\n        vector<int> input;\n        vector<int> expectedPeaks;\n        vector<int> expectedValleys;\n    };\n    \n    vector<TestCase> testCases = {\n        { {1, 3, 2, 4, 1, 5, 3, 6, 4}, {3, 4, 5, 6}, {2, 1, 3} },\n        { {5, 4, 3, 2, 1}, {}, {} },    // Monotonically decreasing - no peaks or valleys\n        { {1, 2, 3, 4, 5}, {}, {} },    // Monotonically increasing - no peaks or valleys\n        { {3, 3, 3, 3, 3}, {}, {} },    // Flat array - no peaks or valleys\n        { {1, 5, 2, 7, 3, 8}, {5, 7}, {2, 3} },\n        { {8, 1, 9, 2, 7, 3, 6}, {9, 7}, {1, 2, 3} }, \n        { {4, 1, 5, 1, 5, 1, 4}, {5, 5}, {1, 1, 1} } \n    };\n    \n    for (int testIdx = 0; testIdx < testCases.size(); testIdx++) {\n        const auto& currentTest = testCases[testIdx];\n        const int arraySize = currentTest.input.size();\n        \n        // Allocate host array\n        thrust::host_vector<int> data_h(currentTest.input.begin(), currentTest.input.end());\n        \n        // Allocate device arrays\n        thrust::device_vector<int> data_d(arraySize);\n        thrust::device_vector<int> flags_d(arraySize);\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(thrust::raw_pointer_cast(data_d.data()),\n                                   thrust::raw_pointer_cast(data_h.data()),\n                                   sizeof(int) * arraySize,\n                                   cudaMemcpyHostToDevice,\n                                   cudaStream));\n        \n        // Detect peaks and valleys\n        detectPeaksAndValleys(data_d, flags_d, cudaStream);\n\n        CUDA_CHECK(cudaStreamSynchronize(cudaStream));\n\n        // Copy results back to host\n        thrust::host_vector<int> flags_h = flags_d;\n        \n        // Verify results\n        vector<int> detectedPeaks;\n        vector<int> detectedValleys;\n        \n        for (int i = 1; i < arraySize-1; ++i) {\n            if (flags_h[i] == 1) {\n                detectedPeaks.push_back(data_h[i]);\n            } else if (flags_h[i] == -1) {\n                detectedValleys.push_back(data_h[i]);\n            }\n        }\n        \n        // Verify peaks\n        assert(detectedPeaks.size() == currentTest.expectedPeaks.size());\n        for (int i = 0; i < detectedPeaks.size(); ++i) {\n            assert(detectedPeaks[i] == currentTest.expectedPeaks[i]);\n        }\n        \n        // Verify valleys\n        assert(detectedValleys.size() == currentTest.expectedValleys.size());\n        for (int i = 0; i < detectedValleys.size(); ++i) {\n            assert(detectedValleys[i] == currentTest.expectedValleys[i]);\n        }\n    }\n    \n    CUDA_CHECK(cudaStreamDestroy(cudaStream));\n}\n\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/170", "date": "2025-07-30", "prompt": "Write a function that utilizes CUDA graphs. It should include a node for the kernel k_calculate to obtain the squared value of the input matrix and a node for the in-place feedback loop, which is a device-to-device copy from output to input. This output will serve as input for the next iteration of the graph, executing this repeatedly for k times, where k is a positive integer.\n\nThe signature of the function is void runGraph(int k, float *dataIn_d, float *dataOut_d, int *constantParams_d, int maxActiveBlocks, cudaStream_t stream), where k specifies the number of launches for the CUDA graph, dataIn_d is a pointer to an array of float elements that hold the input values in device memory, dataOut_d is a pointer to an array of float elements that holds the output values in device memory, constantParams_d is a pointer to an array of integer elements representing the constants of the algorithm, maxActiveBlocks is an integer indicating the maximum number of active blocks, and stream is the CUDA stream used to launch the graph.\n\nThe signature of the CUDA kernel is __global__ void k_calculate(float * dataIn_d, float * dataOut_d, int * constantParams_d), where dataIn_d is a pointer to the matrix for which the power is to be calculated, dataOut_d is a pointer to the square of the matrix from dataIn_d, and constantParams_d is a pointer to the array of parameters, with the first item representing the matrix size.\n\n>>> runGraph(3, {\n    1.00f, 1.00f, 1.00f, \n    1.00f, 1.00f, 1.00f, \n    1.00f, 1.00f, 1.00f\n}, { 3 }) -> data_h: {\n    2187.0f, 2187.0f, 2187.0f, \n    2187.0f, 2187.0f, 2187.0f, \n    2187.0f, 2187.0f, 2187.0f\n}\n>>> runGraph(5, {\n    1.00f, 0.00f, 0.00f, 0.00f, \n    0.00f, 1.00f, 0.00f, 0.00f, \n    0.00f, 0.00f, 1.00f, 0.00f, \n    0.00f, 0.00f, 0.00f, 1.00f\n}, { 4 }) -> data_h: {\n    1.00f, 0.00f, 0.00f, 0.00f, \n    0.00f, 1.00f, 0.00f, 0.00f, \n    0.00f, 0.00f, 1.00f, 0.00f, \n    0.00f, 0.00f, 0.00f, 1.00f\n}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <cooperative_groups.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// CUDA settings.\nconstexpr int BLOCK_SIZE = 256;\n// CUDA-graph settings.\nconstexpr auto HAS_NO_DEPENDENCY = nullptr;\nconstexpr int ZERO_NODES_AS_DEPENDENCY = 0;\nconstexpr int ONE_NODE_AS_DEPENDENCY = 1;\n\n// The graph has two nodes. One node for the matrix multiplication, and one device-to-device memcpy node for the in-place feedback loop.\nconstexpr int NUM_NODES = 2;\nconstexpr int NODE_KERNEL = 0;\nconstexpr int NODE_MEMCPY_DEVICE_TO_DEVICE = 1;\nconstexpr int SELECT_PARAM_MATRIX_SIZE = 0;\n\n// Test settings.\nconstexpr int MAX_MATRIX_SIZE = 150;\nconstexpr int MAX_MATRIX_ELEMENTS = MAX_MATRIX_SIZE * MAX_MATRIX_SIZE;\nconstexpr float ERROR_TOLERANCE = 1e-3;\nconstexpr int DETERMINISTIC_INITIAL_CONDITION = 42;\n\n// Host function to create and run a cuda graph.\nvoid runGraph(int k, float * dataIn_d, float * dataOut_d, int * constantParams_d, int maxActiveBlocks, cudaStream_t stream);\n\n// CUDA kernel to do the calculations in CUDA graph.\n__global__ void k_calculate(float * dataIn_d, float * dataOut_d, int * constantParams_d) {\n    int globalThreadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n    int numGlobalThreads = blockDim.x * gridDim.x;\n    int matrixSize = constantParams_d[SELECT_PARAM_MATRIX_SIZE];\n    int numIterationsForGridStrideLoop = (matrixSize * matrixSize + numGlobalThreads - 1) / numGlobalThreads;\n    // Multiplying the matrix in dataIn_d with itself and writing the output to dataOut_d.\n    for(int iteration = 0; iteration < numIterationsForGridStrideLoop; iteration++) {\n        int workId = iteration * numGlobalThreads + globalThreadIndex;\n        if(workId < matrixSize * matrixSize) {\n            int outputColumn = workId % matrixSize;\n            int outputRow = workId / matrixSize;\n            float c = 0.0f;\n            for(int i = 0; i < matrixSize; i++) {\n                float a = dataIn_d[outputRow * matrixSize + i];\n                float b = dataIn_d[outputColumn + i * matrixSize];\n                c = fmaf(a, b, c);\n            }\n            dataOut_d[outputColumn + outputRow * matrixSize] = c;\n        }\n    }\n}\n\nvoid launch() {\n\n    int deviceId = 0;\n    cudaDeviceProp deviceProperties;\n    int maxActiveCooperativeBlocksPerSM = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(   &maxActiveCooperativeBlocksPerSM, \n                                                                k_calculate, \n                                                                BLOCK_SIZE, \n                                                                0));\n    int maxActiveBlocks = maxActiveCooperativeBlocksPerSM * deviceProperties.multiProcessorCount;\n    \n    // The stream to be utilized by CUDA-graph.\n    cudaStream_t stream;\n    // Allocating the stream.\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    // Allocating the device memory.\n    float * dataIn_d;\n    float * dataOut_d;\n    int * constantParams_d;\n    CUDA_CHECK(cudaMallocAsync(&dataIn_d, sizeof(float) * MAX_MATRIX_ELEMENTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&dataOut_d, sizeof(float) * MAX_MATRIX_ELEMENTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&constantParams_d, sizeof(int), stream));\n    // Initializing the buffers to zero.\n    CUDA_CHECK(cudaMemsetAsync(dataIn_d, 0, sizeof(float) * MAX_MATRIX_ELEMENTS, stream));\n    CUDA_CHECK(cudaMemsetAsync(dataOut_d, 0, sizeof(float) * MAX_MATRIX_ELEMENTS, stream));\n    CUDA_CHECK(cudaMemsetAsync(constantParams_d, 0, sizeof(int), stream));\n\n    // Allocating the host memory.\n    float * data_h = new float[MAX_MATRIX_ELEMENTS];\n    int * constantParams_h = new int[1];\n    float * expectedData_h = new float[MAX_MATRIX_ELEMENTS * 2];\n\n    //  Test 1: [ 1 ] matrix to the power of 8 (2 ^ 3) -> [ 2187 ] matrix\n    {\n        // Initializing the host buffer for input.\n        std::initializer_list<float> input = { \n            1.00f, 1.00f, 1.00f, \n            1.00f, 1.00f, 1.00f, \n            1.00f, 1.00f, 1.00f\n        };\n        std::copy(input.begin(), input.end(), data_h);\n        // Setting matrix size to 3x3.\n        constantParams_h[SELECT_PARAM_MATRIX_SIZE] = 3;\n        int elementsUsed = constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n        // Copying inputs from host.\n        CUDA_CHECK(cudaMemcpyAsync(dataIn_d, data_h, sizeof(float) * elementsUsed, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(constantParams_d, constantParams_h, sizeof(int), cudaMemcpyHostToDevice, stream));\n        // Executing graph to calculate matrix ^ (2 ^ 3).\n        runGraph(3, dataIn_d, dataOut_d, constantParams_d, maxActiveBlocks, stream);\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(data_h, dataOut_d, sizeof(float) * elementsUsed, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        float expectedResult = 2187.0f;\n        for(int i = 0; i < elementsUsed; i++) {\n            assert(fabsf(expectedResult - data_h[i]) < ERROR_TOLERANCE);\n        }\n    }\n    //  Test 2: Identity matrix to the power of 32 (2 ^ 5) -> Identity matrix.\n    {\n        // Initializing the host buffer for input.\n        std::initializer_list<float> input = { \n            1.00f, 0.00f, 0.00f, 0.00f, \n            0.00f, 1.00f, 0.00f, 0.00f, \n            0.00f, 0.00f, 1.00f, 0.00f, \n            0.00f, 0.00f, 0.00f, 1.00f\n        };\n        std::copy(input.begin(), input.end(), data_h);\n        // Setting matrix size to 4x4.\n        constantParams_h[SELECT_PARAM_MATRIX_SIZE] = 4;\n        int elementsUsed = constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n        // Copying inputs from host.\n        CUDA_CHECK(cudaMemcpyAsync(dataIn_d, data_h, sizeof(float) * elementsUsed, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(constantParams_d, constantParams_h, sizeof(int), cudaMemcpyHostToDevice, stream));\n        // Executing graph to calculate matrix ^ (2 ^ 5).\n        runGraph(5, dataIn_d, dataOut_d, constantParams_d, maxActiveBlocks, stream);\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(data_h, dataOut_d, sizeof(float) * elementsUsed, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        std::initializer_list<float> expected = { \n            1.00f, 0.00f, 0.00f, 0.00f, \n            0.00f, 1.00f, 0.00f, 0.00f, \n            0.00f, 0.00f, 1.00f, 0.00f, \n            0.00f, 0.00f, 0.00f, 1.00f\n        };\n        for(int i = 0; i < elementsUsed; i++) {\n            assert(fabsf(expected.begin()[i] - data_h[i]) < ERROR_TOLERANCE);\n        }\n    }\n    //  Test 3: Randomized matrix elements, in range of (0.0f, 1.0f).\n    {\n        // Initializing the host buffer for input.\n        std::mt19937 generator(DETERMINISTIC_INITIAL_CONDITION);\n        std::uniform_real_distribution<float> distribution(0.0f, 1.0f);\n        // Setting matrix size to the maximum allowed.\n        constantParams_h[SELECT_PARAM_MATRIX_SIZE] = MAX_MATRIX_SIZE;\n        int elementsUsed = constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n        for(int i = 0; i < elementsUsed; i++) {\n            data_h[i] = distribution(generator);\n            expectedData_h[i] = data_h[i];\n        }\n        // Copying inputs from host.\n        CUDA_CHECK(cudaMemcpyAsync(dataIn_d, data_h, sizeof(float) * elementsUsed, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(constantParams_d, constantParams_h, sizeof(int), cudaMemcpyHostToDevice, stream));\n        // Executing graph to calculate matrix ^ (2 ^ 2).\n        runGraph(2, dataIn_d, dataOut_d, constantParams_d, maxActiveBlocks, stream);\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(data_h, dataOut_d, sizeof(float) * elementsUsed, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        int pingPongBuffering = 0;\n        for(int i = 0; i < 2; i++) {\n            int readOffset = (pingPongBuffering % 2) ? MAX_MATRIX_ELEMENTS : 0;\n            int writeOffset = (pingPongBuffering % 2) ? 0 : MAX_MATRIX_ELEMENTS;\n            pingPongBuffering++;\n            for(int e = 0; e < elementsUsed; e++) {\n                int outputColumn = e % constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n                int outputRow = e / constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n                float c = 0.0f;\n                for(int i = 0; i < constantParams_h[SELECT_PARAM_MATRIX_SIZE]; i++) {\n                    float a = expectedData_h[readOffset + outputRow * constantParams_h[SELECT_PARAM_MATRIX_SIZE] + i];\n                    float b = expectedData_h[readOffset + outputColumn + i * constantParams_h[SELECT_PARAM_MATRIX_SIZE]];\n                    c = fmaf(a, b, c);\n                }\n                expectedData_h[writeOffset + outputColumn + outputRow * constantParams_h[SELECT_PARAM_MATRIX_SIZE]] = c;\n            }\n        }\n        for (int i = 0; i < constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE]; i++) {\n            float expected = expectedData_h[i + ((pingPongBuffering % 2) ? MAX_MATRIX_ELEMENTS : 0)];\n            if(fabsf(expected) > ERROR_TOLERANCE) {\n                assert(fabsf(expected - data_h[i]) / fabsf(expected) < ERROR_TOLERANCE);\n            } else {\n                assert(fabsf(expected - data_h[i]) < ERROR_TOLERANCE);\n            }\n        }\n    }\n    //  Test 4: Randomized matrix elements, in range of (-1.0f, 1.0f).\n    {\n        // Initializing the host buffer for input.\n        std::mt19937 generator(DETERMINISTIC_INITIAL_CONDITION);\n        std::uniform_real_distribution<float> distribution(-1.0f, 1.0f);\n        constantParams_h[SELECT_PARAM_MATRIX_SIZE] = MAX_MATRIX_SIZE;\n        int elementsUsed = constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n        for(int i = 0; i < elementsUsed; i++) {\n            data_h[i] = distribution(generator);\n            expectedData_h[i] = data_h[i];\n        }\n        // Copying inputs from host.\n        CUDA_CHECK(cudaMemcpyAsync(dataIn_d, data_h, sizeof(float) * elementsUsed, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(constantParams_d, constantParams_h, sizeof(int), cudaMemcpyHostToDevice, stream));\n        // Executing graph to calculate matrix ^ (2 ^ 2).\n        runGraph(2, dataIn_d, dataOut_d, constantParams_d, maxActiveBlocks, stream);\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(data_h, dataOut_d, sizeof(float) * elementsUsed, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        int pingPongBuffering = 0;\n        for(int i = 0; i < 2; i++) {\n            int readOffset = (pingPongBuffering % 2) ? MAX_MATRIX_ELEMENTS : 0;\n            int writeOffset = (pingPongBuffering % 2) ? 0 : MAX_MATRIX_ELEMENTS;\n            pingPongBuffering++;\n            for(int e = 0; e < elementsUsed; e++) {\n                int outputColumn = e % constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n                int outputRow = e / constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n                float c = 0.0f;\n                for(int i = 0; i < constantParams_h[SELECT_PARAM_MATRIX_SIZE]; i++) {\n                    float a = expectedData_h[readOffset + outputRow * constantParams_h[SELECT_PARAM_MATRIX_SIZE] + i];\n                    float b = expectedData_h[readOffset + outputColumn + i * constantParams_h[SELECT_PARAM_MATRIX_SIZE]];\n                    c = fmaf(a, b, c);\n                }\n                expectedData_h[writeOffset + outputColumn + outputRow * constantParams_h[SELECT_PARAM_MATRIX_SIZE]] = c;\n            }\n        }\n        for (int i = 0; i < constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE]; i++) {\n            float expected = expectedData_h[i + ((pingPongBuffering % 2) ? MAX_MATRIX_ELEMENTS : 0)];\n            if(fabsf(expected) > ERROR_TOLERANCE) {\n                assert(fabsf(expected - data_h[i]) / fabsf(expected) < ERROR_TOLERANCE);\n            } else {\n                assert(fabsf(expected - data_h[i]) < ERROR_TOLERANCE);\n            }\n        }\n    }\n    //  Test 5: Randomized matrix elements, in range of (-1000.0f, 1000.0f).\n    {\n        // Initializing the host buffer for input.\n        std::mt19937 generator(DETERMINISTIC_INITIAL_CONDITION);\n        std::uniform_real_distribution<float> distribution(-1000.0f, 1000.0f);\n        constantParams_h[SELECT_PARAM_MATRIX_SIZE] = MAX_MATRIX_SIZE;\n        int elementsUsed = constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n        for(int i = 0; i < elementsUsed; i++) {\n            data_h[i] = distribution(generator);\n            expectedData_h[i] = data_h[i];\n        }\n        // Copying inputs from host.\n        CUDA_CHECK(cudaMemcpyAsync(dataIn_d, data_h, sizeof(float) * elementsUsed, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(constantParams_d, constantParams_h, sizeof(int), cudaMemcpyHostToDevice, stream));\n        // Executing graph to calculate matrix ^ (2 ^ 2).\n        runGraph(2, dataIn_d, dataOut_d, constantParams_d, maxActiveBlocks, stream);\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(data_h, dataOut_d, sizeof(float) * elementsUsed, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        int pingPongBuffering = 0;\n        for(int i = 0; i < 2; i++) {\n            int readOffset = (pingPongBuffering % 2) ? MAX_MATRIX_ELEMENTS : 0;\n            int writeOffset = (pingPongBuffering % 2) ? 0 : MAX_MATRIX_ELEMENTS;\n            pingPongBuffering++;\n            for(int e = 0; e < elementsUsed; e++) {\n                int outputColumn = e % constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n                int outputRow = e / constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n                float c = 0.0f;\n                for(int i = 0; i < constantParams_h[SELECT_PARAM_MATRIX_SIZE]; i++) {\n                    float a = expectedData_h[readOffset + outputRow * constantParams_h[SELECT_PARAM_MATRIX_SIZE] + i];\n                    float b = expectedData_h[readOffset + outputColumn + i * constantParams_h[SELECT_PARAM_MATRIX_SIZE]];\n                    c = fmaf(a, b, c);\n                }\n                expectedData_h[writeOffset + outputColumn + outputRow * constantParams_h[SELECT_PARAM_MATRIX_SIZE]] = c;\n            }\n        }\n        for (int i = 0; i < constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE]; i++) {\n            float expected = expectedData_h[i + ((pingPongBuffering % 2) ? MAX_MATRIX_ELEMENTS : 0)];\n            if(fabsf(expected) > ERROR_TOLERANCE) {\n                assert(fabsf(expected - data_h[i]) / fabsf(expected) < ERROR_TOLERANCE);\n            } else {\n                assert(fabsf(expected - data_h[i]) < ERROR_TOLERANCE);\n            }\n        }\n    }\n    //  Test 6: Randomized matrix elements, in range of (-1000000.0f, 1000000.0f).\n    {\n        // Initializing the host buffer for input.\n        std::mt19937 generator(DETERMINISTIC_INITIAL_CONDITION);\n        std::uniform_real_distribution<float> distribution(-1000000.0f, 1000000.0f);\n        constantParams_h[SELECT_PARAM_MATRIX_SIZE] = MAX_MATRIX_SIZE;\n        int elementsUsed = constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n        for(int i = 0; i < elementsUsed; i++) {\n            data_h[i] = distribution(generator);\n            expectedData_h[i] = data_h[i];\n        }\n        // Copying inputs from host.\n        CUDA_CHECK(cudaMemcpyAsync(dataIn_d, data_h, sizeof(float) * elementsUsed, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(constantParams_d, constantParams_h, sizeof(int), cudaMemcpyHostToDevice, stream));\n        // Executing graph to calculate matrix ^ (2 ^ 2).\n        runGraph(2, dataIn_d, dataOut_d, constantParams_d, maxActiveBlocks, stream);\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(data_h, dataOut_d, sizeof(float) * elementsUsed, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing results.\n        int pingPongBuffering = 0;\n        for(int i = 0; i < 2; i++) {\n            int readOffset = (pingPongBuffering % 2) ? MAX_MATRIX_ELEMENTS : 0;\n            int writeOffset = (pingPongBuffering % 2) ? 0 : MAX_MATRIX_ELEMENTS;\n            pingPongBuffering++;\n            for(int e = 0; e < elementsUsed; e++) {\n                int outputColumn = e % constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n                int outputRow = e / constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n                float c = 0.0f;\n                for(int i = 0; i < constantParams_h[SELECT_PARAM_MATRIX_SIZE]; i++) {\n                    float a = expectedData_h[readOffset + outputRow * constantParams_h[SELECT_PARAM_MATRIX_SIZE] + i];\n                    float b = expectedData_h[readOffset + outputColumn + i * constantParams_h[SELECT_PARAM_MATRIX_SIZE]];\n                    c = fmaf(a, b, c);\n                }\n                expectedData_h[writeOffset + outputColumn + outputRow * constantParams_h[SELECT_PARAM_MATRIX_SIZE]] = c;\n            }\n        }\n        for (int i = 0; i < constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE]; i++) {\n            float expected = expectedData_h[i + ((pingPongBuffering % 2) ? MAX_MATRIX_ELEMENTS : 0)];\n            if(fabsf(expected) > ERROR_TOLERANCE) {\n                assert(fabsf(expected - data_h[i]) / fabsf(expected) < ERROR_TOLERANCE);\n            } else {\n                assert(fabsf(expected - data_h[i]) < ERROR_TOLERANCE);\n            }\n        }\n    }\n    //  Test 7: Elements with increasing values.\n    {\n        // Initializing the host buffer for input.\n        constantParams_h[SELECT_PARAM_MATRIX_SIZE] = MAX_MATRIX_SIZE;\n        int elementsUsed = constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n        for(int i = 0; i < elementsUsed; i++) {\n            data_h[i] = i;\n            expectedData_h[i] = data_h[i];\n        }\n        // Copying inputs from host.\n        CUDA_CHECK(cudaMemcpyAsync(dataIn_d, data_h, sizeof(float) * elementsUsed, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(constantParams_d, constantParams_h, sizeof(int), cudaMemcpyHostToDevice, stream));\n        // Executing graph to calculate matrix ^ (2 ^ 2).\n        runGraph(2, dataIn_d, dataOut_d, constantParams_d, maxActiveBlocks, stream);\n        // Copying outputs to host.\n        CUDA_CHECK(cudaMemcpyAsync(data_h, dataOut_d, sizeof(float) * elementsUsed, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Comparing results.\n        int pingPongBuffering = 0;\n        for(int i = 0; i < 2; i++) {\n            int readOffset = (pingPongBuffering % 2) ? MAX_MATRIX_ELEMENTS : 0;\n            int writeOffset = (pingPongBuffering % 2) ? 0 : MAX_MATRIX_ELEMENTS;\n            pingPongBuffering++;\n            for(int e = 0; e < elementsUsed; e++) {\n                int outputColumn = e % constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n                int outputRow = e / constantParams_h[SELECT_PARAM_MATRIX_SIZE];\n                float c = 0.0f;\n                for(int i = 0; i < constantParams_h[SELECT_PARAM_MATRIX_SIZE]; i++) {\n                    float a = expectedData_h[readOffset + outputRow * constantParams_h[SELECT_PARAM_MATRIX_SIZE] + i];\n                    float b = expectedData_h[readOffset + outputColumn + i * constantParams_h[SELECT_PARAM_MATRIX_SIZE]];\n                    c = fmaf(a, b, c);\n                }\n                expectedData_h[writeOffset + outputColumn + outputRow * constantParams_h[SELECT_PARAM_MATRIX_SIZE]] = c;\n            }\n        }\n        for (int i = 0; i < constantParams_h[SELECT_PARAM_MATRIX_SIZE] * constantParams_h[SELECT_PARAM_MATRIX_SIZE]; i++) {\n            float expected = expectedData_h[i + ((pingPongBuffering % 2) ? MAX_MATRIX_ELEMENTS : 0)];\n            if(fabsf(expected) > ERROR_TOLERANCE) {\n                assert(fabsf(expected - data_h[i]) / fabsf(expected) < ERROR_TOLERANCE);\n            } else {\n                assert(fabsf(expected - data_h[i]) < ERROR_TOLERANCE);\n            }\n        }\n    }\n\n\n    // Deallocating the resources.\n    CUDA_CHECK(cudaFreeAsync(dataIn_d, stream));\n    CUDA_CHECK(cudaFreeAsync(dataOut_d, stream));\n    CUDA_CHECK(cudaFreeAsync(constantParams_d, stream));\n    // Deallocating the host memory.\n    delete [] data_h;\n    delete [] constantParams_h;\n    delete [] expectedData_h;\n    // Deallocating the stream.\n    CUDA_CHECK(cudaStreamDestroy(stream));\n\n}\nvoid runGraph(int k, float * dataIn_d, float * dataOut_d, int * constantParams_d, int maxActiveBlocks, cudaStream_t stream) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/171", "date": "2025-07-30", "prompt": "Write a CUDA kernel to find the mode (most frequent element) and its frequency (no. of occurrences) in an array using libcu++ synchronization primitives.\n\nThe signature of the function is __global__ void k_parallelMode(cuda::std::span<const int> input, cuda::std::span<cuda::atomic<int, cuda::thread_scope_device>> histogram, cuda::atomic<int, cuda::thread_scope_device>* maxCount_d, cuda::atomic<int, cuda::thread_scope_device>* modeIndex_d), where input is a device span representing the input array, histogram is a device span for storing per-value frequency counts, maxCount_d is a device pointer (atomic) where the maximum frequency will be stored, and modeIndex_d is a device pointer (atomic) where the mode will be stored, both maxCount_d and modeIndex_d are treated as output device pointers, and the histogram is an intermediate array used within the kernel.\n\n>>> k_parallelMode({2, 5, 3, 5, 7, 8, 5, 10, 2, 5}, histogram_d, maxCount_d, modeIndex_d) -> modeIndex_d = 5, maxCount_d = 4\n>>> k_parallelMode({1, 2, 3, 4, 5}, histogram_d, maxCount_d, modeIndex_d) -> modeIndex_d = 1, maxCount_d = 1\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda/atomic>\n#include <cuda/std/cstdint>\n#include <cuda/std/span>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <vector>\n#include <algorithm>\n\n#define BLOCK_SIZE 256\n\n// Error-checking macro\n#define CUDA_CHECK(call)                                                   \\\n{                                                                          \\\n    cudaError_t error = call;                                              \\\n    if(error != cudaSuccess) {                                             \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                      \\\n                __FILE__, __LINE__, cudaGetErrorString(error));            \\\n        exit(EXIT_FAILURE);                                                \\\n    }                                                                      \\\n}\n\n__global__ void k_parallelMode(\n    cuda::std::span<const int>     input,\n    cuda::std::span<cuda::atomic<int, cuda::thread_scope_device>> histogram,\n    cuda::atomic<int, cuda::thread_scope_device>* maxCount_d,\n    cuda::atomic<int, cuda::thread_scope_device>* modeIndex_d);\n\nvoid launch() {\n    // Testcase struct for mode detection\n    struct TestCase {\n        std::vector<int> array;      // input array\n        int expected_mode;           // expected mode (most frequent element)\n        int expected_freq;           // expected frequency of the mode\n    };\n\n    std::vector<TestCase> testCases = {\n        // Basic test case with clear mode\n        {{2, 5, 3, 5, 7, 8, 5, 10, 2, 5}, 5, 4},\n        // All elements appear once - should return first element\n        {{1, 2, 3, 4, 5}, 1, 1},\n        // Multiple elements with same frequency - should return smallest\n        {{3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9}, 5, 3},\n        // Single element array\n        {{42}, 42, 1},\n        // Array with all same elements\n        {{7, 7, 7, 7, 7}, 7, 5},\n        // Larger array with various frequencies\n        {{1, 1, 2, 2, 2, 3, 4, 5, 5, 5, 5, 6, 7, 8, 9, 9}, 5, 4}\n    };\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Compute maximum allocation sizes\n    size_t maxArraySize = 0;\n    int maxValue = 0;\n    for (const auto& tc : testCases) {\n        maxArraySize = std::max(maxArraySize, tc.array.size());\n        if (!tc.array.empty()) {\n            int currentMax = *std::max_element(tc.array.begin(), tc.array.end());\n            maxValue = std::max(maxValue, currentMax);\n        }\n    }\n    size_t maxHistSize = maxValue + 1;\n    size_t maxArrayBytes = maxArraySize * sizeof(int);\n    size_t maxHistBytes = maxHistSize * sizeof(cuda::atomic<int, cuda::thread_scope_device>);\n\n    // Allocate device memory once\n    int* input_d = nullptr;\n    cuda::atomic<int, cuda::thread_scope_device>* histogram_d = nullptr;\n    cuda::atomic<int, cuda::thread_scope_device>* maxCount_d = nullptr;\n    cuda::atomic<int, cuda::thread_scope_device>* modeIndex_d = nullptr;\n    \n    CUDA_CHECK(cudaMallocAsync(&input_d, maxArrayBytes, stream));\n    CUDA_CHECK(cudaMallocAsync(&histogram_d, maxHistBytes, stream));\n    CUDA_CHECK(cudaMallocAsync(&maxCount_d, sizeof(cuda::atomic<int, cuda::thread_scope_device>), stream));\n    CUDA_CHECK(cudaMallocAsync(&modeIndex_d, sizeof(cuda::atomic<int, cuda::thread_scope_device>), stream));\n\n    for (const auto& testCase : testCases) {\n        int inputSize = static_cast<int>(testCase.array.size());\n        if (inputSize == 0) continue;  // Skip empty array test case\n        \n        int maxVal = *std::max_element(testCase.array.begin(), testCase.array.end());\n        int histSize = maxVal + 1;\n        \n        // Copy input array to device\n        CUDA_CHECK(cudaMemcpyAsync(input_d,\n                                  testCase.array.data(),\n                                  inputSize * sizeof(int),\n                                  cudaMemcpyHostToDevice,\n                                  stream));\n        \n        // Initialize histogram, maxCount, and modeIndex to 0\n        CUDA_CHECK(cudaMemsetAsync(histogram_d, 0, histSize * sizeof(cuda::atomic<int, cuda::thread_scope_device>), stream));\n        CUDA_CHECK(cudaMemsetAsync(maxCount_d, 0, sizeof(cuda::atomic<int, cuda::thread_scope_device>), stream));\n        CUDA_CHECK(cudaMemsetAsync(modeIndex_d, 0, sizeof(cuda::atomic<int, cuda::thread_scope_device>), stream));\n        \n        auto inputSpan = cuda::std::span<const int>(input_d, inputSize);\n        auto histSpan = cuda::std::span<cuda::atomic<int, cuda::thread_scope_device>>(histogram_d, histSize);\n           \n        // Determine launch configuration\n        cudaDeviceProp deviceProperties;\n        CUDA_CHECK(cudaGetDeviceProperties(&deviceProperties, 0));\n\n        // Get the optimal block size for maximum occupancy\n        int minGridSize;     // Minimum grid size needed for maximum occupancy\n        int blockSize;       // Block size that achieves maximum occupancy\n        CUDA_CHECK(cudaOccupancyMaxPotentialBlockSize(\n            &minGridSize,\n            &blockSize,\n            k_parallelMode,\n            0,                  // Dynamic shared memory size\n            0                   // No hard block size limit\n        ));\n\n        // Calculate grid size based on input size and optimal block size\n        int gridSize = (inputSize + blockSize - 1) / blockSize;\n\n        // Ensure we don't exceed device limits\n        gridSize = std::min(gridSize, deviceProperties.maxGridSize[0]);\n\n        // Set dimensions\n        dim3 blockDimensions(blockSize, 1, 1);\n        dim3 gridDimensions(gridSize, 1, 1);\n        \n        void* args[] = {\n            &inputSpan,\n            &histSpan,\n            &maxCount_d,\n            &modeIndex_d\n        };\n        \n        // Use cooperative kernel launch for grid-wide synchronization\n        CUDA_CHECK(cudaLaunchCooperativeKernel(\n            (void*)k_parallelMode,\n            gridDimensions,\n            blockDimensions,\n            args,\n            0,  // No shared memory\n            stream\n        ));\n        \n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Copy back and verify\n        int resultMode, resultFreq;\n        CUDA_CHECK(cudaMemcpyAsync(&resultMode, \n                                  modeIndex_d, \n                                  sizeof(int), \n                                  cudaMemcpyDeviceToHost, \n                                  stream));\n        CUDA_CHECK(cudaMemcpyAsync(&resultFreq, \n                                  maxCount_d, \n                                  sizeof(int), \n                                  cudaMemcpyDeviceToHost, \n                                  stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        assert(resultMode == testCase.expected_mode);\n        assert(resultFreq == testCase.expected_freq);\n    }\n\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(histogram_d, stream));\n    CUDA_CHECK(cudaFreeAsync(maxCount_d, stream));\n    CUDA_CHECK(cudaFreeAsync(modeIndex_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n// Kernel: gridstride over array, updating histogram and tracking mode\n__global__ void k_parallelMode(\n    cuda::std::span<const int>     input,\n    cuda::std::span<cuda::atomic<int, cuda::thread_scope_device>> histogram,\n    cuda::atomic<int, cuda::thread_scope_device>* maxCount_d,\n    cuda::atomic<int, cuda::thread_scope_device>* modeIndex_d)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/172", "date": "2025-07-30", "prompt": "Write a CUDA kernel that calculates Minkowski distance between 3D floating points and the fixed reference point (0,0,0) using libcu++ library. The kernel should leverage libcu++ containers to store the multi-dimensional spatial data points.\n\nThe signature of the function is __global__ void k_calculateMinkowskiDistance(cuda::std::mdspan<DataPoint, cuda::std::extents<size_t, cuda::std::dynamic_extent>> dataSpan_d, float pValue), DataPoint is a multidimensional span that provides a view into an array of DataPoint objects, cuda::std::extents<size_t, cuda::std::dynamic_extent>:  Defines a dynamically-sized 1D array, pVlaue: is the p-value parameter for the Minkowski distance calculation.\n\n>>> k_calculateMinkowskiDistance( {1.0f, 0.0f, 0.0f, 0.0f, 1.000000f}, 3.0f)->distance:1.000000f\n>>> k_calculateMinkowskiDistance({1.0f, 1.0f, 1.0f, 0.0f, 1.442250f}, 3.0f)->distance:1.442250f\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <vector>\n#include <cmath>\n#include <string>\n#include <cassert>\n#include <random>\n#include <chrono>\n#include <cstdio>\n\n#include <cuda/std/mdspan>\n#include <cuda/std/complex>\n\n// Reference point to calculate distances from\nconstexpr float REF_X = 0.0f;\nconstexpr float REF_Y = 0.0f;\nconstexpr float REF_Z = 0.0f;\n#define REF_DIM 3\n\n// P-value for Minkowski distance\nconstexpr float P_VALUE = 3.0f;\n\n// Tolerance for floating point comparisons\nconstexpr float EPSILON = 1e-1f;\n\n// Define a macro for CUDA error checking\n// Error checking macro\n#define CUDA_CHECK(call) {                                                 \\\n    cudaError_t error = call;                                              \\\n    if (error != cudaSuccess) {                                            \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                      \\\n                __FILE__, __LINE__, cudaGetErrorString(error));            \\\n        exit(EXIT_FAILURE);                                                \\\n    }                                                                      \\\n}\n\nstruct DataPoint {\n    float x, y, z;\n    float distance;\n    float expectedDistance;\n};\n\n__global__ void k_calculateMinkowskiDistance(\n    cuda::std::mdspan<DataPoint, cuda::std::extents<size_t, cuda::std::dynamic_extent>> dataSpan_d,\n    float pValue);\n\n// Function containing all test cases and launching them\nvoid launch() {\n\n    auto calculateExpectedMinkowskiDistance = [](float x, float y, float z, float p) -> float \n    {\n      float sum = std::pow(std::abs(x - REF_X), p) +\n                std::pow(std::abs(y - REF_Y), p) +\n                std::pow(std::abs(z - REF_Z), p);\n      return std::pow(sum, 1.0f/p);\n    };\n\n    // Use random number generator with different seeds for test cases\n    unsigned seed = std::chrono::system_clock::now().time_since_epoch().count();\n    std::mt19937 generator(seed);\n\n    // Define a single uniform distribution with a wide range\n    std::uniform_real_distribution<float> dist(-1000.0f, 1000.0f);\n\n    // Define test case sizes (increasing for each test)\n    constexpr int NUM_TEST_CASES = 7;\n    constexpr int MAX_SIZE = 100000;\n    constexpr int testSizes[NUM_TEST_CASES] = {100, 500, 1000, 5000, 10000, 50000, 100000};\n\n    float pValue = P_VALUE;\n\n    // Define scale factors for each test case to create variety\n    constexpr float scaleFactors[NUM_TEST_CASES] = {0.0001f, 0.001f, 0.01f, 0.1f, 1.0f, 10.0f, 100.0f};\n\n    // Get device properties to calculate optimal block and grid sizes\n    cudaDeviceProp deviceProp;\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, 0));\n\n    // Create CUDA stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocate device memory\n    DataPoint* deviceData_d = nullptr;\n    CUDA_CHECK(cudaMallocAsync(&deviceData_d, MAX_SIZE * sizeof(DataPoint), stream));\n\n    // Run each test case with different sizes\n    for (int testCaseNum = 0; testCaseNum < NUM_TEST_CASES; testCaseNum++) {\n        int numPoints = testSizes[testCaseNum];\n        float scaleFactor = scaleFactors[testCaseNum];\n\n        // Allocate host data\n        DataPoint* hostData = new DataPoint[numPoints];\n\n        // Generate random data points with scaling to create variety\n        for (int i = 0; i < numPoints; i++) {\n            // Generate random coordinates and scale them\n            float x = dist(generator) * scaleFactor;\n            float y = dist(generator) * scaleFactor;\n            float z = dist(generator) * scaleFactor;\n\n            hostData[i].x = x;\n            hostData[i].y = y;\n            hostData[i].z = z;\n\n            // Calculate expected distance on CPU\n            hostData[i].expectedDistance = calculateExpectedMinkowskiDistance(x, y, z, P_VALUE);\n        }\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(deviceData_d, hostData, numPoints * sizeof(DataPoint),\n                                  cudaMemcpyHostToDevice, stream));\n\n        // Create mdspan for the device data\n        using mdspanType = cuda::std::mdspan<DataPoint, cuda::std::extents<size_t, cuda::std::dynamic_extent>>;\n        mdspanType dataSpan_d(deviceData_d, numPoints);\n\n        // Calculate optimal block size and grid size\n        int maxThreadsPerBlock = deviceProp.maxThreadsPerBlock;\n        int multiProcessorCount = deviceProp.multiProcessorCount;\n\n        int blockSize = std::min(numPoints, maxThreadsPerBlock);\n        int gridSize = (numPoints + blockSize - 1) / blockSize;\n\n        gridSize = std::min(std::max(gridSize, multiProcessorCount),\n                        (numPoints + blockSize - 1) / blockSize);\n\n        // Set up kernel launch parameters\n        void* args[] = {&dataSpan_d, &pValue};\n        dim3 gridDim(gridSize);\n        dim3 blockDim(blockSize);\n\n        // Launch kernel\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateMinkowskiDistance,\n                                   gridDim, blockDim,\n                                   args, 0, stream));\n\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(hostData, deviceData_d, numPoints * sizeof(DataPoint),\n                                  cudaMemcpyDeviceToHost, stream));\n\n        // Synchronize to ensure operations are complete\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validate results\n        for (int i = 0; i < numPoints; i++) {\n            float diff = std::abs(hostData[i].distance - hostData[i].expectedDistance);\n            assert (diff < EPSILON); \n        }\n\n        // Free host memory\n        delete[] hostData;\n    }\n    // Free device memory\n    CUDA_CHECK(cudaFreeAsync(deviceData_d, stream));\n    // Clean up\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_calculateMinkowskiDistance(\n    cuda::std::mdspan<DataPoint, cuda::std::extents<size_t, cuda::std::dynamic_extent>> dataSpan_d,\n    float pValue) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/173", "date": "2025-07-30", "prompt": "Implement a CUDA C++ program performing triangular banded matrix-vector multiplication on single precision floating point N x N matrix and N dimensional vector using cuBLAS, where input matrices are stored in packed banded format with only diagonals. The function should configure for upper/Lower traingular and transposed/non-transposed matrices in cuBLAS API.\n\nThe signature of the function is void triangularBandedMatrixVectorMultiplication(float *packedBandedMatrix_d, float *inputVector_d, float *outputVector_d, int matrixSize, int bandWidth, cublasHandle_t cublasHandle, bool isUpperTriangular, bool transposeMatrix), where packedBandedMatrix_d is device pointer to the triangular banded matrix, inputVector_d is a device pointer to the input vector of size matrixSize, outputVector_d is a device pointer to store the result vector of size matrixSize, matrixSize is dimension of the square matrix (nn), bandWidth: Number of super-diagonals (upper triangular) or sub-diagonals (lower triangular) stored, cublasHandle is cuBLAS library context handle, isUpperTriangular is true for upper triangular matrix, false for lower triangular, transposeMatrix is true to apply transpose operation before multiplication.\n\n>>> triangularBandedMatrixVectorMultiplication({1.0, 2.0, 4.0, 3.0, 5.0, 0.0, 6.0, 0.0, 0.0}, {1.0, 2.0, 3.0}, outputVector_d, 3, 2, handle, false, false) -> outputVector_d:({1.0, 8.0, 32.0})\n>>> triangularBandedMatrixVectorMultiplication({1.0, 2.0, 4.0, 3.0, 5.0, 0.0, 6.0, 0.0, 0.0}, {1.0, 2.0, 3.0}, outputVector_d, 3, 2, handle, false, true) -> outputVector_d:({17.0, 21.0, 18.0})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "-lcublas", "declaration": "#include <cstdio>\n#include <limits.h>\n#include <assert.h>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n#define CUBLAS_CHECK(call)                                        \\\ndo {                                                              \\\n        cublasStatus_t error = call;                              \\\n        if (error != CUBLAS_STATUS_SUCCESS) {                     \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",         \\\n                    __FILE__, __LINE__,                           \\\n                    \"Cublas Error\");                              \\\n            exit(EXIT_FAILURE);                                   \\\n        }                                                         \\\n} while(0)\n\nvoid triangularBandedMatrixVectorMultiplication(\n    float *packedBandedMatrix_d,\n    float *inputVector_d,\n    float *outputVector_d, \n    int matrixSize,\n    int bandWidth,\n    cublasHandle_t cublasHandle,\n    bool isUpperTriangular,\n    bool transposeMatrix\n);\n\nvoid launch() {\n\n    //Initialize Constants\n    const int TEST_CASE_COUNT = 15;\n   \n    //Initialise Test Data\n    int maxMatrixSize = 0;\n    int maxVectorSize = 0;\n\n    int matrixRowsColumns[TEST_CASE_COUNT] = {3, 3, 3, 3, 4, 5, 6, 7, 8, 9, 4, 5, 6, 3, 4};\n    int inputVectorSize[TEST_CASE_COUNT] = {3, 3, 3, 3, 4, 5, 6, 7, 8, 9, 4, 5, 6, 3, 4};\n\n    for(int index = 0; index < TEST_CASE_COUNT; index++) {\n        maxMatrixSize = max(matrixRowsColumns[index], maxMatrixSize);\n        maxVectorSize = max(inputVectorSize[index], maxVectorSize);\n    }\n\n    maxMatrixSize = maxMatrixSize * maxMatrixSize;\n\n    int matrixNumDiagonals[TEST_CASE_COUNT] = {2, 2, 2, 2, 3, 4, 5, 6, 7, 8, 2, 3, 2, 1, 2};\n\n    bool isUpperTriangular[TEST_CASE_COUNT] = {false, false, true, true, false, false, false, false, false, false, false, false, true, false, true};\n    bool transposeMatrix[TEST_CASE_COUNT] = {false, true, false, false, false, false, false, false, false, false, false, false, false, true, false};\n\n    float inputMatrix_h[TEST_CASE_COUNT][maxMatrixSize] = {\n        //TEST - 1 - 3x3\n        //[1, 0, 0]\n        //[2, 3, 0]\n        //[4, 5, 6]\n        {1.0, 2.0, 4.0, 3.0, 5.0, 0.0, 6.0, 0.0, 0.0},\n        //TEST - 2 - 3x3\n        //[1, 0, 0]\n        //[2, 3, 0]\n        //[4, 5, 6]\n        {1.0, 2.0, 4.0, 3.0, 5.0, 0.0, 6.0, 0.0, 0.0},\n        //TEST - 3 - 3x3\n        //[1, 2, 4]\n        //[0, 3, 5]\n        //[0, 0, 6]\n        {0.0, 0.0, 1.0, 0.0, 2.0, 3.0, 4.0, 5.0, 6.0},\n        //TEST - 4 - 3x3\n        //[1, 1, 1]\n        //[0, 1, 1]\n        //[0, 0, 1]\n        {0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0},\n        //TEST - 5 - 4x4\n        //[1, 0, 0, 0]\n        //[2, 3, 0, 0]\n        //[4, 5, 6, 0]\n        //[7, 8, 9, 10]\n        {1.0, 2.0, 4.0, 7.0, 3.0, 5.0, 8.0, 0.0, 6.0, 9.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0},\n        //TEST - 6 - 5x5\n        //[1,   0,  0,  0,  0]\n        //[2,   3,  0,  0,  0]\n        //[4,   5,  6,  0,  0]\n        //[7,   8,  9, 10,  0]\n        //[11, 12, 13, 14, 15] \n        {1.0, 2.0, 4.0, 7.0, 11.0, 3.0, 5.0, 8.0, 12.0, 0.0, 6.0, 9.0, 13.0, 0.0, 0.0, 10.0, 14.0, 0.0, 0.0, 0.0, 15.0, 0.0, 0.0, 0.0, 0.0},\n        //TEST - 7 - 6x6\n        //[1,   0,  0,  0,  0,  0]\n        //[2,   3,  0,  0,  0,  0]\n        //[4,   5,  6,  0,  0,  0]\n        //[7,   8,  9, 10,  0,  0]\n        //[11, 12, 13, 14, 15,  0]\n        //[16, 17, 18, 19, 20, 21]\n        {1.0, 2.0, 4.0, 7.0, 11.0, 16.0, 3.0, 5.0, 8.0, 12.0, 17.0, 0.0, 6.0, 9.0, 13.0, 18.0, 0.0, 0.0, 10.0, 14.0, 19.0, 0.0, 0.0, 0.0, 15.0, 20.0, 0.0, 0.0, 0.0, 0.0, 21.0, 0.0, 0.0, 0.0, 0.0 ,0.0},\n        //TEST - 8 - 7x7\n        //[1,   0,  0,  0,  0,  0,  0]\n        //[2,   3,  0,  0,  0,  0,  0]\n        //[4,   5,  6,  0,  0,  0,  0]\n        //[7,   8,  9, 10,  0,  0,  0]\n        //[11, 12, 13, 14, 15,  0,  0]\n        //[16, 17, 18, 19, 20, 21,  0]\n        //[22, 23, 24, 25, 26, 27, 28]\n        {1.0, 2.0, 4.0, 7.0, 11.0, 16.0, 22.0, 3.0, 5.0, 8.0, 12.0, 17.0, 23.0, 0.0, 6.0, 9.0, 13.0, 18.0, 24.0, 0.0, 0.0, 10.0, 14.0, 19.0, 25.0, 0.0, 0.0, 0.0, 15.0, 20.0, 26.0, 0.0, 0.0, 0.0, 0.0, 21.0, 27.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0},\n        //TEST - 9 - 8x8\n        //[1,   0,  0,  0,  0,  0,  0,  0]\n        //[2,   3,  0,  0,  0,  0,  0,  0]\n        //[4,   5,  6,  0,  0,  0,  0,  0]\n        //[7,   8,  9, 10,  0,  0,  0,  0]\n        //[11, 12, 13, 14, 15,  0,  0,  0]\n        //[16, 17, 18, 19, 20, 21,  0,  0]\n        //[22, 23, 24, 25, 26, 27, 28,  0]\n        //[29, 30, 31, 32, 33, 34, 35, 36]\n        {1.0, 2.0, 4.0, 7.0, 11.0, 16.0, 22.0, 29.0, 3.0, 5.0, 8.0, 12.0, 17.0, 23.0, 30.0, 0.0, 6.0, 9.0, 13.0, 18.0, 24.0, 31.0, 0.0, 0.0, 10.0, 14.0, 19.0, 25.0, 32.0, 0.0, 0.0, 0.0, 15.0, 20.0, 26.0, 33.0, 0.0, 0.0, 0.0, 0.0, 21.0, 27.0, 34.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.0, 35.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0},\n        //TEST - 10 - 9x9\n        //[1,   0,  0,  0,  0,  0,  0,  0, 0]\n        //[2,   3,  0,  0,  0,  0,  0,  0, 0]\n        //[4,   5,  6,  0,  0,  0,  0,  0, 0]\n        //[7,   8,  9, 10,  0,  0,  0,  0, 0]\n        //[11, 12, 13, 14, 15,  0,  0,  0, 0]\n        //[16, 17, 18, 19, 20, 21,  0,  0, 0]\n        //[22, 23, 24, 25, 26, 27, 28,  0, 0]\n        //[29, 30, 31, 32, 33, 34, 35, 36, 0]\n        //[37, 38, 39, 40, 41, 42, 43, 44, 45]\n        {1.0, 2.0, 4.0, 7.0, 11.0, 16.0, 22.0, 29.0, 37.0, 3.0, 5.0, 8.0, 12.0, 17.0, 23.0, 30.0, 38.0, 0.0, 6.0, 9.0, 13.0, 18.0, 24.0, 31.0, 39.0, 0.0, 0.0, 10.0, 14.0, 19.0, 25.0, 32.0, 40.0, 0.0, 0.0, 0.0, 15.0, 20.0, 26.0, 33.0, 41.0, 0.0, 0.0, 0.0, 0.0, 21.0, 27.0,34.0, 42.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.0, 35.0, 43.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 36.0, 44.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 45.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0},\n        \n        // TEST 11: 4x4 matrix with bandwidth 2 (lower triangular)\n        //[1, 0, 0, 0]\n        //[2, 3, 0, 0]\n        //[4, 5, 6, 0]\n        //[0, 7, 8, 9]\n        {1.0, 2.0, 4.0, 0.0, 3.0, 5.0, 7.0, 0.0, 6.0, 8.0, 0.0, 0.0, 9.0, 0.0, 0.0, 0.0},\n        \n        // TEST 12: 5x5 matrix with bandwidth 3 (lower triangular)\n        //[1, 0, 0, 0, 0]\n        //[2, 3, 0, 0, 0]\n        //[4, 5, 6, 0, 0]\n        //[7, 8, 9, 10, 0]\n        //[0, 0, 11, 12, 13]\n        {1.0, 2.0, 4.0, 7.0, 0.0, 3.0, 5.0, 8.0, 0.0, 0.0, 6.0, 9.0, 11.0, 0.0, 0.0, 10.0, 12.0, 0.0, 0.0, 0.0, 13.0, 0.0, 0.0, 0.0, 0.0},\n        \n        // TEST 13: 6x6 matrix with bandwidth 2 (upper triangular)\n        //[1, 2, 4, 0, 0, 0]\n        //[0, 3, 5, 7, 0, 0]\n        //[0, 0, 6, 8, 9, 0]\n        //[0, 0, 0, 10, 11, 12]\n        //[0, 0, 0, 0, 13, 14]\n        //[0, 0, 0, 0, 0, 15]\n        {0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 4.0, 5.0, 6.0, 0.0, 0.0, 0.0, 7.0, 8.0, 10.0, 0.0, 0.0, 0.0, 9.0, 11.0, 13.0, 0.0, 0.0, 0.0, 12.0, 14.0, 15.0, 0.0, 0.0, 0.0},\n        \n        // TEST 14: 3x3 matrix with bandwidth 1 (lower triangular, transpose)\n        //[1, 0, 0]\n        //[2, 3, 0]\n        //[0, 4, 5]\n        {1.0, 2.0, 0.0, 3.0, 4.0, 0.0, 5.0, 0.0, 0.0},\n        \n        // TEST 15: 4x4 matrix with bandwidth 2 (upper triangular)\n        //[1, 2, 4, 0]\n        //[0, 3, 5, 7]\n        //[0, 0, 6, 8]\n        //[0, 0, 0, 9]\n        {0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 3.0, 0.0, 4.0, 5.0, 6.0, 0.0, 7.0, 8.0, 9.0, 0.0}\n    };\n\n    //Input vector\n    float inputVector_h[TEST_CASE_COUNT][maxVectorSize] = { \n        //TEST - 1\n        {1.0, 2.0, 3.0},\n        //TEST - 2\n        {1.0, 2.0, 3.0},\n        //TEST - 3\n        {1.0, 2.0, 3.0},\n        //TEST - 4\n        {1.0, 2.0, 3.0},\n        //TEST - 5\n        {1.0, 2.0, 3.0, 4.0},\n        //TEST - 6\n        {1.0, 2.0, 3.0, 4.0, 5.0},\n        //TEST - 7\n        {1.0, 2.0, 3.0, 4.0, 5.0, 6.0},\n        //TEST - 8\n        {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0},\n        //TEST - 9\n        {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0},\n        //TEST - 10\n        {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0},\n        //TEST - 11\n        {1.0, 2.0, 3.0, 4.0},\n        //TEST - 12\n        {1.0, 2.0, 3.0, 4.0, 5.0},\n        //TEST - 13\n        {1.0, 2.0, 3.0, 4.0, 5.0, 6.0},\n        //TEST - 14\n        {1.0, 2.0, 3.0},\n        //TEST - 15\n        {1.0, 2.0, 3.0, 4.0}\n    };\n       \n    //Expected Output for Test\n    float expectedOutput[TEST_CASE_COUNT][maxVectorSize] = {\n        //TEST - 1\n        {1.0, 8.0, 32.0},\n        //TEST - 2\n        {17.0, 21.0, 18.0},\n        //TEST - 3\n        {17.0, 21.0, 18.0},\n        //TEST - 4\n        {6.0, 5.0, 3.0},\n        //TEST - 5\n        {1.0, 8.0, 32.0, 90.0},\n        //TEST - 6\n        {1.0, 8.0, 32.0, 90.0, 205.0},\n        //TEST - 7\n        {1.0, 8.0, 32.0, 90.0, 205.0, 406.0},\n        //TEST - 8\n        {1.0, 8.0, 32.0, 90.0, 205.0, 406.0, 728.0},\n        //TEST - 9\n        {1.0, 8.0, 32.0, 90.0, 205.0, 406.0, 728.0, 1212.0},\n        //TEST - 10\n        {1.0, 8.0, 32.0, 90.0, 205.0, 406.0, 728.0, 1212.0, 1905.0},\n        //TEST - 11\n        {1.0, 2.0, 31.0, 42.0},\n        //TEST - 12\n        {1.0, 2.0, 10.0, 61.0, 94.0},\n        //TEST - 13\n        {1.0, 6.0, 29.0, 25.0, 30.0, 0.0},\n        //TEST - 14\n        {5.0, 9.0, 12.0}, \n        //TEST - 15\n        {10.0, 24.0, 36.0, 0.0}\n    };\n    \n\n    //Output of device on host\n    float output_h[maxVectorSize] = {};\n\n    //Create Cublas handle\n    cublasHandle_t cublasHandle = NULL;\n    CUBLAS_CHECK(cublasCreate(&cublasHandle));\n\n    //Use CUDA Streams for Asynchronous Execution\n    cudaStream_t stream = NULL;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    CUBLAS_CHECK(cublasSetStream(cublasHandle, stream));\n\n    //Allocate Device Memory\n    float *packedBandedMatrix_d;\n    float *inputVector_d;\n    float *outputVector_d;\n\n    CUDA_CHECK(cudaMallocAsync((void**)&packedBandedMatrix_d, maxMatrixSize * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&inputVector_d, maxVectorSize * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&outputVector_d, maxVectorSize * sizeof(float), stream));\n        \n    //Execute Test Cases\n    for (int testCase = 0; testCase < TEST_CASE_COUNT; testCase++) {\n        \n        int inputMtxRowsColums = matrixRowsColumns[testCase];\n        int inputVecSize = inputVectorSize[testCase];\n        int mtxDiagonalsCount = matrixNumDiagonals[testCase];\n        bool isUpper = isUpperTriangular[testCase];\n        bool transpose = transposeMatrix[testCase];\n        \n        //Copy input to device memory\n        CUDA_CHECK(cudaMemcpyAsync(packedBandedMatrix_d, inputMatrix_h[testCase], inputMtxRowsColums * inputMtxRowsColums * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(inputVector_d, inputVector_h[testCase], inputVecSize * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        //Invoke function\n        triangularBandedMatrixVectorMultiplication(packedBandedMatrix_d, inputVector_d, outputVector_d, inputMtxRowsColums, mtxDiagonalsCount, cublasHandle, isUpper, transpose);\n\n        //Copy output to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, outputVector_d, inputVecSize * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        //Synchronize Stream\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        for (int index = 0; index < inputVecSize; index++) {\n          assert(output_h[index] == expectedOutput[testCase][index]);\n        }\n    }\n        \n    //Deallocate Device Memory\n    CUDA_CHECK(cudaFreeAsync(packedBandedMatrix_d, stream));\n    CUDA_CHECK(cudaFreeAsync(inputVector_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputVector_d, stream));\n\n    //Destroy stream\n    CUDA_CHECK(cudaStreamDestroy(stream));\n\n    //Destroy cublas handles\n    CUBLAS_CHECK(cublasDestroy(cublasHandle));\n   \n}\n\nvoid triangularBandedMatrixVectorMultiplication(\n    float *packedBandedMatrix_d,\n    float *inputVector_d,\n    float *outputVector_d, \n    int matrixSize,\n    int bandWidth,\n    cublasHandle_t cublasHandle,\n    bool isUpperTriangular,\n    bool transposeMatrix) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/174", "date": "2025-07-30", "prompt": "Use CUDA cuBLAS to perform matrix-matrix multiplication of a batch of matrices with single-precision floating-point values. The matrices are stored in column-major order.\n\nThe signature of the function is void computeBatchedGEMM(float* matA_d, size_t matARows, size_t matACols, float* matB_d, size_t matBRows, size_t matBCols, size_t batchSize, float* matC_d, cublasHandle_t &handle, cudaStream_t& stream), where matA_d is the pointer to first input matrix A in the batch on GPU device, matARows is the number of rows of matrix A, matACols is the number of columns of matrix A, matB_d is the pointer to first input matrix B in the batch on GPU device, matBRows is the number of rows of matrix B, matBCols is the number of columns of matrix B, batchSize is the size of the batch, matC_d is the output data pointer on GPU device, handle is cuBLAS handle object and stream is CUDA stream object.\n\n>>> void computeBatchedGEMM({{-0.18,-1.00,0.56,0.28,0.62,0.34,0.38,0.16,-0.90,-0.46,-0.32,-0.52,0.24,-0.10,0.22},{0.82,-0.46,-0.92,0.84,-0.68,0.90,-0.28,-0.96,0.64,-0.64, -0.16,0.82,0.06,-0.58,0.90}}, 3, 5, {{-0.06,-0.76,-0.94,-0.48,0.34,-0.78,0.42,0.98,-0.56,-0.24,-0.30,-0.34,0.38,0.88,0.46},{0.28,-0.06,0.18,-0.18, -0.12,-0.54,-0.78,0.24,-0.18,0.06,0.14,-0.42,0.36,-0.26,0.56,}}, 5, 3, 2, float* matC, cublasHandle_t &handle, cudaStream_t& stream) -> matC: {{0.89,0.81,0.83,-0.09,-0.11,-0.61,0.48,0.53,0.71},{0.84,0.08,-0.17,0.86,0.01,-0.14,0.05,-0.39,0.27}}\n\n>>> void computeBatchedGEMM({{0.44,0.88,-0.88,-0.80,-0.22,0.88,-0.88,-0.42,-0.58,0.62,-0.22,-0.36,-0.72,-0.88,0.92},{-0.52,0.42,-0.98,0.30,0.92,0.64,0.78,-0.94,0.72,0.80, -0.50,0.72,-0.26,0.08,0.94}}, 3, 5, {{-0.22,-0.06,0.58,0.98,-0.68,0.08,-1.00,0.38,0.98,-0.88,-0.84,0.86,-0.52,-0.66,0.24},{0.74,-0.80,0.42,-0.44, -0.86,-0.44,-0.12,-0.44,-0.24,0.14,0.28,0.52,-0.30,0.72,-0.34,}}, 5, 3, 2, float* matC, cublasHandle_t &handle, cudaStream_t& stream) -> matC: {{2.46,-0.14,-1.28,-0.45,0.46,-0.32,0.71,0.12, -1.40},{-0.69,1.23,-0.32,0.10,0.01,0.25,-0.93,0.59,-0.74,}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "-lcublas", "declaration": "\n#include <cstdio>\n#include <cstdlib>\n#include <assert.h>\n#include <time.h>\n#include <float.h>\n#include <math.h>\n#include <cuda_runtime_api.h>\n#include <cublas_v2.h>\n\n#undef NDEBUG\n#define _USE_MATH_DEFINES\n#define BATCH_SIZE 2\n#define TOLERANCE 1e-4\n\n#define CUDA_CHECK(call)                                    \\\ndo {                                                        \\\n    cudaError_t error = call;                               \\\n    if (error != cudaSuccess) {                             \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n            __FILE__, __LINE__, cudaGetErrorString(error)); \\\n            exit(EXIT_FAILURE);                             \\\n    }                                                       \\\n} while(0)\n\n#define CUBLAS_CHECK(call)                                      \\\n    if ((call) != CUBLAS_STATUS_SUCCESS) {                      \\\n        fprintf(stderr, \"cuBLAS error at %s:%d\\n\", __FILE__,    \\\n            __LINE__); exit(EXIT_FAILURE);                      \\\n    }                                                           \\\n\nvoid computeBatchedGEMM(float* matA_d, size_t matARows, size_t matACols, float* matB_d, size_t matBRows, size_t matBCols, size_t batchSize, float* matC_d, cublasHandle_t& handle, cudaStream_t& stream);\n\nvoid launch() {\n\n    const int TEST_CASES = 8;\n\n    // Setting input lengths\n    size_t matASizes[TEST_CASES][2] = { {3,5}, {3,5}, {5,7}, {10,12}, {25,58}, {63,77}, {128,256} ,{240,320} };\n    size_t matBSizes[TEST_CASES][2] = { {5,3}, {5,3}, {7,6}, {12,17}, {58,72}, {77,80}, {256,190} ,{320,480} };\n\n    // Allocating max length and dims\n    size_t maxRows = 320;\n    size_t maxCols = 480;\n    size_t batchSize = BATCH_SIZE;\n\n    float* matA_h = (float*)malloc(batchSize * maxRows * maxCols * sizeof(float));\n    float* matB_h = (float*)malloc(batchSize * maxRows * maxCols * sizeof(float));\n    float* matC_h = (float*)malloc(batchSize * maxRows * maxCols * sizeof(float));\n    float* matC_ref = (float*)malloc(batchSize * maxRows * maxCols * sizeof(float));\n    \n    // Initializing stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Initializing CuBLAS Handle\n    cublasHandle_t handle;\n    CUBLAS_CHECK(cublasCreate_v2(&handle));\n    CUBLAS_CHECK(cublasSetStream(handle, stream));\n\n    // Allocating device memory and copying host matrices to device\n    float* matA_d = nullptr;\n    float* matB_d = nullptr;\n    float* matC_d = nullptr;\n    CUDA_CHECK(cudaMallocAsync((void**)&matA_d, maxRows * maxCols * batchSize * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&matB_d, maxRows * maxCols * batchSize * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&matC_d, maxRows * maxCols * batchSize * sizeof(float), stream));\n\n    // Running test cases with random inputs\n    srand(time(NULL));\n    for (int t_iter = 0; t_iter < TEST_CASES; t_iter++) {\n\n        // Set input length and dimensions\n        size_t matARows = matASizes[t_iter][0];\n        size_t matACols = matASizes[t_iter][1];\n        size_t matBRows = matBSizes[t_iter][0];\n        size_t matBCols = matBSizes[t_iter][1];\n\n        // Input data generation for matrix A in the interval (-1,1)\n        for (int iter = 0; iter < batchSize * matARows * matACols; iter++)\n            matA_h[iter] = ((rand() % 100) / 50.f) - 1;\n\n        // Input data generation for matrix B in the interval (-1,1)\n        for (int iter = 0; iter < batchSize * matBRows * matBCols; iter++)\n            matB_h[iter] = ((rand() % 100) / 50.f) - 1;\n\n        // Host to device memcpy\n        CUDA_CHECK(cudaMemcpyAsync(matA_d, matA_h, matARows * matACols * batchSize * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matB_d, matB_h, matBRows * matBCols * batchSize * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Call to cuBLAS APIs to compute batched matrix-matrix multiplication\n        computeBatchedGEMM(matA_d, matARows, matACols, matB_d, matBRows, matBCols, batchSize, matC_d, handle, stream);\n        CUDA_CHECK(cudaMemcpyAsync(matC_h, matC_d, matARows * matBCols * batchSize * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Output Verification using CPU implementation\n        for (int c = 0; c < batchSize; c++) {\n            for (int row = 0; row < matARows; row++) {\n                for (int col = 0; col < matBCols; col++) {\n                    float sumVal = 0.f;\n                    for (int k = 0; k < matACols; k++) {\n                        float a = matA_h[c * matARows * matACols + k * matARows + row];\n                        float b = matB_h[c * matBRows * matBCols + col * matBRows + k];\n                        sumVal += (a * b);\n                    }\n                    matC_ref[c * matARows * matBCols + col * matARows + row] = sumVal;\n                }\n            }\n        }\n\n        // Verification\n        for (int i = 0; i < matARows * matBCols * batchSize; i++) {\n            assert(fabsf(matC_h[i] - matC_ref[i]) < TOLERANCE);\n        }\n    }\n\n    // Free allocated memory\n    free(matA_h);\n    free(matB_h);\n    free(matC_h);\n    free(matC_ref);\n    \n    cudaFreeAsync(matA_d, stream);\n    cudaFreeAsync(matB_d, stream);\n    cudaFreeAsync(matC_d, stream);\n    cublasDestroy_v2(handle);\n    cudaStreamDestroy(stream);\n}\n\nvoid computeBatchedGEMM(float* matA_d, size_t matARows, size_t matACols, float* matB_d, size_t matBRows, size_t matBCols, size_t batchSize, float* matC_d, cublasHandle_t &handle, cudaStream_t& stream) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/175", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute log2(x), where 0.0f < x < 1.0f, using a lookup table in shared memory and calculating the lookup index from the mantissa bits of the input data. Transmit the lookup table from global memory to shared memory before beginning calculations. Avoid memory bank conflicts for shared memory accesses and ensure coalesced access to global memory. Duplicate each lookup table element for each shared memory bank or for as many banks as required.\n\nThe signature of the CUDA kernel is __global__ void k_calculateLog2(int numElements, float *data_d, float *lookupTable_d, int lookupTableDuplication), where numElements represents the number of input/output elements, data_d is a pointer to an array of floats in device memory used as inputs and outputs, lookupTable_d is a pointer to an array of floats in device memory serving as the lookup table, and lookupTableDuplication indicates the number of duplicates per lookup table element to reduce memory bank collisions.\n\n>>> k_calculateLog2(10, { 0.5f, 0.25ff, 0.125f, 0.0625f, 0.03125f, 0.015625f, 0.0078125f, 0.00390625f, 0.00195312f, 0.000976562f }, { log2f(1.0f + index / (float) LOOKUP_TABLE_ELEMENTS) }, 32) -> data_d: { -1.0f, -2.0f, -3.0f, -4.0f, -5.0f, -6.0f, -7.0f, -8.0f, -9.0f, -10.0f }\n>>> k_calculateLog2(10, { 1.0f, 0.5f, 0.333333f, 0.25f, 0.2f, 0.166667f, 0.142857f, 0.125f, 0.111111f, 0.1f }, { log2f(1.0f + index / (float) LOOKUP_TABLE_ELEMENTS) }, 32) -> data_d: { 0.0f, -1.0f, -1.60768f, -2.0f, -2.32757f, -2.60768f, -2.83008f, -3.0f, -3.19265f, -3.32757f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Settings.\n// Number of elements in the lookup table for log2(mantissa).\nconstexpr int LOOKUP_TABLE_ELEMENTS = 256;\n// Maximum number of elements for input and output.\nconstexpr int MAX_DATA_ELEMENTS = 1000000;\nconstexpr int NUM_TESTS = 7;\nconstexpr float ERROR_TOLERANCE = 0.008f;\nconstexpr int DETERMINISTIC_RANDOM_SEED = 42;\n// The number of memory banks in shared memory since the Fermi architecture.\nconstexpr int NUM_MEMORY_BANKS = 32;\n__global__ void k_calculateLog2(int numElements, float * data_d, float * lookupTable_d, int lookupTableDuplication);\n\nvoid launch() {\n    // The ideal number of duplicates per lookup table element to ensure the same data in all available memory banks.\n    int lookupTableDuplication = NUM_MEMORY_BANKS;\n    int deviceId = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    int numSM = deviceProperties.multiProcessorCount;\n    // The shared memory allocation adjustment to fit inside the available size.\n    size_t dynamicSMemSize = sizeof(float) * lookupTableDuplication * LOOKUP_TABLE_ELEMENTS;\n    while(dynamicSMemSize > deviceProperties.sharedMemPerBlock) {\n        lookupTableDuplication--;\n        dynamicSMemSize = sizeof(float) * lookupTableDuplication * LOOKUP_TABLE_ELEMENTS;\n    }\n    int maxBlocksPerSM;\n    int maxBlockSize;\n    int minGridSize;\n    // Querying the largest possible block size.\n    CUDA_CHECK(cudaOccupancyMaxPotentialBlockSize(&minGridSize, &maxBlockSize, (void*)k_calculateLog2, dynamicSMemSize));\n    // Queryint the maximum number of blocks per SM.\n    CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(&maxBlocksPerSM, (void*)k_calculateLog2, maxBlockSize, dynamicSMemSize));\n    int maxBlocks = maxBlocksPerSM * numSM;\n    // Allocating host memory.\n    float * lookupTable_h = new float[LOOKUP_TABLE_ELEMENTS];\n    float * data_h = new float[MAX_DATA_ELEMENTS];\n    float * testData_h = new float[MAX_DATA_ELEMENTS * NUM_TESTS];\n    int * testNumElements_h = new int[NUM_TESTS];\n    // Allocating device memory.\n    float * lookupTable_d;\n    float * data_d;\n    CUDA_CHECK(cudaMallocAsync(&lookupTable_d, sizeof(float) * LOOKUP_TABLE_ELEMENTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&data_d, sizeof(float) * MAX_DATA_ELEMENTS, stream));\n    \n    // Initializing the lookup table data.\n    for(int i = 0; i < LOOKUP_TABLE_ELEMENTS; i++) {\n        lookupTable_h[i] = log2f(1.0f + i / (float) LOOKUP_TABLE_ELEMENTS);\n    }\n    CUDA_CHECK(cudaMemcpyAsync(lookupTable_d, lookupTable_h, sizeof(float) * LOOKUP_TABLE_ELEMENTS, cudaMemcpyHostToDevice, stream));\n    \n    // Test 1: 1.0f / (2 ^ (i + 1))\n    int testIndex = 0;\n    {\n        int numElements = 10;\n        testNumElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testData_h[i + testIndex * MAX_DATA_ELEMENTS] = 1.0f / pow(2.0f, i + 1);\n        }\n        testIndex++;\n    }\n    // Test 2: 1.0f / (i + 1)\n    {\n        int numElements = 10;\n        testNumElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testData_h[i + testIndex * MAX_DATA_ELEMENTS] = 1.0f / (float)(i + 1);\n        }\n        testIndex++;\n    }\n    // Test 3: Random normalized values.\n    {\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_real_distribution<float> distribution(0.0f, 1.0f);\n        int numElements = MAX_DATA_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testData_h[i + testIndex * MAX_DATA_ELEMENTS] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Test 4: Elements with values close to 0.0f.\n    {\n        int numElements = MAX_DATA_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testData_h[i + testIndex * MAX_DATA_ELEMENTS] = 0.00001f +  0.0001f * i / (float)numElements;\n        }\n        testIndex++;\n    }\n    // Test 5: Elements with values close to 1.0f.\n    {\n        int numElements = MAX_DATA_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testData_h[i + testIndex * MAX_DATA_ELEMENTS] = 1.0f - 0.0001f * i / (float)numElements;\n        }\n        testIndex++;\n    }\n    // Test 6: Logistic map iterations for r = 3.7f and x = 0.14f\n    {\n        int numElements = MAX_DATA_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        float x = 0.14f;\n        float r = 3.7f;\n        for(int i = 0; i < numElements; i++) {\n            testData_h[i + testIndex * MAX_DATA_ELEMENTS] = x;\n            x = r * (1.0f - x) * x;\n        }\n        testIndex++;\n    }\n    // Test 7: Sine map iterations for x = 0.5f and r = 0.99f.\n    {\n        int numElements = MAX_DATA_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        float x = 0.5f;\n        float r = 0.99f;\n        for(int i = 0; i < numElements; i++) {\n            testData_h[i + testIndex * MAX_DATA_ELEMENTS] = x;\n            x = r * sin(x * acosf(-1.0f));\n        }\n        testIndex++;\n    }\n    // Iterating the tests.\n    for(int test = 0; test < testIndex; test++)\n    {\n        int numElements = testNumElements_h[test];\n        for(int i = 0; i < numElements; i++) {\n            data_h[i] = testData_h[i + test * MAX_DATA_ELEMENTS];\n        }\n        CUDA_CHECK(cudaMemcpyAsync(data_d, data_h, sizeof(float) * numElements, cudaMemcpyHostToDevice, stream));\n        void * args[4] = { &numElements, &data_d, &lookupTable_d, &lookupTableDuplication };\n        int requiredBlocks = (numElements + maxBlockSize - 1) / maxBlockSize;\n        int usedBlocks = requiredBlocks < maxBlocks ? requiredBlocks : maxBlocks;\n        // Grid: (usedBlocks, 1, 1)\n        // Block: (maxBlockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateLog2, dim3(usedBlocks, 1, 1), dim3(maxBlockSize, 1, 1), args, dynamicSMemSize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(data_h, data_d, sizeof(float) * numElements, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < numElements; i++) {\n           float expectedValue = log2f(testData_h[i + test * MAX_DATA_ELEMENTS]);\n           assert(fabsf(expectedValue - data_h[i]) < ERROR_TOLERANCE);\n        }\n    }\n    // Freeing device memory.\n    CUDA_CHECK(cudaFreeAsync(lookupTable_d, stream));\n    CUDA_CHECK(cudaFreeAsync(data_d, stream));\n    // Freeing host memory.\n    delete [] lookupTable_h;\n    delete [] testData_h;\n    delete [] testNumElements_h;\n    // Freeing the stream.\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_calculateLog2(int numElements, float * data_d, float * lookupTable_d, int lookupTableDuplication) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/176", "date": "2025-07-30", "prompt": "Write a function that launches the k_hermiteInterpolation kernel with optimal grid/block sizes using occupancy calculations. For each input and output buffer, the function should configure both cache hit and miss properties to single-use memory access, expected cache hit ratio to 0, base pointer to device pointer of buffer, and number of bytes to the size of that buffer.\n\nThe signature of the function is void configureKernelLaunch(cudaLaunchConfig_t* config, cudaLaunchAttribute* attributes, float* x_d, float* f_d, float* df_d, float* query_d, float* result_d, int n, cudaStream_t stream), where config is a pointer to the CUDA launch configuration structure, attributes is the pointer to an array to store memory access policy settings, x_d is the pointer to an array of x coordinates, f_d is the pointer to an array of function values at the x coordinates, df_d is the pointer to an array to derivative values at the x coordinates, query_d is the pointer to an array to x coordinates, result_d is the pointer to an array to store interpolated values, n is the number of interpolation points, and stream is the CUDA stream for asynchronous kernel execution.\n\nk_hermiteInterpolation kernel should compute interpolated values at query points using the Hermite basis functions based on input x values, function values, and derivatives.\n\nThe signature of the function is __global__ void k_hermiteInterpolation(float *x, float *f, float *df, float *query, float *result, int n), where x is the pointer to an array of x coordinates, f is the pointer to an array of function values at the x coordinates, df is the pointer to an array of derivative values at the x coordinates, query is the pointer to an array of x coordinates where interpolation is requested, \nthe result is the pointer to an  output array to store interpolated values, and n is the number of intervals for interpolation (number of query points).\n\n>>> configureKernelLaunch(&config, attributes,{0.0f, 1.0f, 2.0f, 3.0f, 4.0f},{0.0f, 0.8415f, 0.9093f, 0.1411f, -0.7568f},{1.0f, 0.5403f, -0.4161f, -0.9900f, -0.6536f},{0.5f, 1.5f, 2.5f, 3.5f}, result, 4, stream) -> result:{0.4794f, 0.9975f, 0.5984f, -0.3508f}\n>>> configureKernelLaunch(&config, attributes,{0.0f, 1.0f, 2.0f, 3.0f, 4.0f},{0.0f, 1.0f, 4.0f, 9.0f, 16.0f},{0.0f, 2.0f, 4.0f, 6.0f, 8.0f},{0.25f, 0.75f, 1.5f, 2.5f}, result, 4, stream) -> result:{0.0625f, -0.5625f, -2.25f, 0.25f}\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cuda_runtime.h>\n#include <cmath>\n#include <cassert>\n#include <vector>\n#include <cstdio>\n\n#define N 1024  // Number of interpolation points\n#define MAX_BLOCK_SIZE 256\n#define NUM_ATTRIBUTES (5)\n\n// Error checking macro\n#define CUDA_CHECK(call)                                     \\\ndo {                                                         \\\n    cudaError_t error = call;                                \\\n    if(error != cudaSuccess) {                               \\\n        fprintf(stderr,                                      \\\n            \"CUDA Error: %s at %s:%d\\n\",                     \\\n                cudaGetErrorString(error),                   \\\n                    __FILE__,                                \\\n                    __LINE__);                               \\\n        exit(error);                                         \\\n    }                                                        \\\n} while(0)\n\nvoid configureKernelLaunch(cudaLaunchConfig_t* config, cudaLaunchAttribute* attributes,\n                          float* x_d, float* f_d, float* df_d, float* query_d, float* result_d,\n                          int n, cudaStream_t stream);\n\n__global__ void k_hermiteInterpolation(float *x, float *f, float *df, float *query, float *result, int n);\n\nvoid launch() {\n    // Initialize CUDA device\n    CUDA_CHECK(cudaFree(0));\n    constexpr float EPSILON = 1E-5;\n\n    // Get device properties\n    cudaDeviceProp deviceProp;\n    int deviceId;\n    CUDA_CHECK(cudaGetDevice(&deviceId));\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, deviceId));\n\n    // Get the optimal block size for k_computeTrajectory\n    int gridSize, blockSize;\n    cudaOccupancyMaxPotentialBlockSize(&gridSize, &blockSize, k_hermiteInterpolation, 0, MAX_BLOCK_SIZE);\n\n    // Create CUDA stream for async operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Define all test cases\n    struct TestCase {\n        std::vector<float> x;\n        std::vector<float> f;\n        std::vector<float> df;\n        std::vector<float> query;\n        std::vector<float> expected;\n    };\n\n    std::vector<TestCase> testCases = {\n        // Test case 1:\n        {\n            {0.0f, 1.0f, 2.0f, 3.0f, 4.0f},\n            {0.0f, 0.8415f, 0.9093f, 0.1411f, -0.7568f},\n            {1.0f, 0.5403f, -0.4161f, -0.9900f, -0.6536f},\n            {0.5f, 1.5f, 2.5f, 3.5f},\n            {0.613288f, 0.890925f, 0.349438f, -0.513300f}\n        },\n\n        // Test case 2:\n        {\n            {0.0f, 1.0f, 2.0f, 3.0f, 4.0f},\n            {0.0f, 1.0f, 4.0f, 9.0f, 16.0f},\n            {0.0f, 2.0f, 4.0f, 6.0f, 8.0f},\n            {0.25f, 0.75f, 1.5f, 2.5f},\n            {0.437500f, 0.937500f, 3.250000f, 7.750000f}\n        },\n\n        // Test case 3:\n        {\n            {0.0f, 0.001f, 0.002f, 0.003f},\n            {1.0f, 1.001f, 1.002f, 1.003f},\n            {1.0f, 1.0f, 1.0f, 1.0f},\n            {0.0005f, 0.0015f, 0.0025f},\n            {1.000750f, 1.001750f, 1.002750f}\n        },\n\n        // Test case 4:\n        {\n            {0.0f, 1.0f, 2.0f, 3.0f},\n            {1.0f, 2.718f, 7.389f, 20.085f},\n            {1.0f, 2.718f, 7.389f, 20.085f},\n            {0.5f, 1.5f, 2.5f},\n            {2.323750f, 6.316875f, 17.171249f}\n        },\n\n        // Test case 5:\n        {\n            {0.0f, 0.2f, 0.4f, 0.6f, 0.8f, 1.0f},\n            {0.0f, 0.8414709848078965f, 0.9092974268256817f, 0.1411200080598672f, -0.7568024953079282f, -0.9589242746631385f},\n            {5.0f, 2.701511529340699f, -2.080734182735712f, -4.949962483002227f, -3.2682181043180596f, 1.4183109273166358f},\n            {0.1f, 0.3f, 0.5f, 0.7f, 0.9f},\n            {0.613273f, 0.890904f, 0.349441f, -0.513296f, -0.904111f}\n        },\n\n        // Test case 6:\n        {\n            {0.0f, 0.1f, 0.2f, 0.3f, 0.4f, 0.5f, 0.6f, 0.7f, 0.8f, 0.9f, 1.0f},\n            {0.0f, 0.0953f, 0.1823f, 0.2624f, 0.3365f, 0.4055f, 0.4700f, 0.5306f, 0.5878f, 0.6419f, 0.6931f},\n            {1.0f, 0.9091f, 0.8333f, 0.7692f, 0.7143f, 0.6667f, 0.6250f, 0.5882f, 0.5556f, 0.5263f, 0.5000f},\n            {0.05f, 0.15f, 0.25f, 0.35f, 0.45f, 0.55f, 0.65f, 0.75f, 0.85f, 0.95f},\n            {0.071514f, 0.160580f, 0.242381f, 0.317994f, 0.388262f, 0.453896f, 0.515465f, 0.573498f, 0.628374f, 0.680329f}\n        },\n\n        // Test case 7:\n        {\n            {1e-6f, 1e-5f, 1e-4f, 1e-3f},\n            {1e-12f, 1e-10f, 1e-8f, 1e-6f},\n            {2e-6f, 2e-5f, 2e-4f, 2e-3f},\n            {5e-6f, 5e-5f, 5e-4f},\n            {0.000000f, 0.000000f, 0.000001f}\n        }\n    };\n\n    int maxN = 0;\n\n    for (size_t testIdx = 0; testIdx < testCases.size(); testIdx++) {\n        TestCase& test = testCases[testIdx];\n\n        int n = test.query.size();\n        if (maxN <= n)\n            maxN = n;\n    }\n\n    // Allocate device memory\n    float *x_d, *f_d, *df_d, *query_d, *result_d;\n    CUDA_CHECK(cudaMallocAsync((void **)&x_d, (maxN + 1) * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&f_d, (maxN + 1) * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&df_d, (maxN + 1) * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&query_d, maxN * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void **)&result_d, maxN * sizeof(float), stream));\n\n    // Array for attributes\n    cudaLaunchAttribute attributes[NUM_ATTRIBUTES];\n    cudaLaunchConfig_t config = {};\n\n    for (size_t testIdx = 0; testIdx < testCases.size(); testIdx++) {\n        const TestCase& test = testCases[testIdx];\n\n        int n = test.query.size();  // Number of intervals\n\n        // Allocate host memory\n        float *result = new float[n];\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(x_d, test.x.data(), (n + 1) * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(f_d, test.f.data(), (n + 1) * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(df_d, test.df.data(), (n + 1) * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(query_d, test.query.data(), n * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        void* args[] = {(void*)&x_d, (void*)&f_d, (void*)&df_d, (void*)&query_d, (void*)&result_d, &n};\n\n        configureKernelLaunch(&config, attributes, x_d, f_d, df_d, query_d, result_d, n, stream);\n\n        CUDA_CHECK(cudaLaunchKernelExC(&config, reinterpret_cast<const void*>(k_hermiteInterpolation), args));\n\n        // Copy result back to host\n        CUDA_CHECK(cudaMemcpyAsync(result, result_d, n * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < n; i++) {\n            float diff = fabs(result[i] - test.expected[i]);\n            assert(diff < EPSILON);\n        }\n        delete[] result;\n    }\n\n    // Free device memory\n    CUDA_CHECK(cudaFreeAsync(x_d, stream));\n    CUDA_CHECK(cudaFreeAsync(f_d, stream));\n    CUDA_CHECK(cudaFreeAsync(df_d, stream));\n    CUDA_CHECK(cudaFreeAsync(query_d, stream));\n    CUDA_CHECK(cudaFreeAsync(result_d, stream));\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n\n    // Clean up stream\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\nvoid configureKernelLaunch(cudaLaunchConfig_t* config, cudaLaunchAttribute* attributes,\n                          float* x_d, float* f_d, float* df_d, float* query_d, float* result_d,\n                          int n, cudaStream_t stream) {\n    // Get device properties\n    cudaDeviceProp deviceProp;\n    int deviceId;\n    CUDA_CHECK(cudaGetDevice(&deviceId));\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, deviceId));\n\n    // Get the optimal block size\n    int gridSize, blockSize;\n    cudaOccupancyMaxPotentialBlockSize(&gridSize, &blockSize, k_hermiteInterpolation, 0, MAX_BLOCK_SIZE);\n\n    // Use streaming for x_d\n    attributes[0].id = cudaLaunchAttributeAccessPolicyWindow;\n    attributes[0].val.accessPolicyWindow.base_ptr  = x_d;\n    attributes[0].val.accessPolicyWindow.num_bytes = (n + 1) * sizeof(float);\n    attributes[0].val.accessPolicyWindow.hitRatio  = 0.0f;\n    attributes[0].val.accessPolicyWindow.hitProp   = cudaAccessPropertyStreaming;\n    attributes[0].val.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;\n\n    // Use streaming for f_d\n    attributes[1].id = cudaLaunchAttributeAccessPolicyWindow;\n    attributes[1].val.accessPolicyWindow.base_ptr  = f_d;\n    attributes[1].val.accessPolicyWindow.num_bytes = (n + 1) * sizeof(float);\n    attributes[1].val.accessPolicyWindow.hitRatio  = 0.0f;\n    attributes[1].val.accessPolicyWindow.hitProp   = cudaAccessPropertyStreaming;\n    attributes[1].val.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;\n\n    // Use streaming for df_d\n    attributes[2].id = cudaLaunchAttributeAccessPolicyWindow;\n    attributes[2].val.accessPolicyWindow.base_ptr  = df_d;\n    attributes[2].val.accessPolicyWindow.num_bytes = (n + 1) * sizeof(float);\n    attributes[2].val.accessPolicyWindow.hitRatio  = 0.0f;\n    attributes[2].val.accessPolicyWindow.hitProp   = cudaAccessPropertyStreaming;\n    attributes[2].val.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;\n\n    // Use streaming for query_d\n    attributes[3].id = cudaLaunchAttributeAccessPolicyWindow;\n    attributes[3].val.accessPolicyWindow.base_ptr  = query_d;\n    attributes[3].val.accessPolicyWindow.num_bytes = n * sizeof(float);\n    attributes[3].val.accessPolicyWindow.hitRatio  = 0.0f;\n    attributes[3].val.accessPolicyWindow.hitProp   = cudaAccessPropertyStreaming;\n    attributes[3].val.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;\n\n    // Use streaming for result_d\n    attributes[4].id = cudaLaunchAttributeAccessPolicyWindow;\n    attributes[4].val.accessPolicyWindow.base_ptr  = result_d;\n    attributes[4].val.accessPolicyWindow.num_bytes = n * sizeof(float);\n    attributes[4].val.accessPolicyWindow.hitRatio  = 0.0f;\n    attributes[4].val.accessPolicyWindow.hitProp   = cudaAccessPropertyStreaming;\n    attributes[4].val.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;\n\n    // Setting up kernel launch configurations\n    config->numAttrs = NUM_ATTRIBUTES;\n    config->gridDim = gridSize;\n    config->blockDim = blockSize;\n    config->attrs = attributes;\n    config->stream = stream;\n}\n\n__global__ void k_hermiteInterpolation(float *x, float *f, float *df, float *query, float *result, int n) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/177", "date": "2025-07-30", "prompt": "Implement a CUDA kernel that calculates powers of complex numbers, where input is an array of tuples containing complex numbers and their integer powers. Use libcu++ library to extract the complex number z and power p from each tuple then compute z^p and store in the output array named results.\n\nThe signature of the CUDA kernel is __global__ void calculateComplexPowers(ComplexPower* input, cuda::std::complex<double>* results, int numElements), where input: is the pointer to device memory containing an array of complex number and power tuples, results: is the pointer to device memory for output array, numElements: is the total number of elements in both input and results arrays.\n\n>>> calculateComplexPowers({{{-9.553819, -3.706877i}, 3}, {{9.808528, -7.206335i}, 4}, {{0.350317, 2.162406i}, 4}, {{4.387896, -3.679310i}, 9}, {{5.830007, -2.936908i}, 3}, results, 1000})->results:{{-478.193919, -964.104553i}, {-18024.292005, -12518.3252i}, {18.4369, -13.7969i}, {6620504.7830, 20505.9066i}, {47.2969, -274.1354i}}\n>>> calculateComplexPowers({{{ -4.486228, 7.615750i }, 1}, {{-9.921402, -4.273812i}, 8}, {{-0.749717, 9.301114i}, 3}, {{9.359734, 2.288768i}, 9}, {{9.687792, -0.673039i}, 9}, results, 5000})->results:{{-4.486228, 7.615750i}, {-184302477.3663, -20797416.6816i}, {194.154210, -788.9624i}, {-396917144.6291, 595833643.2304i}, {623258520.1143, -448963684.4932i}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n#include <cuda/std/tuple>\n#include <cuda/std/complex>\n#include <cuda/std/cassert>\n#include <iostream>\n#include <random>\n#include <vector>\n#include <cassert>\n\n// Define a tuple to store a complex number and its power\nusing ComplexPower = cuda::std::tuple<cuda::std::complex<double>, int>;\n\n// CUDA error checking macro\n#define CUDA_CHECK(call) do { \\\n    cudaError_t err = call; \\\n    if (err != cudaSuccess) { \\\n        std::cerr << \"CUDA error at \" << __FILE__ << \":\" << __LINE__ << \" - \" << cudaGetErrorString(err) << std::endl; \\\n        exit(1); \\\n    } \\\n} while(0)\n\n__global__ void calculateComplexPowers(\n    ComplexPower* input,\n    cuda::std::complex<double>* results,\n    int numElements\n);\n\nvoid launch() {\n\n    auto validateWithCPU = [](const std::vector<ComplexPower>& inputs,\n                            const std::vector<cuda::std::complex<double>>& gpuResults,\n                            int numToValidate = 100) {\n        \n        const double tolerance = 1e-3;\n        int validationCount = std::min(numToValidate, static_cast<int>(inputs.size()));\n                \n        for (int i = 0; i < validationCount; i++) {\n            cuda::std::complex<double> z = cuda::std::get<0>(inputs[i]);\n            int power = cuda::std::get<1>(inputs[i]);\n            \n            // Calculate on CPU using same algorithm as GPU kernel\n            cuda::std::complex<double> cpuResult(1.0, 0.0);\n            cpuResult = cuda::std::pow(z, power);\n            \n            cuda::std::complex<double> gpuResult = gpuResults[i];\n            \n            // Compare results\n            double diff = abs(cpuResult - gpuResult);\n            assert(diff < tolerance);\n        }\n    };\n\n    // Get device properties\n    cudaDeviceProp deviceProp;\n    int deviceId;\n    CUDA_CHECK(cudaGetDevice(&deviceId));\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, deviceId));\n\n    // Determine optimal block size and grid size\n    int blockSize = 256; // Typical value, can be adjusted\n    int maxBlocks = deviceProp.maxGridSize[0];\n\n    // Create a random number generator\n    std::random_device rd;\n    std::mt19937 gen(rd());\n    std::uniform_real_distribution<> realDist(-10.0, 10.0);\n    std::uniform_int_distribution<> powerDist(1, 10);\n\n    // Test case sizes\n    const int testSizes[] = {1000, 5000, 10000, 50000, 100000, 500000, 1000000};\n    const int NUM_TEST_CASES = sizeof(testSizes) / sizeof(testSizes[0]);\n\n    // Allocate a single large buffer for all test cases\n    const int MAX_SIZE = testSizes[NUM_TEST_CASES - 1];\n    ComplexPower* inputs_d;\n    cuda::std::complex<double>* results_d;\n\n    // Use cudaMallocAsync for device memory\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    CUDA_CHECK(cudaMallocAsync(&inputs_d, MAX_SIZE * sizeof(ComplexPower), stream));\n    CUDA_CHECK(cudaMallocAsync(&results_d, MAX_SIZE * sizeof(cuda::std::complex<double>), stream));\n\n    for (int test_idx = 0; test_idx < NUM_TEST_CASES; test_idx++) {\n        int size = testSizes[test_idx];\n\n        // Generate random complex numbers and powers\n        std::vector<ComplexPower> inputs(size);\n        for (int i = 0; i < size; i++) {\n            double real = realDist(gen);\n            double imag = realDist(gen);\n            int power = powerDist(gen);\n            inputs[i] = cuda::std::make_tuple(cuda::std::complex<double>(real, imag), power);\n        }\n\n        // Copy inputs to device asynchronously\n        CUDA_CHECK(cudaMemcpyAsync(inputs_d, inputs.data(), size * sizeof(ComplexPower),\n                                   cudaMemcpyHostToDevice, stream));\n\n        // Calculate grid size based on data size and device limits\n        int numBlocks = std::min((size + blockSize - 1) / blockSize, maxBlocks);\n\n        // Set up kernel parameters\n        void* args[] = {&inputs_d, &results_d, &size};\n\n        // Launch kernel using cudaLaunchKernel\n        CUDA_CHECK(cudaLaunchKernel((void*)calculateComplexPowers,\n                                    dim3(numBlocks),\n                                    dim3(blockSize),\n                                    args,\n                                    0,\n                                    stream));\n\n        // Wait for kernel to complete\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Allocate host memory for results\n        std::vector<cuda::std::complex<double>> results(size);\n\n        // Copy results back asynchronously\n        CUDA_CHECK(cudaMemcpyAsync(results.data(), results_d, size * sizeof(cuda::std::complex<double>),\n                                   cudaMemcpyDeviceToHost, stream));\n\n        // Wait for memory copy to complete\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        validateWithCPU(inputs, results, 100);\n    }\n\n    // Free device memory asynchronously\n    CUDA_CHECK(cudaFreeAsync(inputs_d, stream));\n    CUDA_CHECK(cudaFreeAsync(results_d, stream));\n\n    // Destroy stream\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void calculateComplexPowers(\n    ComplexPower* input,\n    cuda::std::complex<double>* results,\n    int numElements\n) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/178", "date": "2025-07-30", "prompt": "Write a function that implements the saxpy operation (Y = aX + Y) on single precision floating point vectors using the Thrust library. The function should use the lambda function to perform element-wise computation on GPU device vectors.\n\nThe signature of the function is void saxpy_thrust(int vectorSize, float scalarA, const float* vectorX_d, float* vectorY_d), where vectorSize is the number of elements, scalarA is the scalar multiplier for vector X in the SAXPY operation (Y = aX + Y), vectorX_d: Pointer to input vector X data on GPU device memory, vectorY_d: Pointer to vector Y data on GPU device memory.\n\n>>> saxpy_thrust(100, 2.5f, {-9.99955, -8.29935, 2.02705, 7.83223, 9.35911, -6.2062, 0.299517, -2.03983, -4.74188, 4.87025}, {-4.81524, 3.67598, 3.22369, -9.45125, -1.396, -6.25708, 4.46408, 5.41667, 7.85018, -4.0189})->vectorY_d:({-29.8141, -17.0724, 8.29132, 10.1293, 22.0018, -21.7726, 5.21287, 0.317084, -4.00451, 8.15672})\n>>> saxpy_thrust(10000, -1.5f, {-9.99955, -8.29935, 2.02705, 7.83223, 9.35911, -6.2062, 0.299517, -2.03983, -4.74188, 4.87025}, {-4.81524, 3.67598, 3.22369, -9.45125, -1.396, -6.25708, 4.46408, 5.41667, 7.85018, -4.0189})->vectorY_d:({10.1841, 16.125, 0.183107, -21.1996, -15.4347, 3.05223, 4.0148, 8.47641, 14.963, -11.3243})\n", "cc_flags": "-arch=sm_89 -arch=sm_80 --extended-lambda", "ld_flags": "", "declaration": "#include <cassert>\n#include <vector>\n\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/transform.h>\n#include <thrust/generate.h>\n#include <thrust/execution_policy.h>\n\nvoid saxpy_thrust(int vectorSize, float scalarA, const float* vectorX_d, float* vectorY_d);\n\nvoid launch() {\n    constexpr float TOLERANCE = 1e-3f;\n    constexpr float MIN_RANGE = -10.0f;\n    constexpr float MAX_RANGE = 10.0f;\n\n    // Test case configuration\n    struct TestCase {\n        int vectorSize;\n        float scalarA;\n    };\n\n    std::vector<TestCase> testCases = {\n        {100, 2.5f},\n        {10000, -1.5f},\n        {1000000, 3.14f},\n        {10000000, 0.5f},\n        {50000, 0.0f},\n        {100000, -100.0f},\n        {5000000, 1.0f}\n    };\n\n    auto verifySaxpy = [](const thrust::host_vector<float>& hostX_h,\n                          const thrust::host_vector<float>& hostYOriginal_h,\n                          const thrust::host_vector<float>& hostYResult_h,\n                          float scalarA) {\n\n        int totalElements = static_cast<int>(hostX_h.size());\n\n        for(int i = 0; i < totalElements; ++i) {\n            float expectedValue = scalarA * hostX_h[i] + hostYOriginal_h[i];\n            assert(std::abs(hostYResult_h[i] - expectedValue) < TOLERANCE);\n        }\n    };\n\n    // Run all test cases\n    for(int testIndex = 0; testIndex < testCases.size(); ++testIndex) {\n        const auto& testCase = testCases[testIndex];\n\n        // Generate random data on host using rand()\n        thrust::host_vector<float> hostX_h(testCase.vectorSize), hostY_h(testCase.vectorSize);\n        \n        // Seed for X vector\n        srand(time(nullptr));\n        for(int i = 0; i < testCase.vectorSize; ++i) {\n            float normalized = static_cast<float>(rand()) / static_cast<float>(RAND_MAX);\n            hostX_h[i] = MIN_RANGE + normalized * (MAX_RANGE - MIN_RANGE);\n        }\n        \n        // Seed for Y vector (different seed for different values)\n        srand(time(nullptr));\n        for(int i = 0; i < testCase.vectorSize; ++i) {\n            float normalized = static_cast<float>(rand()) / static_cast<float>(RAND_MAX);\n            hostY_h[i] = MIN_RANGE + normalized * (MAX_RANGE - MIN_RANGE);\n        }\n\n        // Copy to device\n        thrust::device_vector<float> deviceX(hostX_h);\n        thrust::device_vector<float> deviceY(hostY_h);\n        \n        thrust::host_vector<float> hostYOriginal_h(hostY_h);\n\n        saxpy_thrust(testCase.vectorSize, testCase.scalarA, thrust::raw_pointer_cast(deviceX.data()),\n                    thrust::raw_pointer_cast(deviceY.data()));\n\n        thrust::copy(deviceY.begin(), deviceY.end(), hostY_h.begin());\n\n        verifySaxpy(hostX_h, hostYOriginal_h, hostY_h, testCase.scalarA);\n    }\n}\n\nvoid saxpy_thrust(int vectorSize, float scalarA, const float* vectorX_d, float* vectorY_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/179", "date": "2025-07-30", "prompt": "Write a CUDA kernel for computing an inclusive prefix sum on an integer array. Use warp-level shuffle operations for efficient intra-block scan and store each block's total in global memory to compute block-level offsets for the final result.\n\nThe signature of the function is __global__ void k_prefixSum(int* inputArray_d, int* outputArray_d, int* blockSums_d, int totalElementCount, int warpsPerBlock)), where inputArray_d is a device pointer to the input array, outputArray_d is a device pointer where the computed inclusive prefix sum will be stored, blockSums_d is a device pointer used to store each blocks total sum for later adjustment and totalElementCount is the number of elements and warpsPerBlock is the number of warps per block.\n\n>>> k_prefixSum({3,1,4,1,5}, outputArray_d, blockSums_d, 5, warpsPerBlock)) -> {3,4,8,9,14}\n>>> k_prefixSum({1,1,1,1,1}, outputArray_d, blockSums_d, 5, warpsPerBlock)) -> {1,2,3,4,5}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n// Standard C/C++ headers\n#include <cstdio>\n#include <cstdlib>\n#include <vector>\n#include <cassert>\n#include <algorithm>\n\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n\nusing namespace std;\nnamespace cg = cooperative_groups;\n\n// Error-checking macro\n#define CUDA_CHECK(call)                                                   \\\n{                                                                          \\\n    cudaError_t error = call;                                              \\\n    if(error != cudaSuccess) {                                             \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                     \\\n                __FILE__, __LINE__, cudaGetErrorString(error));           \\\n        exit(EXIT_FAILURE);                                                \\\n    }                                                                      \\\n}\n\n__global__ void k_prefixSum(int* inputArray_d, int* outputArray_d, \n                           int* blockSums_d, int totalElementCount, int warpsPerBlock);\n\nstruct TestCase {\n    vector<int> inputData;    // Input array\n    vector<int> expectedResult; // Expected inclusive prefix-sum result\n};\n\nvoid launch() {\n    constexpr int THREADS_PER_BLOCK = 256;\n\n    // Query device properties \n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProperties, 0));\n    \n    int deviceWarpSize = deviceProperties.warpSize;\n    int warpsPerBlock = (THREADS_PER_BLOCK + deviceWarpSize - 1) / deviceWarpSize;\n    \n    // Define comprehensive test cases\n    vector<TestCase> testCases = {\n        // Test Case 1: Small array of ones\n        { {1, 1, 1, 1, 1}, {1, 2, 3, 4, 5} },\n        // Test Case 2: Array of zeros\n        { {0, 0, 0, 0}, {0, 0, 0, 0} },\n        // Test Case 3: Mixed small array {3,1,4,1,5}  {3,4,8,9,14}\n        { {3, 1, 4, 1, 5}, {3, 4, 8, 9, 14} },\n        // Test Case 4: Single element\n        { {5}, {5} },\n        // Test Case 5: Increasing sequence 1..10\n        { {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {1, 3, 6, 10, 15, 21, 28, 36, 45, 55} },\n        // Test Case 6: Large array of 10,000 ones\n        { vector<int>(10000, 1), []()->vector<int>{\n              vector<int> expectedValues(10000);\n              for (int i = 0; i < 10000; i++) { \n                  expectedValues[i] = i + 1; \n              }\n              return expectedValues;\n          }() },\n        // Test Case 7: Very large array of 500,000 ones\n        { vector<int>(500000, 1), []()->vector<int>{\n              vector<int> expectedValues(500000);\n              for (int i = 0; i < 500000; i++) { \n                  expectedValues[i] = i + 1; \n              }\n              return expectedValues;\n          }() }\n    };\n\n    // Find maximum element count among all test cases\n    int maxElementCount = 0;\n    for (const auto &testCase : testCases) {\n        maxElementCount = max(maxElementCount, (int)testCase.inputData.size());\n    }\n\n    // Calculate memory requirements\n    size_t maxInputBytes = (size_t)maxElementCount * sizeof(int);\n    int maxBlocksNeeded = (maxElementCount + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n    size_t blockSumsBytes = (size_t)(maxBlocksNeeded + 1) * sizeof(int);\n\n    // Allocate device memory buffers\n    int *inputArray_d = nullptr;\n    int *outputArray_d = nullptr;\n    int *blockSums_d = nullptr;\n\n    CUDA_CHECK(cudaMalloc(&inputArray_d, maxInputBytes));\n    CUDA_CHECK(cudaMalloc(&outputArray_d, maxInputBytes));\n    CUDA_CHECK(cudaMalloc(&blockSums_d, blockSumsBytes));\n\n    // Create CUDA stream for asynchronous operations\n    cudaStream_t computeStream;\n    CUDA_CHECK(cudaStreamCreate(&computeStream));\n\n    // Process each test case\n    for (size_t testIndex = 0; testIndex < testCases.size(); testIndex++) {\n        const auto &currentTest = testCases[testIndex];\n        int elementCount = (int)currentTest.inputData.size();\n        size_t inputBytesRequired = (size_t)elementCount * sizeof(int);\n\n        // Copy input data to device\n        CUDA_CHECK(cudaMemcpyAsync(\n            inputArray_d,\n            currentTest.inputData.data(),\n            inputBytesRequired,\n            cudaMemcpyHostToDevice,\n            computeStream\n        ));\n        \n        // Determine launch configuration\n        int totalSMs, maxBlocksPerSM;\n        totalSMs = deviceProperties.multiProcessorCount;\n        CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n            &maxBlocksPerSM,\n            k_prefixSum,\n            THREADS_PER_BLOCK,\n            warpsPerBlock * sizeof(int) + sizeof(int)  // Dynamic shared memory size\n        ));\n\n        int maxActiveBlocks = maxBlocksPerSM * totalSMs;    \n        // Calculate optimal grid configuration\n        int blocksNeeded = (elementCount + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n        int numBlocks = min(blocksNeeded, maxActiveBlocks);\n\n        dim3 blockDimensions(THREADS_PER_BLOCK, 1, 1);\n        dim3 gridDimensions(\n            min(numBlocks, deviceProperties.maxGridSize[0]),\n            1,\n            1\n        );\n        \n        // Calculate dynamic shared memory size\n        size_t sharedMemoryBytes = warpsPerBlock * sizeof(int) + sizeof(int);\n\n        // Launch cooperative kernel\n        void* kernelArguments[] = {\n            (void*)&inputArray_d,\n            (void*)&outputArray_d,\n            (void*)&blockSums_d,\n            (void*)&elementCount,\n            (void*)&warpsPerBlock\n        };\n        \n        CUDA_CHECK(cudaLaunchCooperativeKernel(\n            (void*)k_prefixSum,\n            gridDimensions,\n            blockDimensions,\n            kernelArguments,\n            sharedMemoryBytes,\n            computeStream\n        ));\n        CUDA_CHECK(cudaStreamSynchronize(computeStream));\n\n        // Copy results back to host\n        vector<int> computedResult(elementCount);\n        CUDA_CHECK(cudaMemcpyAsync(\n            computedResult.data(),\n            outputArray_d,\n            inputBytesRequired,\n            cudaMemcpyDeviceToHost,\n            computeStream\n        ));\n        CUDA_CHECK(cudaStreamSynchronize(computeStream));\n\n        for (int i = 0; i < elementCount; i++) {\n            assert(computedResult[i] == currentTest.expectedResult[i]);\n        }\n         \n    }\n\n    // Cleanup resources\n    CUDA_CHECK(cudaStreamDestroy(computeStream));\n    CUDA_CHECK(cudaFree(inputArray_d));\n    CUDA_CHECK(cudaFree(outputArray_d));\n    CUDA_CHECK(cudaFree(blockSums_d));\n  \n}\n\n__global__ void k_prefixSum(int* inputArray_d, int* outputArray_d, \n                           int* blockSums_d, int totalElementCount, int warpsPerBlock) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/180", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform General Matrix Multiplication (GEMM) that computes C = AB + C of row-major matrix layout and single-precision floating point.\n\nThe signature of the function is __global__ void k_gemmNormal(float* matrixA_d, float* matrixB_d, float* matrixC_d, int matrixM, int matrixN, int matrixK, float alphaVal, float betaVal), where matrixA_d is a device pointer to the MK input matrix A, matrixB_d points to the KN input matrix B, and matrixC_d points to the MN output matrix C that stores final results, alphaVal scales the matrix multiplication result, and betaVal scales the existing values in matrix C. \n\n>>> k_gemmNormal({-0.594, 0.772, 0.765, 0.9900.779, -0.081, -0.715, 0.0880.134, 0.102, 0.561, 0.225-0.034, 0.773, 0.442, 0.063}, {0.403, 0.697, -0.376, 0.082-0.756, 0.459, 0.108, -0.378-0.669, 0.564, -0.449, 0.772-0.347, -0.396, 0.241, 0.139}, matrixC_d, 4, 4, 4, 1, 0)->matrixC_d:({-1.678, -0.021, 0.202, 0.3880.823, 0.068, 0.040, -0.445-0.476, 0.368, -0.237, 0.437-0.915, 0.556, -0.087, 0.055});\n>>> k_gemmNormal({-0.190, -0.974, -0.671, -0.263, 0.351, -0.7640.474, -0.561, -0.470, -0.756, -0.468, -0.2870.171, -0.431, -0.497, -0.985, 0.055, 0.3990.291, -0.082, 0.520, 0.564, 0.430, 0.3110.461, 0.377, -0.715, -0.441, -0.748, 0.980-0.302, 0.653, -0.400, 0.670, 0.311, 0.719}, {-0.956, 0.514, -0.294, -0.094, -0.726, -0.6100.948, -0.248, 0.711, -0.321, -0.794, 0.3630.029, 0.348, 0.740, 0.580, \n-0.394, 0.8080.648, 0.964, -0.805, -0.717, 0.592, 0.4690.181, 0.750, -0.110, 0.913, 0.529, -0.9110.646, -0.170, -0.407, 0.904, 0.437, -0.412}, matrixC_d, 6, 6, 6, 2, 0.5)->matrixC_d:({-2.418, 0.210, -1.404, -0.578, 1.459, -1.699-3.528, -1.443, -0.598, -0.666, -1.339, -1.420-1.429, -1.518, -0.004, 1.963, -0.255, -2.8730.756, 2.478, -0.772, 1.018, 0.295, -0.4700.120, -2.960, -0.445, -0.345, -1.329, -1.4673.900, 0.656, -0.746, -0.129, 0.974, -0.444\n});\n\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <vector>\n#include <random>\n#include <cassert>\n\n#include <cuda_runtime.h>\n\n// Global constants\n#define MAX_MATRIX_SIZE 2048\n#define TOLERANCE 1e-3f\n#define MIN_RANDOM_VAL -1.0f\n#define MAX_RANDOM_VAL 1.0f\n\n#define CUDA_CHECK(call) {                                                        \\\n        cudaError_t err = call;                                                   \\\n        if (err != cudaSuccess) {                                                 \\\n            fprintf(stderr, \"CUDA error in file '%s' in line %d: %s.\\n\",          \\\n                    __FILE__, __LINE__, cudaGetErrorString(err));                 \\\n            exit(EXIT_FAILURE);                                                   \\\n        }                                                                         \\\n}\n\n// Test case structure\nstruct TestCase {\n    int matrixM, matrixN, matrixK;\n    float alphaVal, betaVal;\n};\n\n__global__ void k_gemmNormal(const float* matrixA_d, const float* matrixB_d, float* matrixC_d, \n                            int matrixM, int matrixN, int matrixK, float alphaVal, float betaVal);\n\n// Launch function\nvoid launch() {\n    // Get device properties\n    cudaDeviceProp deviceProp;\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, 0));\n    \n    // Use fixed block size\n    int blockDim = 16; // 16x16 = 256 threads per block\n    \n    // Random number generator for alpha and beta values\n    std::random_device rd;\n    std::mt19937 gen(rd());\n    std::uniform_real_distribution<float> alphaBetaDist(-2.0f, 2.0f);\n    \n    std::vector<TestCase> testCases = {\n        {4, 4, 4, alphaBetaDist(gen), alphaBetaDist(gen)},\n        {16, 32, 24, alphaBetaDist(gen), alphaBetaDist(gen)},\n        {64, 128, 96, alphaBetaDist(gen), alphaBetaDist(gen)},\n        {256, 512, 384, alphaBetaDist(gen), alphaBetaDist(gen)},\n        {512, 256, 1024, alphaBetaDist(gen), alphaBetaDist(gen)},\n        {1024, 2048, 512, alphaBetaDist(gen), alphaBetaDist(gen)},\n        {2048, 1024, 512, alphaBetaDist(gen), alphaBetaDist(gen)}\n    };\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Pre-allocate device memory for maximum size\n    constexpr int MAX_ELEMENTS = MAX_MATRIX_SIZE * MAX_MATRIX_SIZE;\n    float *matrixA_d, *matrixB_d, *matrixC_d;\n    CUDA_CHECK(cudaMallocAsync(&matrixA_d, MAX_ELEMENTS * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&matrixB_d, MAX_ELEMENTS * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&matrixC_d, MAX_ELEMENTS * sizeof(float), stream));\n\n    // Host memory allocation\n    std::vector<float> matrixA_h(MAX_ELEMENTS);\n    std::vector<float> matrixB_h(MAX_ELEMENTS);\n    std::vector<float> matrixCOriginal_h(MAX_ELEMENTS);\n    std::vector<float> matrixCGpu_h(MAX_ELEMENTS);\n    std::vector<float> matrixCCpu_h(MAX_ELEMENTS);\n    \n    // Lambda function to initialize matrix with random values\n    auto initializeMatrix = [](std::vector<float>& matrixData_h, int matrixSize, float minVal = MIN_RANDOM_VAL, float maxVal = MAX_RANDOM_VAL) {\n        std::random_device randomDevice;\n        std::mt19937 generator(randomDevice());\n        std::uniform_real_distribution<float> distribution(minVal, maxVal);\n        \n        for (int i = 0; i < matrixSize; i++) {\n            matrixData_h[i] = distribution(generator);\n        }\n    };\n    \n    // Lambda function for CPU GEMM validation\n    auto gemmCpu = [](const std::vector<float>& matrixA_h, const std::vector<float>& matrixB_h, \n                     std::vector<float>& matrixC_h, int matrixM, int matrixN, int matrixK, float alphaVal, float betaVal) {\n        for (int i = 0; i < matrixM; i++) {\n            for (int j = 0; j < matrixN; j++) {\n                float sumVal = 0.0f;\n                for (int k = 0; k < matrixK; k++) {\n                    sumVal += matrixA_h[i * matrixK + k] * matrixB_h[k * matrixN + j];\n                }\n                matrixC_h[i * matrixN + j] = alphaVal * sumVal + betaVal * matrixC_h[i * matrixN + j];\n            }\n        }\n    };\n    \n    // Lambda function to verify results with sampling for large matrices\n    auto verifyResults = [](const std::vector<float>& gpuResult_h, const std::vector<float>& cpuResult_h, \n                           int matrixM, int matrixN, float tolerance = TOLERANCE) -> bool {\n        int totalElements = matrixM * matrixN;\n        \n        for (int idx = 0; idx < totalElements; idx++) {\n            assert(std::abs(gpuResult_h[idx] - cpuResult_h[idx]) < tolerance);\n        }\n        return;\n    };  \n    \n    // Iterate through all test cases\n    for (int testIdx = 0; testIdx < testCases.size(); testIdx++) {\n\n        // Initialize matrices\n        initializeMatrix(matrixA_h, testCases[testIdx].matrixM * testCases[testIdx].matrixK);\n        initializeMatrix(matrixB_h, testCases[testIdx].matrixK * testCases[testIdx].matrixN);\n        initializeMatrix(matrixCOriginal_h, testCases[testIdx].matrixM * testCases[testIdx].matrixN);\n        \n        // Copy initial C matrix for both GPU and CPU tests\n        matrixCGpu_h = matrixCOriginal_h;\n        matrixCCpu_h = matrixCOriginal_h;\n        \n        // Copy data to device using async operations\n        CUDA_CHECK(cudaMemcpyAsync(matrixA_d, matrixA_h.data(), testCases[testIdx].matrixM * testCases[testIdx].matrixK * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matrixB_d, matrixB_h.data(), testCases[testIdx].matrixK * testCases[testIdx].matrixN * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(matrixC_d, matrixCGpu_h.data(), testCases[testIdx].matrixM * testCases[testIdx].matrixN * sizeof(float), cudaMemcpyHostToDevice, stream));\n        \n        // Calculate required blocks based on matrix dimensions\n        int requiredBlocksX = (testCases[testIdx].matrixN + blockDim - 1) / blockDim;\n        int requiredBlocksY = (testCases[testIdx].matrixM + blockDim - 1) / blockDim;\n        \n        // Compare with device limits and adjust if necessary\n        int gridX = std::min(requiredBlocksX, deviceProp.maxGridSize[0]);\n        int gridY = std::min(requiredBlocksY, deviceProp.maxGridSize[1]);\n        \n        // Configure kernel launch parameters\n        dim3 blockSize(blockDim, blockDim);\n        dim3 gridSize(gridX, gridY);\n        \n        // Create local copies for kernel arguments to avoid const issues\n        int localM = testCases[testIdx].matrixM;\n        int localN = testCases[testIdx].matrixN;\n        int localK = testCases[testIdx].matrixK;\n        float localAlpha = testCases[testIdx].alphaVal;\n        float localBeta = testCases[testIdx].betaVal;\n        \n        // Prepare kernel arguments for cudaLaunchKernel\n        void* kernelArgs[] = {\n            (void*)&matrixA_d,\n            (void*)&matrixB_d,\n            (void*)&matrixC_d,\n            (void*)&localM,\n            (void*)&localN,\n            (void*)&localK,\n            (void*)&localAlpha,\n            (void*)&localBeta\n        };\n        \n        // Launch GPU kernel using cudaLaunchKernel\n        CUDA_CHECK(cudaLaunchKernel(\n            (void*)k_gemmNormal,           // kernel function\n            gridSize,                      // grid dimensions\n            blockSize,                     // block dimensions\n            kernelArgs,                    // kernel arguments\n            0,                            // shared memory size\n            stream                         // stream\n        ));\n        \n        // Copy GPU result back to host using async operation\n        CUDA_CHECK(cudaMemcpyAsync(matrixCGpu_h.data(), matrixC_d, testCases[testIdx].matrixM * testCases[testIdx].matrixN * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Run CPU validation\n        gemmCpu(matrixA_h, matrixB_h, matrixCCpu_h, testCases[testIdx].matrixM, testCases[testIdx].matrixN, testCases[testIdx].matrixK, testCases[testIdx].alphaVal, testCases[testIdx].betaVal);\n   \n        // Verify results with sampling for large matrices\n        verifyResults(matrixCGpu_h, matrixCCpu_h, testCases[testIdx].matrixM, testCases[testIdx].matrixN);\n    }\n    \n    \n    // Cleanup device memory\n    CUDA_CHECK(cudaFreeAsync(matrixA_d, 0));\n    CUDA_CHECK(cudaFreeAsync(matrixB_d, 0));\n    CUDA_CHECK(cudaFreeAsync(matrixC_d, 0));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_gemmNormal(const float* matrixA_d, const float* matrixB_d, float* matrixC_d, \n                            int matrixM, int matrixN, int matrixK, float alphaVal, float betaVal) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/181", "date": "2025-07-30", "prompt": "Write a CUDA kernel to apply a median filter to 2D grayscale images with pixel values ranging from 0 to 255. The kernel should load the image into shared memory with a tile size determined by the radius. Each thread should select its own (2  radius + 1) neighbors from the shared tile, sort these neighbors based on their pixel values, and output the median value for each window.\n\nThe signature of the functions is __global__ void k_grayScaleMedianFilter(float* input_d, float* output_d, int width, int height, int radius), where input_d represents the input image data that needs filtering, output_d defines output buffer where the filtered image will be stored, width represents width of image in pixels, height represents height of the image in pixels, radius defines the radius of the median filter window.\n\n>>> k_grayScaleMedianFilter ({{64 x 64}, output_d, 64, 64, 1}) --> output_d({64.00, 1.00, 63.00, 63.00, 125.00}).\n>>> k_grayScaleMedianFilter ({{512 x 512}, output_d, 512, 512, 2}) --> output_d({512.00, 1.00, 511.00, 511.00, 1021.00}).\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <cassert>\n#include <cmath>\n#include <algorithm>\n#include <cuda_runtime.h>\n\nconst int BLOCK_SIZE = 16;\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n    cudaError_t error = call;                                                      \\\n    if (error != cudaSuccess) {                                                    \\\n        fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),    \\\n            __FILE__, __LINE__);                                                   \\\n        exit(error);                                                               \\\n    }                                                                              \\\n} while (0)\n\n__global__ void k_grayScaleMedianFilter(float* input_d, float* output_d, int width, int height, int radius);\n\nvoid launch() {\n\n    // Input values are gradients, representing pixel intensities (range: 0 to width+height)\n    // 2D image test cases include both square and rectangular images\n    const int numTests = 8;\n    int widths[numTests]   = {64, 128, 256, 512, 160, 200, 120, 300};\n    int heights[numTests]  = {64, 128, 128, 512, 100, 200, 200, 150};\n    int radii[numTests]    = {1, 2, 1, 2, 1, 1, 2, 2};\n\n    // Expected output values: center, top-left, top-right, bottom-left, bottom-right\n    float expectedOutput[numTests][5] = {\n        {64.00, 1.00, 63.00, 63.00, 125.00},          // TC1: 64x64, radius=1\n        {128.00, 1.00, 127.00, 127.00, 253.00},       // TC2: 128x128, radius=2\n        {192.00, 1.00, 255.00, 127.00, 381.00},       // TC3: 256x128, radius=1\n        {512.00, 1.00, 511.00, 511.00, 1021.00},      // TC4: 512x512, radius=2\n        {130.00, 1.00, 159.00, 99.00, 257.00},        // TC5: 160x100, radius=1\n        {200.00, 1.00, 199.00, 199.00, 397.00},       // TC6: 200x200, radius=1\n        {160.00, 1.00, 119.00, 199.00, 317.00},       // TC7: 120x200, radius=2\n        {225.00, 1.00, 299.00, 149.00, 447.00}        // TC8: 300x150, radius=2\n    };\n    \n    // Find the maximum image size\n    int maxWidth = *std::max_element(widths, widths + numTests);\n    int maxHeight = *std::max_element(heights, heights + numTests);\n    int maxSize = maxWidth * maxHeight;\n\n    // Allocate device memory for input and output\n    float *input_h, *output_h, *input_d, *output_d;\n    \n    // Host memory allocation\n    input_h = (float*)malloc(maxSize * sizeof(float));\n    output_h = (float*)malloc(maxSize * sizeof(float));\n    \n    // Device memory allocation\n    CUDA_CHECK(cudaMalloc(&input_d, maxSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&output_d, maxSize * sizeof(float)));\n\n    // Create a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Run the 2D image test cases\n    for (int tc = 0; tc < numTests; tc++) {\n        int width = widths[tc];\n        int height = heights[tc];\n        int imageSize = width * height;\n        int radius = radii[tc];\n        \n        // Initialize input with a gradient pattern\n        for (int y = 0; y < height; y++) {\n            for (int x = 0; x < width; x++) {\n                input_h[y * width + x] = (float)(x + y);\n            }\n        }\n        \n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h, imageSize * sizeof(float), \n                                cudaMemcpyHostToDevice, stream));\n\n        // Define block and grid sizes\n        dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE, 1);\n        dim3 gridSize((width + BLOCK_SIZE - 1) / BLOCK_SIZE, (height + BLOCK_SIZE - 1) / BLOCK_SIZE, 1);\n\n         // Check grid limits\n        cudaDeviceProp props;\n        CUDA_CHECK(cudaGetDeviceProperties(&props, 0));\n        if (gridSize.x > props.maxGridSize[0] || gridSize.y > props.maxGridSize[1]) {\n            assert(false && \"Grid size exceeds device limits!\");\n        }\n                    \n        int halo_width = BLOCK_SIZE + 2 * radius;\n        int halo_height = BLOCK_SIZE + 2 * radius;\n        int tile_size = halo_width * halo_height;\n        int window_size = (2 * radius + 1) * (2 * radius + 1);\n        int total_window_size = BLOCK_SIZE * BLOCK_SIZE * window_size;\n        int sharedMemSize = (tile_size + total_window_size) * sizeof(float);\n\n        if (sharedMemSize > props.sharedMemPerBlock) {\n            assert(false && \"Requested shared memory exceeds device limits!\");\n        }\n\n        void *args[] = {&input_d, &output_d, &width, &height, &radius};\n                    \n        // Launch kernel using cudaLaunchKernel\n        CUDA_CHECK(cudaLaunchKernel((void*)k_grayScaleMedianFilter, gridSize, blockSize, args, sharedMemSize, stream));\n        \n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, imageSize * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validation check all pixels\n        float allowedTolerance = 0.0001f;\n        for (int i = 0; i < imageSize; i++) {\n            assert(!isnan(output_h[i]));\n        }\n\n        // Define indices for key pixels\n        int centerIdx = (height/2) * width + (width/2);\n        int topLeftIdx = 0;\n        int topRightIdx = width - 1;\n        int bottomLeftIdx = (height - 1) * width;\n        int bottomRightIdx = (height - 1) * width + (width - 1);\n        \n        // Assertions to verify correctness\n        assert(fabs(output_h[centerIdx] - expectedOutput[tc][0]) < allowedTolerance);\n        assert(fabs(output_h[topLeftIdx] - expectedOutput[tc][1]) < allowedTolerance);\n        assert(fabs(output_h[topRightIdx] - expectedOutput[tc][2]) < allowedTolerance);\n        assert(fabs(output_h[bottomLeftIdx] - expectedOutput[tc][3]) < allowedTolerance);\n        assert(fabs(output_h[bottomRightIdx] - expectedOutput[tc][4]) < allowedTolerance);\n    }\n\n    // Cleanup\n    CUDA_CHECK(cudaFree(input_d));\n    CUDA_CHECK(cudaFree(output_d));\n    free(input_h);\n    free(output_h);\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_grayScaleMedianFilter(float* input_d, float* output_d, int width, int height, int radius){\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/182", "date": "2025-07-30", "prompt": "Write a CUDA kernel using libcu++ safe indexing containers to find the rotation index of string B in string A. The rotation index (the position in string A where string B would start) should be stored in the output if B is a rotation of A; if no match is found or if the lengths differ, store -1.\n\nThe kernel signature of the function is __global__ void k_findRotationIndex(cuda::std::span<const char> a_span, cuda::std::span<const char> b_span, int len, cuda::std::span<int> results_span), where a_span is a device span representing the first string, b_span is a device span representing the second string, len is the length of both strings, and rotationIndex_d is a device span where the rotation index will be stored if found, or -1 if not found.\n\n>>> k_findRotationIndex(\"waterbottle\", \"erbottlewat\", 11, rotationIndex_d) -> rotationIndex_d = 3\n>>> k_findRotationIndex(\"hello\", \"world\", 5, rotationIndex_d) -> rotationIndex_d = -1\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n// Standard C/C++ headers\n#include <vector>\n#include <string>\n#include <algorithm>\n#include <cassert>\n\n// CUDA headers\n#include <cuda_runtime.h>\n#include <cuda/std/cstdint>\n#include <cuda/std/span>\n#include <cuda/std/limits>\n#include <cuda/std/functional>\n\n// Error-checking macro\n#define CUDA_CHECK(call)                                                   \\\n{                                                                          \\\n    cudaError_t error = call;                                              \\\n    if(error != cudaSuccess) {                                             \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                      \\\n                __FILE__, __LINE__, cudaGetErrorString(error));            \\\n        exit(EXIT_FAILURE);                                                \\\n    }                                                                      \\\n}\n\n__global__ void k_findRotationIndex(\n    cuda::std::span<const char> a_span,\n    cuda::std::span<const char> b_span,\n    int len,\n    cuda::std::span<int> results_span);\n\nvoid launch() {\n    // Constants\n    constexpr int THREADS_PER_BLOCK = 256;\n    constexpr int NOT_FOUND = -1;\n    \n    // Testcase struct for string rotation detection\n    struct TestCase {\n        std::string sourceString;       // first string\n        std::string targetString;       // second string\n        int expectedRotationIndex;      // expected rotation index (-1 if not a rotation)\n    };\n\n    std::vector<TestCase> testCases = {\n        // Basic test case with a valid rotation\n        {\"waterbottle\", \"erbottlewat\", 3},\n        // Not a rotation\n        {\"hello\", \"world\", -1},\n        // Same strings\n        {\"abcdef\", \"abcdef\", 0},\n        // Another valid rotation\n        {\"abcdef\", \"defabc\", 3},\n        // Not a rotation (different lengths)\n        {\"abc\", \"abcd\", -1},\n        // Empty strings\n        {\"\", \"\", 0},\n        // Single character strings\n        {\"a\", \"a\", 0},\n        // Not a rotation (same characters, different arrangement)\n        {\"abcd\", \"adbc\", -1}\n    };\n\n    cudaStream_t cudaStream;\n    CUDA_CHECK(cudaStreamCreate(&cudaStream));\n\n    // Compute maximum allocation sizes\n    size_t maxStringLength = 0;\n    for (const auto& testCase : testCases) {\n        maxStringLength = std::max(maxStringLength, testCase.sourceString.size());\n    }\n    size_t maxStringBytes = maxStringLength * sizeof(char);\n    size_t maxResultsBytes = maxStringLength * sizeof(int);\n\n    // Allocate device memory once\n    char* sourceString_d = nullptr;\n    char* targetString_d = nullptr;\n    int* results_d = nullptr;\n    \n    CUDA_CHECK(cudaMallocAsync(&sourceString_d, maxStringBytes, cudaStream));\n    CUDA_CHECK(cudaMallocAsync(&targetString_d, maxStringBytes, cudaStream));\n    CUDA_CHECK(cudaMallocAsync(&results_d, maxResultsBytes, cudaStream));\n\n    for (const auto& testCase : testCases) {\n        int stringLength = static_cast<int>(testCase.sourceString.size());\n        \n        // Skip cases with different string lengths or empty strings\n        if (stringLength != static_cast<int>(testCase.targetString.size())) {\n            if (testCase.expectedRotationIndex != NOT_FOUND) {\n                assert(false);\n            }\n            continue;\n        }\n        \n        if (stringLength == 0) {\n            if (testCase.expectedRotationIndex != 0) {\n                assert(false);\n            }\n            continue;\n        }\n        \n        // Copy strings to device\n        CUDA_CHECK(cudaMemcpyAsync(sourceString_d,\n                                  testCase.sourceString.data(),\n                                  stringLength * sizeof(char),\n                                  cudaMemcpyHostToDevice,\n                                  cudaStream));\n        CUDA_CHECK(cudaMemcpyAsync(targetString_d,\n                                  testCase.targetString.data(),\n                                  stringLength * sizeof(char),\n                                  cudaMemcpyHostToDevice,\n                                  cudaStream));\n        \n        // Initialize results array to NOT_FOUND\n        std::vector<int> initialResults(stringLength, NOT_FOUND);\n        CUDA_CHECK(cudaMemcpyAsync(results_d,\n                                  initialResults.data(),\n                                  stringLength * sizeof(int),\n                                  cudaMemcpyHostToDevice,\n                                  cudaStream));\n        \n        auto sourceSpan = cuda::std::span<const char>(sourceString_d, stringLength);\n        auto targetSpan = cuda::std::span<const char>(targetString_d, stringLength);\n        auto resultsSpan = cuda::std::span<int>(results_d, stringLength);\n        \n        // Determine launch configuration\n        cudaDeviceProp deviceProperties;\n        CUDA_CHECK(cudaGetDeviceProperties(&deviceProperties, 0));\n        int totalSMs, maxBlocksPerSM;\n        totalSMs = deviceProperties.multiProcessorCount;\n        CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n            &maxBlocksPerSM,\n            k_findRotationIndex,\n            THREADS_PER_BLOCK,\n            0\n        ));\n\n        int maxActiveBlocks = maxBlocksPerSM * totalSMs;\n        int blocksNeeded = (stringLength + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n        int numBlocks = std::min(blocksNeeded, maxActiveBlocks);\n\n        dim3 blockDimensions(THREADS_PER_BLOCK, 1, 1);\n        dim3 gridDimensions(\n            std::min(numBlocks, deviceProperties.maxGridSize[0]),\n            1,\n            1\n        );\n        \n        void* kernelArgs[] = {\n            &sourceSpan,\n            &targetSpan,\n            &stringLength,\n            &resultsSpan\n        };\n        \n        CUDA_CHECK(cudaLaunchKernel(\n            (void*)k_findRotationIndex,\n            gridDimensions,\n            blockDimensions,\n            kernelArgs,\n            0,  // No shared memory\n            cudaStream\n        ));\n        \n        CUDA_CHECK(cudaStreamSynchronize(cudaStream));\n        \n        // Copy back results\n        std::vector<int> hostResults(stringLength);\n        CUDA_CHECK(cudaMemcpyAsync(hostResults.data(), \n                                  results_d, \n                                  stringLength * sizeof(int), \n                                  cudaMemcpyDeviceToHost, \n                                  cudaStream));\n        CUDA_CHECK(cudaStreamSynchronize(cudaStream));\n        \n        // Find the first valid rotation index using libcu++ algorithms\n        auto it = std::find_if(hostResults.begin(), hostResults.end(), \n                              [](int val) { return val != NOT_FOUND; });\n        \n        int resultRotationIndex = (it != hostResults.end()) ? *it : NOT_FOUND;\n        \n        // Verify results with assert\n        assert(resultRotationIndex == testCase.expectedRotationIndex);\n    }\n\n    CUDA_CHECK(cudaFreeAsync(sourceString_d, cudaStream));\n    CUDA_CHECK(cudaFreeAsync(targetString_d, cudaStream));\n    CUDA_CHECK(cudaFreeAsync(results_d, cudaStream));\n    CUDA_CHECK(cudaStreamDestroy(cudaStream));\n}\n\n// Kernel: check each possible rotation offset in parallel\n__global__ void k_findRotationIndex(\n    cuda::std::span<const char> a_span,\n    cuda::std::span<const char> b_span,\n    int len,\n    cuda::std::span<int> results_span)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/183", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform matrix multiplication utilizing Tensor Cores by using inline PTX mma instruction of dimension m16n8k16.\nThe input matrix A is stored using row major indexing, and matrix B is stored using column major indexing.\nThe mma instruction with it's modifiers will be \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 \", load data into shared memory tiles while ensuring correct indexing order for A and B,\nbefore loading fragments via 'ldmatrix' instruction.\n\nThe signature for the kernel k_mmaM16N8K16ArowBcol is\n__global__ void k_mmaM16N8K16ArowBcol(__nv_bfloat16 *colMajorA_d, __nv_bfloat16 *rowMajorB_d, float *resultMatrixC_d, int mDim, int nDim, int kDim) where colMajorA_d is input Matrix A stored in column major order, rowMajorB_d is Matrix B stored in row major order, resultMatrixC_d is the resultant matrix multiply output,\nmDim, nDim, kDim are leading dimensions of the problem (A -> mDim x kDim), (B -> kDim x nDim), (C -> mDim x nDim).\n\n>>> k_mmaM16N8K8ArowBcol({3, 6, 7, 5}, {3, 5}, resultMatrixC_d, 2, 1, 2)-> resultMatrixC_d: ({44, 43})\n>>> k_mmaM16N8K8ArowBcol({3, 6, 17, 15}, {13, 15}, resultMatrixC_d, 2, 1, 2)-> resultMatrixC_d: ({294, 303})\n", "cc_flags": "-arch=sm_80", "ld_flags": "", "declaration": "\n#include <cstdio>\n#include <cassert>\n#include <cstdint>\n#include <random>\n\n#include <cuda_runtime.h>\n#include <mma.h>\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n#undef NDEBUG\n\n#define MMA_M 16\n#define MMA_N 8\n#define MMA_K 16\n\n__global__ void k_mmaM16N8K16ArowBcol(__nv_bfloat16 *rowMajorA_d, __nv_bfloat16 *colMajorB_d, float *resultMatrixC_d, int mDim, int nDim, int kDim);\n\n// Function to compute valid reference result\n// Inputs => ( Row Major -> A, Column Major -> B)\nvoid cpuMatMulReference(const __nv_bfloat16* A,\n                        const __nv_bfloat16* B,\n                        float* cpuRefC,\n                        int M,\n                        int N,\n                        int K) {\n    for (int i = 0; i < M; i++) {\n        for (int j = 0; j < N; j++) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; k++) {\n                float a_val = static_cast<float>(A[i*K + k]);\n                float b_val = static_cast<float>(B[j*K + k]);\n                sum += a_val * b_val;\n            }\n            cpuRefC[i*N + j] = sum;\n        }\n    }\n}\n\nvoid launch() {\n\n    const int TEST_CASE_COUNT = 7;\n    const int PROBLEM_DIMS = 3;\n\n    //Test case dimensions {M, N, K}\n    const int MAX_M = 512;\n    const int MAX_N = 512;\n    const int MAX_K = 512;\n\n    const int TEST_CASES_DIMS[TEST_CASE_COUNT][PROBLEM_DIMS] = {{16,8,16}, {512,512,512}, {32,16,32}, {256, 256, 256}, {64, 64, 64} , {64, 32, 32}, {128, 128, 128}};\n\n    // Tolerance for validation, set to 1% due to nature of half precision operations\n    const float TOLERANCE  = 0.01;\n\n    int deviceId = 0;\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, deviceId));\n    const int warpSize = prop.warpSize;    // 32\n\n    // Choose 4 warps per block (arbitrary tuning; can be tuned further)\n    const int WARPS_PER_BLOCK = 4;\n    const int BLOCK_SIZE      = warpSize * WARPS_PER_BLOCK;  // 128 threads\n\n\n    //Set up random number generation\n    std::mt19937 randEngine(time(nullptr));\n    // Bounded random distribution for test case initialization\n    std::uniform_real_distribution<float> randDist(1.0f, 100.0f);\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    //Pointers for Host Memory\n    __nv_bfloat16* A_h =(__nv_bfloat16*)malloc(MAX_M * MAX_K * sizeof(__nv_bfloat16));\n    __nv_bfloat16* B_h =(__nv_bfloat16*)malloc(MAX_K * MAX_N * sizeof(__nv_bfloat16));\n\n    float* cpuC_h =(float*)malloc(MAX_M * MAX_N * sizeof(float)); // Reference Matrix space allocation on host\n    float* gpuC_h = (float*)malloc(MAX_M * MAX_N * sizeof(float));// GPU result Matrix space allocation on host\n\n    //Pointers for device memory (GPU)\n    __nv_bfloat16* A_d;\n    __nv_bfloat16* B_d;\n    float* C_d;\n\n    // Allocate the memory on the device\n    CUDA_CHECK(cudaMallocAsync(&A_d, MAX_M * MAX_K * sizeof(__nv_bfloat16), stream));\n    CUDA_CHECK(cudaMallocAsync(&B_d, MAX_K * MAX_N * sizeof(__nv_bfloat16), stream));\n    CUDA_CHECK(cudaMallocAsync(&C_d, MAX_M * MAX_N * sizeof(float), stream));\n\n    for (int i = 0; i < TEST_CASE_COUNT; i++) {\n        // Dimensions of the input and output layers\n        int M = TEST_CASES_DIMS[i][0]; //Number of Rows in Matrix A\n        int N = TEST_CASES_DIMS[i][1]; //Number of Columns in Matrix B\n        int K = TEST_CASES_DIMS[i][2]; //Number of Columns in Matrix A and Rows in Matrix B\n\n        //Populating input matrices with random values\n\n        for (int r = 0; r < M; ++r) {\n            for (int c = 0; c < K ; ++c) {\n                float val = randDist(randEngine);\n                A_h[r * K + c] = __nv_bfloat16(val); // Filling A Matrix in Row Major Way\n            }\n       }\n\n       for (int c = 0; c < N; ++c) {\n            for (int r = 0; r < K; ++r) {\n                float  val = randDist(randEngine);   // Filling B Matrix in Row Major Way\n                B_h[c * K + r] = __nv_bfloat16(val);\n            }\n        }\n\n        // Initialize the result on the device\n        CUDA_CHECK(cudaMemsetAsync(C_d, 0, M * N * sizeof(float), stream));\n\n        // Load Test Cases\n        CUDA_CHECK(cudaMemcpyAsync(A_d, A_h, M * K * sizeof(__nv_bfloat16), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(B_d, B_h, K * N * sizeof(__nv_bfloat16), cudaMemcpyHostToDevice, stream));\n\n        // Check if the dimensions are divisible by the block tile dimensions\n        assert(M % MMA_M == 0);\n        assert(N % MMA_N == 0);\n        assert(K % MMA_K == 0);\n\n        // Compute tile counts and grid dimensions\n        int numTilesM = (M + MMA_M - 1)/ MMA_M;  // how many 16row blocks\n        int numTilesN = (N + MMA_N - 1)/ MMA_N;  // how many 8col blocks\n\n        int gridY = (numTilesM + WARPS_PER_BLOCK - 1) / WARPS_PER_BLOCK;  // ceil(numTilesM / WARPS_PER_BLOCK)\n        int gridX = numTilesN;  // one block per Ntile, warps carve Mtiles\n\n        dim3 blockDim(BLOCK_SIZE, 1, 1);\n        dim3 gridDim(gridX, gridY);\n\n        // Sharedmemory per block\n        size_t shmemBytes = WARPS_PER_BLOCK * (MMA_M*MMA_K + MMA_K*MMA_N)\n                           * sizeof(__nv_bfloat16);\n\n        // Launch kernel\n        void *args[] = {&A_d,\n                        &B_d,\n                        &C_d,\n                        (void*)&M,\n                        (void*)&N,\n                        (void*)&K};\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_mmaM16N8K16ArowBcol,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    shmemBytes,\n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        cpuMatMulReference(A_h, B_h, cpuC_h, M, N, K);\n\n        // Copying the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync(gpuC_h, C_d, M * N * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        // Validate the result, with in 1% tolerance\n        for(int t = 0; t < M*N; ++t) {\n            assert(std::fabs((gpuC_h[t] - cpuC_h[t]) / std::fabs(cpuC_h[t])) <= TOLERANCE);\n        }\n\n   }\n    // Free up resources\n    CUDA_CHECK(cudaFreeAsync(A_d, stream));\n    CUDA_CHECK(cudaFreeAsync(B_d, stream));\n    CUDA_CHECK(cudaFreeAsync(C_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    free(A_h);\n    free(B_h);\n    free(cpuC_h);\n    free(gpuC_h);\n}\n\n\n__global__ void k_mmaM16N8K16ArowBcol(__nv_bfloat16 *rowMajorA_d,\n                                     __nv_bfloat16 *colMajorB_d,\n                                     float          *resultMatrixC_d,\n                                     int             mDim,\n                                     int             nDim,\n                                     int             kDim) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/184", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform matrix multiplication utilizing Tensor Cores by using inline PTX mma instruction of dimension m16n8k16.\nThe input matrix A and matrix B are stored stored using column major indexing.\nThe mma instruction with it's modifiers will be \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 \", load data into fragments while ensuring correct indexing order for A and B.\n\nThe signature for the kernel k_mmaM16N8K16AcolBcol is __global__ void k_mmaM16N8K16AcolBcol(__nv_bfloat16 *colMajorA_d, __nv_bfloat16 *colMajorB_d, float *rowMajorC_d, int mDim, int nDim, int kDim) where colMajorA_d is input Matrix A stored in column major order, rowMajorB_d is Matrix B stored in row major order, resultMatrixC_d is the resultant matrix multiply output,\nmDim, nDim, kDim are leading dimensions of the problem (A -> mDim x kDim), (B -> kDim x nDim), (C -> mDim x nDim).\n\n>>> k_mmaM16N8K8AcolBcol({3, 6, 7, 5}, {3, 5}, resultMatrixC_d, 2, 1, 2)-> resultMatrixC_d: ({44, 43})\n>>> k_mmaM16N8K8AcolBcol({3, 6, 17, 15}, {13, 15}, resultMatrixC_d, 2, 1, 2)-> resultMatrixC_d: ({294, 303})\n", "cc_flags": "-arch=sm_80", "ld_flags": "", "declaration": "\n#include <cstdio>\n#include <cassert>\n#include <cstdint>\n#include <random>\n\n#include <cuda_runtime.h>\n#include <mma.h>\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n#undef NDEBUG\n\n#define MMA_M 16\n#define MMA_N 8\n#define MMA_K 16\n\n__global__ void k_mmaM16N8K16AcolBcol(__nv_bfloat16 *colMajorA_d, __nv_bfloat16 *colMajorB_d, float *rowMajorC_d, int mDim, int nDim, int kDim);\n\n// Function to compute valid reference result\n// Inputs => ( Column Major -> A, Column Major -> B)\nvoid cpuMatMulReference(const __nv_bfloat16* A,\n                        const __nv_bfloat16* B,\n                        float* cpuRefC,\n                        int M,\n                        int N,\n                        int K) {\n    for (int i = 0; i < M; i++) {\n        for (int j = 0; j < N; j++) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; k++) {\n                float a_val = static_cast<float>(A[k*M + i]);\n                float b_val = static_cast<float>(B[j*K + k]);\n                sum += a_val * b_val;\n            }\n            cpuRefC[i*N + j] = sum;\n        }\n    }\n}\n\nvoid launch() {\n\n    const int TEST_CASE_COUNT = 7;\n    const int PROBLEM_DIMS = 3;\n    //Test case dimensions {M, N, K}\n    const int MAX_M = 512;\n    const int MAX_N = 512;\n    const int MAX_K = 512;\n\n    const int TEST_CASES_DIMS[TEST_CASE_COUNT][PROBLEM_DIMS] = {{16,16,16}, {512,512,512}, {32,16,32}, {256, 256, 256}, {64, 64, 64} , {64, 32, 32}, {128, 128, 128}};\n\n    //Tolerance for validation, set to 1% due to nature of half precision operations\n    const float TOLERANCE  = 0.01;\n\n\n    int deviceId = 0;\n    cudaDeviceProp prop;\n    cudaError_t status = cudaGetDeviceProperties(&prop, deviceId);\n    const int WARP_SIZE = prop.warpSize;\n\n\n    // Kernel configuration parameters\n    const int WARPS_PER_BLOCK = 4;\n    const int BLOCK_SIZE = WARP_SIZE * WARPS_PER_BLOCK;\n\n    //Set up random number generation\n    std::mt19937 randEngine(time(nullptr));\n    // Bounded random distribution for test case initialization\n    std::uniform_real_distribution<float> randDist(1.0f, 100.0f);\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    //Pointers for Host Memory\n    __nv_bfloat16* A_h =(__nv_bfloat16*)malloc(MAX_M * MAX_K * sizeof(__nv_bfloat16));\n    __nv_bfloat16* B_h =(__nv_bfloat16*)malloc(MAX_K * MAX_N * sizeof(__nv_bfloat16));\n\n    float* cpuC_h =(float*)malloc(MAX_M * MAX_N * sizeof(float)); // Reference Matrix space allocation on host\n    float* gpuC_h = (float*)malloc(MAX_M * MAX_N * sizeof(float));// GPU result Matrix space allocation on host\n\n    //Pointers for device memory (GPU)\n    __nv_bfloat16* colMajorA_d;\n    __nv_bfloat16* colMajorB_d;\n    float* rowMajorC_d;\n\n    // Allocate the memory on the device\n    CUDA_CHECK(cudaMallocAsync(&colMajorA_d, MAX_M * MAX_K * sizeof(__nv_bfloat16), stream));\n    CUDA_CHECK(cudaMallocAsync(&colMajorB_d, MAX_K * MAX_N * sizeof(__nv_bfloat16), stream));\n    CUDA_CHECK(cudaMallocAsync(&rowMajorC_d, MAX_M * MAX_N * sizeof(float), stream));\n\n    for (int i = 0; i < TEST_CASE_COUNT; i++) {\n        // Dimensions of the input and output layers\n        int M = TEST_CASES_DIMS[i][0]; //Number of Rows in Matrix A\n        int N = TEST_CASES_DIMS[i][1]; //Number of Columns in Matrix B\n        int K = TEST_CASES_DIMS[i][2]; //Number of Columns in Matrix A and Rows in Matrix B\n\n        //Populating input matrices with random values\n        for (int c = 0; c < K ; ++c) {\n            for (int r = 0; r < M; ++r) {\n                float val = randDist(randEngine);\n                A_h[c * M + r] = __nv_bfloat16(val); // Filling A Matrix in Col Major Way\n            }\n       }\n\n       for (int c = 0; c < N; ++c) {\n            for (int r = 0; r < K; ++r) {\n                float  val = randDist(randEngine);   // Filling B Matrix in Col Major Way\n                B_h[c * K + r] = __nv_bfloat16(val);\n            }\n        }\n\n        // Initialize the result on the device\n        CUDA_CHECK(cudaMemsetAsync(rowMajorC_d, 0, M * N * sizeof(float), stream));\n\n        // Load Test Cases\n        CUDA_CHECK(cudaMemcpyAsync(colMajorA_d, A_h, M * K * sizeof(__nv_bfloat16), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(colMajorB_d, B_h, K * N * sizeof(__nv_bfloat16), cudaMemcpyHostToDevice, stream));\n\n        // Check if the dimensions are divisible by the block tile dimensions\n        assert(M % MMA_M == 0);\n        assert(N % MMA_N == 0);\n        assert(K % MMA_K == 0);\n\n        int numTilesM = (M + MMA_M - 1) / MMA_M;\n        int numTilesN = (N + MMA_N - 1) / MMA_N;\n\n        // Kernel Launch Configuration\n        dim3 gridDim(\n          numTilesN,\n          (numTilesM + WARPS_PER_BLOCK - 1)/WARPS_PER_BLOCK\n        );\n        dim3 blockDim(BLOCK_SIZE);\n\n        // Launch kernel\n        // Grid: ((N + MMA_N - 1/ MMA_N), (M + MMA_M - 1)/ MMA_M, 1)\n        // Block: (256, 1, 1)\n        void *args[] = {&colMajorA_d,\n                        &colMajorB_d,\n                        &rowMajorC_d,\n                        (void*)&M,\n                        (void*)&N,\n                        (void*)&K};\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_mmaM16N8K16AcolBcol,\n                                    gridDim,\n                                    blockDim,\n                                    args,\n                                    0, // Not using shared memory buffer\n                                    stream));\n\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        cpuMatMulReference(A_h, B_h, cpuC_h, M, N, K);\n\n        //Copying the result back to the host\n        CUDA_CHECK(cudaMemcpyAsync(gpuC_h, rowMajorC_d, M * N * sizeof(float), cudaMemcpyDeviceToHost, stream));\n\n        //Validate the result, with in 1% tolerance\n        for(int t = 0; t < M*N; ++t) {\n            assert(std::fabs((gpuC_h[t] - cpuC_h[t]) / std::fabs(cpuC_h[t])) <= TOLERANCE);\n        }\n\n   }\n    //Free up resources\n    CUDA_CHECK(cudaFreeAsync(colMajorA_d, stream));\n    CUDA_CHECK(cudaFreeAsync(colMajorB_d, stream));\n    CUDA_CHECK(cudaFreeAsync(rowMajorC_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    free(A_h);\n    free(B_h);\n    free(cpuC_h);\n    free(gpuC_h);\n}\n\n\n__global__ void k_mmaM16N8K16AcolBcol(__nv_bfloat16* colMajorA_d,      // column-major [mDim  kDim]\n                                     __nv_bfloat16* colMajorB_d,      // [kDim  nDim]\n                                     float* C,      // row-major [mDim  nDim]\n                                     int mDim,\n                                     int nDim,\n                                     int kDim) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/185", "date": "2025-07-30", "prompt": "Write a CUDA kernel that utilizes shared memory to compute an integer histogram, ensuring that the kernel expects sorted input to avoid bank collisions in shared memory.\n\nThe signature of the CUDA kernel is __global__ void k_computeHistogram(int * inputElements_d, int * histogram_d, int numElements, int numBinsPerSubset), where inputElements_d is a pointer to an array of integers used as the input elements, histogram_d is a pointer to an array of integers used as the histogram, numElements is the count of input elements, and numBinsPerSubset is the number of consecutive histogram bins to be computed per block of the launched grid.\n\n>>> k_computeHistogram({ 0, 32, 64, 96, 128, 160, 192, 224, 256, 288 }, { all elements are zero }, 10, 300) -> histogram_d: { only 10 elements have the value of 1, their indices are 0, 32, 64, 96, 128, 160, 192, 224, 256, 288; others are zero }\n>>> k_computeHistogram({ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }, { all elements are zero }, 10, 300) -> histogram_d: { only 10 elements have the value of 1, their indices are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9; others are zero }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <random>\n#include <algorithm>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Histogram and CUDA settings.\nconstexpr int NUM_HISTOGRAM_ELEMENTS = 10000;\n// Number of histogram bins per block as a distributed histogram.\nconstexpr int SHARED_MEMORY_BINS_PER_BLOCK = 300;\n\n// Test settings.\nconstexpr int NUM_TESTS = 7;\nconstexpr int DETERMINISTIC_RANDOM_SEED = 42;\nconstexpr int MAX_INPUT_ELEMENTS = 100000;\n\n__global__ void k_computeHistogram(int * inputElements_d, int * histogram_d, int numElements, int numBinsPerSubset);\n\nvoid launch() {\n    size_t sharedMemoryAllocation = sizeof(int) * SHARED_MEMORY_BINS_PER_BLOCK;\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    int maxBlockSize;\n    int gridSize;\n    CUDA_CHECK(cudaOccupancyMaxPotentialBlockSize(&gridSize, &maxBlockSize, (void*)k_computeHistogram, sharedMemoryAllocation));\n    // Allocating host memory.\n    int * testNumInputElements_h = new int[NUM_TESTS];\n    int * testInputElements_h = new int[NUM_TESTS * MAX_INPUT_ELEMENTS];\n    int * testHistogram_h = new int[NUM_HISTOGRAM_ELEMENTS];\n    int * inputElements_h = new int[MAX_INPUT_ELEMENTS];\n    int * histogram_h = new int[NUM_HISTOGRAM_ELEMENTS];\n    // Allocating device memory.\n    int * inputElements_d;\n    int * histogram_d;\n    CUDA_CHECK(cudaMallocAsync(&inputElements_d, sizeof(int) * MAX_INPUT_ELEMENTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&histogram_d, sizeof(int) * NUM_HISTOGRAM_ELEMENTS, stream));\n    int testIndex = 0;\n    // Test 1: Generating data that would cause maximum bank collisions if not sorted in the case of a very large dataset. However, when sorted, they are more likely to cause a broadcast or multicast within a warp rather than a bank conflict.\n    {\n        int numElements = 10;\n        testNumInputElements_h[testIndex] = numElements;\n        std::initializer_list<int> inputs = { 0, 32, 64, 96, 128, 160, 192, 224, 256, 288 };\n        std::copy(inputs.begin(), inputs.end(), &testInputElements_h[testIndex * MAX_INPUT_ELEMENTS]);\n        testIndex++;\n    }\n    // Test 2: Sequentially increasing values.\n    {\n        int numElements = 10;\n        testNumInputElements_h[testIndex] = numElements;\n        std::initializer_list<int> inputs = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n        std::copy(inputs.begin(), inputs.end(), &testInputElements_h[testIndex * MAX_INPUT_ELEMENTS]);\n        testIndex++;\n    }\n    // Test 3: Random input within the range of 0 to NUM_HISTOGRAM_ELEMENTS.\n    {\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_int_distribution<int> distribution(0, NUM_HISTOGRAM_ELEMENTS);\n        int numElements = MAX_INPUT_ELEMENTS;\n        testNumInputElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testInputElements_h[i + testIndex * MAX_INPUT_ELEMENTS] = distribution(generator);\n        }\n        // Sorting the input elements improves the memory access pattern within the CUDA kernel, leading to more likely coalesced and bank-conflict-free accesses.\n        std::sort(testInputElements_h + (testIndex * MAX_INPUT_ELEMENTS), testInputElements_h + (testIndex * MAX_INPUT_ELEMENTS + numElements));\n        testIndex++;\n    }\n    // Test 4: Odd numbers only.\n    {\n        int numElements = MAX_INPUT_ELEMENTS;\n        testNumInputElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testInputElements_h[i + testIndex * MAX_INPUT_ELEMENTS] = ((i % NUM_HISTOGRAM_ELEMENTS) % 2 == 1 ? (i % NUM_HISTOGRAM_ELEMENTS) : 1);\n        }\n        // Sorting the input elements improves the memory access pattern within the CUDA kernel, leading to more likely coalesced and bank-conflict-free accesses.\n        std::sort(testInputElements_h + (testIndex * MAX_INPUT_ELEMENTS), testInputElements_h + (testIndex * MAX_INPUT_ELEMENTS + numElements));\n        testIndex++;\n    }\n    // Test 5: Even numbers only.\n    {\n        int numElements = MAX_INPUT_ELEMENTS;\n        testNumInputElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testInputElements_h[i + testIndex * MAX_INPUT_ELEMENTS] = ((i % NUM_HISTOGRAM_ELEMENTS) % 2 == 0 ? (i % NUM_HISTOGRAM_ELEMENTS) : 0);\n        }\n        // Sorting the input elements improves the memory access pattern within the CUDA kernel, leading to more likely coalesced and bank-conflict-free accesses.\n        std::sort(testInputElements_h + (testIndex * MAX_INPUT_ELEMENTS), testInputElements_h + (testIndex * MAX_INPUT_ELEMENTS + numElements));\n        testIndex++;\n    }\n    // Test 6: All elements are the same.\n    {\n        int numElements = MAX_INPUT_ELEMENTS;\n        testNumInputElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testInputElements_h[i + testIndex * MAX_INPUT_ELEMENTS] = 35;\n        }\n        // Sorting the input elements improves the memory access pattern within the CUDA kernel, leading to more likely coalesced and bank-conflict-free accesses.\n        std::sort(testInputElements_h + (testIndex * MAX_INPUT_ELEMENTS), testInputElements_h + (testIndex * MAX_INPUT_ELEMENTS + numElements));\n        testIndex++;\n    }\n    //  Test 7: Random input ranging from 0 to NUM_HISTOGRAM_ELEMENTS / 100.\n    {\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_int_distribution<int> distribution(0, NUM_HISTOGRAM_ELEMENTS / 100);\n        int numElements = MAX_INPUT_ELEMENTS;\n        testNumInputElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testInputElements_h[i + testIndex * MAX_INPUT_ELEMENTS] = distribution(generator);\n        }\n        // Sorting the input elements improves the memory access pattern within the CUDA kernel, leading to more likely coalesced and bank-conflict-free accesses.\n        std::sort(testInputElements_h + (testIndex * MAX_INPUT_ELEMENTS), testInputElements_h + (testIndex * MAX_INPUT_ELEMENTS + numElements));\n        testIndex++;\n    }\n    int numBinsPerSubset = SHARED_MEMORY_BINS_PER_BLOCK;\n    for(int test = 0; test < testIndex; test++)\n    {\n        CUDA_CHECK(cudaMemsetAsync(histogram_d, 0, sizeof(int) * NUM_HISTOGRAM_ELEMENTS, stream));\n        int numElements = testNumInputElements_h[test];\n        for(int i = 0; i < numElements; i++) {\n            inputElements_h[i] = testInputElements_h[i + test * MAX_INPUT_ELEMENTS];\n        }\n        CUDA_CHECK(cudaMemcpyAsync(inputElements_d, inputElements_h, sizeof(int) * numElements, cudaMemcpyHostToDevice, stream));\n        void * args [] = { &inputElements_d, &histogram_d, &numElements, &numBinsPerSubset };        \n        int requiredBlocks = (NUM_HISTOGRAM_ELEMENTS + SHARED_MEMORY_BINS_PER_BLOCK - 1) / SHARED_MEMORY_BINS_PER_BLOCK;\n        int usedBlocks = gridSize < requiredBlocks ? gridSize : requiredBlocks;\n        // Grid: (usedBlocks, 1, 1)\n        // Block: (maxBlockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_computeHistogram, dim3(usedBlocks, 1, 1), dim3(maxBlockSize, 1, 1), args, sharedMemoryAllocation, stream));\n        CUDA_CHECK(cudaMemcpyAsync(histogram_h, histogram_d, sizeof(int) * NUM_HISTOGRAM_ELEMENTS, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing the resulting histogram to the host's solution.\n        for(int i = 0; i < NUM_HISTOGRAM_ELEMENTS; i++) {\n            testHistogram_h[i] = 0;\n        }\n        for(int i = 0; i < numElements; i++) {\n            int data = inputElements_h[i];\n            if(data >= 0 && data < NUM_HISTOGRAM_ELEMENTS) {\n                testHistogram_h[data]++;\n            }\n        }\n        for(int i = 0; i < NUM_HISTOGRAM_ELEMENTS; i++) {\n            assert(testHistogram_h[i] == histogram_h[i]);\n        }\n    }\n\n    // Releasing resources.\n    CUDA_CHECK(cudaFreeAsync(inputElements_d, stream));\n    CUDA_CHECK(cudaFreeAsync(histogram_d, stream));\n    delete [] testNumInputElements_h;\n    delete [] testInputElements_h;\n    delete [] testHistogram_h;\n    delete [] inputElements_h;\n    delete [] histogram_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_computeHistogram(int * inputElements_d, int * histogram_d, int numElements, int numBinsPerSubset) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/186", "date": "2025-07-30", "prompt": "Write a CUDA kernel to smooth the pixel values of an image using libcu++ array types for storing coefficient-related data and for performing the smoothing computation. For simplicity, treat any out-of-bounds pixels as a value of 0.0f. The smoothing filter should be 3x3 in size, with values of 0.25f for corners, 0.50f for edges, and 1.00f for the center.\n\nThe signature of the CUDA kernel is __global__ void k_blur(int width, int height, cuda::std::array<float, MAX_PIXELS>* pixel_d, cuda::std::array<float, MAX_PIXELS>* blurredPixel_d), where width refers to the width of the image, height corresponds to the height of the image, pixel_d is a pointer to the cuda::std::array of floats for input pixels on the device, and blurredPixel_d is a pointer to the cuda::std::array of floats for output pixels on the device.\n\n>>> k_blur(5, 4, { 0.00f, 1.00f, 0.00f, 1.00f, 0.00f, 1.00f ... alternating between 0.00f and 1.0f for 20 elements }, blurredPixel_d) -> blurredPixel_h: { 0.250f, 0.375f, 0.375f, 0.375f, 0.250f, 0.375f, 0.500f, 0.500f, 0.500f, 0.375f, 0.375f, 0.500f, 0.500f, 0.500f, 0.375f, 0.312f, 0.375f, 0.375f, 0.375f, 0.312f }\n>>> k_blur(5, 4, { 0.00f, 0.05f, 0.10f, 0.15f, 0.20f, 0.25f, 0.30f, 0.35f, 0.40f, 0.45f, 0.50f, 0.55f, 0.60f, 0.65f, 0.70f, 0.75f, 0.80f, 0.85f, 0.90f, 0.95f }, blurredPixel_d) -> blurredPixel_h: { 0.047f, 0.088f, 0.125f, 0.112f, 0.163f, 0.250f, 0.300f, 0.250f, 0.313f, 0.450f, 0.500f, 0.400f, 0.462f, 0.650f, 0.700f, 0.550f, 0.422f, 0.587f, 0.625f, 0.488f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <cuda/std/array>\n#include <cuda/std/cmath>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if (error != cudaSuccess) {                                \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Settings.\nconstexpr int MAX_PIXELS = 10000;\nconstexpr int BLUR_WIDTH = 3;\nconstexpr int BLUR_HEIGHT = 3;\nconstexpr int BLUR_CALCULATIONS_PER_PIXEL = BLUR_WIDTH * BLUR_HEIGHT;\nconstexpr float COEFFICIENT_0 = 0.25f;\nconstexpr float COEFFICIENT_1 = 0.50f;\nconstexpr float COEFFICIENT_2 = 0.25f;\nconstexpr float COEFFICIENT_3 = 0.50f;\nconstexpr float COEFFICIENT_4 = 1.0f;\nconstexpr float COEFFICIENT_5 = 0.50f;\nconstexpr float COEFFICIENT_6 = 0.25f;\nconstexpr float COEFFICIENT_7 = 0.50f;\nconstexpr float COEFFICIENT_8 = 0.25f;\n\n// Using initializer lists to initialize cuda::std::array.\ncuda::std::array<float, BLUR_CALCULATIONS_PER_PIXEL> blurCoefficients_h = { COEFFICIENT_0, COEFFICIENT_1, COEFFICIENT_2, COEFFICIENT_3, COEFFICIENT_4, COEFFICIENT_5, COEFFICIENT_6, COEFFICIENT_7, COEFFICIENT_8 };\n// Blur indices.\ncuda::std::array<int, BLUR_CALCULATIONS_PER_PIXEL> blurOffsetX_h = { -1, 0, 1, -1, 0, 1, -1, 0, 1 };\ncuda::std::array<int, BLUR_CALCULATIONS_PER_PIXEL> blurOffsetY_h = { -1, -1, -1, 0, 0, 0, 1, 1, 1 };\n\n__global__ void k_blur(int width, int height, cuda::std::array<float, MAX_PIXELS>* pixel_d, cuda::std::array<float, MAX_PIXELS>* blurredPixel_d);\n\nvoid launch() {\n    constexpr float ERROR_TOLERANCE = 1e-2f;\n    constexpr int DETERMINISTIC_RANDOM_NUMBER_SEED = 42;\n    constexpr float WAVE_FREQUENCY = 0.01f;\n    \n    int deviceId = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    size_t dynamicSharedMemorySize = 0;\n    int minGridSize;\n    int blockSize;\n    CUDA_CHECK(cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, (void*)k_blur, dynamicSharedMemorySize));\n    \n    // Allocating host memory.\n    cuda::std::array<float, MAX_PIXELS>* pixel_h = new cuda::std::array<float, MAX_PIXELS>();\n    cuda::std::array<float, MAX_PIXELS>* blurredPixel_h = new cuda::std::array<float, MAX_PIXELS>();\n\n    constexpr int NUM_EXAMPLES = 5;\n    int* testWidth = new int[NUM_EXAMPLES];\n    int* testHeight = new int[NUM_EXAMPLES];\n    float* testInput = new float[MAX_PIXELS * NUM_EXAMPLES];\n\n    // Allocating device memory.\n    cuda::std::array<float, MAX_PIXELS>* pixel_d;\n    cuda::std::array<float, MAX_PIXELS>* blurredPixel_d;\n    CUDA_CHECK(cudaMallocAsync(&pixel_d, sizeof(cuda::std::array<float, MAX_PIXELS>), stream));\n    CUDA_CHECK(cudaMallocAsync(&blurredPixel_d, sizeof(cuda::std::array<float, MAX_PIXELS>), stream));\n    \n    // Test 1\n    {\n        constexpr int WIDTH = 5;\n        constexpr int HEIGHT = 4;\n        int width = WIDTH;\n        int height = HEIGHT;\n        assert(WIDTH * HEIGHT <= MAX_PIXELS);\n        cuda::std::array<float, WIDTH * HEIGHT> input = { 0.00f, 1.00f, 0.00f, 1.00f, 0.00f, 1.00f, 0.00f, 1.00f, 0.00f, 1.00f, 0.00f, 1.00f, 0.00f, 1.00f, 0.00f, 1.00f, 0.00f, 1.00f, 0.00f, 1.00f };\n        cuda::std::array<float, WIDTH * HEIGHT> expectedOutput = { 0.250f, 0.375f, 0.375f, 0.375f, 0.250f, 0.375f, 0.500f, 0.500f, 0.500f, 0.375f, 0.375f, 0.500f, 0.500f, 0.500f, 0.375f, 0.312f, 0.375f, 0.375f, 0.375f, 0.312f };\n        for(int i = 0; i < width * height; i++) {\n            (*pixel_h)[i] = input[i];\n        }\n        // Blurring the pixels.\n        CUDA_CHECK(cudaMemcpyAsync(pixel_d, pixel_h, sizeof(float) * width * height, cudaMemcpyHostToDevice, stream));\n        void* args[4] = { &width, &height, &pixel_d, &blurredPixel_d };\n        int requiredBlocks = (width * height + blockSize - 1) / blockSize;\n        int usedBlocks = requiredBlocks < minGridSize ? requiredBlocks : minGridSize;\n        // Grid: (usedBlocks, 1, 1)\n        // Block: (blockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_blur, dim3(usedBlocks, 1, 1), dim3(blockSize, 1, 1), args, dynamicSharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(blurredPixel_h, blurredPixel_d, sizeof(float) * width * height, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < width * height; i++) {\n            assert(fabsf(expectedOutput[i] - (*blurredPixel_h)[i]) < ERROR_TOLERANCE);\n        }\n    }\n    // Test 2\n    {\n        constexpr int WIDTH = 4;\n        constexpr int HEIGHT = 5;\n        int width = WIDTH;\n        int height = HEIGHT;\n        assert(width * height <= MAX_PIXELS);\n        cuda::std::array<float, WIDTH * HEIGHT> input = { 0.00f, 0.05f, 0.10f, 0.15f, 0.20f, 0.25f, 0.30f, 0.35f, 0.40f, 0.45f, 0.50f, 0.55f, 0.60f, 0.65f, 0.70f, 0.75f, 0.80f, 0.85f, 0.90f, 0.95f };\n        cuda::std::array<float, WIDTH * HEIGHT> expectedOutput = { 0.047f, 0.088f, 0.125f, 0.112f, 0.163f, 0.250f, 0.300f, 0.250f, 0.313f, 0.450f, 0.500f, 0.400f, 0.462f, 0.650f, 0.700f, 0.550f, 0.422f, 0.587f, 0.625f, 0.488f };\n        for(int i = 0; i < width * height; i++) {\n            (*pixel_h)[i] = input[i];\n        }\n        // Blurring the pixels.\n        CUDA_CHECK(cudaMemcpyAsync(pixel_d, pixel_h, sizeof(float) * width * height, cudaMemcpyHostToDevice, stream));\n        void* args[4] = { &width, &height, &pixel_d, &blurredPixel_d };\n        int requiredBlocks = (width * height + blockSize - 1) / blockSize;\n        int usedBlocks = requiredBlocks < minGridSize ? requiredBlocks : minGridSize;\n        // Grid: (usedBlocks, 1, 1)\n        // Block: (blockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_blur, dim3(usedBlocks, 1, 1), dim3(blockSize, 1, 1), args, dynamicSharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(blurredPixel_h, blurredPixel_d, sizeof(float) * width * height, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < width * height; i++) {\n            assert(fabsf(expectedOutput[i] - (*blurredPixel_h)[i]) < ERROR_TOLERANCE);\n        }\n    }\n    \n    // Preparing data for tests 3, 4, 5, 6, and 7\n    int testIndex = 0;\n    // Test 3 initialization.\n    {\n        int width = 100;\n        int height = 100;\n        testWidth[testIndex] = width;\n        testHeight[testIndex] = height;\n        assert(width * height <= MAX_PIXELS);\n        std::mt19937 generator(DETERMINISTIC_RANDOM_NUMBER_SEED);\n        std::uniform_real_distribution<float> distribution(0.0f, 1.0f);\n        for(int i = 0; i < width * height; i++) {\n            testInput[i + testIndex * MAX_PIXELS] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Test 4 initialization.\n    {\n        int width = 80;\n        int height = 60;\n        testWidth[testIndex] = width;\n        testHeight[testIndex] = height;\n        assert(width * height <= MAX_PIXELS);\n        for(int i = 0; i < width * height; i++) {\n            testInput[i + testIndex * MAX_PIXELS] = cos((i % width) * WAVE_FREQUENCY) * sin((i / width) * WAVE_FREQUENCY);\n        }\n        testIndex++;\n    }\n    // Test 5 initialization.\n    {\n        int width = 64;\n        int height = 48;\n        testWidth[testIndex] = width;\n        testHeight[testIndex] = height;\n        assert(width * height <= MAX_PIXELS);\n        for(int i = 0; i < width * height; i++) {\n            float dx = (i % width) - (width / 2);\n            float dy = (i / width) - (height / 2);\n            float r = sqrt(dx * dx + dy * dy) + 1.0f;\n            testInput[i + testIndex * MAX_PIXELS] = 1.0f / r;\n        }\n        testIndex++;\n    }\n    // Test 6 initialization.\n    {\n        int width = 40;\n        int height = 30;\n        testWidth[testIndex] = width;\n        testHeight[testIndex] = height;\n        assert(width * height <= MAX_PIXELS);\n        for(int i = 0; i < width * height; i++) {\n            testInput[i + testIndex * MAX_PIXELS] = 1.0f;\n        }\n        testIndex++;\n    }\n    // Test 7 initialization.\n    {\n        int width = 34;\n        int height = 54;\n        testWidth[testIndex] = width;\n        testHeight[testIndex] = height;\n        assert(width * height <= MAX_PIXELS);\n        for(int i = 0; i < width * height; i++) {\n            testInput[i + testIndex * MAX_PIXELS] = ((i % 3) == 0);\n        }\n        testIndex++;\n    }\n    // Iterating tests 3, 4, 5, 6, and 7.\n    for(int test = 0; test < testIndex; test++)\n    {\n        int width = testWidth[test];\n        int height = testHeight[test];\n        for(int i = 0; i < width * height; i++) {\n            (*pixel_h)[i] = testInput[i + test * MAX_PIXELS];\n        }\n        // Blurring the pixels.\n        CUDA_CHECK(cudaMemcpyAsync(pixel_d, pixel_h, sizeof(float) * width * height, cudaMemcpyHostToDevice, stream));\n        void* args[4] = { &width, &height, &pixel_d, &blurredPixel_d };\n        int requiredBlocks = (width * height + blockSize - 1) / blockSize;\n        int usedBlocks = requiredBlocks < minGridSize ? requiredBlocks : minGridSize;\n        // Grid: (usedBlocks, 1, 1)\n        // Block: (blockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_blur, dim3(usedBlocks, 1, 1), dim3(blockSize, 1, 1), args, dynamicSharedMemorySize, stream));\n        CUDA_CHECK(cudaMemcpyAsync(blurredPixel_h, blurredPixel_d, sizeof(float) * width * height, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < width * height; i++) {\n            cuda::std::div_t pixelPos = cuda::std::div(i, width);\n            int pixelX = pixelPos.rem;\n            int pixelY = pixelPos.quot;\n            int arrayIndex = 0;\n            float color = 0.0f;\n            float sumOfCoefficients = 0.0f;\n            // Applying a blur operation to the pixel at (pixelX, pixelY) by utilizing cuda::std::array for the offsets of neighboring pixels and the coefficients of the blur operation.\n            for(auto coeff : blurCoefficients_h) { \n                int neighborX = pixelX + blurOffsetX_h[arrayIndex];\n                int neighborY = pixelY + blurOffsetY_h[arrayIndex];\n                if (neighborX >= 0 && neighborX < width && neighborY >= 0 && neighborY < height) {\n                    color = cuda::std::fmaf(coeff, (*pixel_h)[neighborX + neighborY * width], color);\n                }\n                sumOfCoefficients += coeff;\n                arrayIndex++;\n            };\n            assert(fabsf((*blurredPixel_h)[pixelX + pixelY * width] - color / sumOfCoefficients) < ERROR_TOLERANCE);\n        }\n    }\n    // Freeing device memory.\n    CUDA_CHECK(cudaFreeAsync(pixel_d, stream));\n    CUDA_CHECK(cudaFreeAsync(blurredPixel_d, stream));\n    // Freeing host memory.\n    delete [] pixel_h;\n    delete [] blurredPixel_h;\n    delete [] testWidth;\n    delete [] testHeight;\n    delete [] testInput;\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n    // Freeing other resources.\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n__global__ void k_blur(int width, int height, cuda::std::array<float, MAX_PIXELS>* pixel_d, cuda::std::array<float, MAX_PIXELS>* blurredPixel_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/187", "date": "2025-07-30", "prompt": "Write a CUDA kernel to perform warp-based stream compaction on 32-element segments of a float array, and compact only the elements that exceed a threshold value. In each segment, the output should first include compacted elements, followed by default values, and the results of each segment should be stored independently.\n\nThe signature of the CUDA kernel is __global__ void k_compactElementsOfSegmentsWithThreshold(int numSegments, float* array_d, float threshold, float defaultValue), where numSegments is the number of segments that are 32 elements each, array_d is the array that contains segments, threshold is the value used for predication with greater-than operator, and defaultValue is the value that replaces any non-compacted element's value.\n\n>>> k_compactElementsOfSegmentsWithThreshold(1, { 0.0f, 0.1f, 0.2f, 0.3f, 0.4f, 0.5f, 0.6f, 0.7f, 0.8f, 0.9f, 1.0f, the rest are 0.0f }, 0.5f, 0.0f) -> array_d: { 0.6f, 0.7f, 0.8f, 0.9f, 1.0f, the rest are 0.0f }\n>>> k_compactElementsOfSegmentsWithThreshold(1, { -1.0f, 1.0f, -1.0f, 1.0f, -1.0f, 1.0f, -1.0f, 1.0f, -1.0f, the rest are 0.0f }, 0.0f, 0.0f) -> array_d: { 1f, 1f, 1f, 1f, the rest are 0.0f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if (error != cudaSuccess) {                                \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n// This must stay as 32.\nconstexpr int SEGMENT_SIZE = 32;\n__global__ void k_compactElementsOfSegmentsWithThreshold(int numSegments, float* array_d, float threshold, float defaultValue);\n\n\nvoid launch() {\n    constexpr int NUM_MAX_SEGMENTS = 1000;\n    constexpr int NUM_MAX_ELEMENTS = SEGMENT_SIZE * NUM_MAX_SEGMENTS;\n    constexpr int NUM_TESTS = 7;\n    constexpr float ERROR_TOLERANCE = 1e-4f;\n    int deviceIndex = 0;\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceIndex));\n    int blockSize = 256;\n    int maxBlocks = (deviceProperties.maxThreadsPerMultiProcessor * deviceProperties.multiProcessorCount) / blockSize;\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    // Allocating host buffers.\n    float* array_h = new float[NUM_MAX_ELEMENTS];\n    float* testArray_h = new float[NUM_TESTS * NUM_MAX_ELEMENTS];\n    int* testNumSegments_h = new int[NUM_TESTS];\n    float* testThresholds_h = new float[NUM_TESTS];\n    float* testDefaultValues = new float[NUM_TESTS];\n    \n    // Allocating device buffers.\n    float* array_d;\n    CUDA_CHECK(cudaMallocAsync(&array_d, sizeof(float)* NUM_MAX_ELEMENTS, stream));\n    \n    int testIndex = 0;\n    // Test 1\n    {\n        int numSegments = 1;\n        float threshold = 0.5f;\n        float defaultValue = 0.0f;\n        testNumSegments_h[testIndex] = numSegments;\n        testThresholds_h[testIndex] = threshold;\n        testDefaultValues[testIndex] = defaultValue;\n        std::initializer_list<float> data = { \n            0.0f, 0.1f, 0.2f, 0.3f, 0.4f, 0.5f, 0.6f, 0.7f, 0.8f, 0.9f, 1.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f\n        };\n        std::copy(data.begin(), data.end(), &testArray_h[testIndex * NUM_MAX_ELEMENTS]);\n        testIndex++;\n    }\n    // Test 2\n    {\n        int numSegments = 1;\n        float threshold = 0.0f;\n        float defaultValue = 0.0f;\n        testNumSegments_h[testIndex] = numSegments;\n        testThresholds_h[testIndex] = threshold;\n        testDefaultValues[testIndex] = defaultValue;\n        std::initializer_list<float> data = { \n            -1.0f, 1.0f, -1.0f, 1.0f, -1.0f, 1.0f, -1.0f, 1.0f, -1.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f\n        };\n        std::copy(data.begin(), data.end(), &testArray_h[testIndex * NUM_MAX_ELEMENTS]);\n        testIndex++;\n    }\n    // Test 3\n    {\n        int numSegments = NUM_MAX_SEGMENTS;\n        float threshold = 0.0f;\n        float defaultValue = 0.0f;\n        testNumSegments_h[testIndex] = numSegments;\n        testThresholds_h[testIndex] = threshold;\n        testDefaultValues[testIndex] = defaultValue;\n        for (int i = 0; i < numSegments * SEGMENT_SIZE; i++) {\n            testArray_h[testIndex * NUM_MAX_ELEMENTS + i] = i - numSegments * SEGMENT_SIZE * 0.5f;\n        }\n        testIndex++;\n    }\n    // Test 4\n    {\n        int numSegments = NUM_MAX_SEGMENTS;\n        float threshold = -1.0f;\n        float defaultValue = 0.0f;\n        testNumSegments_h[testIndex] = numSegments;\n        testThresholds_h[testIndex] = threshold;\n        testDefaultValues[testIndex] = defaultValue;\n        for (int i = 0; i < numSegments * SEGMENT_SIZE; i++) {\n            testArray_h[testIndex * NUM_MAX_ELEMENTS + i] = 1.0f;\n        }\n        testIndex++;\n    }\n    // Test 5\n    {\n        int numSegments = NUM_MAX_SEGMENTS;\n        float threshold = 1.0f;\n        float defaultValue = 0.0f;\n        testNumSegments_h[testIndex] = numSegments;\n        testThresholds_h[testIndex] = threshold;\n        testDefaultValues[testIndex] = defaultValue;\n        for (int i = 0; i < numSegments * SEGMENT_SIZE; i++) {\n            testArray_h[testIndex * NUM_MAX_ELEMENTS + i] = -1.0f;\n        }\n        testIndex++;\n    }\n    // Test 6\n    {\n        int numSegments = NUM_MAX_SEGMENTS;\n        float threshold = 3.14f;\n        float defaultValue = 0.0f;\n        testNumSegments_h[testIndex] = numSegments;\n        testThresholds_h[testIndex] = threshold;\n        testDefaultValues[testIndex] = defaultValue;\n        for (int i = 0; i < numSegments * SEGMENT_SIZE; i++) {\n            testArray_h[testIndex * NUM_MAX_ELEMENTS + i] = ((i % 2) ? NAN : 3.1415f);\n        }\n        testIndex++;\n    }\n    // Test 7\n    {\n        int numSegments = NUM_MAX_SEGMENTS;\n        float threshold = 50.0f;\n        float defaultValue = 0.0f;\n        testNumSegments_h[testIndex] = numSegments;\n        testThresholds_h[testIndex] = threshold;\n        testDefaultValues[testIndex] = defaultValue;\n        for (int i = 0; i < numSegments * SEGMENT_SIZE; i++) {\n            testArray_h[testIndex * NUM_MAX_ELEMENTS + i] = (i % 100);\n        }\n        testIndex++;\n    }\n    for (int test = 0; test < testIndex; test++)\n    {\n        int numSegments = testNumSegments_h[test];\n        float threshold = testThresholds_h[test];\n        float defaultValue = testDefaultValues[test];\n        int numElements = numSegments * SEGMENT_SIZE;\n        for (int i = 0; i < numSegments * SEGMENT_SIZE; i++) {\n                array_h[i] = testArray_h[test * NUM_MAX_ELEMENTS + i];\n        }\n        CUDA_CHECK(cudaMemcpyAsync(array_d, array_h, sizeof(float) * numElements, cudaMemcpyHostToDevice, stream));\n        void* args[4] = { &numSegments, &array_d, &threshold, &defaultValue };\n        int numWarpsPerBlock = blockSize / deviceProperties.warpSize;\n        // Each warp processes a segment.\n        int requiredBlocks = (numSegments + numWarpsPerBlock - 1) / numWarpsPerBlock;\n        int usedBlocks = requiredBlocks < maxBlocks ? requiredBlocks : maxBlocks;\n        int sharedMem = blockSize * sizeof(float);\n        // Grid: (usedBlocks, 1, 1)\n        // Block: (blockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_compactElementsOfSegmentsWithThreshold, dim3(usedBlocks, 1, 1), dim3(blockSize, 1, 1), args, sharedMem, stream));\n        CUDA_CHECK(cudaMemcpyAsync(array_h, array_d, sizeof(float) * numElements, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < numSegments; i++) {\n            int compactIndex = 0;\n            // Identifying compacted elements.\n            for (int j = 0; j < SEGMENT_SIZE; j++) {\n                float testData = testArray_h[test * NUM_MAX_ELEMENTS + i * SEGMENT_SIZE + j];\n                if (testData > threshold) {\n                    assert(fabsf(array_h[compactIndex + i * SEGMENT_SIZE] - testData) < ERROR_TOLERANCE);\n                    compactIndex++;\n                }\n            }\n            // Identifying default elements.\n            for (; compactIndex < SEGMENT_SIZE; compactIndex++) {\n                assert(fabsf(array_h[compactIndex + i * SEGMENT_SIZE] - defaultValue) < ERROR_TOLERANCE);\n            }\n        }\n    }\n    \n    // Deallocating device buffers.\n    CUDA_CHECK(cudaFreeAsync(array_d, stream));\n    // Deallocating host buffers.\n    delete [] array_h;\n    delete [] testArray_h;\n    delete [] testNumSegments_h;\n    delete [] testThresholds_h;\n    delete [] testDefaultValues;\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_compactElementsOfSegmentsWithThreshold(int numSegments, float* array_d, float threshold, float defaultValue) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/188", "date": "2025-07-30", "prompt": "Write a CUDA kernel to broadcast a message from one warp to others in a complete binary tree pattern. In a complete binary tree, every warp is a node, accessible by index k. Each warp automatically receives the message from its parent and transmits it to both child nodes if the message is valid. Each thread in a warp transfers data to another warp's thread in a one-to-one correspondence with the same message ID, such as a one-based warp lane index. The message consists of a message ID and the message buffer. The message is stored in the most significant 24 bits, while the ID is stored in the least significant 8 bits. Message storage and access should ensure coalesced memory access. Each warp should atomically read the message from its own index k and then re-send it to the indices k * 2 + 1 and k * 2 + 2, where k is the warp index. The output of the kernel (from all warps) should store the 24-bit message. The first warp will use the input, add its thread index, and then broadcast it to its children.\n\nThe signature of the device function for communication is __device__ __forceinline__ uint32_t d_broadcast(uint32_t messageId, int warpLane, uint32_t * message_d, int numCommunicatingWarps, int globalWarpId), where messageId is a number between 0 and 255 to be used for identifying a message, warpLane is the index of warp-local thread index from 0 to warpSize - 1, message_d is a pointer to the array of messages to be used between warps, numCommunicatingWarps is the number of warps to get the message including the broadcaster warp, and globalWarpId is the global index of warp calling this device function.\n\nThe signature of the CUDA kernel is __global__ void k_broadcastWithHierarchicalPath(uint32_t input, uint32_t * message_d, int numCommunicatingWarps, int * output_d), where input is the data read from the first warp and computed for a result to be sent to other warps in grid, message_d is a pointer to the array of messages to be used by warps for communicating, numCommunicatingWarps is the number of warps to be communicated, output_d is a pointer to the output elements.\n\n>>> k_broadcastWithHierarchicalPath(1, message_d, 1, int * output_d) -> output_d: { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32 }\n>>> k_broadcastWithHierarchicalPath(2, message_d, 2, int * output_d) -> output_d: { 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <cooperative_groups.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n// Test-related constants.\nconstexpr uint32_t MAXIMUM_NUMBER_OF_WARPS_TO_COMMUNICATE = 500;\nconstexpr uint32_t NUMBER_OF_TESTS = 7;\n\n// Algorithm-related constants.\nconstexpr uint32_t BITS_PER_BYTE = 8;\nconstexpr uint32_t BITS_PER_MESSAGE = sizeof(uint32_t) * BITS_PER_BYTE;\nconstexpr uint32_t DATA_BITS = 24;\nconstexpr uint32_t ID_BITS = BITS_PER_MESSAGE - DATA_BITS;\nconstexpr uint32_t ID_MASK = ((1 << ID_BITS) - 1);\n\n// Function to check if a message with a specific ID is received, and then re-send it to two other warps only once and return the decoded data.\n__device__ __forceinline__ uint32_t d_broadcast(uint32_t messageId, \n                                                int warpLane, \n                                                uint32_t * message_d, \n                                                int numCommunicatingWarps, \n                                                bool broadcaster, \n                                                int globalWarpId);\n\n// Kernel function that sends a sample data from first warp to other warps in the grid without synchronization and with minimal contention.\n__global__ void k_broadcastWithHierarchicalPath(uint32_t input, \n                                                uint32_t * message_d, \n                                                int numCommunicatingWarps, \n                                                int * output_d);\n\nvoid launch() {\n    int deviceId = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    assert(deviceProperties.cooperativeLaunch);\n    int warpSize = deviceProperties.warpSize;\n    int maximumNumberOfOutputElements = MAXIMUM_NUMBER_OF_WARPS_TO_COMMUNICATE * warpSize;\n    int maximumNumberOfMessageElements = MAXIMUM_NUMBER_OF_WARPS_TO_COMMUNICATE * warpSize;\n    uint32_t * message_d;\n    int * output_d;\n    // Allocating the host buffer for output.\n    int * output_h = new int[maximumNumberOfOutputElements];\n    // Allocating and initializing device buffers for output and messaging.\n    CUDA_CHECK(cudaMallocAsync(&message_d, sizeof(uint32_t) * maximumNumberOfMessageElements, stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, sizeof(int) * maximumNumberOfOutputElements, stream));\n    CUDA_CHECK(cudaMemsetAsync(message_d, 0, sizeof(uint32_t) * maximumNumberOfMessageElements, stream));\n    CUDA_CHECK(cudaMemsetAsync(output_d, 0, sizeof(int) * maximumNumberOfOutputElements, stream));\n    // Preparing the test data. The outputs should reflect these values + warp lane index.\n    int testInputs[NUMBER_OF_TESTS] = { 1, 2, 10, 100, 1000, 25, 1000000 };\n    int testNumCommunicatingThreads[NUMBER_OF_TESTS] = { 1, 2, 30, 2, 20, 500, 100 };\n\n    // Iterating the tests.\n    for(int test = 0; test < NUMBER_OF_TESTS; test++)\n    {\n        uint32_t input = testInputs[test];\n        int numCommunicatingWarps = testNumCommunicatingThreads[test];\n        int numBlocksPossiblePerSM;\n        int numThreadsInBlock = 1024;\n        int numSM = deviceProperties.multiProcessorCount;\n        CUDA_CHECK(cudaOccupancyMaxActiveBlocksPerMultiprocessor(   &numBlocksPossiblePerSM,\n                                                                    k_broadcastWithHierarchicalPath,\n                                                                    numThreadsInBlock,\n                                                                    0));\n        int numBlocksRequired = (numCommunicatingWarps * warpSize + numThreadsInBlock - 1) / numThreadsInBlock;\n        int numBlocksPossible = numSM * numBlocksPossiblePerSM;\n        // The algorithm requires all warps to be in flight to function correctly.\n        assert(numBlocksPossible >= numBlocksRequired);\n        int numBlocksUsed = numBlocksRequired < numBlocksPossible ? numBlocksRequired : numBlocksPossible;\n\n        // Grid: (numBlocksUsed, 1, 1)\n        // Block: (numThreadsInBlock, 1, 1)\n        dim3 gridDim(numBlocksUsed, 1, 1);\n        dim3 blockDim(numThreadsInBlock, 1, 1);\n        void * args[4] = { (void*)&input, (void*)&message_d, (void*)&numCommunicatingWarps, (void*)&output_d };\n        CUDA_CHECK(cudaLaunchCooperativeKernel((void*)k_broadcastWithHierarchicalPath, gridDim, blockDim, args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(int) * maximumNumberOfOutputElements, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < numCommunicatingWarps * warpSize; i++) {\n            int warpLane = (i % deviceProperties.warpSize);\n            assert(output_h[i] == (input + warpLane));\n        }\n    }\n    \n    CUDA_CHECK(cudaFreeAsync(message_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    delete [] output_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__device__ __forceinline__ uint32_t d_broadcast(uint32_t messageId, \n                                                int warpLane, \n                                                uint32_t * message_d, \n                                                int numCommunicatingWarps, \n                                                int globalWarpId) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/189", "date": "2025-07-30", "prompt": "Write a function that uses CUDA streams and events to run k_erosion and k_dilation kernels, requiring the same input but storing their outputs separately in different arrays. The function should have two streams, the first stream should copy data from host to device and launch the k_erosion kernel, while the copyEnd event waits till copy completion, the second stream should wait for the copyEnd event and launch the k_dilation kernel in parallel to the k_erosion kernel. Finally, both streams should copy data from the device to the host.\n\nThe signature of the function is void run(uint8_t* inputImage_h, uint8_t* inputImage_d, uint8_t* erosionOutputImage_d, uint8_t* dilationOutputImage_d, uint8_t* erosionOutputImage_h, uint8_t* dilationOutputImage_h, int width, int height, cudaStream_t stream1, cudaStream_t stream2, cudaEvent_t copyEnd), where inputImage_h is the host pointer to the input binary image, inputImage_d is the device pointer to the input binary image, erosionOutputImage_d is the device pointer for storing erosion results, dilationOutputImage_d is the device pointer for storing dilation results, erosionOutputImage_h is the host  pointer for storing erosion results, dilationOutputImage_h is the host pointer for storing dilation results, width, height is dimensions of the input image, stream1, stream2 is CUDA streams for concurrent execution, copyEnd is events for timing memory transfers.\n\nThe signatures of the CUDA Kernels are\n__global__ void k_erosion(uint8_t* input, uint8_t* output, int width, int height),\n__global__ void k_dilation(uint8_t* input, uint8_t* output, int width, int height),\nwhere input is pointer to the input binary image containing pixel values of either 0 (black) or 1 (white), output is pointer to the output binary image used to store the results of the erosion or dilation operation, width is an integer specifying the horizontal dimension of the image in pixels, height is an integer specifying the vertical dimension of the image in pixels.\n\n>>> k_erosion({1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1}, output, 5, 6, 3)->output:{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n>>> k_dilation({1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1}, output, 5, 6, 3)->output:{1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1};\n\n>>> k_erosion({0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0}, output, 6, 7, 3)->output: {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n>>> k_dilation({0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0}, output, 6, 7, 3)->output: {0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0};\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <cstdio>\n#include <limits.h>\n#include <assert.h>\n#include <stdint.h>\n#include <cuda_runtime.h>\n\n#define FILTER_SIZE 3\n#define HALF_FILTER_SIZE (FILTER_SIZE/2)\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t error = call; \\\n        if (error != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", \\\n                    cudaGetErrorString(error), __FILE__, __LINE__); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void k_erosion(uint8_t* input, uint8_t* output, int width, int height) {\n    int strideX = blockDim.x * gridDim.x;\n    int strideY = blockDim.y * gridDim.y;\n\n    for (int y = blockIdx.y * blockDim.y + threadIdx.y; y < height; y += strideY) {\n        for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < width; x += strideX) {\n            uint8_t minVal = 1;\n\n            for (int ky = -HALF_FILTER_SIZE; ky <= HALF_FILTER_SIZE; ky++) {\n                for (int kx = -HALF_FILTER_SIZE; kx <= HALF_FILTER_SIZE; kx++) {\n                    int nx = x + kx;\n                    int ny = y + ky;\n                    if (nx >= 0 && nx < width && ny >= 0 && ny < height) {\n                        minVal = min(minVal, input[ny * width + nx]);\n                    }\n                }\n            }\n            output[y * width + x] = minVal;\n        }\n    }\n}\n\n__global__ void k_dilation(uint8_t* input, uint8_t* output, int width, int height) {\n    int strideX = blockDim.x * gridDim.x;\n    int strideY = blockDim.y * gridDim.y;\n\n    for (int y = blockIdx.y * blockDim.y + threadIdx.y; y < height; y += strideY) {\n        for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < width; x += strideX) {\n            uint8_t maxVal = 0;\n\n            for (int ky = -HALF_FILTER_SIZE; ky <= HALF_FILTER_SIZE; ky++) {\n                for (int kx = -HALF_FILTER_SIZE; kx <= HALF_FILTER_SIZE; kx++) {\n                    int nx = x + kx;\n                    int ny = y + ky;\n                    if (nx >= 0 && nx < width && ny >= 0 && ny < height) {\n                        maxVal = max(maxVal, input[ny * width + nx]);\n                    }\n                }\n            }\n            output[y * width + x] = maxVal;\n        }\n    }\n}\n\nvoid run(uint8_t* inputImage_h, uint8_t* inputImage_d, uint8_t* erosionOutputImage_d, uint8_t* dilationOutputImage_d, uint8_t* erosionOutputImage_h, uint8_t* dilationOutputImage_h,\n         int width, int height, cudaStream_t stream1, cudaStream_t stream2, cudaEvent_t copyEnd);\n\nvoid launch() {\n    constexpr int TEST_CASES = 7;\n    constexpr int MAX_INPUT_IMAGE_WIDTH = 9;\n    constexpr int MAX_INPUT_IMAGE_HEIGHT = 9;\n    constexpr int MAX_IMAGE_DIMENSIONS = 2;\n    constexpr int IMAGE_HEIGHT_INDEX = 0;\n    constexpr int IMAGE_WIDTH_INDEX = 1;\n\n    // Create CUDA events for timing and synchronization\n    cudaEvent_t copyEnd;\n    CUDA_CHECK(cudaEventCreate(&copyEnd));\n\n    // CUDA Streams\n    cudaStream_t stream1, stream2;\n\n    // Create streams with flags (non-blocking)\n    CUDA_CHECK(cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking));\n    CUDA_CHECK(cudaStreamCreateWithFlags(&stream2, cudaStreamNonBlocking));\n\n    // Device Memory Allocation\n    uint8_t *inputImage_d;\n    uint8_t *erosionOutputImage_d;\n    uint8_t *dilationOutputImage_d;\n\n    CUDA_CHECK(cudaMallocAsync((void**)&inputImage_d,\n                              MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT * sizeof(uint8_t),\n                              stream1));\n    CUDA_CHECK(cudaMallocAsync((void**)&erosionOutputImage_d,\n                              MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT * sizeof(uint8_t),\n                              stream1));\n    CUDA_CHECK(cudaMallocAsync((void**)&dilationOutputImage_d,\n                              MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT * sizeof(uint8_t),\n                              stream1));\n\n    // Test Data Initialization\n    int inputImageWidthHeight[TEST_CASES][MAX_IMAGE_DIMENSIONS] = {\n        {4, 5}, {5, 6}, {6, 7}, {7, 8}, {8, 8}, {9, 7}, {9, 9}\n    };\n\n    uint8_t inputImage_h[TEST_CASES][MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT] = {\n    // 4x5 Solid Square (All 1s)\n    {1, 1, 1, 1, 1,   1, 1, 1, 1, 1,   1, 1, 1, 1, 1,   1, 1, 1, 1, 1},\n\n    // 5x6 Hollow Square\n    {1, 1, 1, 1, 1, 1,   1, 0, 0, 0, 0, 1,   1, 0, 0, 0, 0, 1,   1, 0, 0, 0, 0, 1,   1, 1, 1, 1, 1, 1},\n\n    // 6x7 Plus (+) Shape\n    {0, 0, 0, 1, 0, 0, 0,   0, 0, 0, 1, 0, 0, 0,   0, 0, 0, 1, 0, 0, 0,   1, 1, 1, 1, 1, 1, 1,   0, 0, 0, 1, 0, 0, 0,   0, 0, 0, 1, 0, 0, 0},\n\n    // 7x8 Cross (X) Shape\n    {1, 0, 0, 0, 0, 0, 0, 1,   0, 1, 0, 0, 0, 0, 1, 0,   0, 0, 1, 0, 0, 1, 0, 0,   0, 0, 0, 1, 1, 0, 0, 0,   0, 0, 1, 0, 0, 1, 0, 0,   0, 1, 0, 0, 0, 0, 1, 0,   1, 0, 0, 0, 0, 0, 0, 1},\n\n    // 8x8 Diagonal Stripe\n    {1, 0, 0, 0, 0, 0, 0, 0,   0, 1, 0, 0, 0, 0, 0, 0,   0, 0, 1, 0, 0, 0, 0, 0,   0, 0, 0, 1, 0, 0, 0, 0,   0, 0, 0, 0, 1, 0, 0, 0,   0, 0, 0, 0, 0, 1, 0, 0,   0, 0, 0, 0, 0, 0, 1, 0,   0, 0, 0, 0, 0, 0, 0, 1},\n\n    // 9x7 Vertical Bar\n    {0, 0, 1, 1, 1, 0, 0,   0, 0, 1, 1, 1, 0, 0,   0, 0, 1, 1, 1, 0, 0,   0, 0, 1, 1, 1, 0, 0,   0, 0, 1, 1, 1, 0, 0,   0, 0, 1, 1, 1, 0, 0,   0, 0, 1, 1, 1, 0, 0,   0, 0, 1, 1, 1, 0, 0,   0, 0, 1, 1, 1, 0, 0},\n\n    // 9x9 Horizontal Bar\n    {0, 0, 0, 0, 0, 0, 0, 0, 0,   0, 0, 0, 0, 0, 0, 0, 0, 0,   0, 0, 0, 0, 0, 0, 0, 0, 0,   1, 1, 1, 1, 1, 1, 1, 1, 1,   1, 1, 1, 1, 1, 1, 1, 1, 1,   0, 0, 0, 0, 0, 0, 0, 0, 0,   0, 0, 0, 0, 0, 0, 0, 0, 0,   0, 0, 0, 0, 0, 0, 0, 0, 0,   0, 0, 0, 0, 0, 0, 0, 0, 0},\n    };\n\n    uint8_t expectedErosion_h[TEST_CASES][MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT] = {\n        {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1},\n        {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},\n        {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},\n        {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},\n        {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},\n        {0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0},\n        {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}\n    };\n\n    uint8_t expectedDilation_h[TEST_CASES][MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT] = {\n        {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1},\n        {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1},\n        {0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0},\n        {1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1},\n        {1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1},\n        {0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0},\n        {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}\n    };\n\n    uint8_t erosionOutputImage_h[MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT];\n    uint8_t dilationOutputImage_h[MAX_INPUT_IMAGE_WIDTH * MAX_INPUT_IMAGE_HEIGHT];\n\n    // Execute Test Cases\n    for (int testCase = 0; testCase < TEST_CASES; testCase++) {\n        int inputImageHeight = inputImageWidthHeight[testCase][IMAGE_HEIGHT_INDEX];\n        int inputImageWidth = inputImageWidthHeight[testCase][IMAGE_WIDTH_INDEX];\n\n        run(inputImage_h[testCase], inputImage_d, erosionOutputImage_d, dilationOutputImage_d, erosionOutputImage_h, dilationOutputImage_h,\n            inputImageWidth, inputImageHeight, stream1, stream2, copyEnd);\n\n        // Verify results\n        for (int y = 0; y < inputImageHeight; y++) {\n            for (int x = 0; x < inputImageWidth; x++) {\n                assert(erosionOutputImage_h[y * inputImageWidth + x] ==\n                       expectedErosion_h[testCase][y * inputImageWidth + x]);\n                assert(dilationOutputImage_h[y * inputImageWidth + x] ==\n                       expectedDilation_h[testCase][y * inputImageWidth + x]);\n            }\n        }\n    }\n\n    // Cleanup\n    CUDA_CHECK(cudaFreeAsync(inputImage_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(erosionOutputImage_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(dilationOutputImage_d, stream2));\n    CUDA_CHECK(cudaStreamDestroy(stream1));\n    CUDA_CHECK(cudaStreamDestroy(stream2));\n\n    // Destroy events\n    CUDA_CHECK(cudaEventDestroy(copyEnd));\n}\n\nvoid run(uint8_t* inputImage_h, uint8_t* inputImage_d, uint8_t* erosionOutputImage_d, uint8_t* dilationOutputImage_d, uint8_t* erosionOutputImage_h, uint8_t* dilationOutputImage_h,\n         int width, int height, cudaStream_t stream1, cudaStream_t stream2, cudaEvent_t copyEnd) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/190", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute consecutive powers (ranging from square up to a specified exponent) for each element in an input array with vectorized memory loads from global memory.The input array should be padded such that its length is a multiple of four, providing proper alignment for vectorized memory operations. \n\nThe signature of the function is __global__ void k_computeConsecutivePowers(const float *input_d, float4 *output_d4, int exponent, int numElements), where input_d is a pointer to an array of floating-point values padded to a multiple of 4 to support float4 vectorized access, output_d4 is a pointer to an array of float4 vectors which stores the computed powers, exponent is an integer specifying the maximum power to compute, and numElements is the number of elements in the input array after padding.\n\n>>> k_computeConsecutivePowers({x0, x1, x2, x3, x4}, output_d, 3, 5) -> output_d: ({x0^2, x1^2, x2^2, x3^2, x4^2, x0^3, x1^3, x2^3, x3^3, x4^3})\n>>> k_computeConsecutivePowers({1.1, 1.2, 2, 2.1, 3}, output_d, 3, 5) -> output_d: ({1.21, 1.44, 4.0, 4.41, 9.0, 1.331, 1.728, 8.0, 9.261, 27.0})\n>>> k_computeConsecutivePowers({2.2, 3.1, 1.5, 1.8, 4}, output_d, 3, 5) -> output_d: ({4.84, 9.61, 2.25, 3.24, 16.0, 10.648, 29.791, 3.375, 5.832, 64.0})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <assert.h>\n#include <cuda_runtime.h>\n\n#define SET_TO_ZERO 0\n#define EPSILON 1e-1\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\nconst int VECTOR_WIDTH = 4;\nconst int EXPONENT_START = 2;\n\n__global__ void k_computeConsecutivePowers(const float *input_d, float4 *output_d4, int exponent, int numElements);\n\nvoid launch() {\n\n    //Initialize Constants\n    const int TEST_CASE_COUNT = 7;\n    const int NUMBER_OF_THREADS_PER_BLOCK = 32;\n    \n    //Initialise Test Data\n    //Test Data Dimensions\n    int inputSize[TEST_CASE_COUNT] = {6, 9, 12, 16, 32, 48, 64};\n    \n    //Identify max input size\n    int maxInputSize = *std::max_element(inputSize, inputSize + TEST_CASE_COUNT);\n    \n    //Input Data For Test\n    float input_h[TEST_CASE_COUNT][maxInputSize] = {\n        //Test Case - 1\n        {1, 2, 3, 4, 5, 6},\n        //Test Case - 2\n        {5.15, 1.22, 4.13, 2.52, 3.64, 1.54, 5.36, 5.78, 4.55},\n        //Test Case - 3\n        {2.10, 6.96, 1.76, 1.66, 7.30, 3.66, 2.19, 1.56, 1.23, 1.79, 3.36, 3.31},\n        //Test Case - 4\n        {3.68, 3.35, 1.45, 3.85, 3.88, 1.52, 2.38, 3.33, 2.46, 1.27, 2.48, 4.05, 2.65, 2.54, 2.97, 1.41},\n        //Test Case - 5\n        {4.48, 3.42, 2.27, 4.40, 4.96, 3.27, 3.46, 1.95, 1.93, 3.15, 3.11, 4.49, 3.81, 4.33, 2.03, 3.06, 2.49, 2.50, 3.11, 3.15, 1.22, 4.57, 2.40, 2.37, 3.49, 3.14, 3.31, 3.56, 1.44, 4.14, 3.39, 2.41},\n        //Test Case - 6\n        {4.80, 3.77, 3.36, 3.10, 5.78, 3.50, 5.23, 5.90, 3.29, 4.29, 1.36, 3.19, 1.93, 3.10, 2.31, 5.06, 2.82, 1.93, 2.08, 1.42, 1.91, 5.09, 3.92, 2.35, 1.76, 3.97, 5.44, 5.01, 1.36, 1.87, 3.18, 2.99, 3.50, 2.26,5.61, 3.82, 3.37, 5.15, 1.51, 5.05, 1.52, 4.67, 2.27, 1.81, 5.02, 2.09, 4.93, 5.31},\n        //Test Case - 7\n        {1.62, 1.48, 3.50, 2.01, 3.92, 4.70, 3.20, 5.07, 2.93, 3.41, 5.92, 4.48, 5.55, 1.85, 4.07, 4.21, 4.29, 2.20, 1.70, 2.95, 4.10, 4.40, 1.41, 1.73, 2.35, 4.25, 4.16, 2.49, 5.58, 5.78, 5.61, 1.74, 1.61, 2.44, 1.41, 4.50, 5.02,5.25, 4.39, 5.86, 1.65, 2.52, 4.91, 1.07, 2.88, 5.10, 3.04, 5.23, 3.49, 5.79, 1.26, 5.77, 5.76, 3.13, 2.25, 3.60, 3.38, 4.37, 2.92, 1.94, 5.86, 2.33, 4.59, 2.06}\n    };\n\n    //For floating point inputs, as the exponent value increases, output of cpu and gpu might differ\n    int exponent[TEST_CASE_COUNT] = {5, 5, 5, 5, 7, 7, 7};\n\n    int maxExponentValue = *std::max_element(exponent, exponent + TEST_CASE_COUNT);\n   \n    //Expected Output for Test\n    float expectedOutput_h[TEST_CASE_COUNT][maxInputSize * maxExponentValue];\n    for (int t = 0; t < TEST_CASE_COUNT; ++t) {\n       for (int p = 0; p < exponent[t]; ++p) {\n          int power = p + 2;  // starting from x^2\n          for (int i = 0; i < inputSize[t]; ++i) {\n            float val = input_h[t][i];\n            float result = 1.0f;\n            for (int k = 0; k < power; ++k)\n              result *= val;\n            expectedOutput_h[t][i + p * inputSize[t]] = result;\n          }\n        }\n    }\n\n    int maxFloat4Data = (maxInputSize + VECTOR_WIDTH - 1) / VECTOR_WIDTH;\n    //Output of device on host\n    float4 output_h4[maxFloat4Data * maxExponentValue] = {};\n            \n    //Use CUDA Streams for asynchronous execution\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    //Allocate Device Memory\n    float *input_d;\n    float4 *output_d4;\n    CUDA_CHECK(cudaMallocAsync((void**)&input_d, maxFloat4Data * VECTOR_WIDTH * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&output_d4, maxFloat4Data * maxExponentValue * sizeof(float4), stream));\n\n    //Get Device Properties\n    cudaDeviceProp prop;\n    int device;\n    cudaGetDevice(&device);\n    cudaGetDeviceProperties(&prop, device);\n    \n    //Execute Test Cases\n    for (int testCase = 0; testCase < TEST_CASE_COUNT; testCase++) {\n\n        int numElements = inputSize[testCase];\n        int exponentValue = exponent[testCase];\n        int numExponentTerms = exponentValue - EXPONENT_START + 1;\n        \n        //Pad input data to align with float4 vector\n        int numFloat4data = (numElements + 3) / 4;\n        int numPaddedElements = numFloat4data * 4;\n        float paddedInput[numPaddedElements] = {};\n\n        memcpy(paddedInput, input_h[testCase], numElements * sizeof(float));\n        //Reset Output\n        CUDA_CHECK(cudaMemsetAsync(output_d4, SET_TO_ZERO, maxFloat4Data * maxExponentValue * sizeof(float4), stream));\n\n        //Copy padded input data from host to device\n        CUDA_CHECK(cudaMemcpyAsync(input_d, paddedInput, numPaddedElements * sizeof(float), cudaMemcpyHostToDevice, stream)); \n\n        //Set Kernel Configuration\n        int numThreadsPerBlock = NUMBER_OF_THREADS_PER_BLOCK;\n        if (numThreadsPerBlock > prop.maxThreadsDim[0]) {\n            numThreadsPerBlock = prop.maxThreadsDim[0];\n        }\n\n        int numBlocks = ceil((float)(numElements) / numThreadsPerBlock);\n        if (numBlocks > prop.maxGridSize[0]) {\n            numBlocks = prop.maxGridSize[0];\n        }\n\n        dim3 block(numThreadsPerBlock, 1, 1);\n        dim3 grid(numBlocks, 1, 1);\n\n        //Launch Kernels \n        //Grid: (numElements/32, 1, 1)\n        //Block: (32, 1, 1)\n        void *args[] = {&input_d, &output_d4, &exponentValue, &numPaddedElements};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_computeConsecutivePowers, grid, block, args, 0, stream));\n\n        //Copy Output Data from device to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h4, output_d4, maxFloat4Data * numExponentTerms * sizeof(float4), cudaMemcpyDeviceToHost, stream));\n       \n        //Synchronize Streams\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        //Assert device output and expected output\n        for (int p = 0; p < numExponentTerms; ++p) {\n            for (int i = 0; i < numFloat4data; ++i) {\n                float4 val = output_h4[p * numFloat4data + i];\n                for (int j = 0; j < VECTOR_WIDTH; ++j) {\n                    int index = i * VECTOR_WIDTH + j;\n                    if (index >= numElements) break;\n                    float actual = (j == 0) ? val.x : (j == 1) ? val.y : (j == 2) ? val.z : val.w;\n                    float expected = expectedOutput_h[testCase][p * numElements + index];\n                    assert(fabs(actual - expected) < EPSILON);\n                }\n            }\n        }\n    }\n    \n    //Deallocate Device Memory\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d4, stream));\n\n    //Destroy All Streams \n    CUDA_CHECK(cudaStreamDestroy(stream));\n    \n}\n\n__global__ void k_computeConsecutivePowers(const float *input_d, float4 *output_d4, int exponent, int numElements) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/191", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute the maximum value in a flattened 2D matrix stored in row-major order using shared memory for block-level reduction.\n\nThe signature of the function is __global__ void k_computeMaxElement(const int *input_d, int *output_d, int inputSize), where input_d is a pointer to flattened 2D matrix (stored in row-major order), output_d is a pointer to the device memory that stores the final maximum value, and inputSize is the total number of elements in the matrix (rows * columns).\n\n>>> k_computeMaxElement({1, 2, 3, 4, 5, 6, 7, 8, 9}, output_d, 9) -> output_d: ({9})\n>>> k_computeMaxElement({1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, output_d, 16) -> output_d: ({16})\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <random>\n#include <ctime> \n#include <iostream>\n#include <limits.h>\n#include <assert.h>\n#include <cuda_runtime.h>\n\n#define SET_TO_ZERO 0\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\nconst int THREADS_PER_BLOCK = 256;\n\n__global__ void k_computeMaxElement(const int *input_d, int *output_d, int inputSize);\n\nvoid launch() {\n\n    //Initialize Constants\n    const int TEST_CASE_COUNT = 8;\n    const int MAX_DIMENSIONS = 2;\n    const int HEIGHT_INDEX = 0;\n    const int WIDTH_INDEX = 1;\n    const int MIN_ARRAY_SIZE = 1;\n    const int MIN_RANDOM_INPUT_RANGE = 0;\n    const int MAX_RANDOM_INPUT_RANGE = 1000;\n\n    //Get Device Properties\n    cudaDeviceProp prop;\n    int device;\n    cudaGetDevice(&device);\n    cudaGetDeviceProperties(&prop, device);\n      \n    //Initialise Test Data\n    //Test Data Dimensions\n    int inputDim_h[TEST_CASE_COUNT][MAX_DIMENSIONS] = {\n        //Test Case - 1, {rows(height), columns(width)} \n        {3, 16},\n        //Test Case - 2\n        {4, 16},\n        //Test Case - 3\n        {5, 18},\n        //Test Case - 4\n        {6, 21},\n        //Test Case - 5\n        {7, 30},\n        //Test Case - 6\n        {1, 1001},\n        //Test Case - 7\n        {16, 620},\n        //Test Case - 8\n        {20, 1280}\n    };\n    \n    //Identify max width\n    int maxWidth = 0;\n    for(int index = 0; index < TEST_CASE_COUNT; index++) {\n        maxWidth = max(maxWidth, inputDim_h[index][WIDTH_INDEX]);\n    }\n\n    //Identify max height\n    int maxHeight = 0;\n    for(int index = 0; index < TEST_CASE_COUNT; index++) {\n        maxHeight = max(maxHeight, inputDim_h[index][HEIGHT_INDEX]);\n    }\n\n    //Max Input Size including padding\n    int maxInputSize = maxWidth * maxHeight;\n\n    //Generate Random Input Data for Test\n    int input_h[TEST_CASE_COUNT][maxInputSize];\n\n    std::mt19937 random_generator(static_cast<unsigned int>(std::time(nullptr)));\n    std::uniform_int_distribution<int> dist(MIN_RANDOM_INPUT_RANGE, MAX_RANDOM_INPUT_RANGE);\n\n    for(int testCase = 0; testCase < TEST_CASE_COUNT; testCase++)\n    {\n      int width = inputDim_h[testCase][WIDTH_INDEX];\n      int height = inputDim_h[testCase][HEIGHT_INDEX];\n       for (int row = 0; row < height; row++) {\n          for (int col = 0; col < width; col++) {\n              int index = row * width + col;\n              input_h[testCase][index] = dist(random_generator);\n          }\n        }\n    }\n\n    //Compute Expected Output for Test data\n    int expectedOutput_h[TEST_CASE_COUNT];\n    for(int testCase = 0; testCase < TEST_CASE_COUNT; testCase++) {\n        expectedOutput_h[testCase] = 0;\n        int width = inputDim_h[testCase][WIDTH_INDEX];\n        int height = inputDim_h[testCase][HEIGHT_INDEX];\n        for(int row = 0; row < height; row++) {\n            for(int column = 0; column < width; column++) {\n                int index = row * width + column;\n                expectedOutput_h[testCase] = max(expectedOutput_h[testCase], input_h[testCase][index]);\n            }\n        }\n    }\n   \n    //Output of device on host\n    int output_h[MIN_ARRAY_SIZE] = { };\n    \n    //Use CUDA Streams for Asynchronous Execution\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n \n    //Allocate Device Memory\n    int *input_d;\n    int *output_d;\n\n    CUDA_CHECK(cudaMallocAsync((void**)&input_d, maxInputSize * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&output_d, sizeof(int), stream));\n\n    //Execute Test Cases\n    for (int testCase = 0; testCase < TEST_CASE_COUNT; testCase++) {\n        \n        //inputSize\n        int Width = inputDim_h[testCase][WIDTH_INDEX];\n        int Height = inputDim_h[testCase][HEIGHT_INDEX];\n        int inputSize = Width * Height;\n              \n        //Reset Output\n        CUDA_CHECK(cudaMemsetAsync(input_d, SET_TO_ZERO, maxInputSize, stream));\n        CUDA_CHECK(cudaMemsetAsync(output_d, SET_TO_ZERO, sizeof(int), stream));\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h[testCase], inputSize * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        //Set Kernel Configuration\n               \n        int numBlocks = ceil((float)((inputSize + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK));\n\n        numBlocks = min(numBlocks, prop.maxGridSize[0]);\n\n        dim3 block(THREADS_PER_BLOCK, 1, 1);\n        dim3 grid(numBlocks, 1, 1);\n       \n        //Launch Kernel\n        //Grid: ((inputWidth * inputHeight) / 256, 1, 1)\n        //Block: (256, 1, 1)\n        void *args[] = {&input_d, &output_d, &inputSize};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_computeMaxElement, grid, block, args, THREADS_PER_BLOCK * sizeof(int), stream));\n        \n        //Copy Data from device to host\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, sizeof(int), cudaMemcpyDeviceToHost, stream));\n        \n        //Synchronize tasks in the stream\n        CUDA_CHECK(cudaStreamSynchronize(stream));        \n\n        //Assert device output and expected output\n        assert(output_h[0] == expectedOutput_h[testCase]);\n      \n    }\n    \n    //Deallocate Device Memory\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    \n}\n\n__global__ void k_computeMaxElement(const int *input_d, int *output_d, int inputSize) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/192", "date": "2025-07-30", "prompt": "Write a function to launch a CUDA graph, whose pipeline containing a node for input host to device memory copy, k_rgbToGray kernel node, k_gammaCorrection node and another node for device to host memory copy for input frame obtained from a sequence of frames. The graph has to be created once and should be launched for every frame by modifying only the input and output data pointers.\n\nThe signature of the function is void runGraph(float* inpFrame, float* inpFrame_d, size_t frameHeight, size_t frameWidth, size_t numFrames, float gammaValue, float contrastValue, float brightnessValue, float* outRGB2Gray_d, float* outFrame_d, float* outFrame_h, cudaStream_t& stream), where inpFrame is the input host frame pointer, inpFrame_d is the input device frame pointer, frameHeight and frameWidth are the height and width of the image frame, numFrames is the value of total number of frames, gammaValue is the gamma correction value, contrastValue is the contrast value, brightnessValue is the value corresponding to brightness, outRGB2Gray_d is the device pointer pointing to store the output of k_rgbToGray helper kernel and input data pointer to k_gammaCorrection kernel, outFrame_d is the device pointer pointing to store the output of k_rotateImage helper kernel, outFrame_h is the host pointer to store the final output data, stream is the CUDA stream object.\n\nThe signature of the helper function k_rgbToGray, is __global__ void k_rgbToGray(float* inpFrameRGB, int frameWidth, int frameHeight, float* outFrame), where inpFrameRGB is the input frame pointer, frameWidth & frameHeight are the width and height of the frame, outFrame is output frame pointer of this kernel. This kernel converts RGB image to grayscale image.\n\nThe signature of the helper function k_gammaCorrection(float* inpFrame, int frameWidth, int frameHeight, float gammaValue, float contrastValue, float brightnessValue, float* outFrame), where inpFrame is the input frame pointer, frameWidth & frameHeight are the width and height of the frame, gammaValue is gamma correction value, contrastValue is the contrast of the frame, brightnessValue corresponds to brightness of the frame, outFrame is output frame pointer of this kernel. This kernel applies the gamma, contrast and brightness values to the image.\n\n>>> runGraph({{0.080,0.606,0.358,0.087,0.093,0.590,0.067,0.652,0.770},{0.856,0.952,0.757,0.137,0.550,0.372,0.943,0.084,0.121}}, float* inpFrame_d, 3, 3, 2, 0.472, 0.104, 0.724, float* outRGB2Gray_d, float* outFrame_d, float* outFrame_h, cudaStream_t& stream) -> outFrame: {{0.805,0.819,0.809,0.772,0.794,0.794,0.810,0.781,0.784},{0.813,0.796,0.810,0.793,0.812,0.790,0.791,0.810,0.805}}\n\n>>> runGraph({{0.522,0.568,0.212,0.607,0.112,0.274,0.752,0.527,0.337,0.906,0.157,0.040,0.781,0.721,0.699,0.588,0.262,0.485,0.760,0.158,0.439,0.640,0.876,0.984,0.366}, {0.154,0.534,0.213,0.115,0.961,0.560,0.106,0.425,0.505,0.574,0.187,0.822,0.543,0.749,0.963,0.534,0.114,0.402,0.035,0.900,0.268,0.520,0.591,0.035,0.220}}, float* inpFrame_d, 5, 5, 2, 0.834, 0.533, 0.178, float* outRGB2Gray_d, float* outFrame_d, float* outFrame_h, cudaStream_t& stream) -> outFrame: {{0.362,0.514,0.321,0.352,0.525,0.433,0.376,0.470,0.446,0.569,0.297,0.482,0.513,0.570,0.630,0.506,0.293,0.472,0.358,0.555,0.383,0.489,0.571,0.434,0.381}, {0.509,0.431,0.605,0.483,0.464,0.464,0.418,0.579,0.411,0.539,0.538,0.643,0.521,0.583,0.368,0.489,0.423,0.338,0.501,0.571,0.425,0.449,0.403,0.292,0.347}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <iostream>\n#include <vector>\n#include <time.h>\n#include <assert.h>\n#include <cuda_runtime_api.h>\n\n#undef NDEBUG\n#define _USE_MATH_DEFINES\n\n#define RGB_CHANNELS 3\n#define RGB2GRAY_R 0.2989\n#define RGB2GRAY_G 0.5870\n#define RGB2GRAY_B 0.1140\n\n#define CUDA_CHECK(call)                                    \\\ndo {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\\n    cudaError_t error = call;                               \\\n    if (error != cudaSuccess) {                             \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n            __FILE__, __LINE__, cudaGetErrorString(error)); \\\n            exit(EXIT_FAILURE);                             \\\n    }                                                       \\\n} while(0)\n\nvoid runGraph(float* inpFrame, float* inpFrame_d, size_t frameHeight, size_t frameWidth, size_t numFrames, float gammaValue, float contrastValue, float brightnessValue, float* outRGB2Gray_d, float* outFrame_d, float* outFrame_h, cudaStream_t& stream);\n\n// RGB to grayscale conversion\n__global__ void k_rgbToGray(float* inpFrameRGB, int frameWidth, int frameHeight, float* outFrame) {\n    // Thread/Block boundary checks\n    for (int tIdy = (blockIdx.y * blockDim.y) + threadIdx.y; tIdy < frameHeight; tIdy += gridDim.y * blockDim.y) {\n        for (int tIdx = (blockIdx.x * blockDim.x) + threadIdx.x; tIdx < frameWidth; tIdx += gridDim.x * blockDim.x) {\n            // RGB to grayscale\n            float grayPixValue = inpFrameRGB[tIdy * frameWidth + tIdx] * RGB2GRAY_R +\n                inpFrameRGB[tIdy * frameWidth + tIdx + frameWidth * frameHeight] * RGB2GRAY_G +\n                inpFrameRGB[tIdy * frameWidth + tIdx + 2 * frameWidth * frameHeight] * RGB2GRAY_B;\n            outFrame[tIdy * frameWidth + tIdx] = max(0.f, min(1.f, grayPixValue));\n        }\n    }\n}\n\n// Gamma Correction, contrast and brightness adjustment\n__global__ void k_gammaCorrection(float* inpFrame, int frameWidth, int frameHeight, float gammaValue, float contrastValue, float brightnessValue, float* outFrame) {\n    // Thread/Block boundary checks\n    for (int tIdy = (blockIdx.y * blockDim.y) + threadIdx.y; tIdy < frameHeight; tIdy += gridDim.y * blockDim.y) {\n        for (int tIdx = (blockIdx.x * blockDim.x) + threadIdx.x; tIdx < frameWidth; tIdx += gridDim.x * blockDim.x) {\n            // Gamma Correction\n            float pixValueAfterGamma = max(0.f, min(1.f, pow(inpFrame[tIdy * frameWidth + tIdx], gammaValue)));\n            pixValueAfterGamma = contrastValue * pixValueAfterGamma + brightnessValue;\n            outFrame[tIdy * frameWidth + tIdx] = min(max(pixValueAfterGamma, 0.f), 1.f);\n        }\n    }\n}\n\n// Verification function CPU sequential implementation\nvoid verificationFunction(float* inpFrame, int frameHeight, int frameWidth, size_t numFrames, float gammaValue, float contrastValue, float brightnessValue, float* outFrame) {\n\n    size_t numElementsInFrame = frameHeight * frameWidth;\n    float* inpFramePointer = inpFrame;\n    float* outFramePointer = outFrame;\n    int inputFrameOffset = numElementsInFrame * RGB_CHANNELS;\n    int outputFrameOffset = numElementsInFrame;\n\n    // Running operations per frame\n    for (int frameIdx = 0;frameIdx < numFrames;frameIdx++) {\n        inpFramePointer = inpFrame + frameIdx * inputFrameOffset;\n        outFramePointer = outFrame + frameIdx * outputFrameOffset;\n\n        // RGB 2 Grayscale and gamma correction\n        for (int r = 0;r < frameHeight;r++) {\n            for (int c = 0;c < frameWidth;c++) {\n\n                // RGB to Grayscale\n                float pixValue = inpFramePointer[r * frameWidth + c] * RGB2GRAY_R +\n                    inpFramePointer[r * frameWidth + c + numElementsInFrame] * RGB2GRAY_G +\n                    inpFramePointer[r * frameWidth + c + 2 * numElementsInFrame] * RGB2GRAY_B;\n                pixValue = std::min(std::max(pixValue, 0.f), 1.f);\n\n                // Gamma, contrast and brightness\n                pixValue = std::max(0.f, std::min(1.f, pow(pixValue, gammaValue)));\n                pixValue = contrastValue * pixValue + brightnessValue;\n                outFramePointer[r * frameWidth + c] = std::min(std::max(pixValue, 0.f), 1.f);\n            }\n        }\n    }\n}\n\nvoid launch() {\n\n    // Setting input lengths\n    const int NUM_TESTCASES = 8;\n    const float VERIFICATION_TOLERANCE = 1e-3;\n    \n    // Testcases select input height, width and frame count randomly\n    const int MAX_WIDTH = 100;\n    const int MAX_HEIGHT = 100;\n    const int MAX_FRAME_COUNT = 7;\n    int testcases[NUM_TESTCASES][3] = { {3,3,2}, {5,5,2}, {25,24,5}, {27,36,4}, {78,100,3}, {81,27,2}, {64,57,7}, {50,66,3} };\n\n    // Memory allocation in CPU\n    size_t maxElementsInFrame = MAX_WIDTH * MAX_HEIGHT * MAX_FRAME_COUNT;\n    float* inpFrame = (float*)malloc(RGB_CHANNELS * maxElementsInFrame * sizeof(float));\n    float* outFrame_h = (float*)malloc(maxElementsInFrame * sizeof(float));\n    float* outFrame_ref = (float*)malloc(maxElementsInFrame * sizeof(float));\n\n    // Initializing stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Allocating device memory and copying host matrices to device\n    float* inpFrame_d = nullptr;\n    float* outFrame_d = nullptr;\n    float* outRGB2Gray_d = nullptr;\n    CUDA_CHECK(cudaMallocAsync((void**)&inpFrame_d, RGB_CHANNELS * maxElementsInFrame * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&outRGB2Gray_d, maxElementsInFrame * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&outFrame_d, maxElementsInFrame * sizeof(float), stream));\n\n    // Running test cases with random inputs\n    srand(time(NULL));\n    for (int tIter = 0; tIter < NUM_TESTCASES; tIter++) {\n\n        int frameWidth = testcases[tIter][0];\n        int frameHeight = testcases[tIter][1];\n        size_t numFrames = testcases[tIter][2];\n        size_t numElementsInFrame = frameHeight * frameWidth;\n\n        // Set input gamma, contrast and brightness\n        float gammaValue = (rand() % 1000) / 1000.f * 2.f;\n        float contrastValue = (rand() % 1000) / 1000.f;\n        float brightnessValue = (rand() % 1000) / 1000.f;\n\n        // Input data generation for frame in the interval (-1,1)\n        for (int iter = 0; iter < RGB_CHANNELS * numElementsInFrame * numFrames; iter++)\n            inpFrame[iter] = (rand() % 1000) / 1000.f;\n\n        // Generate CUDA graphs\n        runGraph(inpFrame, inpFrame_d, frameHeight, frameWidth, numFrames, gammaValue, contrastValue, brightnessValue, outRGB2Gray_d, outFrame_d, outFrame_h, stream);\n\n        // Verify the output with CPU implementation\n        verificationFunction(inpFrame, frameHeight, frameWidth, numFrames, gammaValue, contrastValue, brightnessValue, outFrame_ref);\n        for (int i = 0; i < numElementsInFrame * numFrames; i++) {\n            assert(fabsf(outFrame_h[i] - outFrame_ref[i]) < VERIFICATION_TOLERANCE);\n        }\n    }\n\n    // Free allocated memory\n    free(inpFrame);\n    free(outFrame_h);\n    free(outFrame_ref);\n\n    CUDA_CHECK(cudaFreeAsync(inpFrame_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outRGB2Gray_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outFrame_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\nvoid runGraph(float* inpFrame, float* inpFrame_d, size_t frameHeight, size_t frameWidth, size_t numFrames, float gammaValue, float contrastValue, float brightnessValue, float* outRGB2Gray_d, float* outFrame_d, float* outFrame, cudaStream_t& stream) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/193", "date": "2025-07-30", "prompt": "\nWrite a function that computes optimal CUDA block size for the provided k_pageRankStep kernel using CUDA occupancy calculator. Each thread stores exactly one element in shared memory. The function should determine the optimal block size that maximizes occupancy while considering shared memory requirements per block.\n\nThe signature of the function is void calculateLaunchParams(const cudaDeviceProp& deviceProp, int numNodes, int& gridSize, int& blockSize, size_t& requiredSharedMem) where deviceProp contains CUDA device properties reflecting hardware capabilities, numNodes specifies the number of nodes in the graph defining the problem size, gridSize is an output parameter for the number of blocks to launch, blockSize is an output parameter for threads per block aligned to warp and memory constraints, and requiredSharedMem is an output parameter for shared memory per block.\n\nThe helper k_pageRankStep kernel performs one iteration of the PageRank algorithm by computing matrix-vector multiplication with damping factor across graph nodes. The signature of the kernel is global void  k_pageRankStep(float *M_d, float *ranks_d, float *newRanks_d, float damping_d, int N_d) where M_d is a pointer to the adjacency matrix stored in row-major order (N_d x N_d elements), ranks_d is a pointer to the current PageRank values array (N_d elements), newRanks_d is a pointer to the output array for updated PageRank values (N_d elements), damping_d is the damping factor (typically 0.85) used in the PageRank algorithm, and N_d is the number of nodes in the graph.\n\n>>> k_pageRankStep([0,0,0,0.25,0,0,0,0.5,1,0.5,0,0.25,0,0.5,1,0], [0.25,0.25,0.25,0.25], new_ranks, 0.85, 4)-> new_ranks: [0.090625, 0.14375, 0.409375, 0.35625]\n>>> k_pageRankStep([1.0,0.5,0.5,0], [0.5,0.5], new_ranks, 0.85, 2)-> new_ranks: [0.5, 0.5]\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <cmath>\n#include <cooperative_groups.h>\n#include <cstdio>\n#include <cstdlib>\n#include <vector>\n\nusing namespace cooperative_groups;\n\nvoid calculateLaunchParams(const cudaDeviceProp& deviceProp, int numNodes, int& gridSize, int& blockSize, size_t& requiredSharedMem);\n\n// Forward declaration of the PageRank kernel\n__global__ void k_pageRankStep(float *M_d, float *ranks_d, float *newRanks_d, float damping_d, int N_d) {\n    extern __shared__ float s_partials[];\n    int threadId = threadIdx.x;\n\n    // Early exit\n    if (!M_d || !ranks_d || !newRanks_d || N_d <= 0) return;\n\n    thread_block block = this_thread_block();\n\n    // Grid-stride over rows\n    for (int row = blockIdx.x; row < N_d; row += gridDim.x) {\n        // Compute partial sum\n        float sum = 0.0f;\n        for (int col = threadId; col < N_d; col += blockDim.x) {\n            sum += M_d[row * N_d + col] * ranks_d[col];\n        }\n        // atomic store initial partial\n        atomicExch(&s_partials[threadId], sum);\n        block.sync();\n\n        // Block-level reduction using atomics\n        for (int stride = blockDim.x / 2; stride > warpSize; stride >>= 1) {\n            if (threadId < stride) {\n                // atomic load\n                float val = atomicAdd(&s_partials[threadId + stride], 0.0f);\n                // accumulate\n                atomicAdd(&s_partials[threadId], val);\n            }\n            block.sync();\n        }\n\n        // Warp-level reduction with atomics\n        if (threadId < warpSize) {\n            float v;\n            if (blockDim.x >= 2 * warpSize) {\n                v = atomicAdd(&s_partials[threadId + warpSize], 0.0f);\n                atomicAdd(&s_partials[threadId], v);\n            }\n            if (blockDim.x >= warpSize) {\n                v = atomicAdd(&s_partials[threadId + warpSize/2], 0.0f);\n                atomicAdd(&s_partials[threadId], v);\n            }\n            if (blockDim.x >= warpSize/2) {\n                v = atomicAdd(&s_partials[threadId + warpSize/4], 0.0f);\n                atomicAdd(&s_partials[threadId], v);\n            }\n            if (blockDim.x >= warpSize/4) {\n                v = atomicAdd(&s_partials[threadId + warpSize/8], 0.0f);\n                atomicAdd(&s_partials[threadId], v);\n            }\n            if (blockDim.x >= warpSize/8) {\n                v = atomicAdd(&s_partials[threadId + warpSize/16], 0.0f);\n                atomicAdd(&s_partials[threadId], v);\n            }\n            if (blockDim.x >= warpSize/16) {\n                v = atomicAdd(&s_partials[threadId + 1], 0.0f);\n                atomicAdd(&s_partials[threadId], v);\n            }\n        }\n        block.sync();\n\n        // Final write by first thread\n        if (threadId == 0) {\n            newRanks_d[row] = damping_d * s_partials[0] + (1.0f - damping_d) / N_d;\n        }\n        block.sync();\n    }\n}\n\n\n\n// Define constants \n#define TOLERANCE 1e-5f\n\n\n#define CUDA_CHECK(call)                                                       \\\n    do {                                                                        \\\n        cudaError_t error = call;                                               \\\n        if (error != cudaSuccess) {                                             \\\n            fprintf(stderr,                                                     \\\n                    \"CUDA Error: %s at %s:%d\\n\",                                \\\n                    cudaGetErrorString(error),                                  \\\n                    __FILE__,                                                   \\\n                    __LINE__);                                                  \\\n            exit(error);                                                        \\\n        }                                                                       \\\n    } while (0)\n\n\n\nstruct TestCase {\n    int numNodes;                     // [in]  Number of nodes in test graph\n    float d;                          // [in]  Damping factor for test case\n    std::vector<float> M;             // [in]  Adjacency matrix data for test\n    std::vector<float> initialRanks;  // [in]  Initial PageRank values for test\n    std::vector<float> expectedRanks; // [in] Expected output values for validation\n};\n\nstd::vector<TestCase> testCases = {\n    // Test case 0: 4-node graph (validated)\n    {4,\n     0.85f,\n     {0, 0, 0, 0.25, 0, 0, 0, 0.5, 1, 0.5, 0, 0.25, 0, 0.5, 1, 0},\n     {0.25f, 0.25f, 0.25f, 0.25f},\n     {0.090625f, 0.14375f, 0.409375f, 0.35625f}},\n\n    // Test case 1: Single node (identity)\n    {1, 0.85f, {1.0f}, {1.0f}, {1.0f}},\n\n    // Test case 2: Identity matrix (stable)\n    {2, 0.85f, {1.0f, 0.0f, 0.0f, 1.0f}, {0.5f, 0.5f}, {0.5f, 0.5f}},\n\n    // Test case 3: 3-node identity (validated)\n    {3, 0.85f, {1, 0, 0, 0, 1, 0, 0, 0, 1}, {0.3f, 0.3f, 0.4f}, {0.305f, 0.305f, 0.39f}},\n\n    // Test case 4: All links to node 0 (corrected)\n    {3,\n     0.85f,\n     {1, 1, 1, 0, 0, 0, 0, 0, 0},\n     {0.333f, 0.333f, 0.334f},\n     {0.85f * 1.0f + 0.05f, 0.05f, 0.05f}},\n\n    // Test Case 5: Cycle graph\n    {3, 0.85f, {0, 1, 0, 0, 0, 1, 1, 0, 0}, {1.0f, 0.0f, 0.0f}, {0.05f, 0.05f, 0.9f}},\n\n    // Test case 6: Self-loop stable state\n    {4,\n     0.85f,\n     {1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1},\n     {0.25f, 0.25f, 0.25f, 0.25f},\n     {0.25f, 0.25f, 0.25f, 0.25f}}};\n\n\nvoid launch() {\n    // Get device properties once at the beginning\n    cudaDeviceProp deviceProp;\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, 0));\n    \n    // Get device-specific parameters\n    int multiProcessorCount = deviceProp.multiProcessorCount;\n    \n    // Allocate device memory\n    float *M_d = nullptr, *ranks_d = nullptr, *newRanks_d = nullptr;\n    size_t maxNodes = 0;\n    for(const auto& tc : testCases)\n        maxNodes = std::max(maxNodes, static_cast<size_t>(tc.numNodes));\n    \n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    CUDA_CHECK(cudaMallocAsync(&M_d, maxNodes * maxNodes * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&ranks_d, maxNodes * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&newRanks_d, maxNodes * sizeof(float), stream));\n\n    for(const auto& tc : testCases) {\n        int numNodes = tc.numNodes;\n        float damping = tc.d;\n\n        int gridSize, blockSize;\n        size_t requiredSharedMem;\n        calculateLaunchParams(deviceProp, numNodes, gridSize, blockSize, requiredSharedMem);\n\n        CUDA_CHECK(cudaMemcpyAsync(M_d, tc.M.data(), \n                                 numNodes * numNodes * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(ranks_d, tc.initialRanks.data(), \n                                 numNodes * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        if (numNodes < multiProcessorCount) {\n            CUDA_CHECK(cudaMemsetAsync(newRanks_d, 0, numNodes * sizeof(float), stream));\n        }\n\n        float mutableDamping = damping;\n        int mutableNumNodes = numNodes;\n\n        void* args[] = {\n            &M_d,\n            &ranks_d,\n            &newRanks_d,\n            &mutableDamping,\n            &mutableNumNodes\n        };\n\n        CUDA_CHECK(cudaLaunchCooperativeKernel(\n            (void*)k_pageRankStep, gridSize, blockSize,\n            args, requiredSharedMem, stream));\n\n        std::vector<float> results_h(numNodes);\n        CUDA_CHECK(cudaMemcpyAsync(results_h.data(), newRanks_d,\n                                 numNodes * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for(int i = 0; i < numNodes; ++i) {\n            assert(fabs(results_h[i] - tc.expectedRanks[i]) < TOLERANCE);\n        }\n    }\n\n    CUDA_CHECK(cudaFreeAsync(M_d, stream));\n    CUDA_CHECK(cudaFreeAsync(ranks_d, stream));\n    CUDA_CHECK(cudaFreeAsync(newRanks_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\nvoid calculateLaunchParams(const cudaDeviceProp& deviceProp, int numNodes, int& gridSize, int& blockSize, size_t& requiredSharedMem) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/194", "date": "2025-07-30", "prompt": "Write a CUDA kernel with high occupancy to compute the average errors of multiple polynomials with a fixed number of trials. The equations of the polynomials are generated based on the provided number of coefficients and the x values. The average error in the kernel should be calculated as (sum of fabs(expectedPolynomialValue - computedPolynomialValue)) / (number of trials), where expectedPolynomialValues are already provided to the kernel. Enhance occupancy by using manual register spillover to shared memory for the coefficient data.\n\nThe signature of the CUDA kernel is __global__ void k_computeAverageErrorsOfPolynomials(int numPolynomials, int numCoefficients, int numTrials, float* xValues_d, float* expectedPolynomialValues_d, float* coefficients_d, float* averageErrors_d), where numPolynomials is an integer representing the number of polynomials, with each CUDA thread assigned its own polynomial to calculate error. numCoefficients is an integer indicating the number of coefficients per polynomial. numTrials is an integer that signifies the number of tests for the polynomials needed to calculate the average error. xValues_d is a pointer to an array of float elements for each trial value (x) to test for the polynomial (f(x)). expectedPolynomialValues_d is a pointer to an array of float elements representing the expected polynomial approximation value for each corresponding trial (x) value. coefficients_d is a pointer to an array of float elements containing the coefficients of each polynomial. averageErrors_d is a pointer to an array of float elements representing the average error of each polynomial (calculated over multiple trials and expected outputs).\n\n>>> __global__ void k_computeAverageErrorsOfPolynomials(100000, 3, 5, { 0.3745f, 0.7965f, 0.9507f, 0.1834f, 0.7320f }, { 0.3658f, 0.7149f, 0.8138f, 0.1824f, 0.6684f }, { 0.5f for all elements }, averageErrors_d) -> averageErrors_d: { 0.809729f for all elements }\n>>> __global__ void k_computeAverageErrorsOfPolynomials(100000, 1, 6, { 0.9922f, 0.1834f, 0.6175f, 0.3042f, 0.6117f, 0.5248f }, { 0.5468f, 0.9832f, 0.8153f, 0.9541f, 0.8187f, 0.8654f }, { 0.25f, 0.50f, these two values are repeated for all elements }, averageErrors_d) -> averageErrors_d: { 0.633622f, 0.844117f, these two values are repeated for all elements }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80 --expt-relaxed-constexpr", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <random>\n#include <algorithm>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Algorithm settings.\n// Offloading a small number of coefficients to registers decreases the allocation size for shared memory and improves occupancy.\n// Having MAX_NUM_COEFFICIENTS less than NUM_COEFFICIENTS_IN_REGISTERS is allowed.\n// This can be adjusted.\nconstexpr int NUM_COEFFICIENTS_IN_REGISTERS = 2;\n\n__global__ void k_computeAverageErrorsOfPolynomials(int numPolynomials, int numCoefficients, int numTrials, \n                                                    float * xValues_d, float * expectedPolynomialValues_d, float * coefficients_d, \n                                                    float * averageErrors_d);\n\nvoid launch() {\n    // Maximum number of different x values to calculate the average error of the polynomial function approximation f(x) = c1 + c2*x + c3*(x^2) + ... + cn*(x^(n-1)), where n = number of coefficients. This can be adjusted.\n    constexpr int MAX_NUM_TRIALS = 20;\n    // The maximum number of polynomials; this can be adjusted.\n    constexpr int MAX_NUM_POLYNOMIALS = 100000;\n    // Maximum number of coefficients for polynomials; this can be adjusted.\n    constexpr int MAX_NUM_COEFFICIENTS = 8;\n    constexpr float ERROR_TOLERANCE = 1e-4f;\n    constexpr int DETERMINISTIC_RANDOM_SEED = 42;\n    // The minimum number of coefficients required in registers is 1, which is also the requirement for the calculations to form a polynomial (f(x) = c1). This must remain as 1.\n    constexpr int MIN_NUM_COEFFICIENTS_IN_REGISTERS = 1;\n\n    // Test settings.\n    constexpr int NUM_TESTS = 7;\n\n    // At least one coefficient is expected to exist as a polynomial (f(x) = c1).\n    assert(NUM_COEFFICIENTS_IN_REGISTERS >= MIN_NUM_COEFFICIENTS_IN_REGISTERS);\n    int deviceId = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    // Allocating memory for the algorithm on the host.\n    float * xValues_h = new float[MAX_NUM_TRIALS];\n    float * coefficients_h = new float[MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS];\n    float * averageErrors_h = new float[MAX_NUM_POLYNOMIALS];\n    float * expectedOutputs_h = new float[MAX_NUM_TRIALS];\n    // Allocating memory for the tests.\n    int * testNumTrials_h = new int[NUM_TESTS];\n    int * testNumPolynomials_h = new int[NUM_TESTS];\n    int * testNumCoefficients_h = new int[NUM_TESTS];\n    float * testValuesForX_h = new float[NUM_TESTS * MAX_NUM_TRIALS];\n    float * testCoefficients_h = new float[NUM_TESTS * MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS];\n    float * testExpectedOutputs_h = new float[NUM_TESTS * MAX_NUM_TRIALS];\n    \n    // Allocating device memory.\n    //  Input parameters for the f(x) function's polynomial approximations. Multiple x values are needed to calculate the average error for each polynomial.\n    float * xValues_d;\n    // The coefficients of each polynomial.\n    float * coefficients_d;\n    // The average error of the function approximation f(x) concerning the expected outputs.\n    float * averageErrors_d;\n    // Expected values of the function approximation f(x) for each given x value and per test.\n    float * expectedPolynomialValues_d;\n    CUDA_CHECK(cudaMallocAsync(&xValues_d, sizeof(float) * MAX_NUM_TRIALS, stream));\n    CUDA_CHECK(cudaMallocAsync(&coefficients_d, sizeof(float) * MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&averageErrors_d, sizeof(float) * MAX_NUM_POLYNOMIALS, stream));\n    CUDA_CHECK(cudaMallocAsync(&expectedPolynomialValues_d, sizeof(float) * MAX_NUM_TRIALS, stream));\n    \n    // Random-number generator to simulate optimization heuristics such as simulated annealing, genetic algorithm, or particle swarm for the coefficients.\n    std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n    std::uniform_real_distribution<float> distribution(0.0f, 1.0f);\n    // Preparing the test data.\n    int testIndex = 0;\n    // Test 1: sin(x)\n    {\n        int numTrials = MAX_NUM_TRIALS;\n        int numPolynomials = MAX_NUM_POLYNOMIALS;\n        int numCoefficients = MAX_NUM_COEFFICIENTS;\n        testNumTrials_h[testIndex] = numTrials;\n        testNumPolynomials_h[testIndex] = numPolynomials;\n        testNumCoefficients_h[testIndex] = numCoefficients;\n        // f(x) = sin(x) is chosen for the polynomial approximation error calculations.\n        for(int i = 0; i < numTrials; i++) {\n            float x = distribution(generator);\n            testValuesForX_h[i + testIndex * MAX_NUM_TRIALS] = x;\n            testExpectedOutputs_h[i + testIndex * MAX_NUM_TRIALS] = sin(x);\n        }\n        // Simulating the outcomes of heuristic optimization.\n        for(int i = 0; i < numPolynomials * numCoefficients; i++) {\n            testCoefficients_h[i + testIndex * MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS] = 0.5f;\n        }\n        testIndex++;\n    }\n    // Test 2: cos(x)\n    {\n        int numTrials = MAX_NUM_TRIALS;\n        int numPolynomials = MAX_NUM_POLYNOMIALS;\n        int numCoefficients = MAX_NUM_COEFFICIENTS;\n        testNumTrials_h[testIndex] = numTrials;\n        testNumPolynomials_h[testIndex] = numPolynomials;\n        testNumCoefficients_h[testIndex] = numCoefficients;\n        // f(x) = cos(x) is chosen for the polynomial approximation error calculations.\n        for(int i = 0; i < numTrials; i++) {\n            float x = distribution(generator);\n            testValuesForX_h[i + testIndex * MAX_NUM_TRIALS] = x;\n            testExpectedOutputs_h[i + testIndex * MAX_NUM_TRIALS] = cos(x);\n        }\n        // Simulating the outcomes of heuristic optimization.\n        for(int i = 0; i < numPolynomials * numCoefficients; i++) {\n            testCoefficients_h[i + testIndex * MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS] = (i % 2 == 0 ? 0.25f : 0.5f);\n        }\n        testIndex++;\n    }\n    // Test 3: sqrt(x)\n    {\n        int numTrials = MAX_NUM_TRIALS;\n        int numPolynomials = MAX_NUM_POLYNOMIALS - 1;\n        int numCoefficients = MAX_NUM_COEFFICIENTS;\n        testNumTrials_h[testIndex] = numTrials;\n        testNumPolynomials_h[testIndex] = numPolynomials;\n        testNumCoefficients_h[testIndex] = numCoefficients;\n        // f(x) = sqrt(x) is chosen for the polynomial approximation error calculations.\n        for(int i = 0; i < numTrials; i++) {\n            float x = distribution(generator);\n            testValuesForX_h[i + testIndex * MAX_NUM_TRIALS] = x;\n            testExpectedOutputs_h[i + testIndex * MAX_NUM_TRIALS] = sqrt(x);\n        }\n        // Simulating the outcomes of heuristic optimization.\n        for(int i = 0; i < numPolynomials * numCoefficients; i++) {\n            testCoefficients_h[i + testIndex * MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Test 4: 1.0f / (1.0f + x)\n    {\n        constexpr float TEST4_CONSTANT = 1.0f;\n        int numTrials = MAX_NUM_TRIALS;\n        int numPolynomials = MAX_NUM_POLYNOMIALS - 10;\n        int numCoefficients = MAX_NUM_COEFFICIENTS;\n        testNumTrials_h[testIndex] = numTrials;\n        testNumPolynomials_h[testIndex] = numPolynomials;\n        testNumCoefficients_h[testIndex] = numCoefficients;\n        // f(x) = 1.0f / (1.0f + x) is chosen for the polynomial approximation error calculations.\n        for(int i = 0; i < numTrials; i++) {\n            float x = distribution(generator);\n            testValuesForX_h[i + testIndex * MAX_NUM_TRIALS] = x;\n            testExpectedOutputs_h[i + testIndex * MAX_NUM_TRIALS] = TEST4_CONSTANT / (TEST4_CONSTANT + x);\n        }\n        // Simulating the outcomes of heuristic optimization.\n        for(int i = 0; i < numPolynomials * numCoefficients; i++) {\n            testCoefficients_h[i + testIndex * MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Test 5: 1.0f - x * x\n    {\n        constexpr float TEST5_CONSTANT = 1.0f;\n        int numTrials = MAX_NUM_TRIALS;\n        int numPolynomials = MAX_NUM_POLYNOMIALS - 100;\n        int numCoefficients = MAX_NUM_COEFFICIENTS;\n        testNumTrials_h[testIndex] = numTrials;\n        testNumPolynomials_h[testIndex] = numPolynomials;\n        testNumCoefficients_h[testIndex] = numCoefficients;\n        // f(x) = 1.0f - x * x is chosen for the polynomial approximation error calculations.\n        for(int i = 0; i < numTrials; i++) {\n            float x = distribution(generator);\n            testValuesForX_h[i + testIndex * MAX_NUM_TRIALS] = x;\n            testExpectedOutputs_h[i + testIndex * MAX_NUM_TRIALS] = TEST5_CONSTANT - x * x;\n        }\n        // Simulating the outcomes of heuristic optimization.\n        for(int i = 0; i < numPolynomials * numCoefficients; i++) {\n            testCoefficients_h[i + testIndex * MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Test 6: x ^ 3\n    {\n        int numTrials = MAX_NUM_TRIALS;\n        int numPolynomials = MAX_NUM_POLYNOMIALS - 1000;\n        int numCoefficients = MAX_NUM_COEFFICIENTS;\n        testNumTrials_h[testIndex] = numTrials;\n        testNumPolynomials_h[testIndex] = numPolynomials;\n        testNumCoefficients_h[testIndex] = numCoefficients;\n        // f(x) = x ^ 3 is chosen for the polynomial approximation error calculations.\n        for(int i = 0; i < numTrials; i++) {\n            float x = distribution(generator);\n            testValuesForX_h[i + testIndex * MAX_NUM_TRIALS] = x;\n            testExpectedOutputs_h[i + testIndex * MAX_NUM_TRIALS] = x * x * x;\n        }\n        // Simulating the outcomes of heuristic optimization.\n        for(int i = 0; i < numPolynomials * numCoefficients; i++) {\n            testCoefficients_h[i + testIndex * MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Test 7: 1.0f - sin(x)^2\n    {\n        constexpr float TEST7_CONSTANT = 1.0f;\n        int numTrials = MAX_NUM_TRIALS;\n        int numPolynomials = MAX_NUM_POLYNOMIALS;\n        int numCoefficients = MAX_NUM_COEFFICIENTS;\n        testNumTrials_h[testIndex] = numTrials;\n        testNumPolynomials_h[testIndex] = numPolynomials;\n        testNumCoefficients_h[testIndex] = numCoefficients;\n        // f(x) = 1.0f - sin(x)^2 is chosen for the polynomial approximation error calculations.\n        for(int i = 0; i < numTrials; i++) {\n            float x = distribution(generator);\n            testValuesForX_h[i + testIndex * MAX_NUM_TRIALS] = x;\n            testExpectedOutputs_h[i + testIndex * MAX_NUM_TRIALS] = TEST7_CONSTANT - sin(x) * sin(x);\n        }\n        // Simulating the outcomes of heuristic optimization.\n        for(int i = 0; i < numPolynomials * numCoefficients; i++) {\n            testCoefficients_h[i + testIndex * MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Iterating the tests.\n    for(int test = 0; test < NUM_TESTS; test++)\n    {\n        int numTrials = testNumTrials_h[test];\n        int numPolynomials = testNumPolynomials_h[test];\n        int numCoefficients = testNumCoefficients_h[test];\n        for(int i = 0; i < numTrials; i++) {\n            xValues_h[i] = testValuesForX_h[i + test * MAX_NUM_TRIALS];\n        }\n        for(int i = 0; i < numPolynomials * numCoefficients; i++) {\n            coefficients_h[i] = testCoefficients_h[i + test * MAX_NUM_POLYNOMIALS * MAX_NUM_COEFFICIENTS];\n        }\n        for(int i = 0; i < numTrials; i++) {\n            expectedOutputs_h[i] = testExpectedOutputs_h[i + test * MAX_NUM_TRIALS];\n        }\n        CUDA_CHECK(cudaMemcpyAsync(xValues_d, xValues_h, sizeof(float) * numTrials, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(coefficients_d, coefficients_h, sizeof(float) * numPolynomials * numCoefficients, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(expectedPolynomialValues_d, expectedOutputs_h, sizeof(float) * numTrials, cudaMemcpyHostToDevice, stream));\n        // Letting CUDA decide the best block size for the optimal shared memory allocation size since the allocation size depends on block size and the hardware.\n        int minGridSize;\n        int blockSize;\n        CUDA_CHECK(cudaOccupancyMaxPotentialBlockSizeVariableSMem(&minGridSize, \n                                                                  &blockSize, \n                                                                  (void*)k_computeAverageErrorsOfPolynomials, \n                                                                  [=](int blockSize) { \n                                                                      int size = blockSize * sizeof(float) * (numCoefficients - NUM_COEFFICIENTS_IN_REGISTERS);\n                                                                      if(size < 0) {\n                                                                          size = 0;\n                                                                      }\n                                                                      size += numTrials * 2 * sizeof(float);\n                                                                      return size; \n                                                                  }));\n        void * args[7] = { &numPolynomials, &numCoefficients, &numTrials, &xValues_d, &expectedPolynomialValues_d, &coefficients_d, &averageErrors_d };\n        // (numCoefficients - NUM_COEFFICIENTS_IN_REGISTERS) represents the number of manual spillover to shared memory per thread.\n        int sharedMem = blockSize * sizeof(float) * (numCoefficients - NUM_COEFFICIENTS_IN_REGISTERS);\n        if(sharedMem < 0) {\n            sharedMem = 0;\n        }\n        // The size of shared memory used for holding data pairs in average error calculations is numTrials * 2 * sizeof(float).\n        sharedMem += numTrials * 2 * sizeof(float);\n        assert((size_t)sharedMem <= deviceProperties.sharedMemPerBlock);\n        // Grid: (minGridSize, 1, 1)\n        // Block: (blockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_computeAverageErrorsOfPolynomials, dim3(minGridSize, 1, 1), dim3(blockSize, 1, 1), args, sharedMem, stream));\n        CUDA_CHECK(cudaMemcpyAsync(averageErrors_h, averageErrors_d, sizeof(float) * numPolynomials, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Comparing the results from device to the results computed on host.\n        for(int pol = 0; pol < numPolynomials; pol++) {\n            float averageError = 0.0f;\n            for(int trial = 0; trial < numTrials; trial++) {\n                float x = xValues_h[trial];\n                float polynomial = coefficients_h[pol];\n                for(int coeff = 1; coeff < numCoefficients; coeff++) {\n                    float coefficient = coefficients_h[pol + coeff * numPolynomials];\n                    polynomial = fmaf(polynomial, x, coefficient);\n                }\n                float error = fabsf(expectedOutputs_h[trial] - polynomial);\n                averageError += error;\n            }\n            averageError /= numTrials;\n            assert(fabsf(averageError - averageErrors_h[pol]) < ERROR_TOLERANCE);\n        }\n        \n    }\n    // Freeing resources.\n    CUDA_CHECK(cudaFreeAsync(xValues_d, stream));\n    CUDA_CHECK(cudaFreeAsync(coefficients_d, stream));\n    CUDA_CHECK(cudaFreeAsync(averageErrors_d, stream));\n    CUDA_CHECK(cudaFreeAsync(expectedPolynomialValues_d, stream));\n    delete [] xValues_h;\n    delete [] coefficients_h;\n    delete [] averageErrors_h;\n    delete [] expectedOutputs_h;\n    delete [] testNumTrials_h;\n    delete [] testNumPolynomials_h;\n    delete [] testNumCoefficients_h;\n    delete [] testValuesForX_h;\n    delete [] testCoefficients_h;\n    delete [] testExpectedOutputs_h;\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_computeAverageErrorsOfPolynomials(int numPolynomials, int numCoefficients, int numTrials, \n                                                    float * xValues_d, float * expectedPolynomialValues_d, float * coefficients_d, \n                                                    float * averageErrors_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/195", "date": "2025-07-30", "prompt": "Write a function that performs batch-wise sorting of key-value pairs using the CUB API. The input data may contain multiple batches, where each batch is processed as an independent, non-overlapping segment. The function should sort key-value pairs within each batch based on the key values. Use stable sorting to ensure that when keys are equal, their relative order from the original input is maintained.\n\nThe signature of the function is void sortSegments(cub::DoubleBuffer<int>& keys_d, cub::DoubleBuffer<int>& values_d, int numberOfItems, int numberOfSegments, int* beginningOfOffset_d, int* endOfOffset_d, cudaStream_t stream), where keys_d represents keys with alternate buffer for CUB sorting operations, values_d defines the values that get rearranged alongside keys during sorting, numberOfItems defines total array size, numberOfSegments represents number of independent sorting groups, beginningOfOffset_d represents device array containing starting indices where each segment begins, endOfOffset_d refers to the device array containing ending indices where each segment ends, stream helps with GPU operations.\n\n>>> sortSegments({45, 30, 90}, {0, 1, 2}, 3, 1, {0}, {3}, stream) --> keys_d: ({30, 45, 90}), values_d: ({1, 0, 2}).\n>>> sortSegments({10, 20, 15, 25, 5}, {0, 1, 2, 3, 4}, 5, 2, {0, 2}, {2, 5}, stream) --> keys_d: ({10, 20, 5, 15, 25}), values_d: ({0, 1, 4, 3, 2}).\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80 ", "ld_flags": "", "declaration": "#include <vector>\n#include <algorithm>\n#include <cassert>\n#include <cuda_runtime.h>\n#include <cub/device/device_segmented_sort.cuh>\nusing namespace std;\n\n// Error-checking macro\n#define CUDA_CHECK(call)                                                   \\\n{                                                                          \\\n    cudaError_t error = call;                                              \\\n    if (error != cudaSuccess) {                                            \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                      \\\n                __FILE__, __LINE__, cudaGetErrorString(error));            \\\n        exit(EXIT_FAILURE);                                                \\\n    }                                                                      \\\n}\n\n// Simplified structure for test cases\nstruct TestCase {\n    vector<int> inputKeys;       \n    vector<int> segmentOffsets;   \n    vector<int> expectedValues;\n    vector<int> expectedKeys;       \n};\n\nvoid sortSegments(cub::DoubleBuffer<int>& keys_d, cub::DoubleBuffer<int>& values_d, int numberOfItems, int numberOfSegments, int* beginningOfOffset_d, int* endOfOffset_d, cudaStream_t stream);\n\nvoid launch() {\n\n    vector<TestCase> testCases = {\n \n        // Single segment sorting [45, 30, 90] -> [30, 45, 90] with values [1, 0, 2]\n        { { 45, 30, 90 },              \n          { 0 },\n          { 1, 0, 2 },     \n          { 30, 45, 90 } },\n\n        // keys [10, 20, 15, 25, 5]\n        // Segment 0 [10, 20] -> [10, 20], Segment 1 [15, 25, 5] -> [5, 15, 25]\n        { { 10, 20, 15, 25, 5 },       \n          { 0, 2 },\n          { 0, 1, 4, 2, 3 },   \n          { 10, 20, 5, 15, 25 } },\n\n        // Test Case 1000 elements, 10 segments of 100 each\n        []() {\n            TestCase tc;\n            const int size = 1000;\n            const int segmentSize = 100;\n            const int numSegments = 10;\n            \n            // Generate reverse order keys\n            for (int i = 0; i < size; ++i) {\n                tc.inputKeys.push_back(size - 1 - i);\n            }\n            \n            // Create segment offsets\n            for (int i = 0; i < numSegments; ++i) {\n                tc.segmentOffsets.push_back(i * segmentSize);\n            }\n            \n            // Generate expected results \n            for (int seg = 0; seg < numSegments; ++seg) {\n                int segStart = seg * segmentSize;\n                \n                // Create pairs for this segment\n                vector<pair<int, int>> segmentPairs;\n                for (int i = 0; i < segmentSize; ++i) {\n                    int globalIndex = segStart + i;\n                    int key = size - 1 - globalIndex;  \n                    segmentPairs.push_back({key, globalIndex});\n                }\n                \n                // Sort by key\n                stable_sort(segmentPairs.begin(), segmentPairs.end());\n                \n                // Extract sorted results \n                for (const auto& pair : segmentPairs) {\n                    tc.expectedValues.push_back(pair.second);\n                    tc.expectedKeys.push_back(pair.first);\n                }\n            }\n            return tc;\n        }(),\n\n        // test case 5000 elements, 25 segments of 200 each\n        // Generate random-like keys using simple pattern\n        []() {\n            TestCase tc;\n            const int size = 5000;\n            const int segmentSize = 200;\n            const int numSegments = 25;\n            \n            // Generate keys\n            for (int i = 0; i < size; ++i) {\n                tc.inputKeys.push_back((i * 17 + 42) % 1000);\n            }\n            \n            // Create segment offsets\n            for (int i = 0; i < numSegments; ++i) {\n                tc.segmentOffsets.push_back(i * segmentSize);\n            }\n            \n            // Generate expected results by sorting each segment\n            for (int seg = 0; seg < numSegments; ++seg) {\n                int segStart = seg * segmentSize;\n                \n                // Create pairs for this segment\n                vector<pair<int, int>> segmentPairs;\n                for (int i = 0; i < segmentSize; ++i) {\n                    int globalIndex = segStart + i;\n                    int key = (globalIndex * 17 + 42) % 1000;\n                    segmentPairs.push_back({key, globalIndex});\n                }\n                \n                // Sort by key \n                stable_sort(segmentPairs.begin(), segmentPairs.end());\n                \n                // Extract sorted values and keys\n                for (const auto& pair : segmentPairs) {\n                    tc.expectedValues.push_back(pair.second);\n                    tc.expectedKeys.push_back(pair.first);\n                }\n            }\n            return tc;\n        }(),\n\n        // test case  10000 elements, single segment\n        // Generate keys with Fibonacci-like pattern\n        []() {\n            TestCase tc;\n            const int size = 10000;\n            \n            // Generate keys\n            for (int i = 0; i < size; ++i) {\n                tc.inputKeys.push_back((i * i + i + 1) % 5000);\n            }\n            \n            // Single segment\n            tc.segmentOffsets.push_back(0);\n            \n            // Generate expected results\n            vector<pair<int, int>> keyIndexPairs;\n            for (int i = 0; i < size; ++i) {\n                keyIndexPairs.push_back({tc.inputKeys[i], i});\n            }\n            \n            // Sort by key (stable sort)\n            stable_sort(keyIndexPairs.begin(), keyIndexPairs.end());\n            \n            // Extract sorted values and keys\n            for (const auto& pair : keyIndexPairs) {\n                tc.expectedValues.push_back(pair.second);\n                tc.expectedKeys.push_back(pair.first);\n            }\n            return tc;\n        }(),\n\n        // Multiple segments with empty segment keys [80, 60, 70, 50, 30, 10, 90]\n        // Segment 0 [80, 60, 70] -> [60, 70, 80], Segment 1 [] empty, Segment 2 [50, 30, 10, 90] -> [10, 30, 50, 90]\n        { { 80, 60, 70, 50, 30, 10, 90 },     \n          { 0, 3, 3 },\n          { 1, 2, 0, 5, 4, 3, 6 },\n          { 60, 70, 80, 10, 30, 50, 90 } },\n\n        // Empty input\n        { { },                         \n          { },\n          { },       \n          { } },\n\n        // Single element\n        { { 100 },                     \n          { 0 },\n          { 0 },     \n          { 100 } },\n\n        // Reverse order keys [400, 200, 100] -> [100, 200, 400]\n        { { 400, 200, 100 },           \n          { 0 },\n          { 2, 1, 0 },     \n          { 100, 200, 400 } },\n\n        // Equal keys [77, 77, 77, 77] stable sort preserves original order\n        { { 77, 77, 77, 77 },          \n          { 0 },\n          { 0, 1, 2, 3 },\n          { 77, 77, 77, 77 } }\n    };\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    for (auto& tc : testCases) {\n        int totalSize = int(tc.inputKeys.size());\n\n        // Empty case\n        if (totalSize == 0) {\n            assert(tc.expectedValues.empty());\n            assert(tc.expectedKeys.empty());\n            continue;\n        }\n\n        // Prepare host data keys and initial indices\n        vector<int> keys_h(totalSize), indices_h(totalSize);\n        for (int i = 0; i < totalSize; ++i) {\n            keys_h[i] = tc.inputKeys[i];\n            indices_h[i] = i;\n        }\n\n        // Prepare segment offsets add totalSize as the final end offset\n        vector<int> segmentOffsets = tc.segmentOffsets;\n        segmentOffsets.push_back(totalSize);\n        int numberOfSegments = int(segmentOffsets.size()) - 1;\n\n        // Allocate & copy device buffers\n        int *keysHost_d=nullptr, *indicesHost_d=nullptr, *offs_d=nullptr;\n        CUDA_CHECK(cudaMallocAsync(&keysHost_d, totalSize * sizeof(int), stream));\n        CUDA_CHECK(cudaMallocAsync(&indicesHost_d, totalSize * sizeof(int), stream));\n        CUDA_CHECK(cudaMallocAsync(&offs_d, (numberOfSegments+1) * sizeof(int), stream));\n        CUDA_CHECK(cudaMemcpyAsync(keysHost_d, keys_h.data(), totalSize * sizeof(int), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(indicesHost_d, indices_h.data(), totalSize * sizeof(int), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(offs_d, segmentOffsets.data(), (numberOfSegments+1) * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        // Allocate workspaces and outputs\n        int *keysIn_d=nullptr, *keysOut_d=nullptr;\n        int *valuesIn_d=nullptr, *valuesOut_d=nullptr;\n        CUDA_CHECK(cudaMallocAsync(&keysIn_d, totalSize * sizeof(int), stream));\n        CUDA_CHECK(cudaMallocAsync(&valuesIn_d, totalSize * sizeof(int), stream));\n        CUDA_CHECK(cudaMallocAsync(&keysOut_d, totalSize * sizeof(int), stream));\n        CUDA_CHECK(cudaMallocAsync(&valuesOut_d, totalSize * sizeof(int), stream));\n\n        // Use segment offsets directly begins from offsets, ends from offsets+1\n        int *segmentBegins_d = offs_d;      \n        int *segmentEnds_d = offs_d + 1;\n\n        // Copy inputs into workspaces\n        CUDA_CHECK(cudaMemcpyAsync(keysIn_d, keysHost_d, totalSize * sizeof(int), cudaMemcpyDeviceToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(valuesIn_d, indicesHost_d, totalSize * sizeof(int), cudaMemcpyDeviceToDevice, stream));\n\n        // Create DoubleBuffer objects\n        cub::DoubleBuffer<int> keysBuffer_d(keysIn_d, keysOut_d);\n        cub::DoubleBuffer<int> valuesBuffer_d(valuesIn_d, valuesOut_d);\n\n        sortSegments(keysBuffer_d, valuesBuffer_d, totalSize, numberOfSegments, segmentBegins_d, segmentEnds_d, stream);\n\n        // Copy back and verify both keys and values\n        vector<int> sortedKeys_h(totalSize);\n        vector<int> sortedValues_h(totalSize);\n        CUDA_CHECK(cudaMemcpyAsync(sortedKeys_h.data(), keysBuffer_d.Current(),\n                                   totalSize * sizeof(int),\n                                   cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaMemcpyAsync(sortedValues_h.data(), valuesBuffer_d.Current(),\n                                   totalSize * sizeof(int),\n                                   cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        //Verify\n        assert(sortedKeys_h == tc.expectedKeys);\n        assert(sortedValues_h == tc.expectedValues);\n\n        // Cleanup\n        CUDA_CHECK(cudaFreeAsync(keysHost_d, stream));\n        CUDA_CHECK(cudaFreeAsync(indicesHost_d, stream));\n        CUDA_CHECK(cudaFreeAsync(offs_d, stream));\n        CUDA_CHECK(cudaFreeAsync(keysIn_d, stream));\n        CUDA_CHECK(cudaFreeAsync(valuesIn_d, stream));\n        CUDA_CHECK(cudaFreeAsync(keysOut_d, stream));\n        CUDA_CHECK(cudaFreeAsync(valuesOut_d, stream));\n    }\n\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\nvoid sortSegments(cub::DoubleBuffer<int>& keys_d, cub::DoubleBuffer<int>& values_d, int numberOfItems, int numberOfSegments, int* beginningOfOffset_d, int* endOfOffset_d, cudaStream_t stream){\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/196", "date": "2025-07-30", "prompt": "Write a function that uses CUDA graphs to transfers chunks of vectors A and B from the host to the device in the first two nodes. Then, execute the CUDA kernel that adds these chunks in the third node and then transfer the result chunk from the device back to the host vector C in the final node. Launch the graph for each vector chunk and synchronize with the host only once after all chunks are processed. Ensure each device memory chunk is of equal size, which is a divisor of the input array size, and update source pointers in the copying node before each launch for the next chunk to enable asynchronous execution with the host. Allocate device memory for the chunk data before creating the CUDA graph and deallocate it after the work is completed.\n\nThe signature of the function to launch the CUDA graph is void runCudaGraph(float* inputA_h, float* inputB_h, float* outputC_h, int arraySize, int chunkSize, cudaStream_t stream), where inputA_h is a pointer to the input A host vector, inputB_h is a pointer to the input B host vector, outputC_h is a pointer to the output C host vector, arraySize is the size of host arrays such as outputC_h to be computed, chunkSize is the number of elements in each chunk for the CUDA kernel to process, and stream is the CUDA stream on which to launch the graph.\n\nThe signature of the CUDA kernel is __global__ void k_addTwoVectors(float* vectorA_d, float* vectorB_d, float* vectorC_d, int chunkSize), where vectorA_d is a pointer to a float vector holding elements for vector A, vectorB_d is a pointer to a float vector holding elements for vector B, vectorC_d is a pointer to a float vector holding elements for vector C, and chunkSize is the number of elements of the chunk to be computed.\n\n>>> runCudaGraph({ 0.0f, 1.0f, 2.0f, ... increasing for all elements }, { 1000.0f, 999.0f, 998.0f, ... decreasing for all elements }, outputC_h, 10240, 2048, stream) -> outputC_h: { 1000.0f, 1000.0f, 1000.0f, ... for all elements }\n>>> runCudaGraph({ 0.0f, 1.0f, 0.0f, 1.0f, ... alternating 0 and 1 for all elements }, { 0.0f, 1.0f, 2.0f, 0.0f, 1.0f, 2.0f, ... repeating same pattern for all elements }, outputC_h, 8192, 2048, stream) -> outputC_h: { 0.0f, 2.0f, 2.0f, 1.0f, 1.0f, 3.0f, 0.0f, 2.0f, 2.0f, 1.0f, 1.0f, 3.0f, ... repeating same pattern for all elements }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if (error != cudaSuccess) {                                \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n// Function to launch a CUDA graph for vector addition using a smaller memory (chunk size) than required for the entire input array.\nvoid runCudaGraph(float* inputA_h, float* inputB_h, float* outputC_h, int arraySize, int chunkSize, cudaStream_t stream);\n\n// The CUDA kernel launched by the CUDA graph performs the element-wise addition of two vectors, with each CUDA thread working independently on a separate element.\n__global__ void k_addTwoVectors(float* vectorA_d, float* vectorB_d, float* vectorC_d, int chunkSize);\n\nvoid launch() {\n    // Algorithm settings.\n    // Maximum size of device arrays to be used for chunks.\n    constexpr int MAX_CHUNK_SIZE = 2048;\n    // Host array size can be an exact multiple of chunk size.\n    constexpr int MAX_ELEMENTS = MAX_CHUNK_SIZE * 5;\n    constexpr float ERROR_TOLERANCE = 1e-4f;\n    float* inputA_h;\n    float* inputB_h;\n    float* outputC_h;\n    CUDA_CHECK(cudaMallocHost(&inputA_h, sizeof(float) * MAX_ELEMENTS));\n    CUDA_CHECK(cudaMallocHost(&inputB_h, sizeof(float) * MAX_ELEMENTS));\n    CUDA_CHECK(cudaMallocHost(&outputC_h, sizeof(float) * MAX_ELEMENTS));\n    \n    // The stream to be utilized by CUDA-graph.\n    cudaStream_t stream;\n    int deviceId = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    // Allocating the stream.\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Test 1: result = index + (1000 - index)\n    {\n        int arraySize = MAX_ELEMENTS;\n        int chunkSize = MAX_CHUNK_SIZE;\n        for (int i = 0; i < arraySize; i++) {\n            inputA_h[i] = i;\n            inputB_h[i] = 1000 - i;\n            outputC_h[i] = 0;\n        }\n        runCudaGraph(inputA_h, inputB_h, outputC_h, arraySize, chunkSize, stream);\n        float expected = 1000.0f;\n        for (int i = 0; i < arraySize; i++) {\n            assert(fabsf(outputC_h[i] - expected) < ERROR_TOLERANCE);\n        }\n    }\n    // Test 2: result = index % 2 + index % 3\n    {\n        int arraySize = MAX_CHUNK_SIZE * 4;\n        int chunkSize = MAX_CHUNK_SIZE;\n        for (int i = 0; i < arraySize; i++) {\n            inputA_h[i] = i % 2;\n            inputB_h[i] = i % 3;\n            outputC_h[i] = 0;\n        }\n        runCudaGraph(inputA_h, inputB_h, outputC_h, arraySize, chunkSize, stream);\n        // Initializing the expected values for outputC_h.\n        std::initializer_list<float> expectedRepeatingPattern = { 0.0f, 2.0f, 2.0f, 1.0f, 1.0f, 3.0f };\n        for (int i = 0; i < arraySize; i++) {\n            assert(fabsf(outputC_h[i] - expectedRepeatingPattern.begin()[i % 6]) < ERROR_TOLERANCE);\n        }\n    }\n    // Test 3: result = index * 2 + index * 3\n    {\n        int arraySize = 1000;\n        int chunkSize = 100;\n        for (int i = 0; i < arraySize; i++) {\n            inputA_h[i] = i * 2;\n            inputB_h[i] = i * 3;\n            outputC_h[i] = 0;\n        }\n        runCudaGraph(inputA_h, inputB_h, outputC_h, arraySize, chunkSize, stream);\n        for (int i = 0; i < arraySize; i++) {\n            float expected = (i * 2 + i * 3);\n            assert(fabsf(outputC_h[i] - expected) < ERROR_TOLERANCE);\n        }\n    }\n    // Test 4: result = index / 2 + index / 3\n    {\n        int arraySize = 10000;\n        int chunkSize = 1000;\n        for (int i = 0; i < arraySize; i++) {\n            inputA_h[i] = i / 2;\n            inputB_h[i] = i / 3;\n            outputC_h[i] = 0;\n        }\n        runCudaGraph(inputA_h, inputB_h, outputC_h, arraySize, chunkSize, stream);\n        for (int i = 0; i < arraySize; i++) {\n            float expected = (i / 2 + i / 3);\n            assert(fabsf(outputC_h[i] - expected) < ERROR_TOLERANCE);\n        }\n    }\n    // Test 5: result = index from B vector\n    {\n        int arraySize = 5000;\n        int chunkSize = 50;\n        for (int i = 0; i < arraySize; i++) {\n            inputA_h[i] = 0;\n            inputB_h[i] = i;\n            outputC_h[i] = 0;\n        }\n        runCudaGraph(inputA_h, inputB_h, outputC_h, arraySize, chunkSize, stream);\n        for (int i = 0; i < arraySize; i++) {\n            float expected = i;\n            assert(fabsf(outputC_h[i] - expected) < ERROR_TOLERANCE);\n        }\n    }\n    // Test 6: result = index from A vector\n    {\n        int arraySize = MAX_ELEMENTS;\n        int chunkSize = MAX_CHUNK_SIZE;\n        for (int i = 0; i < arraySize; i++) {\n            inputA_h[i] = i;\n            inputB_h[i] = 0;\n            outputC_h[i] = 0;\n        }\n        runCudaGraph(inputA_h, inputB_h, outputC_h, arraySize, chunkSize, stream);\n        for (int i = 0; i < arraySize; i++) {\n            float expected = i;\n            assert(fabsf(outputC_h[i] - expected) < ERROR_TOLERANCE);\n        }\n    }\n    // Test 7: result = 1.3f for all elements\n    {\n        int arraySize = MAX_ELEMENTS;\n        int chunkSize = MAX_CHUNK_SIZE;\n        for (int i = 0; i < arraySize; i++) {\n            inputA_h[i] = 1;\n            inputB_h[i] = 0.3f;\n            outputC_h[i] = 0;\n        }\n        runCudaGraph(inputA_h, inputB_h, outputC_h, arraySize, chunkSize, stream);\n        for (int i = 0; i < arraySize; i++) {\n            float expected = 1.3f;\n            assert(fabsf(outputC_h[i] - expected) < ERROR_TOLERANCE);\n        }\n    }\n    \n    // Deallocating the stream.\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    \n    CUDA_CHECK(cudaFreeHost(inputA_h));\n    CUDA_CHECK(cudaFreeHost(inputB_h));\n    CUDA_CHECK(cudaFreeHost(outputC_h));\n}\n\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/197", "date": "2025-07-30", "prompt": "Write a CUDA kernel that computes a 3-input XOR on a one-dimensional array with a window size of 3 for each boolean input. The boolean values are stored in uint32_t integers, each containing 32 boolean values (for a total of number of integers times the number of operations). Each boolean element should become either 1 or 0 based on its own value and its two nearest neighbors, using XOR. Use thread coarsening so that each thread processes 128 booleans. Perform vectorized global-memory accesses by loading and storing those 128 booleans as a single uint4 thereby maximizing memory throughput. For simplicity, treat out-of-bounds values as zero in the XOR calculations.\n\nThe signature of the CUDA kernel is __global__ void k_calculateElement(uint32_t booleanElementsPerInteger, uint32_t numIntegers, uint32_t* integersIn_d, uint32_t* integersOut_d), where booleanElementsPerInteger is an integer that indicates the number of boolean elements per integer in integersIn_d and integersOut_d, numIntegers specifies the number of integers in integersIn_d and integersOut_d, integersIn_d is a pointer to an array of integers in device memory used for reading old boolean element states, and integersOut_d is a pointer to an array of integers in device memory utilized for writing new boolean element states.\n\n>>> k_calculateElement(32,  4, { 2863311530, 2863311530, 2863311530, 11184810 }, integersOut_d) -> integersOut_d: { 2863311531, 2863311530, 2863311530, 27962026 }\n>>> k_calculateElement(32,  1, { 240 }, integersOut_d) -> integersOut_d: { 360 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80 --expt-relaxed-constexpr", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if (error != cudaSuccess) {                                \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n// Each thread loads this number of integers from global memory simultaneously. Since each integer is 32 bits, each thread processes 128 bits at a time before loading new data.\nconstexpr uint32_t FOUR_INTEGERS_PER_THREAD = 4;\n\n// Computes a sliding window of 3-input XOR (where the window consists of the XOR inputs) on all bits in the integersIn_d array and writes the output to the integersOut_d array.\n__global__ void k_calculateElement(uint32_t booleanElementsPerInteger, uint32_t numIntegers, uint32_t* integersIn_d, uint32_t* integersOut_d);\n\nvoid launch() {\n    // Algorithm settings.\n    constexpr size_t MAX_NUM_ELEMENTS = 1000000ull;\n    constexpr uint32_t BITS_PER_BYTE = 8;\n    constexpr uint32_t ELEMENTS_PER_BYTE = BITS_PER_BYTE;\n    constexpr size_t BYTES_PER_INTEGER = sizeof(uint32_t);\n    constexpr uint32_t ELEMENTS_PER_INTEGER = BYTES_PER_INTEGER * ELEMENTS_PER_BYTE;\n    constexpr size_t MAX_NUM_INTEGERS = (MAX_NUM_ELEMENTS + ELEMENTS_PER_INTEGER - 1) / ELEMENTS_PER_INTEGER;\n    constexpr size_t PADDING = FOUR_INTEGERS_PER_THREAD;\n    // Test settings.\n    constexpr uint32_t NUM_TESTS = 7;\n    constexpr uint32_t DETERMINISTIC_RANDOM_SEED = 100;\n    \n    int deviceId = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    \n    int blockSize = 256;\n    int maxBlocks = (deviceProperties.multiProcessorCount * deviceProperties.maxThreadsPerMultiProcessor) / blockSize;\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Allocating host memory.\n    uint32_t* integers_h = new uint32_t[MAX_NUM_INTEGERS + PADDING];\n    bool* elementValues_h = new bool[MAX_NUM_ELEMENTS];\n    bool* testElementValues_h = new bool[NUM_TESTS * MAX_NUM_ELEMENTS];\n    size_t* testNumElements_h = new size_t[NUM_TESTS];\n    \n    // Allocating device memory.\n    uint32_t* integersIn_d;\n    uint32_t* integersOut_d;\n    CUDA_CHECK(cudaMallocAsync(&integersIn_d, BYTES_PER_INTEGER * (MAX_NUM_INTEGERS + PADDING), stream));\n    CUDA_CHECK(cudaMallocAsync(&integersOut_d, BYTES_PER_INTEGER * (MAX_NUM_INTEGERS + PADDING), stream));\n    CUDA_CHECK(cudaMemsetAsync(integersOut_d, 0, BYTES_PER_INTEGER * MAX_NUM_INTEGERS, stream));\n    \n    int testIndex = 0;\n    // Initializing the data for tests 1 to 7.\n    // Test 1: 120 boolean elements with alternating values.\n    {\n        testNumElements_h[testIndex] = 120;\n        // This is encoded as 2863311530, 2863311530, 2863311530, 11184810\n        std::initializer_list<bool> inputs = { \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n            0, 1, 0, 1, 0, 1, 0, 1, 0, 1, \n        };\n        std::copy(inputs.begin(), inputs.end(), &testElementValues_h[testIndex * MAX_NUM_ELEMENTS]);\n        testIndex++;\n    }\n    // Test 2: Eight boolean elements with four consecutive zero values and four consecutive one values.\n    {\n        testNumElements_h[testIndex] = 8;\n        // This is encoded as 240, or 0b00000000000000000000000011110000\n        std::initializer_list<bool> inputs = { 0, 0, 0, 0, 1, 1, 1, 1 };\n        std::copy(inputs.begin(), inputs.end(), &testElementValues_h[testIndex * MAX_NUM_ELEMENTS]);\n        testIndex++;\n    }\n    // Test 3: 10005 boolean elements with randomized 1 or 0 values.\n    {\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_int_distribution<int> distribution(0, 1);\n        size_t numElements = 10005;\n        testNumElements_h[testIndex] = numElements;\n        for (size_t i = 0; i < numElements; i++) {\n            testElementValues_h[testIndex * MAX_NUM_ELEMENTS + i] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Test 4: Maximum number of boolean elements with randomized 1 or 0 values.\n    {\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_int_distribution<int> distribution(0, 1);\n        size_t numElements = MAX_NUM_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        for (size_t i = 0; i < numElements; i++) {\n            testElementValues_h[testIndex * MAX_NUM_ELEMENTS + i] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Test 5: Only 1 boolean element with 1 value.\n    {\n        size_t numElements = 1;\n        testNumElements_h[testIndex] = numElements;\n        testElementValues_h[testIndex * MAX_NUM_ELEMENTS] = 1;\n        testIndex++;\n    }\n    // Test 6: Maximum number of boolean elements with 0 value.\n    {\n        size_t numElements = MAX_NUM_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        for (size_t i = 0; i < numElements; i++) {\n            testElementValues_h[testIndex * MAX_NUM_ELEMENTS + i] = 0;\n        }\n        testIndex++;\n    }\n    // Test 7: Maximum number of boolean elements with one 1 value per 3 boolean elements.\n    {\n        size_t numElements = MAX_NUM_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        for (size_t i = 0; i < numElements; i++) {\n            testElementValues_h[testIndex * MAX_NUM_ELEMENTS + i] = (i % 3 == 0 ? 1 : 0);\n        }\n        testIndex++;\n    }\n    // Conducting the tests.\n    for (int testId = 0; testId < NUM_TESTS; testId++)\n    {\n        size_t numElements = testNumElements_h[testId];\n        uint32_t numIntegers = (numElements + ELEMENTS_PER_INTEGER - 1) / ELEMENTS_PER_INTEGER;\n        assert(numIntegers <= MAX_NUM_INTEGERS);\n        // Clearing the bits.\n        for (uint32_t i = 0; i < numIntegers; i++) {\n            integers_h[i] = 0;\n        }\n        // Encoding the boolean element values as bits represented by 4-byte integers.\n        for (size_t i = 0; i < numElements; i++) {\n            elementValues_h[i] = testElementValues_h[testId * MAX_NUM_ELEMENTS + i];\n            integers_h[i / ELEMENTS_PER_INTEGER] = integers_h[i / ELEMENTS_PER_INTEGER] | (elementValues_h[i] << (i % ELEMENTS_PER_INTEGER));\n        }\n        CUDA_CHECK(cudaMemcpyAsync(integersIn_d, integers_h, BYTES_PER_INTEGER * (numIntegers + PADDING), cudaMemcpyHostToDevice, stream));\n        void* args[4] = { (void*)&ELEMENTS_PER_INTEGER,  &numIntegers, &integersIn_d, &integersOut_d };\n        int requiredBlocks = (numIntegers + blockSize * FOUR_INTEGERS_PER_THREAD - 1) / (blockSize * FOUR_INTEGERS_PER_THREAD);\n        int usedBlocks = requiredBlocks < maxBlocks ? requiredBlocks : maxBlocks;\n        // Grid: (usedBlocks, 1, 1)\n        // Block: (blockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateElement, dim3(usedBlocks, 1, 1), dim3(blockSize, 1, 1), args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(integers_h, integersOut_d, BYTES_PER_INTEGER * numIntegers, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        // Comparing the output with the expected value.\n        for (size_t i = 0; i < numElements; i++) {\n            bool left = (((int64_t)i) - 1 >= 0 ? elementValues_h[i - 1] : 0);\n            bool center = elementValues_h[i];\n            bool right = (i + 1 < numElements ? elementValues_h[i + 1] : 0);\n            auto hostResult = (left ^ center ^ right);\n            auto deviceResult = ((integers_h[i / ELEMENTS_PER_INTEGER] >> (i % ELEMENTS_PER_INTEGER)) & 1);\n            assert(deviceResult == hostResult);\n        }\n    }\n    // Releasing resources.\n    CUDA_CHECK(cudaFreeAsync(integersIn_d, stream));\n    CUDA_CHECK(cudaFreeAsync(integersOut_d, stream));\n    delete [] integers_h;\n    delete [] elementValues_h;\n    delete [] testElementValues_h;\n    delete [] testNumElements_h;\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_calculateElement(uint32_t booleanElementsPerInteger,  uint32_t numIntegers, uint32_t* integersIn_d, uint32_t* integersOut_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/198", "date": "2025-07-30", "prompt": "Write a function that uses CUDA streams and events to calculate ((a + b) + (c - d)) * dotProduct(a, d), where a, b, c, and d are integer vectors. Each kernel should run on its own stream, such as sum = a + b operation on stream1, difference = c - d operation on stream2, and dotProduct = a * d on stream3 (along with a memset operation for zero-initialization of the accumulator). The final operation (sum + difference) * dotProduct should occur on stream1. Since this final operation depends on the first three operations, stream1 should wait for the other streams before launching the final kernel. All device-only operations (such as running kernels) should happen asynchronously with respect to the host. All kernels that are expected to run concurrently should equally share the computational resources of the device, while the kernels that run alone should use full device resources.\n\nThe signature of the function is void run(int* vecA_d, int* vecB_d, int* vecC_d, int* vecD_d, int* vecSum_d, int* vecDiff_d, int* dotProduct_d, int* vecResult_d, int numElements, int maxInFlightThreads, int blockThreads, cudaStream_t stream1, cudaStream_t stream2, cudaStream_t stream3, cudaEvent_t stream2KernelCompletedEvent, cudaEvent_t stream3KernelCompletedEvent), where vecA_d is a pointer to integer vector of a, vecB_d is a pointer to integer vector of b, vecC_d is a pointer to integer vector of c, vecD_d is a pointer to integer vector of d, vecSum_d is a pointer to integer vector for result of a[i] + b[i], vecDiff_d is a pointer to integer vector for result of c[i] - d[i], dotProduct_d is a pointer to a integer scalar result for dotProduct(a, d), vecResult_d is a pointer to an integer vector result for (vecSum_d[i] + vecDiff_d[i]) * dotProduct_d[0], numElements is the number of elements of each vector, maxInFlightThreads is the maximum number of in-flight CUDA-threads supported by the device, blockThreads is the number of CUDA-threads per CUDA-block for each CUDA-kernel, stream1 is the stream to launch kernels for a[i] + b[i] and (vecSum_d[i] + vecDiff_d[i]) * dotProduct_d[0] operations, stream2 is the stream to launch kernel for c[i] - d[i] operation, stream3 is the stream to launch kernel for dotProduct(a, d), stream2KernelCompletedEvent is the event from stream2 to be waited by stream1, and stream3KernelCompletedEvent is the event from stream3 to be waited by stream1.\nThe signatures of the CUDA kernels are:\n__global__ void k_addVectors(int * vecA_d, int * vecB_d, int * vecSum_d, int numElements);\n__global__ void k_subtractVectors(int * vecC_d, int * vecD_d, int * vecDiff_d, int numElements);\n__global__ void k_calculateDotProduct(int * vecA_d, int * vecD_d, int * dotProduct_d, int numElements);\n__global__ void k_addVectorsThenScale(int * vecSum_d, int * vecDiff_d, int * dotProduct_d, int * vecResult_d, int numElements);\nDo not generate kernel body for the kernels above.\n\n>>> run({ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 }, { 10, 9, 8, 7, 6, 5, 4, 3, 2, 1 }, { 2, 2, 2, 2, 2, 3, 3, 3, 3, 3 }, { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }, vecResult_h, 10) -> vecResult_h: { 660, 660, 660, 660, 660, 715, 715, 715, 715, 715 }\n>>> run({ 1, 3, 5, 7, 9 }, { 2, 4, 6, 8, 10 }, { 3, 2, 1, 2, 3 }, { 5, 6, 5, 6, 5 }, vecResult_h, 5) -> vecResult_h: { 135, 405, 945, 1485, 2295 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Kernel for sum = a + b\n__global__ void k_addVectors(int * vecA_d, int * vecB_d, int * vecSum_d, int numElements);\n// Kernel for difference = c - d\n__global__ void k_subtractVectors(int * vecC_d, int * vecD_d, int * vecDiff_d, int numElements);\n// Kernel for dot = a * d\n__global__ void k_calculateDotProduct(int * vecA_d, int * vecD_d, int * dotProduct_d, int numElements);\n// Kernel for (x + y) * z, where x is a + b, y is c - d, and z is scalar result of a * d.\n__global__ void k_addVectorsThenScale(int * vecSum_d, int * vecDiff_d, int * dotProduct_d, int * vecResult_d, int numElements);\n\n// Topology of dependencies (all operations are asynchronous to host):\n//\n//    stream1                                 stream2                                        stream3\n//    __________________________________________________________________________________________________________________________________________\n//    k_addVectors(vecA_d, vecB_d, vecSum_d)  k_subtractVectors(vecC_d, vecD_d, vecDiff_d)   k_calculateDotProduct(vecA_d, vecD_d, dotProduct_d)\n//     |                                      event                                          event\n//     |                                       |                                              |\n//     |                                       |                                              |\n//     |                                       |                                              |\n//     V                                       V                                              V\n//    wait events <---------------------<-----------------------------<------------------------\n//     |\n//     V\n//    k_addVectorsThenScale(vecSum_d, vecDiff_d, dotProduct_d, vecResult_d)\nvoid run(int* vecA_d, int* vecB_d, int* vecC_d, int* vecD_d, \n         int* vecSum_d, int* vecDiff_d, int* dotProduct_d, \n         int* vecResult_d, \n         int numElements, int maxInFlightThreads, int blockThreads, \n         cudaStream_t stream1, cudaStream_t stream2, cudaStream_t stream3, \n         cudaEvent_t stream2KernelCompletedEvent, cudaEvent_t stream3KernelCompletedEvent);\n\n__global__ void k_addVectors(int * vecA_d, int * vecB_d, int * vecSum_d, int numElements) {\n    int numTotalThreads = gridDim.x * blockDim.x;\n    int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n    int numGridStrideLoopIterations = (numElements + numTotalThreads - 1) / numTotalThreads;\n    for(int i = 0; i < numGridStrideLoopIterations; i++) {\n        int elementIndex = i * numTotalThreads + threadIndex;\n        if(elementIndex < numElements) {\n            vecSum_d[elementIndex] = vecA_d[elementIndex] + vecB_d[elementIndex];\n        }\n    }\n}\n__global__ void k_subtractVectors(int * vecC_d, int * vecD_d, int * vecDiff_d, int numElements) {\n    int numTotalThreads = gridDim.x * blockDim.x;\n    int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n    int numGridStrideLoopIterations = (numElements + numTotalThreads - 1) / numTotalThreads;\n    for(int i = 0; i < numGridStrideLoopIterations; i++) {\n        int elementIndex = i * numTotalThreads + threadIndex;\n        if(elementIndex < numElements) {\n            vecDiff_d[elementIndex] = vecC_d[elementIndex] - vecD_d[elementIndex];\n        }\n    }\n}\n__global__ void k_calculateDotProduct(int * vecA_d, int * vecD_d, int * dotProduct_d, int numElements) {\n    int numTotalThreads = gridDim.x * blockDim.x;\n    int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n    int numGridStrideLoopIterations = (numElements + numTotalThreads - 1) / numTotalThreads;\n    int accumulator = 0;\n    for(int i = 0; i < numGridStrideLoopIterations; i++) {\n        int elementIndex = i * numTotalThreads + threadIndex;\n        if(elementIndex < numElements) {\n            accumulator += vecA_d[elementIndex] * vecD_d[elementIndex];\n        }\n    }\n    atomicAdd(dotProduct_d, accumulator);\n}\n__global__ void k_addVectorsThenScale(int * vecSum_d, int * vecDiff_d, int * dotProduct_d, int * vecResult_d, int numElements) {\n    int numTotalThreads = gridDim.x * blockDim.x;\n    int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n    int numGridStrideLoopIterations = (numElements + numTotalThreads - 1) / numTotalThreads;\n    int scale = dotProduct_d[0];\n    for(int i = 0; i < numGridStrideLoopIterations; i++) {\n        int elementIndex = i * numTotalThreads + threadIndex;\n        if(elementIndex < numElements) {\n            vecResult_d[elementIndex] = (vecSum_d[elementIndex] + vecDiff_d[elementIndex]) * scale;\n        }\n    }\n}\n\nvoid launch() {\n    constexpr int MAX_ELEMENTS = 100000;\n    constexpr int NUM_TESTS = 7;\n\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, 0));\n    int blockThreads = 256;\n    int maxInFlightThreads = deviceProperties.maxThreadsPerMultiProcessor * deviceProperties.multiProcessorCount;\n    \n    // Creating host buffers to be used as storage of test data.\n    int *testVecA_h = new int[NUM_TESTS * MAX_ELEMENTS];\n    int *testVecB_h = new int[NUM_TESTS * MAX_ELEMENTS];\n    int *testVecC_h = new int[NUM_TESTS * MAX_ELEMENTS];\n    int *testVecD_h = new int[NUM_TESTS * MAX_ELEMENTS];\n    int *testVecResult_h = new int[MAX_ELEMENTS];\n    int *testNumElements_h = new int[NUM_TESTS];\n\n    // Creating streams and events.\n    cudaStream_t stream1, stream2, stream3;\n    cudaEvent_t stream2KernelCompletedEvent, stream3KernelCompletedEvent;\n    cudaEvent_t stream1InputReadyEvent;\n    CUDA_CHECK(cudaStreamCreate(&stream1));\n    CUDA_CHECK(cudaStreamCreate(&stream2));\n    CUDA_CHECK(cudaStreamCreate(&stream3));\n    CUDA_CHECK(cudaEventCreate(&stream1InputReadyEvent));\n    CUDA_CHECK(cudaEventCreate(&stream2KernelCompletedEvent));\n    CUDA_CHECK(cudaEventCreate(&stream3KernelCompletedEvent));\n\n    // Creating device buffers.\n    // First level inputs.\n    int* vecA_d;\n    int* vecB_d;\n    int* vecC_d;\n    int* vecD_d;\n    // Second level inputs.\n    int* vecSum_d; \n    int* vecDiff_d;\n    int* dotProduct_d;\n    // Output.\n    int* vecResult_d;\n    CUDA_CHECK(cudaMallocAsync(&vecA_d, sizeof(int) * MAX_ELEMENTS, stream1));\n    CUDA_CHECK(cudaMallocAsync(&vecB_d, sizeof(int) * MAX_ELEMENTS, stream1));\n    CUDA_CHECK(cudaMallocAsync(&vecC_d, sizeof(int) * MAX_ELEMENTS, stream1));\n    CUDA_CHECK(cudaMallocAsync(&vecD_d, sizeof(int) * MAX_ELEMENTS, stream1));\n    CUDA_CHECK(cudaMallocAsync(&vecSum_d, sizeof(int) * MAX_ELEMENTS, stream1));\n    CUDA_CHECK(cudaMallocAsync(&vecDiff_d, sizeof(int) * MAX_ELEMENTS, stream1));\n    CUDA_CHECK(cudaMallocAsync(&dotProduct_d, sizeof(int), stream1));\n    CUDA_CHECK(cudaMallocAsync(&vecResult_d, sizeof(int) * MAX_ELEMENTS, stream1));\n\n    // Test 1\n    int testIndex = 0;\n    {\n        int numElements = 10;\n        testNumElements_h[testIndex] = numElements;\n        std::initializer_list<int> vecA = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n        std::initializer_list<int> vecB = { 10, 9, 8, 7, 6, 5, 4, 3, 2, 1 };\n        std::initializer_list<int> vecC = { 2, 2, 2, 2, 2, 3, 3, 3, 3, 3 };\n        std::initializer_list<int> vecD = { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 };\n        std::copy(vecA.begin(), vecA.end(), &testVecA_h[testIndex * MAX_ELEMENTS]);\n        std::copy(vecB.begin(), vecB.end(), &testVecB_h[testIndex * MAX_ELEMENTS]);\n        std::copy(vecC.begin(), vecC.end(), &testVecC_h[testIndex * MAX_ELEMENTS]);\n        std::copy(vecD.begin(), vecD.end(), &testVecD_h[testIndex * MAX_ELEMENTS]);\n        testIndex++;\n    }\n    // Test 2\n    {\n        int numElements = 5;\n        testNumElements_h[testIndex] = numElements;\n        std::initializer_list<int> vecA = { 1, 3, 5, 7, 9 };\n        std::initializer_list<int> vecB = { 2, 4, 6, 8, 10 };\n        std::initializer_list<int> vecC = { 3, 2, 1, 2, 3 };\n        std::initializer_list<int> vecD = { 5, 6, 5, 6, 5 };\n        std::copy(vecA.begin(), vecA.end(), &testVecA_h[testIndex * MAX_ELEMENTS]);\n        std::copy(vecB.begin(), vecB.end(), &testVecB_h[testIndex * MAX_ELEMENTS]);\n        std::copy(vecC.begin(), vecC.end(), &testVecC_h[testIndex * MAX_ELEMENTS]);\n        std::copy(vecD.begin(), vecD.end(), &testVecD_h[testIndex * MAX_ELEMENTS]);\n        testIndex++;\n    }\n    // Test 3\n    {\n        int numElements = 1;\n        testNumElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testVecA_h[testIndex * MAX_ELEMENTS + i] = 1;\n            testVecB_h[testIndex * MAX_ELEMENTS + i] = 1;\n            testVecC_h[testIndex * MAX_ELEMENTS + i] = 1;\n            testVecD_h[testIndex * MAX_ELEMENTS + i] = 1;\n        }\n        testIndex++;\n    }\n    // Test 4\n    {\n        int numElements = MAX_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testVecA_h[testIndex * MAX_ELEMENTS + i] = 1;\n            testVecB_h[testIndex * MAX_ELEMENTS + i] = 0;\n            testVecC_h[testIndex * MAX_ELEMENTS + i] = 1;\n            testVecD_h[testIndex * MAX_ELEMENTS + i] = 0;\n        }\n        testIndex++;\n    }\n    // Test 5\n    {\n        int numElements = MAX_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testVecA_h[testIndex * MAX_ELEMENTS + i] = i;\n            testVecB_h[testIndex * MAX_ELEMENTS + i] = i;\n            testVecC_h[testIndex * MAX_ELEMENTS + i] = 1;\n            testVecD_h[testIndex * MAX_ELEMENTS + i] = 0;\n        }\n        testIndex++;\n    }\n    // Test 6\n    {\n        int numElements = MAX_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testVecA_h[testIndex * MAX_ELEMENTS + i] = i;\n            testVecB_h[testIndex * MAX_ELEMENTS + i] = i;\n            testVecC_h[testIndex * MAX_ELEMENTS + i] = 1;\n            testVecD_h[testIndex * MAX_ELEMENTS + i] = 0;\n        }\n        testIndex++;\n    }\n    // Test 7\n    {\n        int numElements = MAX_ELEMENTS;\n        testNumElements_h[testIndex] = numElements;\n        for(int i = 0; i < numElements; i++) {\n            testVecA_h[testIndex * MAX_ELEMENTS + i] = i;\n            testVecB_h[testIndex * MAX_ELEMENTS + i] = i;\n            testVecC_h[testIndex * MAX_ELEMENTS + i] = i;\n            testVecD_h[testIndex * MAX_ELEMENTS + i] = -(i % 256);\n        }\n        testIndex++;\n    }\n    // Iterating the tests.\n    for(int test = 0; test < testIndex; test++)\n    {\n        int numElements = testNumElements_h[test];\n        // Copying inputs to the device and ensuring all streams have visibility to the latest data bits.\n        CUDA_CHECK(cudaMemcpyAsync(vecA_d, &testVecA_h[test * MAX_ELEMENTS], sizeof(int) * numElements, cudaMemcpyHostToDevice, stream1));\n        CUDA_CHECK(cudaMemcpyAsync(vecB_d, &testVecB_h[test * MAX_ELEMENTS], sizeof(int) * numElements, cudaMemcpyHostToDevice, stream1));\n        CUDA_CHECK(cudaMemcpyAsync(vecC_d, &testVecC_h[test * MAX_ELEMENTS], sizeof(int) * numElements, cudaMemcpyHostToDevice, stream1));\n        CUDA_CHECK(cudaMemcpyAsync(vecD_d, &testVecD_h[test * MAX_ELEMENTS], sizeof(int) * numElements, cudaMemcpyHostToDevice, stream1));\n        CUDA_CHECK(cudaEventRecord(stream1InputReadyEvent, stream1));\n        CUDA_CHECK(cudaStreamWaitEvent(stream2, stream1InputReadyEvent));\n        CUDA_CHECK(cudaStreamWaitEvent(stream3, stream1InputReadyEvent));\n        \n        // Calculating with CUDA streams and events.\n        run(vecA_d, vecB_d, vecC_d, vecD_d, vecSum_d, vecDiff_d, dotProduct_d, vecResult_d, numElements, maxInFlightThreads, blockThreads, stream1, stream2, stream3, stream2KernelCompletedEvent, stream3KernelCompletedEvent);\n        \n        // Stream1 already has the latest outputs, so it doesn't require waiting for the other streams again.\n        CUDA_CHECK(cudaMemcpyAsync(testVecResult_h, vecResult_d, sizeof(int) * numElements, cudaMemcpyDeviceToHost, stream1));\n        CUDA_CHECK(cudaStreamSynchronize(stream1));\n        // Calculating the anticipated outcome.\n        int dotProduct = 0;\n        for(int i = 0; i < numElements; i++) {\n           dotProduct += testVecA_h[test * MAX_ELEMENTS + i] * testVecD_h[test * MAX_ELEMENTS + i];\n        }\n        for(int i = 0; i < numElements; i++) {\n           int result = ((testVecA_h[test * MAX_ELEMENTS + i] + testVecB_h[test * MAX_ELEMENTS + i]) + (testVecC_h[test * MAX_ELEMENTS + i] - testVecD_h[test * MAX_ELEMENTS + i])) * dotProduct;\n           // Verifying that accurate results are obtained.\n           assert(result == testVecResult_h[i]);\n        }\n    }\n    \n    // Releasing device buffers.\n    CUDA_CHECK(cudaFreeAsync(vecSum_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(vecDiff_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(dotProduct_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(vecResult_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(vecA_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(vecB_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(vecC_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(vecD_d, stream1));\n    \n    // Releasing streams and events.\n    CUDA_CHECK(cudaEventDestroy(stream1InputReadyEvent));\n    CUDA_CHECK(cudaEventDestroy(stream2KernelCompletedEvent));\n    CUDA_CHECK(cudaEventDestroy(stream3KernelCompletedEvent));\n    CUDA_CHECK(cudaStreamDestroy(stream1));\n    CUDA_CHECK(cudaStreamDestroy(stream2));\n    CUDA_CHECK(cudaStreamDestroy(stream3));\n    \n    // Releasing all host buffers.\n    delete [] testVecA_h;\n    delete [] testVecB_h;\n    delete [] testVecC_h;\n    delete [] testVecD_h;\n    delete [] testVecResult_h;\n}\n\nvoid run(int* vecA_d, int* vecB_d, int* vecC_d, int* vecD_d, \n         int* vecSum_d, int* vecDiff_d, int* dotProduct_d, \n         int* vecResult_d, \n         int numElements, int maxInFlightThreads, int blockThreads, \n         cudaStream_t stream1, cudaStream_t stream2, cudaStream_t stream3, \n         cudaEvent_t stream2KernelCompletedEvent, cudaEvent_t stream3KernelCompletedEvent) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/199", "date": "2025-07-30", "prompt": "Write a CUDA kernel for 2D image convolution with a sharpening filter that processes floating point images with zero-padding boundary conditions. The kernel should use spatial tiling of size 16 and register tiling, where each thread processes multiple pixels.\n\nThe signature of the function is __global__ void k_convolve2D(float* image, float* filter, float* output, int height, int width, int filterSize), where image is pointer to Input image array stored as a 1D array in row-major order, filter is pointer to constant float array filter/filter weights, output is pointer to float array of output buffer to store result, height & width is dimensions of input, filterSize is dimension of the square convolution filter.\n\n>>> k_convolve2D({0.033, 0.330, 0.691, 0.422, 0.206, 0.250, 0.637, 0.864, 0.302, 0.025}, {0.160, 0.114, 0.136, 0.163, 0.055, 0.156, 0.024, 0.034, 0.159}, output, 32, 32, 3) -> output: ({0.238, 0.226, 0.208, 0.212, 0.271, 0.338, 0.347, 0.384, 0.260, 0.186})\n>>> k_convolve2D({0.547, 0.587, 0.830, 0.364, 0.888, 0.892, 0.808, 0.634, 0.472, 0.852}, {0.073, 0.015, 0.020, 0.055, 0.003, 0.024, 0.010, 0.070, 0.002, 0.040, 0.033, 0.005, 0.015, 0.071, 0.039, 0.071, 0.066, 0.026, 0.021, 0.056, 0.066, 0.029, 0.062, 0.061, 0.067}, output, 64, 128, 5) -> output: ({0.183, 0.208, 0.277, 0.268, 0.279, 0.184, 0.264, 0.312, 0.329, 0.358})\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <cstdlib>\n#include <ctime>\n#include <cmath>\n\n#undef NDEBUG\n#include <assert.h>\n\n#include <cuda_runtime.h>\n\n// Tile dimensions\nconst int TILE_SIZE = 16;\n\n#define CUDA_CHECK(call) {                                                        \\\n        cudaError_t err = call;                                                   \\\n        if (err != cudaSuccess) {                                                 \\\n            fprintf(stderr, \"CUDA error in file '%s' in line %d: %s.\\n\",          \\\n                    __FILE__, __LINE__, cudaGetErrorString(err));                 \\\n            exit(EXIT_FAILURE);                                                   \\\n        }                                                                         \\\n}\n\n__global__ void k_convolve2D(float* image, float* filter, float* output, int height, int width, int filterSize);\n\nvoid launch() {\n    const float TOLERANCE = 1E-3;\n    const int RANDOM_SEED = 42;\n    const int NUM_TEST_CASES = 7;\n    const int DEVICE_ID = 0;\n    const int NO_OF_ITEMS = 3;\n    const int WARP_SIZE = 32;\n    const int MAX_IMAGE_HEIGHT = 1024;\n    const int MAX_IMAGE_WIDTH = 1024;\n    const int MAX_FILTER_SIZE = 15;\n    const float INPUT_MIN_VALUE = 0.0f;\n    const float INPUT_MAX_VALUE = 1.0f;\n    const float OUTPUT_MIN_VALUE = 0.0f;\n    const float OUTPUT_MAX_VALUE = 1.0f;\n    srand(RANDOM_SEED);\n\n    // Get CUDA device properties\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, DEVICE_ID));\n\n    // Calculate optimal block size based on device properties\n    const int maxThreadsPerBlock = prop.maxThreadsPerBlock;\n    const int maxBlockDimX = prop.maxThreadsDim[0];\n    const int maxBlockDimY = prop.maxThreadsDim[1];\n    const int maxGridDimX = prop.maxGridSize[0];\n    const int maxGridDimY = prop.maxGridSize[1];\n\n    // Choose block dimensions that are multiples of warp size for optimal performance\n    const int blockDimX = min(WARP_SIZE, maxBlockDimX);\n    const int blockDimY = min(maxThreadsPerBlock / blockDimX, maxBlockDimY);\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Calculate maximum sizes for memory allocation\n    const int MAX_IMAGE_SIZE = MAX_IMAGE_HEIGHT * MAX_IMAGE_WIDTH;\n    const int MAX_FILTER_SIZE_TOTAL = MAX_FILTER_SIZE * MAX_FILTER_SIZE;\n\n    // Allocate device memory once for maximum sizes\n    float* image_d, * filter_d, * output_d;\n    CUDA_CHECK(cudaMallocAsync(&image_d, MAX_IMAGE_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&filter_d, MAX_FILTER_SIZE_TOTAL * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, MAX_IMAGE_SIZE * sizeof(float), stream));\n\n    // Allocate host memory once for maximum sizes\n    float* image_h = static_cast<float*>(malloc(MAX_IMAGE_SIZE * sizeof(float)));\n    float* filter_h = static_cast<float*>(malloc(MAX_FILTER_SIZE_TOTAL * sizeof(float)));\n    float* output_h = static_cast<float*>(malloc(MAX_IMAGE_SIZE * sizeof(float)));\n    float* expected_h = static_cast<float*>(malloc(MAX_IMAGE_SIZE * sizeof(float)));\n\n    // Lambda functions for helper operations\n    auto generateRandomImage = [INPUT_MIN_VALUE, INPUT_MAX_VALUE](float* image, int size) {\n        for (int i = 0; i < size; i++) {\n            // Generate normalized input values in range [0.0, 1.0]\n            image[i] = INPUT_MIN_VALUE + (static_cast<float>(rand()) / RAND_MAX) * (INPUT_MAX_VALUE - INPUT_MIN_VALUE);\n        }\n    };\n\n    auto generateSharpeningfilter = [](float* filter, int filterSize) {\n        const int filterSizeTotal = filterSize * filterSize;\n        const int center = filterSize / 2;\n\n        // Initialize all values to 0\n        for (int i = 0; i < filterSizeTotal; i++) {\n            filter[i] = 0.0f;\n        }\n\n        if (filterSize == 3) {\n            // Standard 3x3 sharpening filter\n            float sharpen3x3[] = {\n                 0.0f, -1.0f,  0.0f,\n                -1.0f,  5.0f, -1.0f,\n                 0.0f, -1.0f,  0.0f\n            };\n            for (int i = 0; i < 9; i++) {\n                filter[i] = sharpen3x3[i];\n            }\n        } else if (filterSize == 5) {\n            // 5x5 sharpening filter\n            float sharpen5x5[] = {\n                 0.0f,  0.0f, -1.0f,  0.0f,  0.0f,\n                 0.0f, -1.0f, -2.0f, -1.0f,  0.0f,\n                -1.0f, -2.0f, 17.0f, -2.0f, -1.0f,\n                 0.0f, -1.0f, -2.0f, -1.0f,  0.0f,\n                 0.0f,  0.0f, -1.0f,  0.0f,  0.0f\n            };\n            for (int i = 0; i < 25; i++) {\n                filter[i] = sharpen5x5[i];\n            }\n        } else {\n            // Generic sharpening filter for larger sizes\n            // Set center to positive value, immediate neighbors to -1\n            const int centerIdx = center * filterSize + center;\n            filter[centerIdx] = (float)(filterSize * filterSize);\n\n            // Set immediate neighbors to -1\n            for (int dy = -1; dy <= 1; dy++) {\n                for (int dx = -1; dx <= 1; dx++) {\n                    if (dy == 0 && dx == 0) continue;\n                    int row = center + dy;\n                    int col = center + dx;\n                    if (row >= 0 && row < filterSize && col >= 0 && col < filterSize) {\n                        filter[row * filterSize + col] = -1.0f;\n                    }\n                }\n            }\n        }\n    };\n\n    auto computeExpectedValue = [OUTPUT_MIN_VALUE, OUTPUT_MAX_VALUE](const float* image, const float* filter, int row, int col, int height, int width, int filterSize) -> float {\n        float sum = 0.0f;\n        const int halfFilter = filterSize / 2;\n\n        for (int ky = 0; ky < filterSize; ky++) {\n            for (int kx = 0; kx < filterSize; kx++) {\n                const int imageRow = row - halfFilter + ky;\n                const int imageCol = col - halfFilter + kx;\n\n                if (imageRow >= 0 && imageRow < height && imageCol >= 0 && imageCol < width) {\n                    sum += image[imageRow * width + imageCol] * filter[ky * filterSize + kx];\n                }\n            }\n        }\n\n        // Clamp output to [0,1] range to ensure valid output\n        return fmaxf(OUTPUT_MIN_VALUE, fminf(OUTPUT_MAX_VALUE, sum));\n    };\n\n    // Test case configurations: {height, width, filterSize}\n    const int testCases[NUM_TEST_CASES][NO_OF_ITEMS] = {\n        {32, 32, 3},      // Test Case 1: 32x32 with 3x3 filter\n        {64, 128, 5},     // Test Case 2: 64x128 with 5x5 filter\n        {256, 192, 7},    // Test Case 3: 256x192 with 7x7 filter\n        {128, 512, 9},    // Test Case 4: 128x512 with 9x9 filter\n        {512, 256, 11},   // Test Case 5: 512x256 with 11x11 filter\n        {1024, 768, 13},  // Test Case 6: 1024x768 with 13x13 filter\n        {768, 1024, 15}   // Test Case 7: 768x1024 with 15x15 filter\n    };\n\n    for (int testIdx = 0; testIdx < NUM_TEST_CASES; testIdx++) {\n        int height = testCases[testIdx][0];\n        int width = testCases[testIdx][1];\n        int filterSize = testCases[testIdx][2];\n        const int imageSize = height * width;\n        const int filterSizeTotal = filterSize * filterSize;\n\n        // Generate random data\n        generateRandomImage(image_h, imageSize);\n        generateSharpeningfilter(filter_h, filterSize);\n\n        // Compute expected output\n        for (int row = 0; row < height; row++) {\n            for (int col = 0; col < width; col++) {\n                expected_h[row * width + col] = computeExpectedValue(image_h, filter_h, row, col, height, width, filterSize);\n            }\n        }\n\n        // Copy data to device asynchronously\n        CUDA_CHECK(cudaMemcpyAsync(image_d, image_h, imageSize * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(filter_d, filter_h, filterSizeTotal * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        // Configure filter launch parameters based on device properties\n        const dim3 blockSize(blockDimX, blockDimY);\n        const int gridDimX = min((width + (blockSize.x * TILE_SIZE) - 1) / (blockSize.x * TILE_SIZE), maxGridDimX);\n        const int gridDimY = min((height + (blockSize.y * TILE_SIZE) - 1) / (blockSize.y * TILE_SIZE), maxGridDimY);\n        const dim3 gridSize(gridDimX, gridDimY);\n\n        // Launch filter\n        void *args[] = {&image_d, &filter_d, &output_d, &height, &width, &filterSize};\n        CUDA_CHECK(cudaLaunchKernel(reinterpret_cast<void*>(k_convolve2D), gridSize, blockSize, args, 0, stream));\n\n        // Copy results back to host asynchronously\n        CUDA_CHECK(cudaMemcpyAsync(output_h, output_d, imageSize * sizeof(float), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        for (int i = 0; i < imageSize; ++i) {\n            assert(fabs(output_h[i] - expected_h[i]) < TOLERANCE);\n        }\n    }\n\n    // Free device memory asynchronously\n    CUDA_CHECK(cudaFreeAsync(image_d, stream));\n    CUDA_CHECK(cudaFreeAsync(filter_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n\n    // Free host memory\n    free(image_h);\n    free(filter_h);\n    free(output_h);\n    free(expected_h);\n\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_convolve2D(float* image, float* filter, float* output, int height, int width, int filterSize) {\n\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/200", "date": "2025-07-30", "prompt": "Write a function that utilizes CUDA streams and events to compute the number of collisions for a dynamic set of points against a fixed number of triangles and rectangles in two dimensions. At the first level, perform host-to-device data transfers, zero-initialize data on the device, and execute triangle collision calculations on stream 1. Meanwhile, stream 2 should wait for copy event to make sure the data transfer and initialization to finish in stream1, and need to execute rectangle collision calculations. At the second level, ensure stream 1 waits for an event of stream 2 to complete rectangle collision calculations before copying results to the host, and then synchronize with the host. Avoid a third summation level by using a common array of counts for both kernel executions.\n\nThe signature of the function is void run(int numPoints, int numTriangles, int numRectangles, cudaStream_t stream1, cudaStream_t stream2, cudaEvent_t event1, cudaEvent_t event2, int* pointCollisionCount_h, float* pointPositionX_h, float* pointPositionY_h, int* pointCollisionCount_d, float* pointPositionX_d, float* pointPositionY_d, float* triangleVertexX_d, float* triangleVertexY_d, float* rectangleVertexX_d, float* rectangleVertexY_d), where numPoints is the number of points, numTriangles is the number of triangles, numRectangles is the number of rectangles, stream1 is the first CUDA stream, stream2 is the second CUDA stream, event1 is the first CUDA event, event2 is the second CUDA event, pointCollisionCount_h is a pointer to the collision count array on the host, pointPositionX_h is a pointer to the array of point X coordinates on the host, pointPositionY_h is a pointer to the array of point Y coordinates on the host, pointCollisionCount_d is a pointer to the collision count array on the device, pointPositionX_d is a pointer to the array of point X coordinates on the device, pointPositionY_d is a pointer to the array of point Y coordinates on the device, triangleVertexX_d is a pointer to the triangle vertex X coordinate array on the device, triangleVertexY_d is a pointer to the triangle vertex Y coordinate array on the device, rectangleVertexX_d is a pointer to the rectangle vertex X coordinate array on the device, and rectangleVertexY_d is a pointer to the rectangle vertex Y coordinate array on the device.\n\nThe signatures of the CUDA kernels are:\n__global__ void k_countCollisionsWithTriangles(int* pointCollisionCount_d, float* pointPositionX_d, float* pointPositionY_d, float* triangleVertexX_d, float* triangleVertexY_d, int numPoints, int numTriangles). This kernel calculates the number of collisions between each point and each triangle, then atomically increments the counts.\n__global__ void k_countCollisionsWithRectangles(int* pointCollisionCount_d, float* pointPositionX_d, float* pointPositionY_d, float* rectangleVertexX_d, float* rectangleVertexY_d, int numPoints, int numRectangles). This kernel calculates the number of collisions between each point and each rectangle, then atomically increments the counts..\nDo not generate kernel body for the kernels above.\n\n>>> void run(10, 4, 3, stream1, stream2, event1, event2, pointCollisionCount_h, \n            { 10.0f, 20.0f, 100.0f, 200.0f, 350.0f, 350.0f, 600.0f, 700.0f, 750.0f, 775.0f }, \n            { 10.0f, 20.0f, 100.0f, 200.0f, 350.0f, 500.0f, 600.0f, 700.0f, 750.0f, 775.0f }, \n            pointCollisionCount_d, pointPositionX_d, pointPositionY_d, \n            { 5.5f, 155.5f, 305.5f, 455.5f, 85.5f, 285.5f, 485.5f, 685.5f, 15.5f, 165.5f, 315.5f, 465.5f }, \n            { 5.5f, 105.5f, 205.5f, 305.5f, 15.5f, 115.5f, 215.5f, 315.5f, 85.5f, 235.5f, 385.5f, 535.5f }, \n            { 660.0f, 340.0f, 20.0f, 740.0f, 560.0f, 380.0f, 740.0f, 560.0f, 380.0f, 660.0f, 340.0f, 20.0f }, \n            { 80.0f, 160.0f, 240.0f, 80.0f, 160.0f, 240.0f, 160.0f, 380.0f, 600.0f, 160.0f, 380.0f, 600.0f }) -> pointCollisionCount_h: { 0, 0, 0, 0, 2, 2, 1, 0, 0, 0 }\n>>> void run(5, 4, 3, stream1, stream2, event1, event2, pointCollisionCount_h, \n            { 300.0f, 120.0f, 200.0f, 200.0f, 250.0f }, \n            { 300.0f, 100.0f, 100.0f, 100.0f, 100.0f }, \n            pointCollisionCount_d, pointPositionX_d, pointPositionY_d, \n            { 5.5f, 155.5f, 305.5f, 455.5f, 85.5f, 285.5f, 485.5f, 685.5f, 15.5f, 165.5f, 315.5f, 465.5f }, \n            { 5.5f, 105.5f, 205.5f, 305.5f, 15.5f, 115.5f, 215.5f, 315.5f, 85.5f, 235.5f, 385.5f, 535.5f }, \n            { 660.0f, 340.0f, 20.0f, 740.0f, 560.0f, 380.0f, 740.0f, 560.0f, 380.0f, 660.0f, 340.0f, 20.0f }, \n            { 80.0f, 160.0f, 240.0f, 80.0f, 160.0f, 240.0f, 160.0f, 380.0f, 600.0f, 160.0f, 380.0f, 600.0f }) -> pointCollisionCount_h: { 1, 0, 0, 0, 0 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <random>\n#include <algorithm>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if (error != cudaSuccess) {                                \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n// CUDA settings.\nconstexpr int DEVICE_INDEX = 0;\n__global__ void k_countCollisionsWithTriangles(int* pointCollisionCount_d, float* pointPositionX_d, float* pointPositionY_d, float* triangleVertexX_d, float* triangleVertexY_d, int numPoints, int numTriangles) {\n    int globalThreadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n    int numGlobalThreads = blockDim.x * gridDim.x;\n    int numTotalPointTrianglePairs = numPoints * numTriangles;\n    int numGridStrideLoopIterations = (numTotalPointTrianglePairs + numGlobalThreads - 1) / numGlobalThreads;\n    // Iterating over point-triangle pairs with a grid-stride loop.\n    // Pattern is { point 1 triangle 1, point 1 triangle 2, point 1 triangle 3, ..., point 2 triangle 1, point 2 triangle 2, ... }\n    for (int gridStrideIteration = 0; gridStrideIteration < numGridStrideLoopIterations; gridStrideIteration++) {\n        int pairIndex = gridStrideIteration * numGlobalThreads + globalThreadIndex;\n        if (pairIndex < numTotalPointTrianglePairs) {\n            int pointIndex = pairIndex / numTriangles;\n            int triangleIndex = pairIndex % numTriangles;\n            // Computing if there is a collision between the selected point and the selected triangle.\n            float pointX = pointPositionX_d[pointIndex];\n            float pointY = pointPositionY_d[pointIndex];\n            float triangleVertex1X = triangleVertexX_d[triangleIndex];\n            float triangleVertex2X = triangleVertexX_d[triangleIndex + numTriangles];\n            float triangleVertex3X = triangleVertexX_d[triangleIndex + numTriangles * 2];\n            float triangleVertex1Y = triangleVertexY_d[triangleIndex];\n            float triangleVertex2Y = triangleVertexY_d[triangleIndex + numTriangles];\n            float triangleVertex3Y = triangleVertexY_d[triangleIndex + numTriangles * 2];\n            float distanceToThirdVertexX = pointX - triangleVertex3X;\n            float distanceToThirdVertexY = pointY - triangleVertex3Y;\n            float distanceOfTwoVerticesX = triangleVertex2X - triangleVertex1X;\n            float distanceOfTwoVerticesY = triangleVertex2Y - triangleVertex1Y;\n            float determinant = distanceOfTwoVerticesY * (triangleVertex3X - triangleVertex1X) + distanceOfTwoVerticesX * (triangleVertex3Y - triangleVertex1Y);\n            float partialBarycentricCoord1 = distanceOfTwoVerticesY * distanceToThirdVertexX + distanceOfTwoVerticesX * distanceToThirdVertexY;\n            float partialBarycentricCoord2 = (triangleVertex3Y - triangleVertex1Y) * distanceToThirdVertexX + (triangleVertex1X - triangleVertex3X) * distanceToThirdVertexY;\n            bool collision = false;\n            if (determinant < 0.0f) {\n                collision = (partialBarycentricCoord1 <= 0.0f && partialBarycentricCoord2 <= 0.0f && partialBarycentricCoord1 + partialBarycentricCoord2 >= determinant);\n            } else {\n                collision = (partialBarycentricCoord1 >= 0.0f && partialBarycentricCoord2 >= 0.0f && partialBarycentricCoord1 + partialBarycentricCoord2 <= determinant);\n            }\n            if (collision) {\n                atomicAdd(&pointCollisionCount_d[pointIndex], 1);\n            }\n        }\n    }\n}\n__global__ void k_countCollisionsWithRectangles(int* pointCollisionCount_d, float* pointPositionX_d, float* pointPositionY_d, float* rectangleVertexX_d, float* rectangleVertexY_d, int numPoints, int numRectangles) {\n    int globalThreadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n    int numGlobalThreads = blockDim.x * gridDim.x;\n    int numTotalPointRectanglePairs = numPoints * numRectangles;\n    int numGridStrideLoopIterations = (numTotalPointRectanglePairs + numGlobalThreads - 1) / numGlobalThreads;\n    // Iterating over point-rectangle pairs with a grid-stride loop.\n    // Pattern is { point 1 rectangle 1, point 1 rectangle 2, point 1 rectangle 3, ..., point 2 rectangle 1, point 2 rectangle 2, ... }\n    for (int gridStrideIteration = 0; gridStrideIteration < numGridStrideLoopIterations; gridStrideIteration++) {\n        int pairIndex = gridStrideIteration * numGlobalThreads + globalThreadIndex;\n        if (pairIndex < numTotalPointRectanglePairs) {\n            int pointIndex = pairIndex / numRectangles;\n            int rectangleIndex = pairIndex % numRectangles;\n            // Computing if there is a collision between the selected point and the selected rectangle.\n            float pointX = pointPositionX_d[pointIndex];\n            float pointY = pointPositionY_d[pointIndex];\n            float rectangleVertex1X = rectangleVertexX_d[rectangleIndex];\n            float rectangleVertex2X = rectangleVertexX_d[rectangleIndex + numRectangles];\n            float rectangleVertex3X = rectangleVertexX_d[rectangleIndex + numRectangles * 2];\n            float rectangleVertex4X = rectangleVertexX_d[rectangleIndex + numRectangles * 3];\n            float rectangleVertex1Y = rectangleVertexY_d[rectangleIndex];\n            float rectangleVertex2Y = rectangleVertexY_d[rectangleIndex + numRectangles];\n            float rectangleVertex3Y = rectangleVertexY_d[rectangleIndex + numRectangles * 2];\n            float rectangleVertex4Y = rectangleVertexY_d[rectangleIndex + numRectangles * 3];\n            float edge1 = (rectangleVertex2X - rectangleVertex1X) * (pointY - rectangleVertex1Y) - (pointX - rectangleVertex1X) * (rectangleVertex2Y - rectangleVertex1Y);\n            float edge2 = (rectangleVertex3X - rectangleVertex2X) * (pointY - rectangleVertex2Y) - (pointX - rectangleVertex2X) * (rectangleVertex3Y - rectangleVertex2Y);\n            float edge3 = (rectangleVertex4X - rectangleVertex3X) * (pointY - rectangleVertex3Y) - (pointX - rectangleVertex3X) * (rectangleVertex4Y - rectangleVertex3Y);\n            float edge4 = (rectangleVertex1X - rectangleVertex4X) * (pointY - rectangleVertex4Y) - (pointX - rectangleVertex4X) * (rectangleVertex1Y - rectangleVertex4Y);\n            bool collision = (edge1 >= 0.0f && edge2 >= 0.0f && edge3 >= 0.0f && edge4 >= 0.0f) ||\n                             (edge1 <= 0.0f && edge2 <= 0.0f && edge3 <= 0.0f && edge4 <= 0.0f);\n            if (collision) {\n                atomicAdd(&pointCollisionCount_d[pointIndex], 1);\n            }\n        }\n    }\n}\n// Topology of dependencies:\n// stream 1                                    stream 2\n// memcpy inputs to device                     |\n// zero-initialize counts                      |\n// |                                           |\n// event 1 ----------------------------------> wait for event 1\n// |                                           |\n// count collisions with triangles             count collisions with rectangles\n// |                                           |\n// wait for event 2 <------------------------- event 2\n// |\n// memcpy outputs to host\n// synchronize host\nvoid run(int numPoints, int numTriangles, int numRectangles, cudaStream_t stream1, cudaStream_t stream2, cudaEvent_t event1, cudaEvent_t event2, \n         int* pointCollisionCount_h, float* pointPositionX_h, float* pointPositionY_h, \n         int* pointCollisionCount_d, float* pointPositionX_d, float* pointPositionY_d, \n         float* triangleVertexX_d, float* triangleVertexY_d, \n         float* rectangleVertexX_d, float* rectangleVertexY_d);\n\nvoid launch() {\n    // Test and algorithm settings.\n    constexpr int MAX_NUMBER_OF_POINTS = 100000;\n    constexpr int NUMBER_OF_TRIANGLES = 4;\n    constexpr int NUMBER_OF_RECTANGLES = 3;\n    constexpr int VERTICES_PER_TRIANGLE = 3;\n    constexpr int VERTICES_PER_RECTANGLE = 4;\n    constexpr int NUM_TESTS = 7;\n    constexpr int DETERMINISTIC_RANDOM_SEED = 42;\n    \n    CUDA_CHECK(cudaSetDevice(DEVICE_INDEX));\n    \n    cudaStream_t stream1;\n    cudaStream_t stream2;\n    CUDA_CHECK(cudaStreamCreate(&stream1));\n    CUDA_CHECK(cudaStreamCreate(&stream2));\n    cudaEvent_t event1;\n    cudaEvent_t event2;\n    CUDA_CHECK(cudaEventCreate(&event1));\n    CUDA_CHECK(cudaEventCreate(&event2));\n    // Allocating host memory.\n    int* pointCollisionCount_h = new int[MAX_NUMBER_OF_POINTS];\n    float* pointPositionX_h = new float[MAX_NUMBER_OF_POINTS];\n    float* pointPositionY_h = new float[MAX_NUMBER_OF_POINTS];\n    // Triangles are stored in striped pattern with this order -> First vertices of all triangles, second vertices of all triangles, third vertices of all triangles.\n    float* triangleVertexX_h = new float[NUMBER_OF_TRIANGLES * VERTICES_PER_TRIANGLE];\n    float* triangleVertexY_h = new float[NUMBER_OF_TRIANGLES * VERTICES_PER_TRIANGLE];\n    // Rectangles are stored in striped pattern with this order -> First vertices of all rectangles, second vertices of all rectangles, third vertices of all rectangles, fourth vertices of all rectangles.\n    float* rectangleVertexX_h = new float[NUMBER_OF_RECTANGLES * VERTICES_PER_RECTANGLE];\n    float* rectangleVertexY_h = new float[NUMBER_OF_RECTANGLES * VERTICES_PER_RECTANGLE];\n    int* testNumPoints_h = new int[NUM_TESTS];\n    float* testPointPositionX_h = new float[MAX_NUMBER_OF_POINTS * NUM_TESTS];\n    float* testPointPositionY_h = new float[MAX_NUMBER_OF_POINTS * NUM_TESTS];\n    \n    // Allocating device memory.\n    int* pointCollisionCount_d;\n    float* pointPositionX_d;\n    float* pointPositionY_d;\n    // Triangles are stored in striped pattern with this order -> First vertices of all triangles, second vertices of all triangles, third vertices of all triangles.\n    float* triangleVertexX_d;\n    float* triangleVertexY_d;\n    // Rectangles are stored in striped pattern with this order -> First vertices of all rectangles, second vertices of all rectangles, third vertices of all rectangles, fourth vertices of all rectangles.\n    float* rectangleVertexX_d;\n    float* rectangleVertexY_d;\n    CUDA_CHECK(cudaMallocAsync(&pointCollisionCount_d, sizeof(int) * MAX_NUMBER_OF_POINTS, stream1));\n    CUDA_CHECK(cudaMallocAsync(&pointPositionX_d, sizeof(float) * MAX_NUMBER_OF_POINTS, stream1));\n    CUDA_CHECK(cudaMallocAsync(&pointPositionY_d, sizeof(float) * MAX_NUMBER_OF_POINTS, stream1));\n    CUDA_CHECK(cudaMallocAsync(&triangleVertexX_d, sizeof(float) * NUMBER_OF_TRIANGLES * VERTICES_PER_TRIANGLE, stream1));\n    CUDA_CHECK(cudaMallocAsync(&triangleVertexY_d, sizeof(float) * NUMBER_OF_TRIANGLES * VERTICES_PER_TRIANGLE, stream1));\n    CUDA_CHECK(cudaMallocAsync(&rectangleVertexX_d, sizeof(float) * NUMBER_OF_RECTANGLES * VERTICES_PER_RECTANGLE, stream1));\n    CUDA_CHECK(cudaMallocAsync(&rectangleVertexY_d, sizeof(float) * NUMBER_OF_RECTANGLES * VERTICES_PER_RECTANGLE, stream1));\n    float sceneWidth = 800.0f;\n    float sceneHeight = 600.0f;\n    float sceneCenterX = sceneWidth / 2.0f;\n    float sceneCenterY = sceneHeight / 2.0f;\n    float margin = 10.0f;\n    std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n    std::uniform_real_distribution<float> distributionX(margin, sceneWidth - margin);\n    std::uniform_real_distribution<float> distributionY(margin, sceneHeight - margin);\n    // Preparing the triangles and rectangles.\n    {\n        float startTriangleX = 5.5f;\n        float startTriangleY = 5.5f;\n        float stepTriangleX = 150.0f;\n        float stepTriangleY = 100.0f;\n        for (int i = 0; i < NUMBER_OF_TRIANGLES; i++) {\n            triangleVertexX_h[i] = startTriangleX + i * stepTriangleX;\n            triangleVertexY_h[i] = startTriangleY + i * stepTriangleY;\n            triangleVertexX_h[i + NUMBER_OF_TRIANGLES] = triangleVertexX_h[i] + 80.0f + i * 50.0f;\n            triangleVertexY_h[i + NUMBER_OF_TRIANGLES] = triangleVertexY_h[i] + 10.0f;\n            triangleVertexX_h[i + 2 * NUMBER_OF_TRIANGLES] = triangleVertexX_h[i] + 10.0f;\n            triangleVertexY_h[i + 2 * NUMBER_OF_TRIANGLES] = triangleVertexY_h[i] + 80.0f + i * 50.0f;\n        }\n        float startRectangleX = 700.0f;\n        float startRectangleY = 120.0f;\n        float stepRectangleX = -250.0f;\n        float stepRectangleY = 150.0f;\n        float sizeStart = 40.0f;\n        float sizeStep = 70.0f;\n        for (int i = 0; i < NUMBER_OF_RECTANGLES; i++) {\n            float centerX = startRectangleX + stepRectangleX * i;\n            float centerY = startRectangleY + stepRectangleY * i;\n            float size = sizeStart + sizeStep * i;\n            rectangleVertexX_h[i] = centerX - size;\n            rectangleVertexY_h[i] = centerY - size;\n            rectangleVertexX_h[i + NUMBER_OF_RECTANGLES] = centerX + size;\n            rectangleVertexY_h[i + NUMBER_OF_RECTANGLES] = centerY - size;\n            rectangleVertexX_h[i + 2 * NUMBER_OF_RECTANGLES] = centerX + size;\n            rectangleVertexY_h[i + 2 * NUMBER_OF_RECTANGLES] = centerY + size;\n            rectangleVertexX_h[i + 3 * NUMBER_OF_RECTANGLES] = centerX - size;\n            rectangleVertexY_h[i + 3 * NUMBER_OF_RECTANGLES] = centerY + size;\n        }\n        CUDA_CHECK(cudaMemcpyAsync(triangleVertexX_d, triangleVertexX_h, sizeof(float) * NUMBER_OF_TRIANGLES * VERTICES_PER_TRIANGLE, cudaMemcpyHostToDevice, stream1));\n        CUDA_CHECK(cudaMemcpyAsync(triangleVertexY_d, triangleVertexY_h, sizeof(float) * NUMBER_OF_TRIANGLES * VERTICES_PER_TRIANGLE, cudaMemcpyHostToDevice, stream1));\n        CUDA_CHECK(cudaMemcpyAsync(rectangleVertexX_d, rectangleVertexX_h, sizeof(float) * NUMBER_OF_RECTANGLES * VERTICES_PER_RECTANGLE, cudaMemcpyHostToDevice, stream1));\n        CUDA_CHECK(cudaMemcpyAsync(rectangleVertexY_d, rectangleVertexY_h, sizeof(float) * NUMBER_OF_RECTANGLES * VERTICES_PER_RECTANGLE, cudaMemcpyHostToDevice, stream1));\n    }\n    \n    int testIndex = 0;\n    // Preparing test data.\n    // Test 1: Ten points.\n    {\n        int numPoints = 10;\n        testNumPoints_h[testIndex] = numPoints;\n        std::initializer_list<float> pointX = { 10.0f, 20.0f, 100.0f, 200.0f, 350.0f, 350.0f, 600.0f, 700.0f, 750.0f, 775.0f };\n        std::initializer_list<float> pointY = { 10.0f, 20.0f, 100.0f, 200.0f, 350.0f, 500.0f, 600.0f, 700.0f, 750.0f, 775.0f };\n        std::copy(pointX.begin(), pointX.end(), &testPointPositionX_h[testIndex * MAX_NUMBER_OF_POINTS]);\n        std::copy(pointY.begin(), pointY.end(), &testPointPositionY_h[testIndex * MAX_NUMBER_OF_POINTS]);\n        testIndex++;\n    }\n    // Test 2: Five points.\n    {\n        int numPoints = 5;\n        testNumPoints_h[testIndex] = numPoints;\n        std::initializer_list<float> pointX = { 100.0f, 120.0f, 200.0f, 200.0f, 250.0f };\n        std::initializer_list<float> pointY = { 100.0f, 100.0f, 100.0f, 100.0f, 100.0f };\n        std::copy(pointX.begin(), pointX.end(), &testPointPositionX_h[testIndex * MAX_NUMBER_OF_POINTS]);\n        std::copy(pointY.begin(), pointY.end(), &testPointPositionY_h[testIndex * MAX_NUMBER_OF_POINTS]);\n        testIndex++;\n    }\n    // Test 3: A thousand randomly generated points.\n    {\n        int numPoints = 1000;\n        testNumPoints_h[testIndex] = numPoints;\n        for (int i = 0; i < numPoints; i++) {\n            testPointPositionX_h[testIndex * MAX_NUMBER_OF_POINTS + i] = distributionX(generator);\n            testPointPositionY_h[testIndex * MAX_NUMBER_OF_POINTS + i] = distributionY(generator);\n        }\n        testIndex++;\n    }\n    // Test 4: Maximum number of randomized points.\n    {\n        int numPoints = MAX_NUMBER_OF_POINTS;\n        testNumPoints_h[testIndex] = numPoints;\n        for (int i = 0; i < numPoints; i++) {\n            testPointPositionX_h[testIndex * MAX_NUMBER_OF_POINTS + i] = distributionX(generator);\n            testPointPositionY_h[testIndex * MAX_NUMBER_OF_POINTS + i] = distributionY(generator);\n        }\n        testIndex++;\n    }\n    // Test 5: One point at the center.\n    {\n        int numPoints = 1;\n        testNumPoints_h[testIndex] = numPoints;\n        testPointPositionX_h[testIndex * MAX_NUMBER_OF_POINTS] = sceneCenterX;\n        testPointPositionY_h[testIndex * MAX_NUMBER_OF_POINTS] = sceneCenterY;\n        testIndex++;\n    }\n    // Test 6: Points on a horizontal line\n    {\n        int numPoints = MAX_NUMBER_OF_POINTS;\n        testNumPoints_h[testIndex] = numPoints;\n        for (int i = 0; i < numPoints; i++) {\n            testPointPositionX_h[testIndex * MAX_NUMBER_OF_POINTS + i] = (i / (float)numPoints) * sceneWidth;\n            testPointPositionY_h[testIndex * MAX_NUMBER_OF_POINTS + i] = sceneCenterY;\n        }\n        testIndex++;\n    }\n    // Test 7: Many points are in the same position.\n    {\n        int numPoints = MAX_NUMBER_OF_POINTS;\n        testNumPoints_h[testIndex] = numPoints;\n        for (int i = 0; i < numPoints; i++) {\n            testPointPositionX_h[testIndex * MAX_NUMBER_OF_POINTS + i] = sceneCenterX;\n            testPointPositionY_h[testIndex * MAX_NUMBER_OF_POINTS + i] = sceneCenterY;\n        }\n        testIndex++;\n    }\n    // Processing test data.\n    for (int i = 0; i < testIndex; i++) {\n        int numPoints = testNumPoints_h[i];\n        int numTriangles = NUMBER_OF_TRIANGLES;\n        int numRectangles = NUMBER_OF_RECTANGLES;\n        for (int j = 0; j < numPoints; j++) {\n            pointPositionX_h[j] = testPointPositionX_h[i * MAX_NUMBER_OF_POINTS + j];\n            pointPositionY_h[j] = testPointPositionY_h[i * MAX_NUMBER_OF_POINTS + j];\n        }\n        // Calculating collision counts with multiple streams and events.\n        run(numPoints, numTriangles, numRectangles, stream1, stream2, event1, event2, \n            pointCollisionCount_h, pointPositionX_h, pointPositionY_h, \n            pointCollisionCount_d, pointPositionX_d, pointPositionY_d, \n            triangleVertexX_d, triangleVertexY_d, \n            rectangleVertexX_d, rectangleVertexY_d);\n        // Calculating the expected counts with the host.\n        for (int j = 0; j < numPoints; j++) {\n            int collisions = 0;\n            // Counting collisions with triangles.\n            for (int k = 0; k < numTriangles; k++) {\n                float triangleVertex1X = triangleVertexX_h[k];\n                float triangleVertex2X = triangleVertexX_h[k + numTriangles];\n                float triangleVertex3X = triangleVertexX_h[k + numTriangles * 2];\n                float triangleVertex1Y = triangleVertexY_h[k];\n                float triangleVertex2Y = triangleVertexY_h[k + numTriangles];\n                float triangleVertex3Y = triangleVertexY_h[k + numTriangles * 2];\n                float distanceToThirdVertexX = pointPositionX_h[j] - triangleVertex3X;\n                float distanceToThirdVertexY = pointPositionY_h[j] - triangleVertex3Y;\n                float distanceOfTwoVerticesX = triangleVertex2X - triangleVertex1X;\n                float distanceOfTwoVerticesY = triangleVertex2Y - triangleVertex1Y;\n                float determinant = distanceOfTwoVerticesY * (triangleVertex3X - triangleVertex1X) + distanceOfTwoVerticesX * (triangleVertex3Y - triangleVertex1Y);\n                float partialBarycentricCoord1 = distanceOfTwoVerticesY * distanceToThirdVertexX + distanceOfTwoVerticesX * distanceToThirdVertexY;\n                float partialBarycentricCoord2 = (triangleVertex3Y - triangleVertex1Y) * distanceToThirdVertexX + (triangleVertex1X - triangleVertex3X) * distanceToThirdVertexY;\n                bool coll = false;\n                if (determinant < 0.0f) {\n                    coll = (partialBarycentricCoord1 <= 0.0f && partialBarycentricCoord2 <= 0.0f && partialBarycentricCoord1 + partialBarycentricCoord2 >= determinant);\n                } else {\n                    coll = (partialBarycentricCoord1 >= 0.0f && partialBarycentricCoord2 >= 0.0f && partialBarycentricCoord1 + partialBarycentricCoord2 <= determinant);\n                }\n                if (coll) {\n                    collisions++;\n                }\n            }\n            // Counting collisions with rectangles.\n            for (int k = 0; k < numRectangles; k++) {\n                float rectangleVertex1X = rectangleVertexX_h[k];\n                float rectangleVertex2X = rectangleVertexX_h[k + numRectangles];\n                float rectangleVertex3X = rectangleVertexX_h[k + numRectangles * 2];\n                float rectangleVertex4X = rectangleVertexX_h[k + numRectangles * 3];\n                float rectangleVertex1Y = rectangleVertexY_h[k];\n                float rectangleVertex2Y = rectangleVertexY_h[k + numRectangles];\n                float rectangleVertex3Y = rectangleVertexY_h[k + numRectangles * 2];\n                float rectangleVertex4Y = rectangleVertexY_h[k + numRectangles * 3];\n                float edge1 = (rectangleVertex2X - rectangleVertex1X) * (pointPositionY_h[j] - rectangleVertex1Y) - (pointPositionX_h[j] - rectangleVertex1X) * (rectangleVertex2Y - rectangleVertex1Y);\n                float edge2 = (rectangleVertex3X - rectangleVertex2X) * (pointPositionY_h[j] - rectangleVertex2Y) - (pointPositionX_h[j] - rectangleVertex2X) * (rectangleVertex3Y - rectangleVertex2Y);\n                float edge3 = (rectangleVertex4X - rectangleVertex3X) * (pointPositionY_h[j] - rectangleVertex3Y) - (pointPositionX_h[j] - rectangleVertex3X) * (rectangleVertex4Y - rectangleVertex3Y);\n                float edge4 = (rectangleVertex1X - rectangleVertex4X) * (pointPositionY_h[j] - rectangleVertex4Y) - (pointPositionX_h[j] - rectangleVertex4X) * (rectangleVertex1Y - rectangleVertex4Y);\n                bool coll = (edge1 >= 0.0f && edge2 >= 0.0f && edge3 >= 0.0f && edge4 >= 0.0f) ||\n                            (edge1 <= 0.0f && edge2 <= 0.0f && edge3 <= 0.0f && edge4 <= 0.0f);\n                if (coll) {\n                    collisions++;\n                }\n            }\n            assert(collisions == pointCollisionCount_h[j]);\n        }\n    }\n    \n    // Releasing device memory\n    CUDA_CHECK(cudaFreeAsync(pointCollisionCount_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(pointPositionX_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(pointPositionY_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(triangleVertexX_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(triangleVertexY_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(rectangleVertexX_d, stream1));\n    CUDA_CHECK(cudaFreeAsync(rectangleVertexY_d, stream1));\n    \n    // Releasing host memory.\n    delete [] pointCollisionCount_h;\n    delete [] pointPositionX_h;\n    delete [] pointPositionY_h;\n    delete [] triangleVertexX_h;\n    delete [] triangleVertexY_h;\n    delete [] rectangleVertexX_h;\n    delete [] rectangleVertexY_h;\n    delete [] testNumPoints_h;\n    delete [] testPointPositionX_h;\n    delete [] testPointPositionY_h;\n    CUDA_CHECK(cudaEventDestroy(event1));\n    CUDA_CHECK(cudaEventDestroy(event2));\n    CUDA_CHECK(cudaStreamSynchronize(stream1));\n    CUDA_CHECK(cudaStreamSynchronize(stream2));\n    CUDA_CHECK(cudaStreamDestroy(stream1));\n    CUDA_CHECK(cudaStreamDestroy(stream2));\n}\n\nvoid run(int numPoints, int numTriangles, int numRectangles, cudaStream_t stream1, cudaStream_t stream2, cudaEvent_t event1, cudaEvent_t event2, \n         int* pointCollisionCount_h, float* pointPositionX_h, float* pointPositionY_h, \n         int* pointCollisionCount_d, float* pointPositionX_d, float* pointPositionY_d, \n         float* triangleVertexX_d, float* triangleVertexY_d, \n         float* rectangleVertexX_d, float* rectangleVertexY_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/201", "date": "2025-07-30", "prompt": "Write a CUDA kernel that performs an affine warp transformation with sine-wave distortion to grayscale images. The kernel should map each output pixel with an affine transformation matrix (with rotation, scaling, skew, and translation), then add a sine-based distortion along the x-axis.\n\nThe signature of the function is __global__ void k_affineWarp(unsigned char* input_d, unsigned char* output_d, int width, int height, float m00, float m01, float m10, float m11, float tx, float ty, float amplitude, float frequency), where input_d is a pointer to the input grayscale image, output_d is a pointer to the output image, width, height is dimensions of m00, m01, m10, m11 that defines affine transformation matrix, tx and ty are translation offsets, amplitude is the sine distortion magnitude, frequency is the sine wave frequency.\n\n>>> k_affineWarp({106, 255, 184, 238, 0, 32, 77, 255, 37, 60}, output_d, 1024, 768, 1.03, -0.36, 0.48, 0.69, 0, 0, 10, 0.03)->output_d:({106, 255, 58, 170, 70, 167, 84, 22, 225, 232})\n>>> k_affineWarp({111, 47, 6, 238, 140, 242, 111, 124, 107, 82}, output_d, 1920, 1080, 1.03, -0.36, 0.48, 0.69, 0, 0, 10, 0.03)->output_d:({111, 47, 160, 54, 2, 89, 190, 175, 215, 62})\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <vector>\n#include <random>\n#include <cassert>\n#include <cmath>\n#include <algorithm>\n\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                         \\\n    do {                                         \\\n        cudaError_t err = call;                  \\\n        if (err != cudaSuccess) {                \\\n            fprintf(stderr,                      \\\n                    \"CUDA error: %s at %s:%d\\n\", \\\n                    cudaGetErrorString(err),     \\\n                    __FILE__, __LINE__);         \\\n            exit(EXIT_FAILURE);                  \\\n        }                                        \\\n    } while (0)\n\n__global__ void k_affineWarp(unsigned char* input_d, unsigned char* output_d,\n                             int width, int height,\n                             float m00, float m01, float m10, float m11,\n                             float tx, float ty,\n                             float amplitude, float frequency);\n\nvoid launch() {\n    const int NUM_TEST_CASES = 7;\n    const int MAX_WIDTH = 3840;\n    const int MAX_HEIGHT = 2160;\n    const size_t MAX_SIZE = MAX_WIDTH * MAX_HEIGHT;\n    const float DEG = 30.0f;\n    const float RAD = DEG * 3.14159265f / 180.0f;\n    const float SCALE_X = 1.2f;\n    const float SCALE_Y = 0.8f;\n    const float SKEW_X = 0.2f;\n    const float SKEW_Y = 0.1f;\n    const float AMPLITUDE = 10.0f;\n    const float FREQUENCY = 0.03f;\n    const float TX = 0.0f;\n    const float TY = 0.0f;\n    const int BLOCK_X_DEFAULT = 32;\n    const int BLOCK_Y_DEFAULT = 8;\n    const int TOLERANCE = 255;\n\n    std::pair<int, int> testSizes[NUM_TEST_CASES] = {\n        {1024, 768},\n        {1920, 1080},\n        {2048, 1536},\n        {2560, 1440},\n        {2880, 1620},\n        {3200, 1800},\n        {3840, 2160}     \n    };\n\n    auto cpuAffineWarp = [](const std::vector<unsigned char>& input, int w, int h,\n                            float a, float b, float c, float d,\n                            float tx, float ty,\n                            float amplitude, float frequency) {\n        std::vector<unsigned char> output(w * h);\n        for (int y = 0; y < h; ++y) {\n            for (int x = 0; x < w; ++x) {\n                float xf = a * x + b * y + tx;\n                float yf = c * x + d * y + ty;\n                xf += amplitude * sinf(frequency * yf);\n                int srcX = std::min(std::max(int(xf + 0.5f), 0), w - 1);\n                int srcY = std::min(std::max(int(yf + 0.5f), 0), h - 1);\n                output[y * w + x] = input[srcY * w + srcX];\n            }\n        }\n        return output;\n    };\n\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n    const int MAX_GRID_X = prop.maxGridSize[0];\n    const int MAX_GRID_Y = prop.maxGridSize[1];\n    const int MAX_BLOCK_X = prop.maxThreadsDim[0];\n    const int MAX_BLOCK_Y = prop.maxThreadsDim[1];\n    const int BLOCK_X = std::min(BLOCK_X_DEFAULT, MAX_BLOCK_X);\n    const int BLOCK_Y = std::min(BLOCK_Y_DEFAULT, MAX_BLOCK_Y);\n    const float cosT = cosf(RAD);\n    const float sinT = sinf(RAD);\n    // Matrix elements for affine transformation.\n    const float m00 = cosT * SCALE_X;                  // Controls x-scaling and rotation.\n    const float m01 = (-sinT + SKEW_X) * SCALE_X;      // Controls x-skew and rotation.\n    const float m10 = (sinT + SKEW_Y) * SCALE_Y;       // Controls y-skew and rotation.\n    const float m11 = cosT * SCALE_Y;                  // Controls y-scaling and rotation.\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    unsigned char *input_d, *output_d;\n    CUDA_CHECK(cudaMallocAsync(&input_d, MAX_SIZE, stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, MAX_SIZE, stream));\n\n    auto generateInput = [](size_t size, int seed) {\n        std::vector<unsigned char> data(size);\n        std::mt19937 rng(seed);\n        std::uniform_int_distribution<int> dist(0, 255);\n        for (size_t i = 0; i < size; ++i)\n            data[i] = dist(rng);\n        return data;\n    };\n\n    auto verify = [](const std::vector<unsigned char>& expected, const std::vector<unsigned char>& actual) {\n        assert(expected.size() == actual.size());\n        for (size_t i = 0; i < expected.size(); ++i) {\n            assert(std::abs(static_cast<int>(expected[i]) - static_cast<int>(actual[i])) <= TOLERANCE);\n        }\n    };\n\n    for (int i = 0; i < NUM_TEST_CASES; ++i) {\n        //const std::vector<unsigned char>& expected = expectedData[i];\n        const int w = testSizes[i].first;\n        const int h = testSizes[i].second;\n        const size_t size = w * h;\n\n        auto input_h = generateInput(size, i + 1);\n        std::vector<unsigned char> output_h(size);\n\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h.data(), size, cudaMemcpyHostToDevice, stream));\n\n        void* args[] = {\n            (void*)&input_d, (void*)&output_d,\n            (void*)&w, (void*)&h,\n            (void*)&m00, (void*)&m01, (void*)&m10, (void*)&m11,\n            (void*)&TX, (void*)&TY,\n            (void*)&AMPLITUDE, (void*)&FREQUENCY\n        };\n\n        dim3 block(BLOCK_X, BLOCK_Y);\n        dim3 grid((w + block.x - 1) / block.x, (h + block.y - 1) / block.y);\n        grid.x = std::min(int(grid.x), MAX_GRID_X);\n        grid.y = std::min(int(grid.y), MAX_GRID_Y);\n        \n        CUDA_CHECK(cudaLaunchKernel((void*)k_affineWarp, grid, block, args, 0, stream));        \n        CUDA_CHECK(cudaMemcpyAsync(output_h.data(), output_d, size, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        auto expected = cpuAffineWarp(input_h, w, h, m00, m01, m10, m11, TX, TY, AMPLITUDE, FREQUENCY);\n\n        verify(expected, output_h);\n    }\n\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_affineWarp(unsigned char* input_d, unsigned char* output_d,\n                             int width, int height,\n                             float m00, float m01, float m10, float m11,\n                             float tx, float ty,\n                             float amplitude, float frequency) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/202", "date": "2025-07-30", "prompt": "Write a CUDA kernel to calculate the compound interest for an array of input values. Paralleize the computation, where each thread should compute the interest using the corresponding principal amount, interest rate, and time duration from the input arrays. Store the computed compound interest in the output array. \n\nThe signature of the function \n__global__ void k_compoundInterest (float* principal_d, float* rate_d, float* time_d, float* compInterest_d, int size) where principal_d is the array of principal amounts, rate_d is the array of interest rates, time_d is the array of time duration (number of years), compInterest_d is the output array to store the computed compound interest, size is the total number of inputs.\n\n>>> k_compoundInterest({1000.0f, 2000.0f, 1500.0f}, {5.0f, 7.5f, 6.0f}, {2.0f, 1.5f, 3.0f}, compInterest_d, 3) -> compInterest_d : {102.5f, 228.55f, 286.52f}\n>>> k_compoundInterest({5000.0f, 8000.0f, 1200.0f, 3000.0f}, {10.0f, 4.0f, 8.0f, 6.5f}, {3.0f, 5.0f, 2.0f, 4.0f}, compInterest_d, 4) -> compInterest_d : {1655.0f, 1733.2f, 199.68f, 844.35f}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <random>\n#include <algorithm> \n#include <iostream>\n#include <cmath>\n#include <ctime>\n#include <assert.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\nconst int THREADS_PER_BLOCK = 256;\nconst float EPSILON = 5e-3f;\nconst float PERCENT = 100.0f;\n\n__global__ void k_compoundInterest (float* principal_d, float* rate_d, float* time_d, float* compInterest_d, int size);\n\nvoid launch() {\n\n    const int TEST_CASE_COUNT = 7;\n\n    int input[TEST_CASE_COUNT] = {40, 60, 80, 100, 200, 800, 1000};\n    int maxInputSize = *std::max_element(input, input + TEST_CASE_COUNT);\n\n    float principal_h[TEST_CASE_COUNT][maxInputSize];\n    float rate_h[TEST_CASE_COUNT][maxInputSize];\n    float time_h[TEST_CASE_COUNT][maxInputSize];\n    float expected_h[TEST_CASE_COUNT][maxInputSize];\n    float compInterest_h[maxInputSize];\n   \n    float totalAmount;\n\n    float *principal_d, *rate_d, *time_d, *compInterest_d;\n\n    // Random Input generator\n    std::mt19937 random_generator(static_cast<unsigned>(std::time(nullptr)));\n\n    std::uniform_real_distribution<float> prin_dist(10, 10000);\n    std::uniform_real_distribution<float> rate_dist(0.05f, 0.15f);\n    std::uniform_real_distribution<float> time_dist(1.0f, 10.0f);\n\n    // Generating the input data for all the test cases\n    for(int tc = 0; tc < TEST_CASE_COUNT; tc++) {\n        int N = input[tc];\n        for (int i = 0; i < N; ++i) {\n            principal_h[tc][i] = prin_dist(random_generator);\n            rate_h[tc][i] = rate_dist(random_generator);\n            time_h[tc][i] = time_dist(random_generator);\n            totalAmount = principal_h[tc][i] * powf((1.0f + rate_h[tc][i] / PERCENT), time_h[tc][i]);\n            expected_h[tc][i]= totalAmount - principal_h[tc][i];\n        }\n    }\n\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, 0);\n    int maxBlocks = prop.maxGridSize[0];\n\n    //Use CUDA Streams for Asynchronous Execution\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    CUDA_CHECK(cudaMalloc(&principal_d, maxInputSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&rate_d, maxInputSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&time_d, maxInputSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&compInterest_d, maxInputSize * sizeof(float)));\n   \n    for (int tc = 0; tc < TEST_CASE_COUNT; tc++) {\n        int N = input[tc];\n\n        CUDA_CHECK(cudaMemcpyAsync(principal_d, principal_h[tc], N * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(rate_d, rate_h[tc], N * sizeof(float), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(time_d, time_h[tc], N * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n        int numBlocks = std::min(maxBlocks, (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK);\n        dim3 block(THREADS_PER_BLOCK, 1, 1);\n        dim3 grid(numBlocks, 1, 1);\n\n        //Launch Kernel\n        //Grid: (N / 256, 1, 1)\n        //Block: (256, 1, 1)\n        void *args[] = {&principal_d, &rate_d, &time_d, &compInterest_d, &N};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_compoundInterest, grid, block, args, 0, stream));\n\n        //Copy Data from device to host\n        CUDA_CHECK(cudaMemcpyAsync(compInterest_h, compInterest_d, N * sizeof(float), cudaMemcpyDeviceToHost, stream));\n       \n        //Synchronize tasks in the stream\n        CUDA_CHECK(cudaStreamSynchronize(stream)); \n\n        //Validate the result with expected result\n        for (int i = 0; i < N; ++i) {\n            assert(fabs(compInterest_h[i] - expected_h[tc][i]) < EPSILON);\n        }\n    }\n\n    //Deallocate Device Memory\n    CUDA_CHECK(cudaFree(principal_d));\n    CUDA_CHECK(cudaFree(rate_d));\n    CUDA_CHECK(cudaFree(time_d));\n    CUDA_CHECK(cudaFree(compInterest_d));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void  k_compoundInterest (float* principal_d, float* rate_d, float* time_d, float* compInterest_d, int size) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/203", "date": "2025-07-30", "prompt": "\nWrite a CUDA kernel to find the new particle position in a 3D plane based on the given initial position and particle velocity. The new position for each particle is calculated by summing the coordinate of the initial position with the product of the corresponding velocity and the given time interval.\n\nThe signature of the kernel is __global__ void k_calculateParticleNewPosition(Vector3D* initialPosition_d, Vector3D* particleVelocity_d, Vector3D* newPosition_d, int deltaT_d, int inputSize) where initialPosition_d is the initial coordinate of the particle, particleVelocity_d is the velocity of the particle, newPosition_d is the new position of the particle based on the velocity and time, deltaT_d is the time interval, inputSize is the total size of the input.\n\n>>> k_calculateParticleNewPosition({{-2, 6, -4},{7, -5, 2}},{{8, 7, -10},{-6, 2, -10}},newPosition_d, 1, 2)-> newPosition_d:{{6, 13, -14},{1, -3, -8}}\n>>> k_calculateParticleNewPosition({{-1, -1, -3}, {7, 3, -4}, {-8, -8, -7}, {8, 3, 6}},{{2, -6, 8}, {0, 4, -9}, {-9, -8, 5}, {-1, -10, -7}},newPosition_d, 1, 4)-> newPosition_d:{{1, -7, 5}, {7, 7, -13}, {-17, -16, -2}, {7, -7, -1}}\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <assert.h>\n#include <random>\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\nconstexpr int MAX_SIZE = 10000;\nconstexpr unsigned int BLOCK_SIZE = 256;\n\nstruct Vector3D{\nint x;\nint y;\nint z;\n};\n\n__global__ void k_calculateParticleNewPosition(Vector3D* initialPosition_d, Vector3D* particleVelocity_d, Vector3D* newPosition_d, int deltaT_d, int inputSize);\n\nint launch() {\n\n    // Use a CUDA stream for asynchronous operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Fetch GPU properties\n    cudaDeviceProp deviceProp;\n    int currentDevice;\n    CUDA_CHECK(cudaGetDevice(&currentDevice));\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, currentDevice));\n    int numSMs = deviceProp.multiProcessorCount;\n    int maxBlocksPerSM = deviceProp.maxBlocksPerMultiProcessor;\n\n    // Calculate max no of blocks and grids\n    unsigned int maxGridSize = numSMs * maxBlocksPerSM;\n\n\n    // Create device memory\n    Vector3D *initialPosition_d;\n    Vector3D *particleVelocity_d;\n    Vector3D *newPosition_d;\n\n    // Allocate device memory\n    CUDA_CHECK(cudaMallocAsync(&initialPosition_d, sizeof(Vector3D) * MAX_SIZE, stream));\n    CUDA_CHECK(cudaMallocAsync(&particleVelocity_d, sizeof(Vector3D) * MAX_SIZE, stream));\n    CUDA_CHECK(cudaMallocAsync(&newPosition_d, sizeof(Vector3D) * MAX_SIZE, stream));\n\n    int givenInputSize = 100;\n    int totalTestCases = 7;\n\n    // For each iteration the input sizes are doubled 100, 200, 400, 800, 1600, 3200, 6400\n    for(int i = 0; i < totalTestCases ; i++)\n    {\n       int inputSize = givenInputSize;\n       std::mt19937 randomGenerator(std::random_device{}());\n       std::uniform_int_distribution<int> distribution(-10000, 10000);\n\n       Vector3D initialPosition_h[inputSize];\n       Vector3D newPosition_h[inputSize];\n       Vector3D expectedOutput[inputSize];\n       Vector3D particleVelocity_h[inputSize];\n       int deltaT_h{i+1};\n\n       for(int i = 0; i < inputSize; i++){\n           Vector3D initialPosition;\n           initialPosition.x = distribution(randomGenerator);\n           initialPosition.y = distribution(randomGenerator);\n           initialPosition.z = distribution(randomGenerator);\n\n           initialPosition_h[i] = initialPosition;\n           Vector3D velocity;\n           velocity.x = distribution(randomGenerator);\n           velocity.y = distribution(randomGenerator);\n           velocity.z = distribution(randomGenerator);\n           particleVelocity_h[i] = velocity;\n\n           Vector3D expectedPosition;\n           expectedPosition.x = initialPosition.x + (velocity.x * deltaT_h);\n           expectedPosition.y = initialPosition.y + (velocity.y * deltaT_h);\n           expectedPosition.z = initialPosition.z + (velocity.z * deltaT_h);\n           expectedOutput[i] = expectedPosition;\n       }\n\n       // Copy data to device\n       CUDA_CHECK(cudaMemcpyAsync(initialPosition_d, initialPosition_h, sizeof(Vector3D) * inputSize, cudaMemcpyHostToDevice, stream));\n       CUDA_CHECK(cudaMemcpyAsync(particleVelocity_d, particleVelocity_h, sizeof(Vector3D) * inputSize, cudaMemcpyHostToDevice, stream));\n       CUDA_CHECK(cudaMemcpyAsync(newPosition_d, newPosition_h, sizeof(Vector3D) * inputSize, cudaMemcpyHostToDevice, stream));\n\n       // Calculate the required grid size\n       unsigned int requiredGridSize = (inputSize + BLOCK_SIZE - 1) / BLOCK_SIZE;\n       unsigned int numGrids = std::min(requiredGridSize, maxGridSize);\n       dim3 gridDim(numGrids, 1, 1);\n       dim3 blockDim(BLOCK_SIZE, 1, 1);\n\n       //launch kernel\n       void* args[] = {&initialPosition_d,\n                       &particleVelocity_d,\n                       &newPosition_d,\n                       &deltaT_h,\n                       &inputSize};\n\n       CUDA_CHECK(cudaLaunchKernel((void*)k_calculateParticleNewPosition,\n                                   gridDim,\n                                   blockDim,\n                                   args,\n                                   0,\n                                   stream));\n\n       CUDA_CHECK(cudaStreamSynchronize(stream));\n\n       // Copy data back to host\n       CUDA_CHECK(cudaMemcpyAsync(newPosition_h, newPosition_d, sizeof(Vector3D) * inputSize, cudaMemcpyDeviceToHost, stream));\n\n       // Verify output\n       for(int i = 0; i < inputSize; i++){\n          assert(newPosition_h[i].x == expectedOutput[i].x);\n          assert(newPosition_h[i].y == expectedOutput[i].y);\n          assert(newPosition_h[i].z == expectedOutput[i].z);\n       }\n\n       // Increase input size for next iteration\n       givenInputSize *= 2;\n    }\n\n    //free memory resources\n    CUDA_CHECK(cudaFreeAsync(initialPosition_d, stream));\n    CUDA_CHECK(cudaFreeAsync(particleVelocity_d, stream));\n    CUDA_CHECK(cudaFreeAsync(newPosition_d, stream));\n\n    return 0;\n}\n\n__global__ void k_calculateParticleNewPosition(Vector3D* initialPosition_d, Vector3D* particleVelocity_d, Vector3D* newPosition_d, int deltaT_d, int inputSize){\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/204", "date": "2025-07-30", "prompt": "Write a CUDA kernel that encrypts an input message using the XOR cipher algorithm with a given key. Leverage data parallelism by assigning each thread to process one character, performing an XOR operation with the key to generate the encrypted output.\n\nThe signature of the function \n__global__ void k_xorEncryption(char* input_d, char* output_d, char key, int length) where input_d is an input character array containing the original message to be encrypted, output_d is the output character array where the encrypted message will be stored, key is a hexadecimal value used for encryption where each character in the input is XORed with this key, and length is the number of characters in the input message to be encrypted.\n\n>>> k_xorEncryption(\"GPU\", output_d, 0x2C, 3) -> output_d: k|y\n>>> k_xorEncryption(\"CUDA\", output_d, 0x2C, 4) -> output_d: oyhm\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <iostream>\n#include <cstring>\n#include <assert.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\nconst int THREADS_PER_BLOCK = 256;\n\n__global__ void k_xorEncryption(char* input_d, char* output_d, char key, int length);\n\nvoid launch() {\n\n    const int TEST_CASE_COUNT = 7;\n    const int MAX_LEN = 100;\n    char key = 0x2C;\n\n    char input[TEST_CASE_COUNT][MAX_LEN] = {\n        {\"helloworld\"},\n        {\"XOR encryption using data parallelism\"},\n        {\"Encryption\"},\n        {\"GPU\"},\n        {\"CUDA\"},\n        {\"CUDAPROGRAMMING\"},\n        {\"This program encrypts the input message into encrypted message\"},\n    };\n    \n    //Declare host and device pointers\n    char output[MAX_LEN];\n    char expected[MAX_LEN];\n\n    char *input_d, *output_d;\n\n    //Device properties\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, 0);\n    int maxBlocks = prop.maxGridSize[0];\n\n    //Use CUDA Streams for Asynchronous Execution\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n   \n    CUDA_CHECK(cudaMallocAsync(&input_d, MAX_LEN * sizeof(char), stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, MAX_LEN * sizeof(char), stream));\n\n    // Test case loop\n    for(int tc = 0; tc < TEST_CASE_COUNT; tc++) {\n       \n        int length = std::strlen(input[tc]);\n    \n        CUDA_CHECK(cudaMemcpyAsync(input_d, input[tc], length * sizeof(char), cudaMemcpyHostToDevice, stream));\n\n        int numBlocks = std::min(maxBlocks, (length + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK); \n\n        dim3 blockSize(THREADS_PER_BLOCK, 1, 1);\n        dim3 gridSize(numBlocks, 1, 1);\n\n        //Launch Kernel\n        //Grid: (N / 256, 1, 1)\n        //Block: (256, 1, 1)\n        void *args[] = {&input_d, &output_d, &key, &length};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_xorEncryption, gridSize, blockSize, args, 0, stream));\n \n        CUDA_CHECK(cudaMemcpyAsync(output, output_d, length * sizeof(char), cudaMemcpyDeviceToHost, stream));\n\n        //Synchronize tasks in the stream\n        CUDA_CHECK(cudaStreamSynchronize(stream)); \n   \n        //Expected output \n        for (int i = 0; i < length; ++i)\n            expected[i] = input[tc][i] ^ key;\n\n        // Validate the result with the expected output\n        assert(std::memcmp(expected, output, length) == 0); \n    }\n    cudaFreeAsync(input_d, stream);\n    cudaFreeAsync(output_d, stream);\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_xorEncryption(char* input_d, char* output_d, char key, int length) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/205", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute Local Binary Patterns (LBP) for 8-bit grayscale images with a fixed width of 1024. Each thread should load center pixels and halo regions into shared memory, synchronize, then compute 8-neighbor LBP codes for interior pixels by comparing each neighbor with the center pixel (neighbor >= center contributes bit 1, else 0) in clockwise order starting from top-left, assembling an 8-bit code, and set boundary pixels (pixels on the image edges) to 0.\n\nThe signature of the function is __global__ void k_computeLBP(const unsigned char* input_d, unsigned char* output_d, int width, int height), where input_d is the pointer to the input grayscale image, output_d is the pointer to the output LBP image, width, height is dimension of image.\n\n>>> k_computeLBP({49, 160, 169, 251, 119, 68, 155, 102, 36, 131}, output_d, 1024, 2048)->output_d:({0, 86, 154, 0, 111, 255, 142, 143, 127, 110})\n>>> k_computeLBP({150, 211, 10, 99, 150, 219, 1, 44, 87, 175}, output_d, 1024, 4096)->output_d:({0, 72, 255, 126, 246, 128, 255, 254, 254, 210})\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <cstdio>\n#include <vector>\n#include <random>\n#include <algorithm>\n\n#include <cuda_runtime.h>\n\n#define BIT_TOP_LEFT  7\n#define BIT_TOP 6\n#define BIT_TOP_RIGHT 5\n#define BIT_RIGHT 4\n#define BIT_BOTTOM_RIGHT 3\n#define BIT_BOTTOM 2\n#define BIT_BOTTOM_LEFT 1\n#define BIT_LEFT 0\n\n#define HALO_PADDING 2 // 2 * HALO_SIZE for both sides\n\n#define CUDA_CHECK(call)                                  \\\n    do {                                                  \\\n        cudaError_t err = call;                           \\\n        if (err != cudaSuccess) {                         \\\n            fprintf(stderr,                               \\\n                    \"CUDA error: %s at %s:%d\\n\",          \\\n                    cudaGetErrorString(err),              \\\n                    __FILE__, __LINE__);                  \\\n            exit(EXIT_FAILURE);                           \\\n        }                                                 \\\n    } while (0)\n\n__global__ void k_computeLBP(const unsigned char* input_d, unsigned char* output_d, int width, int height);\n\nvoid launch() {\n    const int NUM_TESTS = 7;\n    const int WIDTH = 1024;\n    const int HEIGHTS[NUM_TESTS] = {64, 128, 256, 512, 1024, 2048, 4096};\n    const int BLOCK_SIZE_X = 32;\n    const int BLOCK_SIZE_Y = 8;\n\n    auto cpuLBP = [](const std::vector<unsigned char>& input, int x, int y, int width) {\n        int idx = y * width + x;\n        unsigned char c = input[idx];\n        unsigned char lbpCode = 0;\n        lbpCode |= (input[(y-1)*width + (x-1)] >= c) << BIT_TOP_LEFT;\n        lbpCode |= (input[(y-1)*width + x    ] >= c) << BIT_TOP;\n        lbpCode |= (input[(y-1)*width + (x+1)] >= c) << BIT_TOP_RIGHT;\n        lbpCode |= (input[y*width + (x+1)]    >= c) << BIT_RIGHT;\n        lbpCode |= (input[(y+1)*width + (x+1)] >= c) << BIT_BOTTOM_RIGHT;\n        lbpCode |= (input[(y+1)*width + x    ] >= c) << BIT_BOTTOM;\n        lbpCode |= (input[(y+1)*width + (x-1)] >= c) << BIT_BOTTOM_LEFT;\n        lbpCode |= (input[y*width + (x-1)]    >= c) << BIT_LEFT;\n        return lbpCode;\n    };\n\n    auto fillRandom = [](std::vector<unsigned char>& input) {\n        std::random_device rd;\n        std::mt19937 rng(rd());\n        std::uniform_int_distribution<int> dist(0, 255);\n        for (auto& val : input) val = static_cast<unsigned char>(dist(rng));\n    };\n\n    auto verify = [&](const std::vector<unsigned char>& input, const std::vector<unsigned char>& output, int width, int height) {\n        for (int y = 1; y < height - 1; ++y) {\n            for (int x = 1; x < width - 1; ++x) {\n                int idx = y * width + x;\n                unsigned char expected = cpuLBP(input, x, y, width);\n                assert(output[idx] == expected);\n            }\n        }\n    };\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n    const int maxBlocksPerSM = prop.maxThreadsPerMultiProcessor / (BLOCK_SIZE_X * BLOCK_SIZE_Y);\n    const int maxSharedMem = prop.sharedMemPerBlock;\n    const int numSMs = prop.multiProcessorCount;\n\n    unsigned char *input_d, *output_d;\n    int maxImgSize = WIDTH * HEIGHTS[NUM_TESTS - 1];\n    CUDA_CHECK(cudaMallocAsync((void**)&input_d, maxImgSize * sizeof(unsigned char), stream));\n    CUDA_CHECK(cudaMallocAsync((void**)&output_d, maxImgSize * sizeof(unsigned char), stream));\n\n    for (int test = 0; test < NUM_TESTS; ++test) {\n        int height = HEIGHTS[test];\n        int imgSize = WIDTH * height;\n\n        std::vector<unsigned char> input_h(imgSize);\n        std::vector<unsigned char> output_h(imgSize);\n\n        fillRandom(input_h);\n\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h.data(), imgSize * sizeof(unsigned char), cudaMemcpyHostToDevice, stream));\n\n        dim3 blockDim(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n        size_t requestedSharedMem = (blockDim.x + HALO_PADDING) * (blockDim.y + HALO_PADDING) * sizeof(unsigned char);\n        size_t sharedMem = std::min(requestedSharedMem, static_cast<size_t>(maxSharedMem));\n\n        int gridX = std::min((int)((WIDTH + blockDim.x - 1) / blockDim.x), numSMs * maxBlocksPerSM);\n        int gridY = std::min((int)((height + blockDim.y - 1) / blockDim.y), numSMs * maxBlocksPerSM);\n        dim3 gridDim(gridX, gridY);\n\n        void* args[] = { (void*)&input_d, (void*)&output_d, (void*)&WIDTH, (void*)&height };\n        CUDA_CHECK(cudaLaunchKernel((void*)k_computeLBP, gridDim, blockDim, args, sharedMem, stream));\n\n        CUDA_CHECK(cudaMemcpyAsync(output_h.data(), output_d, imgSize * sizeof(unsigned char), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        verify(input_h, output_h, WIDTH, height);\n    }\n\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_computeLBP(const unsigned char* input_d, unsigned char* output_d, int width, int height) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/206", "date": "2025-07-30", "prompt": "Write a CUDA kernel for parallel Breadth-First Search (BFS) on graphs stored in Compressed Sparse Row (CSR) format. The kernel should compute the distance from a source node to all other reachable nodes and store the results in an output array. Each thread should process nodes in the current frontier, iterate through their neighbors using CSR row and column arrays, update the distances of unvisited neighbors, and add those neighbors to the next frontier for the subsequent BFS level.\n\nThe signature of the CUDA kernel is __global__ void k_bfsKernel(const int* rowOffsets_d, const int* colIndices_d, const int* frontier_d, int frontierSize, int currentLevel, int* distances_d, int* nextFrontier_d, int* nextFrontierSize_d), where rowOffsets_d is pointer to CSR row pointer array that stores the starting index of node i's adjacency list in the colIndices array, colIndices_d is pointer to CSR column indices array where colIndices[j] contains the neighbor node ID at position j in the flattened adjacency representation, frontier_d is pointer to current frontier array that contains the node ID of the k-th node to be processed in this BFS iteration, frontierSize is scalar value that represents the total number of active nodes in the current frontier, currentLevel is current BFS level that represents the distance value assigned to newly discovered nodes, distances is the distance array that stores the shortest path distance from source to node i, nextFrontier_d is pointer to next frontier array that will store newly discovered node IDs for the subsequent BFS level, nextFrontierSize_d is pointer to atomic counter that tracks the number of nodes added to the next frontier during kernel execution.\n\n>>> k_bfsKernel({0, 3, 7, 10, 14, 18, 21, 25, 28, 32, 35}, {1, 4, 7, 0, 2, 5, 8, 1, 3, 6, 2, 4, 9, 0, 3, 5, 7, 1, 4, 6, 2, 5, 8, 6, 7, 4, 6, 9, 5, 7, 8, 3, 8, 9, 2, 9}, {3}, 1, 0, distances_d, nextFrontier_d, nextFrontierSize_d)->output:(distances_d: {2, 3, 2, 0, 1, 1, 2, 1, 2, 2}, nextFrontier_d: {0, 4, 5, 7}, nextFrontierSize_d: 4);\n>>> k_bfsKernel({0, 4, 8, 12, 15, 19, 23, 26, 30, 33, 37, 40}, {2, 6, 8, 10, 0, 3, 7, 9, 1, 4, 5, 11, 1, 6, 8, 2, 7, 9, 10, 0, 3, 4, 11, 2, 5, 8, 1, 4, 6, 9, 2, 5, 7, 0, 3, 6, 10}, {5}, 1, 0, distances_d, nextFrontier_d, nextFrontierSize_d)->output:(distances_d: {3, 2, 1, 1, 2, 0, 3, 2, 1, 2, 3, 1}, nextFrontier_d: {2, 3, 8, 11}, nextFrontierSize_d: 4);\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cassert>\n#include <random>\n#include <vector>\n#include <algorithm>\n#include <numeric>\n#include <queue>\n\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                         \\\n    do {                                         \\\n        cudaError_t err = call;                  \\\n        if (err != cudaSuccess) {                \\\n            fprintf(stderr,                      \\\n                    \"CUDA error: %s at %s:%d\\n\", \\\n                    cudaGetErrorString(err),     \\\n                    __FILE__, __LINE__);         \\\n            exit(EXIT_FAILURE);                  \\\n        }                                        \\\n    } while (0)\n\n__global__ void k_bfsKernel(\n    const int* rowOffsets_d,\n    const int* colIndices_d,\n    const int* frontier_d,\n    int frontierSize,\n    int currentLevel,\n    int* distances_d,\n    int* nextFrontier_d,\n    int* nextFrontierSize_d);\n\n// CPU BFS implementation for validation\nvoid cpuBFS(const std::vector<int>& rowOffsets_h, \n           const std::vector<int>& colIndices_h,\n           int source,\n           std::vector<int>& distances_h) {\n    int numNodes = rowOffsets_h.size() - 1;\n    \n    // Initialize distances\n    std::fill(distances_h.begin(), distances_h.end(), -1);\n    distances_h[source] = 0;\n    \n    // BFS using queue\n    std::queue<int> queue;\n    queue.push(source);\n    \n    while (!queue.empty()) {\n        int u = queue.front();\n        queue.pop();\n        \n        // Process all neighbors of u\n        int start = rowOffsets_h[u];\n        int end = rowOffsets_h[u + 1];\n        \n        for (int i = start; i < end; ++i) {\n            int v = colIndices_h[i];\n            \n            // If neighbor v is unvisited\n            if (distances_h[v] == -1) {\n                distances_h[v] = distances_h[u] + 1;\n                queue.push(v);\n            }\n        }\n    }\n}\n\nvoid launch() {\n    const int MAX_NODES = 1000000;\n    const int MAX_EDGES = 10000000;\n    const int NUM_TEST_CASES = 7;\n    const int BLOCK_SIZE = 256;\n\n    // Get CUDA device properties\n    cudaDeviceProp deviceProp;\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, 0));\n\n    int maxBlocksPerSm = deviceProp.maxThreadsPerMultiProcessor / BLOCK_SIZE;\n    int maxBlocks = maxBlocksPerSm * deviceProp.multiProcessorCount;\n\n    auto generateConnectedGraph = [](\n        std::vector<int>& rowOffsets_h,\n        std::vector<int>& colIndices_h,\n        int numNodes,\n        int numEdges,\n        std::mt19937& rng) -> void {\n\n        // Assert input parameters\n        assert(numNodes > 0);\n        assert(numEdges >= numNodes - 1); // Need at least spanning tree edges\n        assert(numNodes <= MAX_NODES);\n        assert(numEdges <= MAX_EDGES);\n\n        rowOffsets_h.resize(numNodes + 1);\n        colIndices_h.clear();\n        colIndices_h.reserve(numEdges);\n\n        // Step 1: Create adjacency list representation\n        std::vector<std::vector<int>> adjList(numNodes);\n\n        // Step 2: Build spanning tree to ensure connectivity\n        std::vector<int> nodes(numNodes);\n        std::iota(nodes.begin(), nodes.end(), 0);\n        std::shuffle(nodes.begin(), nodes.end(), rng);\n\n        // Connect each node to a random previous node in shuffled order\n        for (int i = 1; i < numNodes; ++i) {\n            std::uniform_int_distribution<int> parentDist(0, i - 1);\n            int parent = nodes[parentDist(rng)];\n            int child = nodes[i];\n\n            // Add bidirectional edges for connectivity\n            adjList[parent].push_back(child);\n            adjList[child].push_back(parent);\n        }\n\n        int edgesAdded = (numNodes - 1) * 2; // Each undirected edge = 2 directed edges\n\n        // Step 3: Add remaining edges randomly\n        std::uniform_int_distribution<int> nodeDist(0, numNodes - 1);\n\n        while (edgesAdded < numEdges) {\n            int u = nodeDist(rng);\n            int v = nodeDist(rng);\n\n            if (u != v) { // No self-loops\n                // Check if edge already exists\n                bool exists = std::find(adjList[u].begin(), adjList[u].end(), v) != adjList[u].end();\n                if (!exists) {\n                    adjList[u].push_back(v);\n                    edgesAdded++;\n                }\n            }\n        }\n\n        // Step 4: Convert adjacency list to CSR format\n        rowOffsets_h[0] = 0;\n        for (int i = 0; i < numNodes; ++i) {\n            // Sort neighbors for better cache locality\n            std::sort(adjList[i].begin(), adjList[i].end());\n\n            for (int neighbor : adjList[i]) {\n                colIndices_h.push_back(neighbor);\n            }\n            rowOffsets_h[i + 1] = colIndices_h.size();\n        }\n\n        // Assert final graph structure\n        assert(rowOffsets_h.size() == numNodes + 1);\n        assert(rowOffsets_h[0] == 0);\n        assert(colIndices_h.size() > 0);\n        assert(colIndices_h.size() <= numEdges);\n\n        // Validate all neighbors are within bounds\n        for (int neighbor : colIndices_h) {\n            assert(neighbor >= 0 && neighbor < numNodes);\n        }\n    };\n\n    // Create CUDA stream for async operations\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Device pointers with _d suffix\n    int *rowOffsets_d, *colIndices_d, *distances_d;\n    int *frontier_d, *nextFrontier_d, *nextFrontierSize_d;\n\n    CUDA_CHECK(cudaMallocAsync(&rowOffsets_d, (MAX_NODES + 1) * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&colIndices_d, MAX_EDGES * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&distances_d, MAX_NODES * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&frontier_d, MAX_NODES * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&nextFrontier_d, MAX_NODES * sizeof(int), stream));\n    CUDA_CHECK(cudaMallocAsync(&nextFrontierSize_d, sizeof(int), stream));\n\n    // Random number generator\n    std::random_device rd;\n    std::mt19937 rng(rd());\n\n    // Test case configurations\n    struct TestConfig {\n        int numNodes;\n        int numEdges;\n    };\n\n    const TestConfig testConfigs[NUM_TEST_CASES] = {\n        {1000, 5000},       // Small graph\n        {10000, 50000},      // Medium graph\n        {50000, 200000},     // Large graph\n        {100000, 500000},    // Very large graph\n        {200000, 1000000},   // Huge graph\n        {500000, 2000000},   // Massive graph\n        {800000, 5000000}    // Maximum graph\n    };\n\n    // Assert test configurations are valid\n    for (int i = 0; i < NUM_TEST_CASES; ++i) {\n        assert(testConfigs[i].numNodes > 0);\n        assert(testConfigs[i].numEdges >= testConfigs[i].numNodes - 1); // Ensure connectivity possible\n        assert(testConfigs[i].numNodes <= MAX_NODES);\n        assert(testConfigs[i].numEdges <= MAX_EDGES);\n    }\n\n    for (int i = 0; i < NUM_TEST_CASES; ++i) {\n        int currentNodes = testConfigs[i].numNodes;\n        int currentEdges = testConfigs[i].numEdges;\n\n        // Assert test case parameters\n        assert(currentNodes > 0 && currentNodes <= MAX_NODES);\n        assert(currentEdges >= currentNodes - 1 && currentEdges <= MAX_EDGES);\n\n        // Host vectors with _h suffix\n        std::vector<int> rowOffsets_h, colIndices_h;\n        generateConnectedGraph(rowOffsets_h, colIndices_h, currentNodes, currentEdges, rng);\n\n        // Assert graph generation results\n        assert(rowOffsets_h.size() == currentNodes + 1);\n        assert(colIndices_h.size() > 0);\n        assert(rowOffsets_h[0] == 0);\n\n        // Random source node\n        std::uniform_int_distribution<int> sourceDist(0, currentNodes - 1);\n        int source = sourceDist(rng);\n\n        // Assert source is valid\n        assert(source >= 0 && source < currentNodes);\n\n        std::vector<int> distances_h(currentNodes, -1);\n        std::vector<int> cpuDistances_h(currentNodes, -1);\n\n        int numNodes = rowOffsets_h.size() - 1;\n        int numEdges = colIndices_h.size();\n\n        // Assert BFS input parameters\n        assert(numNodes == currentNodes);\n        assert(numEdges > 0);\n        assert(source >= 0 && source < numNodes);\n\n        // Run CPU BFS for validation\n        cpuBFS(rowOffsets_h, colIndices_h, source, cpuDistances_h);\n\n        // Copy graph to device asynchronously\n        CUDA_CHECK(cudaMemcpyAsync(rowOffsets_d, rowOffsets_h.data(), (numNodes + 1) * sizeof(int), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(colIndices_d, colIndices_h.data(), numEdges * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        // Initialize distances to -1\n        std::vector<int> initDistances_h(numNodes, -1);\n        initDistances_h[source] = 0;\n        CUDA_CHECK(cudaMemcpyAsync(distances_d, initDistances_h.data(), numNodes * sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        // Initialize frontier with source node\n        int frontierSize = 1;\n        CUDA_CHECK(cudaMemcpyAsync(frontier_d, &source, sizeof(int), cudaMemcpyHostToDevice, stream));\n\n        int currentLevel = 0;\n        int maxIterations = numNodes; // Prevent infinite loops\n        int iterations = 0;\n\n        while (frontierSize > 0) {\n            // Assert loop bounds\n            assert(iterations < maxIterations);\n            assert(frontierSize > 0);\n            assert(currentLevel >= 0);\n\n            CUDA_CHECK(cudaMemsetAsync(nextFrontierSize_d, 0, sizeof(int), stream));\n\n            // Calculate grid size based on device properties and frontier size\n            int gridSize = std::min(maxBlocks, (frontierSize + BLOCK_SIZE - 1) / BLOCK_SIZE);\n\n            // Assert grid configuration\n            assert(gridSize > 0);\n            assert(gridSize <= maxBlocks);\n\n            // Prepare kernel parameters\n            void* kernelArgs[] = {\n                &rowOffsets_d,\n                &colIndices_d,\n                &frontier_d,\n                &frontierSize,\n                &currentLevel,\n                &distances_d,\n                &nextFrontier_d,\n                &nextFrontierSize_d\n            };\n\n            // Launch kernel using cudaLaunchKernel\n            CUDA_CHECK(cudaLaunchKernel(\n                (void*)k_bfsKernel,\n                dim3(gridSize),\n                dim3(BLOCK_SIZE),\n                kernelArgs,\n                0,\n                stream\n            ));\n\n            CUDA_CHECK(cudaMemcpyAsync(&frontierSize, nextFrontierSize_d, sizeof(int), cudaMemcpyDeviceToHost, stream));\n            CUDA_CHECK(cudaStreamSynchronize(stream));\n\n            // Assert frontier size is reasonable\n            assert(frontierSize >= 0);\n            assert(frontierSize <= numNodes);\n\n            std::swap(frontier_d, nextFrontier_d);\n            currentLevel++;\n            iterations++;\n        }\n\n        CUDA_CHECK(cudaMemcpyAsync(distances_h.data(), distances_d, numNodes * sizeof(int), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validate GPU results against CPU results\n        for (int j = 0; j < numNodes; ++j) {\n            assert(distances_h[j] == cpuDistances_h[j]);\n        }\n\n        assert(distances_h[source] == 0); // Source distance should be 0\n\n        int reachableCount = 0;\n        int maxDistance = 0;\n        for (int j = 0; j < numNodes; ++j) {\n            if (distances_h[j] != -1) {\n                assert(distances_h[j] >= 0);\n                assert(distances_h[j] < numNodes); // Distance can't exceed graph diameter\n                reachableCount++;\n                maxDistance = std::max(maxDistance, distances_h[j]);\n            }\n        }\n\n        // Assert result validity\n        assert(reachableCount > 0); // At least source should be reachable\n        assert(reachableCount == numNodes);\n        assert(maxDistance >= 0);\n        assert(maxDistance < numNodes);\n    }\n\n    // Free device memory asynchronously\n    CUDA_CHECK(cudaFreeAsync(rowOffsets_d, stream));\n    CUDA_CHECK(cudaFreeAsync(colIndices_d, stream));\n    CUDA_CHECK(cudaFreeAsync(distances_d, stream));\n    CUDA_CHECK(cudaFreeAsync(frontier_d, stream));\n    CUDA_CHECK(cudaFreeAsync(nextFrontier_d, stream));\n    CUDA_CHECK(cudaFreeAsync(nextFrontierSize_d, stream));\n\n    // Destroy stream\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_bfsKernel(\n    const int* rowOffsets_d,\n    const int* colIndices_d,\n    const int* frontier_d,\n    int frontierSize,\n    int currentLevel,\n    int* distances_d,\n    int* nextFrontier_d,\n    int* nextFrontierSize_d)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/207", "date": "2025-07-30", "prompt": "Write a CUDA kernel that performs FIR filtering where each thread processes with the equation output[n] = (k=0 to filterOrder) coeff[k] * input[n-k] where n is the current output sample, k is tap index. The kernel should take filter coefficients as argument and load it into shared memory for threads to compute output.\n\nThe signature of the function is __global__ void k_firFilter(const float* inputSignal_d, float* outputSignal_d, float* filterCoeffs_d, int signalLength, int filterOrder), where inputSignal_d is pointer to the input signal array, outputSignal_d is pointer to the output signal array, filterCoeffs_d is the filter coefficients array, signalLength is the total number of samples, and filterOrder is the highest coefficient index.\n\n>>> k_firFilter({1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f, 9.0f, 10.0f}, outputSignal_d, {0.293f, 0.586f, 0.293f}, 10, 2) -> outputSignal_d({0.293f, 1.172f, 2.343f, 3.515f, 4.686f, 5.858f, 7.030f, 8.201f, 9.373f, 10.544f});\n>>> k_firFilter({0.601f, 0.708f, 0.021f, 0.970f, 0.832f, 0.212f, 0.182f, 0.183f, 0.304f, 0.525f, 0.432f, 0.291f, 0.612f, 0.139f, 0.292f, 0.366f}, outputSignal_d, {0.293f, 0.586f, 0.293f}, 10, 2) -> outputSignal_d({0.146f, 0.732f, 1.757f, 2.929f, 4.101f, 5.272f, 6.444f, 7.615f, 8.787f, 9.959f});\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <iostream>\n#include <vector>\n#include <cassert>\n#include <cmath>\n#include <algorithm>\n\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n__global__ void k_firFilter(const float* inputSignal_d, float* outputSignal_d, float* filterCoeffs_d, int signalLength, int filterOrder);\n\nint launch() {\n    const int NUM_TESTS = 7;\n    const int INPUT_SIZES[NUM_TESTS] = {4096, 8192, 16384, 32768, 65536, 131072, 262144};\n    const int MAX_FILTER_ORDER = 2;\n    const int NUM_FILTER_COEFFS = MAX_FILTER_ORDER + 1;\n    const int DEVICE_ID = 0;\n    const float FILTER_COEFFS[NUM_FILTER_COEFFS] = {0.2929f, 0.5858f, 0.2929f};\n    const float TOLERANCE = 1e-5f;\n\n    cudaStream_t computeStream;\n    cudaStreamCreate(&computeStream);\n\n    float *filterCoeffs_d;\n    CUDA_CHECK(cudaMallocAsync(&filterCoeffs_d, NUM_FILTER_COEFFS * sizeof(float), computeStream));\n    CUDA_CHECK(cudaMemcpyAsync(filterCoeffs_d, FILTER_COEFFS, NUM_FILTER_COEFFS * sizeof(float), cudaMemcpyHostToDevice, computeStream));\n\n    auto generateTestSignal = [](int signalLength) {\n        std::vector<float> signal(signalLength);\n        for (int i = 0; i < signalLength; ++i)\n            signal[i] = static_cast<float>(rand()) / RAND_MAX;\n        return signal;\n    };\n\n    auto computeReferenceFIR = [&](const std::vector<float>& inputSignal, std::vector<float>& outputSignal) {\n        outputSignal.resize(inputSignal.size(), 0.0f);\n        for (size_t id = 0; id < inputSignal.size(); ++id) {\n            float accumulator = 0.0f;\n            // FIR computation - only feedforward path\n            for (int coeffIndex = 0; coeffIndex <= MAX_FILTER_ORDER; ++coeffIndex)\n                if (id >= coeffIndex)\n                    accumulator += FILTER_COEFFS[coeffIndex] * inputSignal[id - coeffIndex];\n            outputSignal[id] = accumulator;\n        }\n    };\n\n    float *inputSignal_d, *outputSignal_d;\n    const int LARGEST_TEST_INDEX = NUM_TESTS - 1;\n    size_t maxBufferSize = INPUT_SIZES[LARGEST_TEST_INDEX] * sizeof(float);\n    CUDA_CHECK(cudaMallocAsync(&inputSignal_d, maxBufferSize, computeStream));\n    CUDA_CHECK(cudaMallocAsync(&outputSignal_d, maxBufferSize, computeStream));\n\n    int maxGridDimension, maxSharedMemoryPerBlock;\n    cudaDeviceGetAttribute(&maxGridDimension, cudaDevAttrMaxGridDimX, DEVICE_ID);\n    cudaDeviceGetAttribute(&maxSharedMemoryPerBlock, cudaDevAttrMaxSharedMemoryPerBlock, DEVICE_ID);\n\n    for (int testIndex = 0; testIndex < NUM_TESTS; ++testIndex) {\n        int signalLength = INPUT_SIZES[testIndex];\n        auto inputSignal_host = generateTestSignal(signalLength);\n        std::vector<float> outputSignal_host(signalLength), referenceOutput(signalLength);\n\n        CUDA_CHECK(cudaMemcpyAsync(inputSignal_d, inputSignal_host.data(), signalLength * sizeof(float), cudaMemcpyHostToDevice, computeStream));\n\n        const int THREADS_PER_BLOCK = 256;\n        int requiredBlocks = (signalLength + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n        int actualGridSize = std::min(requiredBlocks, maxGridDimension);\n        size_t requiredSharedMemory = (MAX_FILTER_ORDER + 1 + THREADS_PER_BLOCK + MAX_FILTER_ORDER) * sizeof(float);\n        size_t sharedMemorySize = std::min((int)requiredSharedMemory, (int)maxSharedMemoryPerBlock);\n        void* kernelArgs[] = { &inputSignal_d, &outputSignal_d, &filterCoeffs_d,\n                              &signalLength, (void*)&MAX_FILTER_ORDER };\n        CUDA_CHECK(cudaLaunchKernel((void*)k_firFilter,\n                         dim3(actualGridSize), dim3(THREADS_PER_BLOCK),\n                         kernelArgs, sharedMemorySize, computeStream));\n\n        CUDA_CHECK(cudaMemcpyAsync(outputSignal_host.data(), outputSignal_d, signalLength * sizeof(float), cudaMemcpyDeviceToHost, computeStream));\n        CUDA_CHECK(cudaStreamSynchronize(computeStream));\n\n        computeReferenceFIR(inputSignal_host, referenceOutput);\n        for (int id = 0; id < signalLength; ++id) {\n            assert(fabs(outputSignal_host[id] - referenceOutput[id]) < TOLERANCE);\n        }\n    }\n\n    CUDA_CHECK(cudaFreeAsync(inputSignal_d, computeStream));\n    CUDA_CHECK(cudaFreeAsync(outputSignal_d, computeStream));\n    CUDA_CHECK(cudaFreeAsync(filterCoeffs_d, computeStream));\n    CUDA_CHECK(cudaStreamDestroy(computeStream));\n\n    return 0;\n}\n\n__global__ void k_firFilter(const float* inputSignal_d, float* outputSignal_d, float* filterCoeffs_d, int signalLength, int filterOrder) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/208", "date": "2025-07-30", "prompt": "Write a CUDA kernel that computes cross-entropy loss from softmax probabilities and one-hot encoded labels. Each thread should compute -log(max(probability, 1e-6)) only for true class elements (label = 1) and accumulate its partial loss. The kernel should perform warp-level reduction and use atomic operations to accumulate the total loss into a single global output.\n\nThe signature of the function is __global__ void k_crossEntropyLoss(const float* probs_d, const int* labels_d, float* loss_d, int numSamples, int numClasses), where probs_d is pointer to predicted softmax probabilities, labels_d is pointer to one-hot encoded labels, loss_d is pointer to a single float that accumulates total cross-entropy loss, numSamples is number of samples, numClasses is number of classes.\n\n>>> k_crossEntropyLoss({0.0006, 0.0010, 0.0003, 0.0006, 0.0015, 0.0018, 0.0015, 0.0007, 0.0016, 0.0003}, labels_d, loss_d, 128, 1000)->loss_d:(7.089562)\n>>> k_crossEntropyLoss({0.0016, 0.0013, 0.0008, 0.0010, 0.0007, 0.0006, 0.0013, 0.0006, 0.0005, 0.0009}, labels_d, loss_d, 128, 1000)->loss_d:(7.053688)\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <vector>\n#include <cmath>\n#include <numeric>\n#include <cstdio>\n#include <cassert>\n#include <algorithm>\n\n#include <cuda_runtime.h>\n\n#define MIN_PROB 1e-6f\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\n\n__global__ void k_crossEntropyLoss(const float* probs_d, const int* labels_d, float* loss_d, int numSamples, int numClasses);\n\nvoid launch() {\n  const int NUM_TEST_CASES = 7;\n  const int NUM_CLASSES = 1000;\n  const int MAX_SAMPLES = 16384;\n  const int MAX_TOTAL = MAX_SAMPLES * NUM_CLASSES;\n  const float TOLERANCE = 1e-3f;\n\n  cudaStream_t stream;\n  cudaStreamCreate(&stream);\n  srand(42);\n\n  // CUDA properties\n  cudaDeviceProp props;\n  cudaGetDeviceProperties(&props, 0);\n\n  const int BLOCK_SIZE = 256;\n  int maxGridSize = props.multiProcessorCount * (props.maxThreadsPerMultiProcessor / BLOCK_SIZE);\n\n  float *probs_d, *loss_d;\n  int *labels_d;\n  CUDA_CHECK(cudaMallocAsync(&probs_d, MAX_TOTAL * sizeof(float), stream));\n  CUDA_CHECK(cudaMallocAsync(&labels_d, MAX_TOTAL * sizeof(int), stream));\n  CUDA_CHECK(cudaMallocAsync(&loss_d, sizeof(float), stream));\n\n  int testSizes[NUM_TEST_CASES] = {128, 512, 1024, 2048, 4096, 8192, 16384};\n\n  for (int testIdx = 0; testIdx < NUM_TEST_CASES; ++testIdx) {\n    int numSamples = testSizes[testIdx];\n    int totalElems = numSamples * NUM_CLASSES;\n    int reqBlocks = (totalElems + BLOCK_SIZE - 1) / BLOCK_SIZE;\n    int actualBlocks = std::min(maxGridSize, reqBlocks);\n\n    float *probs_h = new float[totalElems];\n    float *labels_h = new float[totalElems];\n\n    auto normalize = [&](int sampleIdx) {\n      float sum = 0.0f;\n      for (int classIdx = 0; classIdx < NUM_CLASSES; ++classIdx) {\n        int elemIdx = sampleIdx * NUM_CLASSES + classIdx;\n        probs_h[elemIdx] = 0.1f + (rand() % 9000) / 10000.0f;\n        sum += probs_h[elemIdx];\n        labels_h[elemIdx] = 0.0f;\n      }\n      int trueClass = rand() % NUM_CLASSES;\n      labels_h[sampleIdx * NUM_CLASSES + trueClass] = 1.0f;\n      for (int classIdx = 0; classIdx < NUM_CLASSES; ++classIdx)\n        probs_h[sampleIdx * NUM_CLASSES + classIdx] /= sum;\n    };\n\n    auto computeCpuLoss = [&]() -> float {\n      float loss = 0.0f;\n      for (int sampleIdx = 0; sampleIdx < numSamples; ++sampleIdx) {\n        for (int classIdx = 0; classIdx < NUM_CLASSES; ++classIdx) {\n          int elemIdx = sampleIdx * NUM_CLASSES + classIdx;\n          if (labels_h[elemIdx] == 1.0f)\n            loss += -logf(fmaxf(probs_h[elemIdx], MIN_PROB));\n        }\n      }\n      return loss / numSamples;\n    };\n\n    for (int sampleIdx = 0; sampleIdx < numSamples; ++sampleIdx)\n      normalize(sampleIdx);\n\n    float cpuLoss = computeCpuLoss();\n    float gpuLoss = 0.0f;\n\n    CUDA_CHECK(cudaMemcpyAsync(probs_d, probs_h, totalElems * sizeof(float), cudaMemcpyHostToDevice, stream));\n    CUDA_CHECK(cudaMemcpyAsync(labels_d, labels_h, totalElems * sizeof(int), cudaMemcpyHostToDevice, stream));\n    CUDA_CHECK(cudaMemsetAsync(loss_d, 0, sizeof(float), stream));\n\n    void* args[] = { &probs_d, &labels_d, &loss_d, (void*)&numSamples, (void*)&NUM_CLASSES };\n    CUDA_CHECK(cudaLaunchKernel((void*)k_crossEntropyLoss, dim3(actualBlocks), dim3(BLOCK_SIZE), args, 0, stream));\n\n    CUDA_CHECK(cudaMemcpyAsync(&gpuLoss, loss_d, sizeof(float), cudaMemcpyDeviceToHost, stream));\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n\n    gpuLoss /= numSamples;\n    assert(fabs(cpuLoss - gpuLoss) < TOLERANCE);\n\n    delete[] probs_h;\n    delete[] labels_h;\n  }\n\n  CUDA_CHECK(cudaFreeAsync(probs_d, stream));\n  CUDA_CHECK(cudaFreeAsync(labels_d, stream));\n  CUDA_CHECK(cudaFreeAsync(loss_d, stream));\n  CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n\n__global__ void k_crossEntropyLoss(const float* probs_d, const int* labels_d, float* loss_d, int numSamples, int numClasses) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/209", "date": "2025-07-30", "prompt": "Write a CUDA kernel that counts the total occurrence of a character within an input DNA sequence. Utilize warp-level intrinsics to aggregate the partial counts within each warp efficiently.\n\nThe signature of the function is \n__global__ void k_characterCount(const char* input_d, char targetChar, int inputLength, unsigned int* count_d) where input_d is a device pointer to the input DNA sequence which contains the bases 'A', 'C', 'G', and 'T', targetChar is the target character to search for within the input sequence, inputLength specifies the total number of characters in the input and count_d is a device pointer to the output where the final count of the target character will be stored.\n\n>>> k_characterCount(\"ACGTAGATCGAGGAATAGAC\", \"A\", 20, count) -> count: 8\n>>> k_characterCount(\"AAAAA\", \"A\", 5, count) -> count: 5\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n#include <string>\n#include <cstdio>\n#include <assert.h>\n#include <cuda.h>\n#include <curand_kernel.h>\n#include <cuda_runtime.h>\n\n#undef NDEBUG\n#define CUDA_CHECK(call)                                    \\\ndo {                                                        \\\n    cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                         \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",   \\\n                    __FILE__, __LINE__,                     \\\n                    cudaGetErrorString(error));             \\\n            exit(EXIT_FAILURE);                             \\\n    }                                                       \\\n} while (0)\n\n__global__ void k_characterCount(const char* input_d, char targetChar, int inputLength, unsigned int* count_d);\n\nvoid launch() {\n\n    const int TEST_CASE_COUNT = 7;\n    const int MAX_LEN = 100;\n    char targetChar = 'A';\n\n    //Input sequences and patterns for testing\n    char input[TEST_CASE_COUNT][MAX_LEN] = {\n        {\"ACGTAGATCGAGGAATAGAC\"},\n        {\"AAAAA\"},\n        {\"ACGTAGATCGAGGAATAGACACGTAGATCGAGGAATAGAC\"},\n        {\"GGGGGGGGGGGGGGGGGG\"},\n        {\"TTTTTTTATTTTTTTT\"},\n        {\"CUDATCGATGCCUDATCGATGCCUDATCGATGC\"},\n        {\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"}\n    };\n\n    //Expected output\n    unsigned int expectedCount[] = {8, 5, 16, 0, 1, 6, 45};\n\n    //Creating cuda streams\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    //Device Memory Allocation\n    char* input_d;\n    unsigned int* count_d;\n    CUDA_CHECK(cudaMallocAsync(&input_d, MAX_LEN * sizeof(char), stream));\n    CUDA_CHECK(cudaMallocAsync(&count_d, sizeof(unsigned int), stream));\n\n    // Fetch GPU properties\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, 0);\n    int maxBlocks = prop.maxGridSize[0];\n    int maxThreads = prop.maxThreadsDim[0];\n    int warpSize = prop.warpSize;\n    \n    //Declaration of test sequence and pattern\n    for (int tc = 0; tc < TEST_CASE_COUNT; tc++) {\n        \n        int inputLength = strlen(input[tc]);\n\n        int threadsPerBlock = std::min(maxThreads, std::max(warpSize, inputLength));\n        // Set the grid dimension to the minimum of needed blocks or GPU max blocks\n        int blocksPerGrid = std::min(maxBlocks, (inputLength + threadsPerBlock - 1) / threadsPerBlock);\n              \n        // Copy sequences to device\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input[tc], inputLength * sizeof(char), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemsetAsync(count_d, 0, sizeof(unsigned int), stream));\n\n        dim3 gridSize(blocksPerGrid, 1, 1);\n        dim3 blockSize(threadsPerBlock, 1, 1);\n\n        //Launch the kernel function\n        //Grid: (inputLength + threadsPerBlock - 1) / threadsPerBlock), 1, 1)\n        //Block: (threadsPerBlock, 1, 1)\n        void *args[] = {&input_d, &targetChar, &inputLength, &count_d};\n        CUDA_CHECK(cudaLaunchKernel((void*)k_characterCount, gridSize, blockSize, args, 0, stream));\n\n        int count_h = 0;\n        // Copy results back to host\n        CUDA_CHECK(cudaMemcpyAsync(&count_h, count_d, sizeof(unsigned int), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        //Validate the results with the expected value\n        assert(count_h == expectedCount[tc]);\n    }\n\n    //Free Device memory\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(count_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_characterCount(const char* input_d, char targetChar, int inputLength, unsigned int* count_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/210", "date": "2025-07-30", "prompt": "Write CUDA kernel for histogram CDF computation. The kernel should first compute total sum of all histogram bins, then compute cumulative distribution across each bin.\n\nThe signature of the function is __global__ void k_histogramCdfKernel(const float* histogram_d, float* cdf_d, int numBins), where histogram_d is input device pointer to histogram array containing frequency counts for each bin, cdf_d is output device pointer to CDF array where cumulative distribution function values will be stored, numBins is number of histogram bins/elements to process.\n\n>>> k_histogramCdfKernel({8459, 3260, 4390, 4047, 7083, 3517, 5161, 7749, 8077, 5437}, cdf_d, 256) -> cdf_d: ({0.003026, 0.009460, 0.017139, 0.018621, 0.024534, 0.030831, 0.035667, 0.040488, 0.041749, 0.045350})\n>>> k_histogramCdfKernel({245, 5510, 8223, 3008, 7768, 6775, 2691, 2843, 6998, 81}, cdf_d, 1024) -> cdf_d: ({0.000014, 0.000335, 0.001330, 0.002371, 0.003184, 0.004129, 0.004562, 0.005912, 0.006145, 0.006671})\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#include <iostream>\n#include <vector>\n#include <algorithm>\n#include <random>\n#include <iomanip>\n#include <cassert>\n#include <cmath>\n\n__global__ void k_histogramCdfKernel(const float* histogram_d, float* cdf_d, int numBins);\n\nvoid launch() {\n    const int TEST_CASES = 7;\n    const float TOLERANCE = 1E-4;\n\n    auto allocateMultiBlockResources = [](int numBins, const cudaDeviceProp& deviceProp) {\n        struct ResourceConfig {\n            int threadsPerBlock;\n            int numBlocks;\n            size_t sharedMemSize;\n        };\n\n        // Calculate optimal multi-block configuration\n        int maxThreadsPerBlock = deviceProp.maxThreadsPerBlock;\n        int maxBlocksPerSM = deviceProp.maxThreadsPerMultiProcessor / maxThreadsPerBlock;\n        int maxTotalBlocks = maxBlocksPerSM * deviceProp.multiProcessorCount;\n\n        ResourceConfig config;\n\n        // Use optimal multi-block configuration\n        config.threadsPerBlock = std::min(1024, maxThreadsPerBlock);\n\n        // Calculate number of blocks needed for grid-stride coverage\n        int minBlocksNeeded = (numBins + config.threadsPerBlock - 1) / config.threadsPerBlock;\n        config.numBlocks = std::min(std::max(minBlocksNeeded, 4), std::min(32, maxTotalBlocks));\n\n        // Shared memory for reduction within each block\n        config.sharedMemSize = config.threadsPerBlock * sizeof(float);\n\n        return config;\n    };\n\n    // Get CUDA device properties\n    cudaDeviceProp deviceProp;\n    cudaGetDeviceProperties(&deviceProp, 0);\n\n    std::vector<int> testBinCounts = {\n        256, 1024, 4096, 16384, 65536, 262144, 1048576\n    };\n\n    std::vector<std::vector<float>> expected = {\n        {0.003026, 0.009460, 0.017139, 0.018621, 0.024534, 0.030831, 0.035667, 0.040488, 0.041749, 0.045350},\n        {0.000014, 0.000335, 0.001330, 0.002371, 0.003184, 0.004129, 0.004562, 0.005912, 0.006145, 0.006671},\n        {0.000010, 0.000285, 0.000442, 0.000852, 0.000955, 0.000999, 0.001159, 0.001420, 0.001478, 0.001592},\n        {0.000061, 0.000151, 0.000201, 0.000241, 0.000306, 0.000418, 0.000491, 0.000523, 0.000525, 0.000614},\n        {0.000011, 0.000025, 0.000027, 0.000044, 0.000056, 0.000068, 0.000087, 0.000093, 0.000109, 0.000127},\n        {0.000004, 0.000010, 0.000013, 0.000018, 0.000020, 0.000022, 0.000029, 0.000030, 0.000031, 0.000037},\n        {0.000001, 0.000001, 0.000001, 0.000003, 0.000003, 0.000003, 0.000004, 0.000006, 0.000008, 0.000009}\n    };\n\n    cudaStream_t streamHandle;\n    cudaStreamCreate(&streamHandle);\n\n    // Initialize random number generator\n    std::mt19937 gen(42);\n    std::uniform_int_distribution<> dis(1, 10000);\n\n    for (int testCase = 0; testCase < TEST_CASES; testCase++) {\n        int numBins = testBinCounts[testCase];\n        size_t bytesSize = numBins * sizeof(float);\n\n        // Allocate multi-block resources\n        auto config = allocateMultiBlockResources(numBins, deviceProp);\n\n        // Allocate memory for this test case\n        float* histogram_d;\n        float* cdf_d;\n\n        cudaMallocAsync(&histogram_d, bytesSize, streamHandle);\n        cudaMallocAsync(&cdf_d, bytesSize, streamHandle);\n\n        // Generate histogram data on host\n        std::vector<float> hostHistogram(numBins);\n        std::vector<float> hostCdf(numBins);\n\n        for (int i = 0; i < numBins; i++) {\n            hostHistogram[i] = static_cast<float>(dis(gen));\n        }\n\n        // Copy histogram to device\n        cudaMemcpyAsync(histogram_d, hostHistogram.data(), bytesSize,\n                       cudaMemcpyHostToDevice, streamHandle);\n\n\n        // Launch multi-block GPU kernel\n        dim3 gridDim(config.numBlocks);\n        dim3 blockDim(config.threadsPerBlock);\n        void* kernelArgs[] = {&histogram_d, &cdf_d, &numBins};\n\n        cudaLaunchKernel((void*)k_histogramCdfKernel, gridDim, blockDim,\n                       kernelArgs, config.sharedMemSize, streamHandle);\n\n        // Copy result back to host\n        cudaMemcpyAsync(hostCdf.data(), cdf_d, bytesSize,\n                       cudaMemcpyDeviceToHost, streamHandle);\n        cudaStreamSynchronize(streamHandle);\n\n        for(int i=0; i < 10; i++){\n          assert(fabs(expected[testCase][i] - hostCdf[i]) < TOLERANCE);\n        }\n\n        // Free memory for this test case\n        cudaFreeAsync(histogram_d, streamHandle);\n        cudaFreeAsync(cdf_d, streamHandle);\n    }\n\n    cudaStreamDestroy(streamHandle);\n}\n\n__global__ void k_histogramCdfKernel(const float* histogram_d, float* cdf_d, int numBins) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/211", "date": "2025-07-30", "prompt": "Write a CUDA kernel to compute polynomial rolling hash values for variable length strings stored in a flattened character array. Each thread should independently process a complete string using the polynomial rolling hash algorithm.\n\nThe signature of the function is __global__ void k_computeStringHash(char* stringData_d, int* stringLengths_d, int* stringOffsets_d, long long* resultArray_d, int numOfStrings), where stringData_d represents character array containing all strings, stringLengths_d defines the array containing the length of each string, stringOffsets_d represents the array containing the starting position of each string in stringData, resultArray_d refers to output array storing computed polynomial rolling hash values for each string, numOfStrings represents total number of strings to process.\n\n>>> k_computeStringHash ({\"abc\", \"hello\", \"world\", \"test\", \"short\", \"name\", \"a\", \"xy\", \"quick\", \"fox\"}, {3, 5, 5, 4, 5, 4, 1, 2, 5, 3}, {0, 3, 8, 13, 17, 22, 26, 27, 29, 34}, resultArray_d, 10) --> resultArray_d({2946, 14222002, 4069362, 614254, 19021340, 161493, 1, 799, 10257421, 23535}).\n>>> k_computeStringHash ({\"hello\", \"world\", \"test\", \"short\", \"name\", \"a\", \"xy\", \"quick\", \"fox\", \"data\"}, {5, 5, 4, 5, 4, 1, 2, 5, 3, 4}, {0, 5, 10, 14, 19, 23, 24, 26, 31, 34}, resultArray_d, 10) --> resultArray_d({14222002, 4069362, 614254, 19021340, 161493, 1, 799, 10257421, 23535, 49046}).\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <cstdio>\n#include <algorithm>\n#include <assert.h>\n#include <cuda_runtime.h>\n#include <cstring>\n#include <random>\n#include <chrono>\n#undef NDEBUG\n\n#define CUDA_CHECK(call)                                                           \\\ndo {                                                                               \\\n        cudaError_t error = call;                                                  \\\n        if (error != cudaSuccess) {                                                \\\n            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", cudaGetErrorString(error),\\\n                    __FILE__, __LINE__);                                           \\\n            exit(error);                                                           \\\n        }                                                                          \\\n} while (0)\n\n#define BLOCK_SIZE 256\n#define BASE_MULTIPLIER 31\n#define MODULUS 1000000009\n\n__global__ void k_computeStringHash(char* stringData_d, int* stringLengths_d, int* stringOffsets_d, long long* resultArray_d, int numOfStrings);\n\nvoid launch() {\n    // Get device properties\n    cudaDeviceProp deviceProp;\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, 0));\n\n    {\n        int numTestCases = 8;\n        // Predefined test data for small test cases\n        const char* testData[numTestCases][10] = {\n            {\"abc\", \"hello\", \"world\", \"test\", \"short\", \"name\", \"a\", \"xy\", \"quick\", \"fox\"},\n            {\"hello\", \"world\", \"test\", \"short\", \"name\", \"a\", \"xy\", \"quick\", \"fox\", \"data\"},\n            {\"world\", \"test\", \"short\", \"name\", \"data\", \"info\", \"proc\", \"comp\", \"file\", \"algo\"},\n            {\"test\", \"name\", \"data\", \"info\", \"proc\", \"comp\", \"file\", \"algo\", \"hash\", \"code\"},\n            {}, // Huge test case - will be random\n            {\"ab\", \"xy\", \"cd\", \"ef\", \"gh\", \"ij\", \"kl\", \"mn\", \"op\", \"qr\"},\n            {}, // Huge test case - will be random\n            {\"xy\", \"pq\", \"rs\", \"tu\", \"vw\", \"mn\", \"op\", \"gh\", \"ij\", \"kl\"}\n        };\n\n        // Calculate dynamic array sizes based on actual test data length\n         int numStrings[numTestCases];\n        for (int tc = 0; tc < numTestCases; tc++) {\n            if (tc == 4) {\n\n                // Huge test case\n                numStrings[tc] = 100000;  \n            } else if (tc == 6) {\n\n                // Huge test case\n                numStrings[tc] = 200000;  \n            } else {\n                int count = 0;\n                for (int i = 0; i < 10; i++) {\n                    if (strlen(testData[tc][i]) > 0) count++;\n                }\n                numStrings[tc] = count;\n            }\n        }\n\n        // Expected results for each test case (corrected calculations)\n        // Formula: char * (position + 1) + char for each character in flattened string\n        int expectedOutput_h[numTestCases][50] = {\n            {2946, 14222002, 4069362, 614254, 19021340, 161493, 1, 799, 10257421, 23535},\n            {14222002, 4069362, 614254, 19021340, 161493, 1, 799, 10257421, 23535, 49046},\n            {4069362, 614254, 19021340, 161493, 49046, 453074, 104362, 489617, 160772, 453965},\n            {614254, 161493, 49046, 453074, 104362, 489617, 160772, 453965, 256626, 153267},\n            {}, // Huge test case\n            {63, 799, 127, 191, 255, 319, 383, 447, 511, 575},\n            {}, // Huge test case\n            {799, 543, 607, 671, 735, 447, 511, 255, 319, 383}\n        };\n\n        // Random number generator setup\n        std::random_device rd;\n        std::mt19937 gen(rd());\n\n        // Create streams for parallel execution\n        cudaStream_t stream;\n        CUDA_CHECK(cudaStreamCreate(&stream));\n\n        // Arrays for device data and expected results\n        char* stringData_d[numTestCases];\n        int* stringLengths_d[numTestCases];\n        int* stringOffsets_d[numTestCases];\n        long long* resultData_d[numTestCases];\n        \n        char* stringData_h[numTestCases];\n        int* stringLengths_h[numTestCases];\n        int* stringOffsets_h[numTestCases];\n        long long* resultData_h[numTestCases];\n        long long* expectedResults_h[numTestCases];\n\n        // Launch all test cases\n        for (int tc = 0; tc < numTestCases; tc++) {\n            int totalStrings = numStrings[tc];\n            \n            // Allocate host memory\n            stringLengths_h[tc] = (int*)malloc(totalStrings * sizeof(int));\n            stringOffsets_h[tc] = (int*)malloc(totalStrings * sizeof(int));\n            resultData_h[tc] = (long long*)malloc(totalStrings * sizeof(long long));\n            expectedResults_h[tc] = (long long*)malloc(totalStrings * sizeof(long long));\n            \n            // Calculate total characters needed and populate string data\n            int totalChars = 0;\n            if (totalStrings <= 100) {\n                // Small test cases - use predefined data\n                for (int i = 0; i < totalStrings; i++) {\n                    stringLengths_h[tc][i] = strlen(testData[tc][i]);\n                    stringOffsets_h[tc][i] = totalChars;\n                    totalChars += stringLengths_h[tc][i];\n                    resultData_h[tc][i] = -1;\n                }\n                \n                // Allocate and fill string data\n                stringData_h[tc] = (char*)malloc(totalChars * sizeof(char));\n                int charIndex = 0;\n                for (int i = 0; i < totalStrings; i++) {\n                    for (int j = 0; j < stringLengths_h[tc][i]; j++) {\n                        stringData_h[tc][charIndex++] = testData[tc][i][j];\n                    }\n                }\n            } else {\n                // Huge test cases - generate random strings\n                std::mt19937 deterministicGen(12345 + tc);\n                std::uniform_int_distribution<> lenDist(3, 8);\n                std::uniform_int_distribution<> charDist(0, 25);\n                \n                for (int i = 0; i < totalStrings; i++) {\n                    stringLengths_h[tc][i] = lenDist(deterministicGen);\n                    stringOffsets_h[tc][i] = totalChars;\n                    totalChars += stringLengths_h[tc][i];\n                    resultData_h[tc][i] = -1;\n                }\n                \n                // Allocate and fill string data\n                stringData_h[tc] = (char*)malloc(totalChars * sizeof(char));\n                int charIndex = 0;\n                for (int i = 0; i < totalStrings; i++) {\n                    for (int j = 0; j < stringLengths_h[tc][i]; j++) {\n                        stringData_h[tc][charIndex++] = 'a' + charDist(deterministicGen);\n                    }\n                }\n            }\n\n            // Allocate device memory\n            CUDA_CHECK(cudaMallocAsync(&stringData_d[tc], totalChars * sizeof(char), stream));\n            CUDA_CHECK(cudaMallocAsync(&stringLengths_d[tc], totalStrings * sizeof(int), stream));\n            CUDA_CHECK(cudaMallocAsync(&stringOffsets_d[tc], totalStrings * sizeof(int), stream));\n            CUDA_CHECK(cudaMallocAsync(&resultData_d[tc], totalStrings * sizeof(long long), stream));\n\n            // Calculate grid size each thread processes ONE string\n            int gridSize = (totalStrings + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n            // Copy data to device\n            CUDA_CHECK(cudaMemcpyAsync(stringData_d[tc], stringData_h[tc], totalChars * sizeof(char), \n                                     cudaMemcpyHostToDevice, stream));\n            CUDA_CHECK(cudaMemcpyAsync(stringLengths_d[tc], stringLengths_h[tc], totalStrings * sizeof(int), \n                                     cudaMemcpyHostToDevice, stream));\n            CUDA_CHECK(cudaMemcpyAsync(stringOffsets_d[tc], stringOffsets_h[tc], totalStrings * sizeof(int), \n                                     cudaMemcpyHostToDevice, stream));\n\n            // Launch kernel - each thread processes ONE string\n            void *args[] = {&stringData_d[tc], &stringLengths_d[tc], &stringOffsets_d[tc], &resultData_d[tc], &totalStrings};\n            CUDA_CHECK(cudaLaunchKernel((void*)k_computeStringHash, \n                                      dim3(gridSize), dim3(BLOCK_SIZE), args, 0, stream));\n\n            // Copy results back\n            CUDA_CHECK(cudaMemcpyAsync(resultData_h[tc], resultData_d[tc], totalStrings * sizeof(long long), \n                                     cudaMemcpyDeviceToHost, stream));\n            \n            // For huge test cases\n            if (totalStrings > 100) {\n                CUDA_CHECK(cudaStreamSynchronize(stream));\n                for (int i = 0; i < totalStrings; i++) {\n                    expectedResults_h[tc][i] = resultData_h[tc][i];\n                }\n            }\n        }\n\n        // Synchronize and verify all test cases with assertions\n        for (int tc = 0; tc < numTestCases; tc++) {\n            CUDA_CHECK(cudaStreamSynchronize(stream));\n            \n            // Verify results\n            if (numStrings[tc] <= 100) {\n                // Small test cases \n                for (int i = 0; i < numStrings[tc]; i++) {\n                    assert(resultData_h[tc][i] == expectedOutput_h[tc][i]);\n                }\n            } else {\n                // Huge test cases \n                for (int i = 0; i < numStrings[tc]; i++) {\n                    // For huge test cases, verify sample points\n                    if (i % (numStrings[tc] / 1000) == 0 || i < 100) {\n                        assert(resultData_h[tc][i] == expectedResults_h[tc][i]);\n                    }\n                }\n            }\n        }\n\n        // Cleanup memory\n        for (int tc = 0; tc < numTestCases; tc++) {\n\n            // Free device memory\n            CUDA_CHECK(cudaFreeAsync(stringData_d[tc], stream));\n            CUDA_CHECK(cudaFreeAsync(stringLengths_d[tc], stream));\n            CUDA_CHECK(cudaFreeAsync(stringOffsets_d[tc], stream));\n            CUDA_CHECK(cudaFreeAsync(resultData_d[tc], stream));\n\n            // Free host memory \n            free(stringData_h[tc]);\n            free(stringLengths_h[tc]);\n            free(stringOffsets_h[tc]);\n            free(resultData_h[tc]);\n            free(expectedResults_h[tc]);\n        }\n        CUDA_CHECK(cudaStreamDestroy(stream));\n    }\n}\n\n__global__ void k_computeStringHash(char* stringData_d, int* stringLengths_d, int* stringOffsets_d, long long* resultArray_d, int numOfStrings) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/212", "date": "2025-07-30", "prompt": "Write a CUDA kernel that performs the Daubechies-4 wavelet transform using DB4 filter coefficients. Each thread should process one output coefficient pair (approximation + detail) by loading 4 consecutive input samples into shared memory to compute DB4 wavelet coefficients.\n\nThe signature of the function is __global__ void k_db4Wavelet(const float* input_d, float* output_d, int numSamples, const float* db4LoFilter_d, const float* db4HiFilter_d), where input_d is pointer to input signal array, output_d is pointer to output wavelet coefficients array, numSamples is total number of input samples, db4LoFilter_d is pointer to constant float array of Low-pass filter coefficients for DB4 wavelet, db4HiFilter_d is pointer to constant float array of High-pass filter coefficients for DB4 wavelet.\n\n>>> k_db4Wavelet({0.375f, 0.951f, 0.732f, 0.599f, 0.156f, 0.156f, 0.058f, 0.866f}, output_d, 8, {0.4821f, 0.836f, 0.224f, -0.129f}, {-0.129, -0.224f, 0.836f, -0.482f}) -> output_d:({1.063f, 0.869f, 0.107f, 0.835f, 0.062f, -0.174f, -0.425f, 0.105f});\n>>> k_db4Wavelet({0.601f, 0.708f, 0.021f, 0.970f, 0.832f, 0.212f, 0.182f, 0.183f, 0.304f, 0.525f, 0.432f, 0.291f, 0.612f, 0.139f, 0.292f, 0.366f}, output_d, 16, {0.4821f, 0.836f, 0.224f, -0.129f}, {-0.129, -0.224f, 0.836f, -0.482f}) -> output_d:({0.762f, 0.980f, 0.597f, 0.242f, 0.645f, 0.571f, 0.430f, 0.482f, -0.688f, 0.374f, -0.092f, -0.064f, 0.064f, 0.323f, -0.043f, 0.010f});\n", "cc_flags": "-arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#include <vector>\n#include <iostream>\n#include <cassert>\n#include <cmath>\n#include <cstdlib>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t error = call; \\\n        if (error != cudaSuccess) { \\\n            std::cerr << \"CUDA error at \" << __FILE__ << \":\" << __LINE__ \\\n                      << \" - \" << cudaGetErrorString(error) << std::endl; \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n// Global constants - used across multiple functions\nconst int DB4_FILTER_LENGTH = 4;\nconst int WAVELET_DECIMATION_FACTOR = 2;\n\n// Host-side Daubechies-4 filter coefficients for CPU reference\nconst float DB4_LO_FILTER_H[DB4_FILTER_LENGTH] = {\n    0.48296291f,   // h0\n    0.83651630f,   // h1\n    0.22414387f,   // h2\n    -0.12940952f   // h3\n};\n\nconst float DB4_HI_FILTER_H[DB4_FILTER_LENGTH] = {\n    -0.12940952f,  // g0 = (-1)^k * h(3-k)\n    -0.22414387f,  // g1\n    0.83651630f,   // g2\n    -0.48296291f   // g3\n};\n\n__global__ void k_db4Wavelet(const float* input_d, float* output_d, int numSamples, \n                             const float* db4LoFilter_d, const float* db4HiFilter_d);\n\nint launch() {\n    // Local constants for launch function\n    const int NUM_TESTS = 7;\n    const int TEST_SIZES[NUM_TESTS] = {1024, 2048, 4096, 8192, 16384, 32768, 65536};\n    const int BLOCK_SIZE = 256;\n    const float TOLERANCE = 1e-4f;\n\n    // CUDA device properties\n    int maxGridDimX = 0, maxSharedMemPerBlock = 0;\n    CUDA_CHECK(cudaDeviceGetAttribute(&maxGridDimX, cudaDevAttrMaxGridDimX, 0));\n    CUDA_CHECK(cudaDeviceGetAttribute(&maxSharedMemPerBlock, cudaDevAttrMaxSharedMemoryPerBlock, 0));\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    auto generateInput = [](int n) {\n        std::vector<float> vec(n);\n        for (int i = 0; i < n; ++i)\n            vec[i] = static_cast<float>(rand()) / RAND_MAX;\n        return vec;\n    };\n\n    auto referenceDb4 = [](const std::vector<float>& input_h, std::vector<float>& output_h) {\n        int n = input_h.size();\n        int halfSize = n / WAVELET_DECIMATION_FACTOR;\n        output_h.resize(n);\n\n        for (int i = 0; i < halfSize; ++i) {\n            float approximationCoeff = 0.0f, detailCoeff = 0.0f;\n            for (int k = 0; k < DB4_FILTER_LENGTH; ++k) {\n                int inputIdx = std::min(std::max(WAVELET_DECIMATION_FACTOR * i + k, 0), n - 1);\n                approximationCoeff += input_h[inputIdx] * DB4_LO_FILTER_H[k];\n                detailCoeff += input_h[inputIdx] * DB4_HI_FILTER_H[k];\n            }\n            output_h[i] = approximationCoeff;\n            output_h[halfSize + i] = detailCoeff;\n        }\n    };\n\n    // Allocate device memory once for the largest test size\n    const int MAX_SIZE = TEST_SIZES[NUM_TESTS - 1];  // Largest test size\n    const size_t MAX_BYTES = MAX_SIZE * sizeof(float);\n\n    float *input_d, *output_d, *db4LoFilter_d, *db4HiFilter_d;\n    CUDA_CHECK(cudaMallocAsync(&input_d, MAX_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&output_d, MAX_BYTES, stream));\n    CUDA_CHECK(cudaMallocAsync(&db4LoFilter_d, DB4_FILTER_LENGTH * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&db4HiFilter_d, DB4_FILTER_LENGTH * sizeof(float), stream));\n\n    // Copy filter coefficients to device (only once)\n    CUDA_CHECK(cudaMemcpyAsync(db4LoFilter_d, DB4_LO_FILTER_H, \n                               DB4_FILTER_LENGTH * sizeof(float), cudaMemcpyHostToDevice, stream));\n    CUDA_CHECK(cudaMemcpyAsync(db4HiFilter_d, DB4_HI_FILTER_H, \n                               DB4_FILTER_LENGTH * sizeof(float), cudaMemcpyHostToDevice, stream));\n\n    for (int t = 0; t < NUM_TESTS; ++t) {\n        int numSamples = TEST_SIZES[t];\n        size_t sizeBytes = numSamples * sizeof(float);\n        int halfSize = numSamples / WAVELET_DECIMATION_FACTOR;\n        int requiredGrids = (halfSize + BLOCK_SIZE - 1) / BLOCK_SIZE;\n        int gridSize = std::min(requiredGrids, maxGridDimX);\n\n        int maxElementsPerBlock = 2 * BLOCK_SIZE + 2;\n        size_t sharedMemBytes = std::min(\n            maxElementsPerBlock * sizeof(float),\n            (size_t)maxSharedMemPerBlock\n        );\n\n        auto input_h = generateInput(numSamples);\n        std::vector<float> gpuOutput_h(numSamples), cpuOutput_h(numSamples);\n\n        // Copy input data to device (only the required size)\n        CUDA_CHECK(cudaMemcpyAsync(input_d, input_h.data(), sizeBytes, cudaMemcpyHostToDevice, stream));\n\n        void* kernelArgs[] = { &input_d, &output_d, &numSamples, &db4LoFilter_d, &db4HiFilter_d };\n\n        CUDA_CHECK(cudaLaunchKernel((void*)k_db4Wavelet,\n                                    dim3(gridSize), dim3(BLOCK_SIZE),\n                                    kernelArgs, sharedMemBytes, stream));\n\n        // Copy output data from device (only the required size)\n        CUDA_CHECK(cudaMemcpyAsync(gpuOutput_h.data(), output_d, sizeBytes, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        referenceDb4(input_h, cpuOutput_h);\n        for (int i = 0; i < numSamples; ++i) {\n            assert(fabs(cpuOutput_h[i] - gpuOutput_h[i]) < TOLERANCE);\n        }\n    }\n\n    // Free device memory once after all tests\n    CUDA_CHECK(cudaFreeAsync(input_d, stream));\n    CUDA_CHECK(cudaFreeAsync(output_d, stream));\n    CUDA_CHECK(cudaFreeAsync(db4LoFilter_d, stream));\n    CUDA_CHECK(cudaFreeAsync(db4HiFilter_d, stream));\n\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    return 0;\n}\n\n__global__ void k_db4Wavelet(const float* input_d, float* output_d, int numSamples, \n                             const float* db4LoFilter_d, const float* db4HiFilter_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.4"}
{"task_id": "CUDA/213", "date": "2025-07-30", "prompt": "Write a CUDA kernel to rotate a 2D matrix by 90 degrees clockwise. Each thread should read an integer value from global memory representing a matrix element, apply the 90 degrees clockwise rotation transformation ((row, col) -> (col, numberOfRows-1-row)), and write the resulting rotated element to the output array.\n\nThe signature of the kernel is __global__ void k_rotateMatrix90(const int* inputMatrix_d, int* outputMatrix_d, int numberOfRows, int numberOfCols), where inputMatrix_d is a device pointer representing the input 2D matrix (stored as 1D array), outputMatrix_d is a device pointer for storing the rotated matrix, numberOfRows and numberOfCols are matrix dimensions.\n\n>>> k_rotateMatrix90({1,2,3,4,5,6,7,8,9,10,11,12}, output_d, 3, 4) -> output_d = {9,5,1,10,6,2,11,7,3,12,8,4} (90 clockwise)\n>>> k_rotateMatrix90({1,2,3,4,5,6,7,8,9}, output_d, 3, 3) -> output_d = {7,4,1,8,5,2,9,6,3} (90 clockwise)\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "// Standard C/C++ headers\n#include <vector>\n#include <algorithm>\n#include <cassert>\n#include <random>\n#include <ctime>\n\n// CUDA headers\n#include <cuda_runtime.h>\n\n// Error-checking macro\n#define CUDA_CHECK(call)                                                   \\\n{                                                                          \\\n    cudaError_t error = call;                                              \\\n    if(error != cudaSuccess) {                                             \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                      \\\n                __FILE__, __LINE__, cudaGetErrorString(error));            \\\n        exit(EXIT_FAILURE);                                                \\\n    }                                                                      \\\n}\n\n__global__ void k_rotateMatrix90(\n    const int* inputMatrix_d,\n    int* outputMatrix_d,\n    int numberOfRows,\n    int numberOfCols);\n\nvoid launch() {\n    const int BLOCK_SIZE = 256;\n\n    // Test case struct for matrix rotation\n    struct TestCase {\n        std::vector<int> inputMatrix;  // Input matrix (1D representation)\n        std::vector<int> expectedOutput;  // Expected rotated matrix\n        int numberOfRows;  // Matrix rows\n        int numberOfCols;  // Matrix columns\n    };\n\n    // Test cases\n    std::vector<TestCase> testCases = {\n        // 3x4 matrix\n        {{1,2,3,4,5,6,7,8,9,10,11,12}, \n         {9,5,1,10,6,2,11,7,3,12,8,4}, 3, 4},\n        \n        // 3x3 matrix\n        {{1,2,3,4,5,6,7,8,9}, \n         {7,4,1,8,5,2,9,6,3}, 3, 3},\n        \n        // 2x2 matrix\n        {{1,2,3,4}, \n         {3,1,4,2}, 2, 2},\n        \n        // Single element matrix\n        {{42}, \n         {42}, 1, 1},\n        \n        // 4x2 matrix\n        {{1,2,3,4,5,6,7,8}, \n         {7,5,3,1,8,6,4,2}, 4, 2},\n        \n        // 1x5 matrix\n        {{1,2,3,4,5}, \n         {1,2,3,4,5}, 1, 5},\n        \n        // 5x1 matrix\n        {{1,2,3,4,5}, \n         {5,4,3,2,1}, 5, 1},\n    };\n    \n    // Add large test case with random values for 90 rotation\n    int largeMatrixRows = 1000;\n    int largeMatrixCols = 500;\n    std::vector<int> largeInputMatrix;\n    std::vector<int> largeExpectedOutput;\n    \n    // Random number generator \n    std::mt19937 generator(static_cast<unsigned int>(std::time(nullptr)));\n    std::uniform_int_distribution<int> distributionInt(1, 10000);\n    \n    // Generate random input matrix\n    for (int i = 0; i < largeMatrixRows * largeMatrixCols; i++) {\n        largeInputMatrix.push_back(distributionInt(generator));\n    }\n    \n    // Calculate expected 90 rotation for large matrix\n    largeExpectedOutput.resize(largeMatrixCols * largeMatrixRows);\n    for (int currentRow = 0; currentRow < largeMatrixRows; currentRow++) {\n        for (int currentCol = 0; currentCol < largeMatrixCols; currentCol++) {\n            int inputIndex = currentRow * largeMatrixCols + currentCol;\n            int newRow = currentCol;\n            int newCol = largeMatrixRows - 1 - currentRow;\n            int outputIndex = newRow * largeMatrixRows + newCol;\n            largeExpectedOutput[outputIndex] = largeInputMatrix[inputIndex];\n        }\n    }\n    \n    testCases.push_back({largeInputMatrix, largeExpectedOutput, largeMatrixRows, largeMatrixCols});\n\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Compute maximum allocation sizes\n    size_t maxMatrixSize = 0;\n    size_t maxOutputSize = 0;\n    for (const auto& testCase : testCases) {\n        maxMatrixSize = std::max(maxMatrixSize, testCase.inputMatrix.size());\n        maxOutputSize = std::max(maxOutputSize, testCase.expectedOutput.size());\n    }\n    size_t maxMatrixBytes = maxMatrixSize * sizeof(int);\n    size_t maxOutputBytes = maxOutputSize * sizeof(int);\n\n    // Allocate device memory once\n    int* inputMatrix_d = nullptr;\n    int* outputMatrix_d = nullptr;\n    \n    CUDA_CHECK(cudaMallocAsync(&inputMatrix_d, maxMatrixBytes, stream));\n    CUDA_CHECK(cudaMallocAsync(&outputMatrix_d, maxOutputBytes, stream));\n\n    for (const auto& currentTestCase : testCases) {\n        int matrixSize = static_cast<int>(currentTestCase.inputMatrix.size());\n        int outputSize = static_cast<int>(currentTestCase.expectedOutput.size());\n        if (matrixSize == 0 || outputSize == 0) continue;\n        \n        // Copy input matrix to device\n        CUDA_CHECK(cudaMemcpyAsync(inputMatrix_d,\n                                  currentTestCase.inputMatrix.data(),\n                                  matrixSize * sizeof(int),\n                                  cudaMemcpyHostToDevice,\n                                  stream));\n           \n        // Determine launch configuration\n        cudaDeviceProp deviceProperties;\n        CUDA_CHECK(cudaGetDeviceProperties(&deviceProperties, 0));\n        int maxActiveBlocks = (deviceProperties.maxThreadsPerMultiProcessor * deviceProperties.multiProcessorCount) / BLOCK_SIZE;\n        int totalElements = currentTestCase.numberOfRows * currentTestCase.numberOfCols;\n        int blocksNeeded = (totalElements + BLOCK_SIZE - 1) / BLOCK_SIZE;\n        int blockCount = std::min(blocksNeeded, maxActiveBlocks);\n\n        dim3 blockDimensions(BLOCK_SIZE, 1, 1);\n        dim3 gridDimensions(\n            std::min(blockCount, deviceProperties.maxGridSize[0]),\n            1,\n            1\n        );\n\n        // Launch kernel\n        void* kernelArgs[] = {\n            (void*)&inputMatrix_d,\n            (void*)&outputMatrix_d,\n            (void*)&currentTestCase.numberOfRows,\n            (void*)&currentTestCase.numberOfCols\n        };\n        CUDA_CHECK(cudaLaunchKernel(\n            (void*)k_rotateMatrix90,\n            gridDimensions,\n            blockDimensions,\n            kernelArgs,\n            0,\n            stream\n        ));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Copy back and verify\n        std::vector<int> hostOutput(outputSize);\n        CUDA_CHECK(cudaMemcpyAsync(hostOutput.data(), \n                                  outputMatrix_d, \n                                  outputSize * sizeof(int), \n                                  cudaMemcpyDeviceToHost, \n                                  stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        \n        // Verify results\n        for (int i = 0; i < outputSize; i++) {\n            assert(hostOutput[i] == currentTestCase.expectedOutput[i]);\n        }\n    }\n\n    CUDA_CHECK(cudaFreeAsync(inputMatrix_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputMatrix_d, stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_rotateMatrix90(\n    const int* inputMatrix_d,\n    int* outputMatrix_d,\n    int numberOfRows,\n    int numberOfCols)\n{\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/214", "date": "2025-07-30", "prompt": "Write a CUDA kernel to convert a BGRA image to RGBA format. Assume BGRA pixel values are stored as one uint32 integer. Each thread should read one 4-byte integer from global memory, permute the bytes using __byte_perm() with a permutation descriptor required for BGRA to RGBA conversion, and then write them to the output. The first channel (as R in RGBA) should correspond to the least significant byte, while the last channel (as A in RGBA) corresponds to the most significant byte of the integer.\n\nThe signature of the CUDA kernel is __global__ void k_permuteColorChannels(uint32_t *pixels_d, int numPixels), where pixels_d is a pointer to an array of integers representing the colors of pixels, and numPixels is the total number of pixels.\n\n>>> k_permuteColorChannels({ 0xFF00FF00, 0xFFFF00FF, 0x000000FF, 0x0000FF00, 0x00100010, 0xFFFFFFFF, 0x12345678, 0x87654321, 0x00000000, 0xABBAABBA }, 10) -> pixels_d: { 0xFF00FF00, 0xFFFF00FF, 0x00FF0000, 0x0000ff00, 0x00100010, 0xFFFFFFFF, 0x12785634, 0x87214365, 0x00000000, 0xABBAABBA }\n>>> k_permuteColorChannels({ 0x00FF00FF, 0xFFFFFF00, 0xFF000000, 0x000000FF, 0x10001000 }, 5) -> pixels_d: { 0x00FF00FF, 0xFF00FFFF, 0xFF000000, 0x00FF0000, 0x10001000 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n__global__ void k_permuteColorChannels(uint32_t * pixels_d, int numPixels);\n\nvoid launch() {\n    constexpr int MAX_NUM_PIXELS = 1000000;\n    constexpr int NUM_TESTS = 7;\n    constexpr int BYTE_MASK = 0xFF;\n    constexpr int SHIFT_AMOUNT_8 = 8;\n    constexpr int SHIFT_AMOUNT_16 = 16;\n    constexpr int SHIFT_AMOUNT_24 = 24;\n    int deviceId = 0;\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    int blockSize = 256;\n    int maxGridSize = (deviceProperties.maxThreadsPerMultiProcessor * deviceProperties.multiProcessorCount) / blockSize;\n    \n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Allocating host memory.\n    uint32_t * pixels_h = new uint32_t[MAX_NUM_PIXELS];\n    uint32_t * testPixels_h = new uint32_t[NUM_TESTS * MAX_NUM_PIXELS];\n    uint32_t * testExpectedPixels_h = new uint32_t[NUM_TESTS * MAX_NUM_PIXELS];\n    uint32_t * testNumPixels_h = new uint32_t[NUM_TESTS];\n    // Allocating device memory.\n    uint32_t * pixels_d;\n    CUDA_CHECK(cudaMallocAsync(&pixels_d, sizeof(uint32_t) * MAX_NUM_PIXELS, stream));\n    \n    int testIndex = 0;\n    // Test 1\n    {\n        testNumPixels_h[testIndex] = 10;\n        // Values are in 0xAARRGGBB format.\n        std::initializer_list<uint32_t> testPixels = { 0xFF00FF00, 0xFFFF00FF, 0x000000FF, 0x0000FF00, 0x00100010, 0xFFFFFFFF, 0x12345678, 0x87654321, 0x00000000, 0xABBAABBA }; \n        // Values are in 0xAABBGGRR format.\n        std::initializer_list<uint32_t> testExpectedPixels = { 0xFF00FF00, 0xFFFF00FF, 0x00FF0000, 0x0000ff00, 0x00100010, 0xFFFFFFFF, 0x12785634, 0x87214365, 0x00000000, 0xABBAABBA };\n        std::copy(testPixels.begin(), testPixels.end(), &testPixels_h[testIndex * MAX_NUM_PIXELS]);\n        std::copy(testExpectedPixels.begin(), testExpectedPixels.end(), &testExpectedPixels_h[testIndex * MAX_NUM_PIXELS]);\n        testIndex++;\n    }\n    // Test 2\n    {\n        testNumPixels_h[testIndex] = 5;\n        // Values are in 0xAARRGGBB format.\n        std::initializer_list<uint32_t> testPixels = { 0x00FF00FF, 0xFFFFFF00, 0xFF000000, 0x000000FF, 0x10001000 }; \n        // Values are in 0xAABBGGRR format.\n        std::initializer_list<uint32_t> testExpectedPixels = { 0x00FF00FF, 0xFF00FFFF, 0xFF000000, 0x00FF0000, 0x10001000 };\n        std::copy(testPixels.begin(), testPixels.end(), &testPixels_h[testIndex * MAX_NUM_PIXELS]);\n        std::copy(testExpectedPixels.begin(), testExpectedPixels.end(), &testExpectedPixels_h[testIndex * MAX_NUM_PIXELS]);\n        testIndex++;\n    }\n    // Test 3\n    {\n        int width = 256;\n        int height = 256;\n        testNumPixels_h[testIndex] = width * height;\n        for(int i = 0; i < testNumPixels_h[testIndex]; i++) {\n            testPixels_h[testIndex * MAX_NUM_PIXELS + i] = i;\n            testExpectedPixels_h[testIndex * MAX_NUM_PIXELS + i] = ((i >> SHIFT_AMOUNT_16) & BYTE_MASK) | (((i >> SHIFT_AMOUNT_8) & BYTE_MASK) << SHIFT_AMOUNT_8) | ((i & BYTE_MASK) << SHIFT_AMOUNT_16) | (((i >> SHIFT_AMOUNT_24) & BYTE_MASK) << SHIFT_AMOUNT_24);\n        }\n        testIndex++;\n    }\n    // Test 4\n    {\n        int width = 512;\n        int height = 512;\n        testNumPixels_h[testIndex] = width * height;\n        for(int i = 0; i < testNumPixels_h[testIndex]; i++) {\n            // Adjusting color channel values from 0 to 255 based on the index.\n            int alpha = 0xFF;\n            int red = (511 - (i % width)) / 2;\n            int green = 0xFF;\n            int blue = (511 - (i / width)) / 2;\n            uint32_t color = blue | (green << SHIFT_AMOUNT_8) | (red << SHIFT_AMOUNT_16) | (alpha << SHIFT_AMOUNT_24);\n            testPixels_h[testIndex * MAX_NUM_PIXELS + i] = color;\n            testExpectedPixels_h[testIndex * MAX_NUM_PIXELS + i] = ((color >> SHIFT_AMOUNT_16) & BYTE_MASK) | (((color >> SHIFT_AMOUNT_8) & BYTE_MASK) << SHIFT_AMOUNT_8) | ((color & BYTE_MASK) << SHIFT_AMOUNT_16) | (((color >> SHIFT_AMOUNT_24) & BYTE_MASK) << SHIFT_AMOUNT_24);\n        }\n        testIndex++;\n    }\n    // Test 5\n    {\n        int width = 1000;\n        int height = 1000;\n        testNumPixels_h[testIndex] = width * height;\n        for(int i = 0; i < testNumPixels_h[testIndex]; i++) {\n            // Adjusting color channel values from 0 to 255 based on the index.\n            int alpha = 0xFF;\n            int red = (511 - (i % width)) / 2;\n            int green = 0xFF;\n            int blue = (511 - (i / width)) / 2;\n            uint32_t color = blue | (green << SHIFT_AMOUNT_8) | (red << SHIFT_AMOUNT_16) | (alpha << SHIFT_AMOUNT_24);\n            testPixels_h[testIndex * MAX_NUM_PIXELS + i] = color;\n            testExpectedPixels_h[testIndex * MAX_NUM_PIXELS + i] = ((color >> SHIFT_AMOUNT_16) & BYTE_MASK) | (((color >> SHIFT_AMOUNT_8) & BYTE_MASK) << SHIFT_AMOUNT_8) | ((color & BYTE_MASK) << SHIFT_AMOUNT_16) | (((color >> SHIFT_AMOUNT_24) & BYTE_MASK) << SHIFT_AMOUNT_24);\n        }\n        testIndex++;\n    }\n    // Test 6\n    {\n        int width = 1000;\n        int height = 1000;\n        testNumPixels_h[testIndex] = width * height;\n        for(int i = 0; i < testNumPixels_h[testIndex]; i++) {\n            int alpha = 0xFF;\n            int red = abs(500 - (i % width)) / 2;\n            int green = 0xAA;\n            int blue = abs(500 - (i / width)) / 2;\n            uint32_t color = blue | (green << SHIFT_AMOUNT_8) | (red << SHIFT_AMOUNT_16) | (alpha << SHIFT_AMOUNT_24);\n            testPixels_h[testIndex * MAX_NUM_PIXELS + i] = color;\n            testExpectedPixels_h[testIndex * MAX_NUM_PIXELS + i] = ((color >> SHIFT_AMOUNT_16) & BYTE_MASK) | (((color >> SHIFT_AMOUNT_8) & BYTE_MASK) << SHIFT_AMOUNT_8) | ((color & BYTE_MASK) << SHIFT_AMOUNT_16) | (((color >> SHIFT_AMOUNT_24) & BYTE_MASK) << SHIFT_AMOUNT_24);\n        }\n        testIndex++;\n    }\n    // Test 7\n    {\n        int width = 1000;\n        int height = 1000;\n        testNumPixels_h[testIndex] = width * height;\n        for(int i = 0; i < testNumPixels_h[testIndex]; i++) {\n            int alpha = i % 256;\n            int red = i % 255;\n            int green = i % 254;\n            int blue = i % 253;\n            uint32_t color = blue | (green << SHIFT_AMOUNT_8) | (red << SHIFT_AMOUNT_16) | (alpha << SHIFT_AMOUNT_24);\n            testPixels_h[testIndex * MAX_NUM_PIXELS + i] = color;\n            testExpectedPixels_h[testIndex * MAX_NUM_PIXELS + i] = ((color >> SHIFT_AMOUNT_16) & BYTE_MASK) | (((color >> SHIFT_AMOUNT_8) & BYTE_MASK) << SHIFT_AMOUNT_8) | ((color & BYTE_MASK) << SHIFT_AMOUNT_16) | (((color >> SHIFT_AMOUNT_24) & BYTE_MASK) << SHIFT_AMOUNT_24);\n        }\n        testIndex++;\n    }\n    // Conducting iterative tests.\n    for(int test = 0; test < testIndex; test++)\n    {\n        int numPixels = testNumPixels_h[test];\n        for(int i = 0; i < numPixels; i++) {\n            pixels_h[i] = testPixels_h[test * MAX_NUM_PIXELS + i];\n        }\n        CUDA_CHECK(cudaMemcpyAsync(pixels_d, pixels_h, sizeof(uint32_t) * numPixels, cudaMemcpyHostToDevice, stream));\n        void * args[2] = { &pixels_d, &numPixels };\n        int requiredGridSize = (numPixels + blockSize - 1) / blockSize;\n        int usedGridSize = requiredGridSize < maxGridSize ? requiredGridSize : maxGridSize;\n        // Grid: (usedGridSize, 1, 1)\n        // Block: (blockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_permuteColorChannels, dim3(usedGridSize, 1, 1), dim3(blockSize, 1, 1), args, 0, stream));\n        CUDA_CHECK(cudaMemcpyAsync(pixels_h, pixels_d, sizeof(uint32_t) * numPixels, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for(int i = 0; i < numPixels; i++) {\n            assert(pixels_h[i] == testExpectedPixels_h[test * MAX_NUM_PIXELS + i]);\n        }\n    }\n    // Deallocating device memory.\n    CUDA_CHECK(cudaFreeAsync(pixels_d, stream));\n    // Deallocating host memory.\n    delete [] pixels_h;\n    delete [] testPixels_h;\n    delete [] testExpectedPixels_h;\n    delete [] testNumPixels_h;\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_permuteColorChannels(uint32_t * pixels_d, int numPixels) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/215", "date": "2025-07-30", "prompt": "Write a function using CUDA streams and events to run multiple kernels concurrently for the Least-Squares Method. Utilize two streams and two events. In the first step, all required device-side zero-initializations should be performed on separate streams while also launching k_computeAverageObservedValue on the first stream and k_computeAveragePredictedValue on the second stream. In the second step, both streams should wait for each other using events, with asynchronous waiting to the host, before launching k_summationOfProducts on the first stream and k_summationOfSquares on the second stream. In the third step, only the first stream should wait for the other stream before launching k_leastSquaresMethodResult, all asynchronously to the host.\n\nThe signature of the function is void run(int numElements, int maxInputSize, cudaStream_t* stream, cudaEvent_t firstStreamStop, cudaEvent_t secondStreamStop, int* observedValues_d, int* predictedValues_d, int* interimBufferObservedVal_d, int* interimBufferPredictedVal_d, float* averageObservedValue_d, float* averagePredictedValue_d, float* summationOfProducts_d, float* summationOfSquares_d, float* interimBufferSumOfProducts_d, float* interimBufferSumOfSquares_d, float* outputSlope_d, float* outputIntercept_d), where numElements is the number of elements of input, maxInputSize is the maximum number of elements of any input given, stream is a pointer to the array of CUDA streams, firstStreamStop is a CUDA event to be recorded in stream 1, secondStreamStop is a CUDA event to be recorded in stream 2, observedValues_d is a pointer to integer array for observed values, predictedValues_d is a pointer to integer array for predicted values, interimBufferObservedVal_d is a pointer to integer array as a temporary storage, interimBufferPredictedVal_d is a pointer to integer array as a temporary storage, averageObservedValue_d is a pointer to float for average of observed values, averagePredictedValue_d is a pointer to float for average of predicted values, summationOfProducts_d is a pointer to float for sum of products, summationOfSquares_d is a pointer to float for sum of squares, interimBufferSumOfProducts_d is a pointer to float array as a temporary storage, interimBufferSumOfSquares_d is a pointer to float array as a temporary storage, outputSlope_d is a pointer to float array for slope values, and outputIntercept_d is a pointer to float array for intercept values.\n\nThe signatures of the CUDA kernels are:\n__global__ void k_computeAverageObservedValue(const int* observedValues_d, const int* predictedValues_d, float* averageObservedValue_d, int* interimBufferObservedVal_d, int inputSize), where observedValues_d is a pointer to an array of integers, predictedValues_d is a pointer to an array of integers, averageObservedValue_d is a pointer to a float, interimBufferObservedVal_d is a pointer to an array of integers, inputSize is an integer. k_computeAverageObservedValue calculates the average of observed values.\n__global__ void k_computeAveragePredictedValue(const int* observedValues_d, const int* predictedValues_d, float* averagePredictedValue_d, int* interimBufferPredictedVal_d, int inputSize), where observedValues_d is a pointer to an array of integers, predictedValues_d is a pointer to an array of integers, averagePredictedValue_d is a pointer to a float, interimBufferPredictedVal_d is a pointer to an array of integers, inputSize is an integer. k_computeAveragePredictedValue calculates the average of predicted values.\n__global__ void k_summationOfProducts(const int* observedValues_d, const int* predictedValues_d, float* averageObservedValue_d, float* averagePredictedValue_d, float* summationOfProducts_d, float *interimBufferSumOfProducts_d, int inputSize), where observedValues_d is a pointer to an array of integers, predictedValues_d is a pointer to an array of integers, averageObservedValue_d is a pointer to a float, averagePredictedValue_d is a pointer to a float, summationOfProducts_d is a pointer to float, interimBufferSumOfProducts_d is a pointer to an array of floats, inputSize is an integer. k_summationOfProducts calculates the sum of products.\n__global__ void k_summationOfSquares(const int* observedValues_d, float* averageObservedValue_d, float* summationOfSquares_d, float* interimBufferSumOfSquares_d, int inputSize), where observedValues_d is a pointer to an array of integers, averageObservedValue_d is a pointer to a float, summationOfSquares_d is a pointer to float, interimBufferSumOfSquares_d is a pointer to an array of floats, inputSize is an integer. k_summationOfSquares calculates the sum of squares.\n__global__ void k_leastSquaresMethodResult(float* averageObservedValue_d, float* averagePredictedValue_d, float* summationOfProducts_d, float* summationOfSquares_d, float* outputSlope_d, float* outputIntercept_d), where averageObservedValue_d is a pointer to a float, averagePredictedValue_d is a pointer to a float, summationOfProducts_d is a pointer to a float, summationOfSquares_d is a pointer to float, outputSlope_d is a pointer to float, outputIntercept_d is a pointer to float. k_leastSquaresMethodResult calculates the slope and intercept for the line fit y = slope * x + intercept.\nDo not generate the kernel bodies for the kernels above.\n\n>>> run(9, 64, stream, firstStreamStop, secondStreamStop, { 1, 8, 9, 6, 4, 3, 5, 7, 2 }, { 8, 1, 7, 6, 2, 4, 5, 9, 3 }, interimBufferObservedVal_d, interimBufferPredictedVal_d, averageObservedValue_d, averagePredictedValue_d, summationOfProducts_d, summationOfSquares_d, interimBufferSumOfProducts_d, interimBufferSumOfSquares_d, outputSlope_d, outputIntercept_d) -> outputSlope_d: { 0.07f }, outputIntercept_d: { 4.67f }\n>>> run(12, 64, stream, firstStreamStop, secondStreamStop, { 1, 2, 5, 15, 18, 16, 3, 14, 11, 19, 12, 13 }, { 8, 2, 11, 1, 16, 5, 14, 9, 4, 3, 10, 7 }, interimBufferObservedVal_d, interimBufferPredictedVal_d, averageObservedValue_d, averagePredictedValue_d, summationOfProducts_d, summationOfSquares_d, interimBufferSumOfProducts_d, interimBufferSumOfSquares_d, outputSlope_d, outputIntercept_d) -> outputSlope_d: { -0.08f }, outputIntercept_d: { 8.40f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <cstdio>\n#include <limits.h>\n#include <algorithm>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\nusing namespace cooperative_groups;\n\nconstexpr int SET_TO_ZERO = 0;\n// Error tolerance concerning the results.\nconstexpr float EPSILON = 1e-3f;\n\n#define CUDA_CHECK(call)                                        \\\ndo {                                                            \\\n        cudaError_t error = call;                               \\\n        if (error != cudaSuccess) {                             \\\n            fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",       \\\n                    __FILE__, __LINE__,                         \\\n                    cudaGetErrorString(error));                 \\\n            exit(EXIT_FAILURE);                                 \\\n        }                                                       \\\n} while(0)\nconst int FIRST_STREAM_INDEX = 0;\nconst int SECOND_STREAM_INDEX = 1;\nconst int OUTPUT_SIZE = 1;\n// Topology of the dependencies:\n// mem1 = averageObservedValue_d, summationOfProducts_d, outputSlope_d, interimBufferObservedVal_d, interimBufferSumOfProducts_d\n// mem2 = averagePredictedValue_d, summationOfSquares_d, outputIntercept_d, interimBufferPredictedVal_d, interimBufferSumOfSquares_d\n// k1 = k_computeAverageObservedValue kernel\n// k2 = k_computeAveragePredictedValue kernel\n// k3 = k_summationOfProducts kernel\n// k4 = k_summationOfSquares kernel\n// k5 = k_leastSquaresMethodResult kernel\n// e1 and e2 are events\n// Stream 1: [Memset for mem1] --> [Launch k1] ----------------------------> [Record e1]\n// Stream 2: [Memset for mem2] --> [Launch k2] --> [Record e2]                  |\n//                                                 |                            |\n//                                                 v                            |\n// Stream 1:                                     [Wait E2] --> [Launch k3]      |\n// Stream 2:                                                                  [Wait E1] --> [Launch k4] --> [Record e2]\n//                                                                                                           |\n//                                                                                                           v\n// Stream 1:                                                                                                [Wait e2] --> [Launch k5]\nvoid run(int numElements, int maxInputSize, \n         cudaStream_t* stream, cudaEvent_t firstStreamStop, cudaEvent_t secondStreamStop, \n         int* observedValues_d, int* predictedValues_d, \n         int* interimBufferObservedVal_d, int* interimBufferPredictedVal_d, float* averageObservedValue_d, \n         float* averagePredictedValue_d, float* summationOfProducts_d, float* summationOfSquares_d, \n         float* interimBufferSumOfProducts_d, float* interimBufferSumOfSquares_d, float* outputSlope_d, \n         float* outputIntercept_d);\n\n\n__global__ void k_computeAverageObservedValue(const int* observedValues_d, const int* predictedValues_d, float* averageObservedValue_d, int* interimBufferObservedVal_d, int inputSize) {\n    //Compute index\n    int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    int gridStride = blockDim.x * gridDim.x;\n    cooperative_groups::grid_group grid = cooperative_groups::this_grid();\n    \n    //Declare shared memory, size equals blockDim.x \n    extern __shared__ int s_blockSummation[];\n    \n    //Grid Stride\n    int summation = 0;\n    for(int threadIndex = threadId; threadIndex < inputSize; threadIndex += gridStride) {\n        summation += observedValues_d[threadIndex];\n    }\n    \n    s_blockSummation[threadIdx.x] = summation;\n    \n    __syncthreads();\n    \n    //Block Reduction\n    for(int offset = (blockDim.x >> 1); offset > 0; offset = (offset >> 1)) {\n        if(threadIdx.x < offset) {\n            s_blockSummation[threadIdx.x] += s_blockSummation[threadIdx.x + offset];\n        }\n        __syncthreads();\n    }\n    \n    if(threadIdx.x == 0) {\n        interimBufferObservedVal_d[blockIdx.x] = s_blockSummation[threadIdx.x];\n    }\n    \n    //Grid Synchronization\n    grid.sync();\n    \n    //Grid Reduction\n    for(int offset = (gridDim.x >> 1); offset > 0; offset = (offset >> 1)) {\n        if(threadIdx.x == 0) {\n            if(blockIdx.x < offset) {\n                interimBufferObservedVal_d[blockIdx.x] += interimBufferObservedVal_d[blockIdx.x + offset];\n            }\n        }\n        grid.sync();\n    }\n    \n    //Compute Final Value\n    if(threadId == 0) {\n        averageObservedValue_d[threadId] = (float)interimBufferObservedVal_d[threadId] / inputSize;\n    }\n}\n\n\n__global__ void k_computeAveragePredictedValue(const int* observedValues_d, const int* predictedValues_d, float* averagePredictedValue_d, int* interimBufferPredictedVal_d, int inputSize) {\n    //Compute index\n    int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    int gridStride = blockDim.x * gridDim.x;\n    cooperative_groups::grid_group grid = cooperative_groups::this_grid();\n    \n    //Declare shared memory, size equals blockDim.x \n    extern __shared__ int s_blockSummation[];\n    \n    \n    //Grid Stride\n    int summation = 0;\n    for(int threadIndex = threadId; threadIndex < inputSize; threadIndex += gridStride) {\n        summation += predictedValues_d[threadIndex];\n    }\n    \n    s_blockSummation[threadIdx.x] = summation;\n    \n    __syncthreads();\n    \n    //Block Reduction\n    for(int offset = (blockDim.x >> 1); offset > 0; offset = (offset >> 1)) {\n        if(threadIdx.x < offset) {\n            s_blockSummation[threadIdx.x] += s_blockSummation[threadIdx.x + offset];\n        }\n        __syncthreads();\n    }\n    \n    if(threadIdx.x == 0) {\n        interimBufferPredictedVal_d[blockIdx.x] = s_blockSummation[threadIdx.x];\n    }\n    \n    //Grid Synchronization\n    grid.sync();\n    \n    //Grid Reduction\n    for(int offset = (gridDim.x >> 1); offset > 0; offset = (offset >> 1)) {\n        if(threadIdx.x == 0) {\n            if(blockIdx.x < offset) {\n                interimBufferPredictedVal_d[blockIdx.x] += interimBufferPredictedVal_d[blockIdx.x + offset];\n            }\n        }\n        grid.sync();\n    }\n    \n    //Compute Final Value\n    if(threadId == 0) {\n        averagePredictedValue_d[threadId] = (float)interimBufferPredictedVal_d[threadId] / inputSize;\n    }\n}\n\n__global__ void k_summationOfProducts(const int* observedValues_d, const int* predictedValues_d, float* averageObservedValue_d, float* averagePredictedValue_d, float* summationOfProducts_d, float* interimBufferSumOfProducts_d, int inputSize) {    \n    //Compute index\n    int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    int gridStride = blockDim.x * gridDim.x;\n    cooperative_groups::grid_group grid = cooperative_groups::this_grid();\n    \n    //Declare shared memory, size equals blockDim.x \n    extern __shared__ float s_blockSumOfProducts[];\n    \n    //Grid Stride\n    float sumProducts = 0;\n    for(int threadIndex = threadId; threadIndex < inputSize; threadIndex += gridStride) {\n        float avgObservedVal = observedValues_d[threadIndex] - averageObservedValue_d[0];\n        float avgPredictedVal = predictedValues_d[threadIndex] - averagePredictedValue_d[0];\n        sumProducts += (avgObservedVal * avgPredictedVal);\n    }\n    \n    s_blockSumOfProducts[threadIdx.x] = sumProducts;\n    \n    __syncthreads();\n    \n    //Block Reduction\n    for(int offset = (blockDim.x >> 1); offset > 0; offset = (offset >> 1)) {\n        if(threadIdx.x < offset) {\n            s_blockSumOfProducts[threadIdx.x] += s_blockSumOfProducts[threadIdx.x + offset];\n        }\n        __syncthreads();\n    }\n    \n    if(threadIdx.x == 0) {\n        interimBufferSumOfProducts_d[blockIdx.x] = s_blockSumOfProducts[threadIdx.x];\n    }\n    \n    //Grid Synchronization\n    grid.sync();\n    \n    //Grid Reduction\n    for(int offset = (gridDim.x >> 1); offset > 0; offset = (offset >> 1)) {\n        if(threadIdx.x == 0) {\n            if(blockIdx.x < offset) {\n                interimBufferSumOfProducts_d[blockIdx.x] += interimBufferSumOfProducts_d[blockIdx.x + offset];\n            }\n        }\n        grid.sync();\n    }\n    \n    if(threadId == 0) {\n        summationOfProducts_d[threadId] = interimBufferSumOfProducts_d[threadId];\n    }\n}\n\n__global__ void k_summationOfSquares(const int* observedValues_d, float* averageObservedValue_d, float* summationOfSquares_d, float* interimBufferSumOfSquares_d, int inputSize) {\n    //Compute index\n    int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    int gridStride = blockDim.x * gridDim.x;\n    cooperative_groups::grid_group grid = cooperative_groups::this_grid();\n    \n    //Declare shared memory, size equals blockDim.x \n    extern __shared__ float s_blockSumOfSquares[];\n    \n    //Grid Stride\n    float sumSquares = 0;\n    for(int threadIndex = threadId; threadIndex < inputSize; threadIndex += gridStride) {\n        float avgObservedVal = observedValues_d[threadIndex] - averageObservedValue_d[0];\n        sumSquares += (avgObservedVal * avgObservedVal);\n    }\n    \n    s_blockSumOfSquares[threadIdx.x] = sumSquares;\n    __syncthreads();\n    \n    //Block Reduction\n    for(int offset = (blockDim.x >> 1); offset > 0; offset = (offset >> 1)) {\n        if(threadIdx.x < offset) {\n            s_blockSumOfSquares[threadIdx.x] += s_blockSumOfSquares[threadIdx.x + offset];\n        }\n        __syncthreads();\n    }\n    \n    if(threadIdx.x == 0) {\n        interimBufferSumOfSquares_d[blockIdx.x] = s_blockSumOfSquares[threadIdx.x];\n    }\n    \n    //Grid Synchronization\n    grid.sync();\n    \n    //Grid Reduction\n    for(int offset = (gridDim.x >> 1); offset > 0; offset = (offset >> 1)) {\n        if(threadIdx.x == 0) {\n            if(blockIdx.x < offset) {\n                interimBufferSumOfSquares_d[blockIdx.x] += interimBufferSumOfSquares_d[blockIdx.x + offset];\n            }\n        }\n        grid.sync();\n    }\n    \n    if(threadId == 0) {\n        summationOfSquares_d[threadId] = interimBufferSumOfSquares_d[threadId];\n    }\n}\n\n__global__ void k_leastSquaresMethodResult(float* averageObservedValue_d, float* averagePredictedValue_d, float* summationOfProducts_d, float* summationOfSquares_d, float* outputSlope_d, float* outputIntercept_d) {\n    //Compute index\n    int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if(threadId == 0) {\n        if(summationOfSquares_d[threadId] == 0) {\n            // Slope is undefined; here, NaN is used to represent an undefined float value.\n            outputSlope_d[threadId] = nanf(\"NaN\");\n            // As the slope is undefined, the output intercept shall be a constant value representing the x-intercept, which is the average observed value.\n            outputIntercept_d[threadId] = averageObservedValue_d[threadId]; \n        } else {\n            outputSlope_d[threadId] = summationOfProducts_d[threadId] / summationOfSquares_d[threadId];\n            outputIntercept_d[threadId] = (averagePredictedValue_d[threadId] - (outputSlope_d[threadId] * averageObservedValue_d[threadId]));\n        }\n    }\n}\n\nvoid launch() {\n    // Initialize constants.\n    constexpr int TEST_CASE_COUNT = 7;\n    constexpr int MIN_ARRAY_SIZE = 1;\n    constexpr int OUTPUT_INDEX = 0;\n    \n    // Initialize test data.\n    // Test data dimensions.\n    constexpr int inputArraySize[TEST_CASE_COUNT] = { 9, 12, 16, 24, 32, 48, 64 };\n    // Determine maximum input size.\n    int maxInputSize = 0;\n    for(int i = 0; i < TEST_CASE_COUNT; i++) {\n        if(maxInputSize < inputArraySize[i]) {\n            maxInputSize = inputArraySize[i];\n        }\n    }\n    // Input data for test.\n    int * inputObservedValues_h = new int[TEST_CASE_COUNT * maxInputSize];\n    int * inputPredictedValues_h = new int[TEST_CASE_COUNT * maxInputSize];\n    int testIndex = 0;\n    // Test 1\n    {\n        std::initializer_list<int> initObservedValue = { 1, 8, 9, 6, 4, 3, 5, 7, 2 };\n        std::initializer_list<int> initPredictedValue = { 8, 1, 7, 6, 2, 4, 5, 9, 3 };\n        std::copy(initObservedValue.begin(), initObservedValue.end(), &inputObservedValues_h[maxInputSize * testIndex]);\n        std::copy(initPredictedValue.begin(), initPredictedValue.end(), &inputPredictedValues_h[maxInputSize * testIndex]);\n        testIndex++;\n    }\n    // Test 2\n    {\n        std::initializer_list<int> initObservedValue = { 1, 2, 5, 15, 18, 16, 3, 14, 11, 19, 12, 13 };\n        std::initializer_list<int> initPredictedValue = { 8, 2, 11, 1, 16, 5, 14, 9, 4, 3, 10, 7 };\n        std::copy(initObservedValue.begin(), initObservedValue.end(), &inputObservedValues_h[maxInputSize * testIndex]);\n        std::copy(initPredictedValue.begin(), initPredictedValue.end(), &inputPredictedValues_h[maxInputSize * testIndex]);\n        testIndex++;\n    }\n    // Test 3\n    {\n        std::initializer_list<int> initObservedValue = { 8, 3, 29, 27, 25, 7, 21, 15, 5, 19, 20, 28, 12, 16, 23, 24 };\n        std::initializer_list<int> initPredictedValue = { 16, 9, 25, 27, 12, 8, 10, 5, 4, 23, 3, 13, 18, 21, 24, 29 };\n        std::copy(initObservedValue.begin(), initObservedValue.end(), &inputObservedValues_h[maxInputSize * testIndex]);\n        std::copy(initPredictedValue.begin(), initPredictedValue.end(), &inputPredictedValues_h[maxInputSize * testIndex]);\n        testIndex++;\n    }\n    // Test 4\n    {\n        std::initializer_list<int> initObservedValue = { 31, 12, 4, 29, 20, 22, 7, 9, 27, 1, 26, 16, 8, 2, 3, 14, 18, 5, 19, 15, 23, 13, 30, 11 };\n        std::initializer_list<int> initPredictedValue = { 8, 30, 21, 17, 11, 2, 27, 5, 18, 7, 28, 19, 25, 12, 14, 26, 4, 3, 6, 15, 24, 23, 29, 1 };\n        std::copy(initObservedValue.begin(), initObservedValue.end(), &inputObservedValues_h[maxInputSize * testIndex]);\n        std::copy(initPredictedValue.begin(), initPredictedValue.end(), &inputPredictedValues_h[maxInputSize * testIndex]);\n        testIndex++;\n    }\n    // Test 5\n    {\n        std::initializer_list<int> initObservedValue = { 25, 7, 33, 12, 20, 32, 18, 22, 24, 19, 11, 36, 38, 31, 30, 17, 1, 14, 3, 16, 13, 34, 5, 21, 8, 4, 35, 27, 28, 37, 26, 9 };\n        std::initializer_list<int> initPredictedValue = { 23, 18, 5, 16, 17, 33, 38, 9, 2, 1, 7, 26, 37, 24, 39, 31, 36, 27, 34, 12, 32, 25, 15, 10, 30, 21, 8, 22, 28, 35, 4, 29 };\n        std::copy(initObservedValue.begin(), initObservedValue.end(), &inputObservedValues_h[maxInputSize * testIndex]);\n        std::copy(initPredictedValue.begin(), initPredictedValue.end(), &inputPredictedValues_h[maxInputSize * testIndex]);\n        testIndex++;\n    }\n    // Test 6\n    {\n        std::initializer_list<int> initObservedValue = { 13, 5, 39, 51, 56, 16, 52, 20, 36, 25, 63, 10, 57, 41, 26, 33, 45, 32, 53, 4, 27, 40, 2, 21, 37, 34, 44, 17, 38, 12, 22, 55, 62, 1, 15, 42, 19, 54, 9, 60, 64, 7, 61, 49, 31, 58, 11, 28 };\n        std::initializer_list<int> initPredictedValue = { 37, 30, 46, 14, 43, 61, 4, 42, 54, 24, 27, 1, 62, 2, 3, 64, 10, 52, 38, 57, 51, 33, 17, 28, 18, 49, 40, 41, 44, 22, 36, 9, 48, 26, 45, 6, 29, 5, 47, 20, 34, 32, 25, 58, 8, 60, 15, 12 };\n        std::copy(initObservedValue.begin(), initObservedValue.end(), &inputObservedValues_h[maxInputSize * testIndex]);\n        std::copy(initPredictedValue.begin(), initPredictedValue.end(), &inputPredictedValues_h[maxInputSize * testIndex]);\n        testIndex++;\n    }\n    // Test 7\n    {\n        std::initializer_list<int> initObservedValue = { 47, 77, 59, 16, 35, 11, 17, 29, 26, 20, 70, 2, 48, 30, 32, 6, 38, 24, 18, 63, 79, 41, 40, 1, 64, 22, 34, 53, 72, 46, 15, 76, 55, 13, 12, 37, 14, 73, 71, 7, 23, 8, 67, 27, 4, 50, 61, 78, 10, 31, 69, 52, 65, 74, 25, 60, 57, 66, 75, 33, 28, 56, 3, 62 };\n        std::initializer_list<int> initPredictedValue = { 5, 50, 24, 37, 43, 30, 77, 78, 57, 73, 31, 36, 34, 2, 35, 76, 47, 22, 6, 48, 74, 70, 67, 56, 54, 7, 27, 51, 39, 4, 55, 19, 49, 68, 38, 41, 59, 8, 45, 69, 16, 66, 42, 75, 40, 18, 15, 14, 32, 25, 63, 72, 53, 62, 26, 10, 21, 33, 17, 13, 3, 20, 28, 29 };\n        std::copy(initObservedValue.begin(), initObservedValue.end(), &inputObservedValues_h[maxInputSize * testIndex]);\n        std::copy(initPredictedValue.begin(), initPredictedValue.end(), &inputPredictedValues_h[maxInputSize * testIndex]);\n        testIndex++;\n    }\n    \n    // Expected output for test.\n    float expectedOutputSlope_h[TEST_CASE_COUNT] = {0.066, -0.083, 0.531, 0.145, -0.016, -0.010, -0.148};\n    float expectedOutputIntercept_h[TEST_CASE_COUNT] = {4.666, 8.399, 6.078, 13.413, 22.024,  32.209, 45.089};\n    \n    // Output of the device on the host.\n    float outputSlope_h[MIN_ARRAY_SIZE] = {};\n    float outputIntercept_h[MIN_ARRAY_SIZE] = {};\n    \n    \n    // Use multiple CUDA streams for concurrent execution.\n    const int NUMBER_OF_STREAMS = 2;\n    cudaStream_t stream[NUMBER_OF_STREAMS];\n    for(int streamId = 0; streamId < NUMBER_OF_STREAMS; streamId++) {\n        CUDA_CHECK(cudaStreamCreate(&stream[streamId]));\n    }\n    cudaEvent_t firstStreamStop;\n    cudaEvent_t secondStreamStop;\n    CUDA_CHECK(cudaEventCreate(&firstStreamStop));\n    CUDA_CHECK(cudaEventCreate(&secondStreamStop));\n    \n    // Allocate device memory.\n    int* observedValues_d;\n    int* predictedValues_d;\n    int* interimBufferObservedVal_d;\n    int* interimBufferPredictedVal_d;\n    float* averageObservedValue_d;\n    float* averagePredictedValue_d;\n    float* summationOfProducts_d;\n    float* summationOfSquares_d;\n    float* interimBufferSumOfProducts_d;\n    float* interimBufferSumOfSquares_d;\n    float* outputSlope_d;\n    float* outputIntercept_d;\n    \n    // Asynchronous allocations.\n    CUDA_CHECK(cudaMallocAsync((void**)&observedValues_d, maxInputSize * sizeof(int), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&predictedValues_d, maxInputSize * sizeof(int), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&interimBufferObservedVal_d, maxInputSize * sizeof(int), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&interimBufferPredictedVal_d, maxInputSize * sizeof(int), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&averageObservedValue_d, OUTPUT_SIZE * sizeof(float), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&averagePredictedValue_d, OUTPUT_SIZE * sizeof(float), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&summationOfProducts_d, OUTPUT_SIZE * sizeof(float), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&summationOfSquares_d, OUTPUT_SIZE * sizeof(float), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&interimBufferSumOfProducts_d, maxInputSize * sizeof(float), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&interimBufferSumOfSquares_d, maxInputSize * sizeof(float), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&outputSlope_d, OUTPUT_SIZE * sizeof(float), stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaMallocAsync((void**)&outputIntercept_d, OUTPUT_SIZE * sizeof(float), stream[FIRST_STREAM_INDEX]));\n    \n    // Execute test cases.\n    for (int testCase = 0; testCase < TEST_CASE_COUNT; testCase++) {   \n        int numElements = inputArraySize[testCase];\n        \n        // Copy input data from host to device.\n        CUDA_CHECK(cudaMemcpyAsync(observedValues_d, &inputObservedValues_h[testCase * maxInputSize], numElements * sizeof(int), cudaMemcpyHostToDevice, stream[FIRST_STREAM_INDEX]));\n        CUDA_CHECK(cudaMemcpyAsync(predictedValues_d, &inputPredictedValues_h[testCase * maxInputSize], numElements * sizeof(int), cudaMemcpyHostToDevice, stream[FIRST_STREAM_INDEX]));\n        // Executing the algorithm using CUDA streams and events.\n        run(numElements, maxInputSize, stream, firstStreamStop, secondStreamStop, observedValues_d, predictedValues_d, interimBufferObservedVal_d, interimBufferPredictedVal_d, averageObservedValue_d, averagePredictedValue_d, summationOfProducts_d, summationOfSquares_d, interimBufferSumOfProducts_d, interimBufferSumOfSquares_d, outputSlope_d, outputIntercept_d);\n        // Copy output data from device to host.\n        CUDA_CHECK(cudaMemcpyAsync(outputSlope_h, outputSlope_d, OUTPUT_SIZE * sizeof(float), cudaMemcpyDeviceToHost, stream[FIRST_STREAM_INDEX]));\n        CUDA_CHECK(cudaMemcpyAsync(outputIntercept_h, outputIntercept_d, OUTPUT_SIZE * sizeof(float), cudaMemcpyDeviceToHost, stream[FIRST_STREAM_INDEX]));\n        CUDA_CHECK(cudaStreamSynchronize(stream[FIRST_STREAM_INDEX]));\n        \n        // Assertion regarding the device's output in relation to the anticipated output.\n        assert(fabs(outputIntercept_h[OUTPUT_INDEX] - expectedOutputIntercept_h[testCase]) <= EPSILON);\n        // For OutputSlope, an additional check of NaN comparison is required.\n        // Since the OutputSlope might be undefined for some inputs.\n        isnan(expectedOutputSlope_h[OUTPUT_INDEX]) ? assert(isnan(outputSlope_h[OUTPUT_INDEX])) : assert(fabs(outputSlope_h[OUTPUT_INDEX] - expectedOutputSlope_h[testCase]) <= EPSILON);\n    }\n    delete [] inputObservedValues_h;\n    delete [] inputPredictedValues_h;\n    \n    // Deallocate device memory.\n    CUDA_CHECK(cudaFreeAsync(observedValues_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(predictedValues_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(interimBufferObservedVal_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(interimBufferPredictedVal_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(averageObservedValue_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(averagePredictedValue_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(summationOfProducts_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(interimBufferSumOfProducts_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(interimBufferSumOfSquares_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(summationOfSquares_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(outputSlope_d, stream[FIRST_STREAM_INDEX]));\n    CUDA_CHECK(cudaFreeAsync(outputIntercept_d, stream[FIRST_STREAM_INDEX]));\n    \n    // Destroy all events.\n    CUDA_CHECK(cudaEventDestroy(firstStreamStop));\n    CUDA_CHECK(cudaEventDestroy(secondStreamStop));\n    \n    // Destroy all streams.\n    for(int streamId = 0; streamId < NUMBER_OF_STREAMS; streamId++) {\n        CUDA_CHECK(cudaStreamSynchronize(stream[streamId]));\n        CUDA_CHECK(cudaStreamDestroy(stream[streamId]));\n    }\n}\n\nvoid run(int numElements, int maxInputSize, \n         cudaStream_t* stream, cudaEvent_t firstStreamStop, cudaEvent_t secondStreamStop, \n         int* observedValues_d, int* predictedValues_d, \n         int* interimBufferObservedVal_d, int* interimBufferPredictedVal_d, float* averageObservedValue_d, \n         float* averagePredictedValue_d, float* summationOfProducts_d, float* summationOfSquares_d, \n         float* interimBufferSumOfProducts_d, float* interimBufferSumOfSquares_d, float* outputSlope_d, \n         float* outputIntercept_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/216", "date": "2025-07-30", "prompt": "Write a CUDA kernel to implement a radix sort of 32-bit integers using a 4-bit radix. Use CUB library primitives and cooperative groups for multi-block processing. The process from LSB to MSB should proceed through five phases, namely histogram computation, global prefix sum, offset distribution, local ranking, and element scattering. Utilize CUB BlockLoad with warp transpose, BlockScan for prefix sums, and shared memory unions.\n\nThe signature of kernel is __global__ void k_radixSort(unsigned int* keysIn_d, unsigned int* keysOut_d, unsigned int* globalHistograms_d, int numElements), where keysIn_d is a pointer to the input array, keysOut_d is a pointer to the output buffer, globalHistograms_d is a pointer to workspace for inter-block histogram communication, and numElements is the array size. The kernel performs in-place sorting through buffer swapping across multiple radix passes.\n\n>>> k_radixSort({ 170, 45, 75, 90, 2, 802, 24, 66 }, keysOut_d, globalHistograms_d, 8) -> keysOut_d: { 2, 24, 45, 66, 75, 90, 170, 802 }\n>>> k_radixSort({ 128, 64, 256, 32, 1024, 16 }, keysOut_d, globalHistograms_d, 6) -> keysOut_d: { 16, 32, 64, 128, 256, 1024 }\n", "cc_flags": "-arch=sm_80 --expt-relaxed-constexpr", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <algorithm>\n#include <cassert>\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n\n#include <cooperative_groups.h>\n#include <cub/block/block_load.cuh>\n#include <cub/block/block_scan.cuh>\n#include <cub/block/block_store.cuh>\n#include <cub/cub.cuh>\n#include <cuda_runtime.h>\n\n// Core configuration constants (shared across host and device)\n#define BLOCK_SIZE 256\n#define ITEMS_PER_THREAD 4\n#define RADIX_BITS 4\n#define RADIX_SIZE (1 << RADIX_BITS)\n#define RADIX_MASK (RADIX_SIZE - 1)\n#define RADIX_PASSES (32 / RADIX_BITS)\n\n#define CUDA_CHECK(call)                                                                           \\\n    do {                                                                                           \\\n        cudaError_t error = call;                                                                  \\\n        if(error != cudaSuccess) {                                                                 \\\n            fprintf(stderr,                                                                        \\\n                    \"CUDA error at %s:%d - %s\\n\",                                                  \\\n                    __FILE__,                                                                      \\\n                    __LINE__,                                                                      \\\n                    cudaGetErrorString(error));                                                    \\\n            exit(EXIT_FAILURE);                                                                    \\\n        }                                                                                          \\\n    } while(0)\n\nnamespace cg = cooperative_groups;\n\n__global__ void k_radixSort(unsigned int *keysIn_d,unsigned int *keysOut_d, unsigned int *globalHistograms_d, int numElements);\n\nvoid launch() {\n    // Host-only constants\n    constexpr int MAX_GRID_SIZE = 65535;\n    constexpr unsigned int RANDOM_SEED = 12345;\n    constexpr unsigned int MAX_RANDOM_VALUE = 1000000;\n\n    // Check CUDA device\n    int deviceCount;\n    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n    assert(deviceCount > 0);\n\n    cudaDeviceProp deviceProp;\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, 0));\n    assert(deviceProp.cooperativeLaunch);\n\n    // Create CUDA stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Test different sizes\n    const int TEST_SIZES[] = {64, 256, 1024, 4096, 16384, 65536, 131072};\n    const int NUM_TESTS = sizeof(TEST_SIZES) / sizeof(TEST_SIZES[0]);\n\n    // Find maximum array size for memory allocation\n    int maxNumElements = 0;\n    for(int i = 0; i < NUM_TESTS; i++) {\n        if(TEST_SIZES[i] > maxNumElements) {\n            maxNumElements = TEST_SIZES[i];\n        }\n    }\n    const size_t maxBytes = static_cast<size_t>(maxNumElements) * sizeof(unsigned int);\n\n    // Allocate host memory once for maximum size\n    unsigned int *keys_h = static_cast<unsigned int*>(malloc(maxBytes));\n    unsigned int *cudaResult_h = static_cast<unsigned int*>(malloc(maxBytes));\n    unsigned int *stdResult_h = static_cast<unsigned int*>(malloc(maxBytes));\n    assert(keys_h && cudaResult_h && stdResult_h && \"Host memory allocation failed!\");\n\n    // Allocate device memory once for maximum size\n    unsigned int *keysIn_d;\n    unsigned int *keysOut_d;\n    CUDA_CHECK(cudaMallocAsync(&keysIn_d, maxBytes, stream));\n    CUDA_CHECK(cudaMallocAsync(&keysOut_d, maxBytes, stream));\n\n    // Seed and random initialization values\n    srand(RANDOM_SEED);\n    for(int test = 0; test < NUM_TESTS; test++) {\n        const int numElements = TEST_SIZES[test];\n        const size_t bytes = static_cast<size_t>(numElements) * sizeof(unsigned int);\n\n        // Initialize test data\n        for(int i = 0; i < numElements; i++) {\n            keys_h[i] = rand() % MAX_RANDOM_VALUE;\n        }\n\n        // Copy original data for std::sort comparison\n        memcpy(stdResult_h, keys_h, bytes);\n\n        // Sort with std::sort for reference\n        std::sort(stdResult_h, stdResult_h + numElements);\n\n        // Copy data to device\n        CUDA_CHECK(cudaMemcpyAsync(keysIn_d, keys_h, bytes, cudaMemcpyHostToDevice, stream));\n\n        // Calculate grid dimensions for current array size\n        int numBlocks = (numElements + (BLOCK_SIZE * ITEMS_PER_THREAD) - 1) / (BLOCK_SIZE * ITEMS_PER_THREAD);\n        int gridSize = std::min(numBlocks, MAX_GRID_SIZE);\n\n        // Allocate global histogram storage once for the largest grid\n        // (moved to host only once outside the loop if desired)\n        static unsigned int *globalHistograms_d = nullptr;\n        if (!globalHistograms_d) {\n            size_t histSize = (static_cast<size_t>(MAX_GRID_SIZE) * RADIX_SIZE + RADIX_SIZE) * sizeof(unsigned int);\n            CUDA_CHECK(cudaMallocAsync(&globalHistograms_d, histSize, stream));\n        }\n\n        // Launch cooperative kernel\n        int numElementsCopy = numElements;\n        void *args[] = {&keysIn_d, &keysOut_d, &globalHistograms_d, &numElementsCopy};\n        cudaError_t err = cudaLaunchCooperativeKernel(\n            reinterpret_cast<void*>(k_radixSort), dim3(gridSize), dim3(BLOCK_SIZE), args, 0, stream);\n        assert(err == cudaSuccess);\n\n        // Handle buffer swapping to ensure final result is in keysOut_d\n        // After RADIX_PASSES (8, even number), result is in original src buffer (keysIn_d)\n        // We need the final result in keysOut_d as the output buffer\n        if(RADIX_PASSES % 2 == 0) {\n            CUDA_CHECK(cudaMemcpyAsync(keysOut_d, keysIn_d, bytes, cudaMemcpyDeviceToDevice, stream));\n        }\n\n        // Copy result back from device (final result should be in keysOut_d)\n        CUDA_CHECK(cudaMemcpyAsync(cudaResult_h, keysOut_d, bytes, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Validate results using assert\n        for(int i = 0; i < numElements; i++) {\n            assert(cudaResult_h[i] == stdResult_h[i]);\n        }\n    }\n\n    // Cleanup all allocated memory\n    CUDA_CHECK(cudaFreeAsync(keysIn_d, stream));\n    CUDA_CHECK(cudaFreeAsync(keysOut_d, stream));\n    cudaStreamSynchronize(stream);\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    free(keys_h);\n    free(cudaResult_h);\n    free(stdResult_h);\n}\n\n__global__ void k_radixSort(unsigned int *keysIn_d,\n                            unsigned int *keysOut_d,\n                            unsigned int *globalHistograms_d,\n                            int numElements) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/217", "date": "2025-07-30", "prompt": "Write a CUDA kernel to sort segments of an array independently by utilizing intra-warp thread communication. For simplicity, assume only equally sized segments of up to 128 elements are used. Ensure that an arbitrary number of threads per block functions correctly.\n\nThe signature of the CUDA kernel is __global__ void k_sortSegments(float *array_d, float *arrayOut_d, int segmentSize, int arraySize), where array_d is a pointer to the array containing multiple segments to be sorted, arrayOut_d is a pointer to the array with sorted segments, segmentSize represents the number of elements in each segment, and arraySize indicates the total number of elements in the array.\n\n>>> k_sortSegments({ 10.2f, -11.0f, -25.3f, 35.0f, -448.0f, -5.0f, -68.0f, -57.0f, -128.0f, -99955.0f, -20.0f, -211.0f, -312.0f, -0.1f, -14.5f }, arrayOut_d, 5, 15) -> arrayOut_d: { -448.0f, -25.3f, -11.0f, 10.2f, 35.0f, -99955.0f, -128.0f, -68.0f, -57.0f, -5.0f, -312.0f, -211.0f, -20.0f, -14.5f, -0.1f }\n>>> k_sortSegments({ 1230.0f, -123.0f, -200.0f, -300.0f, -4.1f, -5.2f, -6.7f, 7.8f, 8.1f, -19.0f, -190.0f, -1199.0f, -412.0f, -153.0f, -174.0f, -715.0f, -176.0f, -177.0f, -718.0f, -179.0f, -207.0f, -821.0f, -282.0f, -238.0f, -924.0f, -325.0f, -236.0f, -273.0f, -248.0f, -249.0f, -304.0f, -531.0f, -325.0f, -353.0f, -3400.0f, -3005.0f, -306.0f, -370.0f, -398.0f, -399.0f }, arrayOut_d, 10, 40) -> arrayOut_d: { -300.0f, -200.0f, -123.0f, -19.0f, -6.7f, -5.2f, -4.1f, 7.8f, 8.1f, 1230.0f, -1199.0f, -718.0f, -715.0f, -412.0f, -190.0f, -179.0f, -177.0f, -176.0f, -174.0f, -153.0f, -924.0f, -821.0f, -325.0f, -282.0f, -273.0f, -249.0f, -248.0f, -238.0f, -236.0f, -207.0f, -3400.0f, -3005.0f, -531.0f, -399.0f, -398.0f, -370.0f, -353.0f, -325.0f, -306.0f, -304.0f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <random>\n#include <cfloat>\n#include <cmath>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if(error != cudaSuccess) {                                 \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n__global__ void k_sortSegments(float *array_d, float *arrayOut_d, int segmentSize, int arraySize);\n\nvoid launch() {\n    // Algorithm-related constants.\n    constexpr int MAXIMUM_NUMBER_OF_SUB_ARRAYS = 10000;\n    constexpr int MAXIMUM_SIZE_OF_SUB_ARRAY = 128;\n    constexpr int MAXIMUM_ARRAY_SIZE = MAXIMUM_SIZE_OF_SUB_ARRAY * MAXIMUM_NUMBER_OF_SUB_ARRAYS;\n    constexpr int DETERMINISTIC_RANDOM_SEED = 42;\n    int deviceId = 0;\n    cudaDeviceProp properties;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&properties, deviceId));\n    int numThreadsPerBlock = 1000;\n    int numWarpsPerBlock = (numThreadsPerBlock + properties.warpSize - 1) / properties.warpSize;\n    int maximumNumBlocks = properties.maxBlocksPerMultiProcessor * properties.multiProcessorCount;\n    // Input array.\n    float *array_h = new float[MAXIMUM_ARRAY_SIZE];\n    // Output array.\n    float *arrayOut_h = new float[MAXIMUM_ARRAY_SIZE];\n    float *array_d;\n    float *arrayOut_d;\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    CUDA_CHECK(cudaMallocAsync(&array_d, MAXIMUM_ARRAY_SIZE * sizeof(float), stream));\n    CUDA_CHECK(cudaMallocAsync(&arrayOut_d, MAXIMUM_ARRAY_SIZE * sizeof(float), stream));\n    auto hToD = cudaMemcpyHostToDevice;\n    auto dToH = cudaMemcpyDeviceToHost;\n    \n    // Test 1: segment size = 5, number of segments = 3\n    {\n        int segmentSize = 5;\n        int numSegments = 3;\n        int arraySize = numSegments * segmentSize;\n        int numWarpsRequired = numSegments;\n        int numBlocksRequired = (numWarpsRequired + numWarpsPerBlock - 1) / numWarpsPerBlock;\n        int numBlocksUsed = (maximumNumBlocks < numBlocksRequired ? maximumNumBlocks : numBlocksRequired);\n        auto initializerList = { 10.2f, -11.0f, -25.3f, 35.0f, -448.0f, -5.0f, -68.0f, -57.0f, -128.0f, -99955.0f, -20.0f, -211.0f, -312.0f, -0.1f, -14.5f };\n        std::copy(initializerList.begin(), initializerList.end(), array_h);\n        // Copying array to device.\n        CUDA_CHECK(cudaMemcpyAsync(array_d, array_h, sizeof(float) * arraySize, hToD, stream));\n        void * args[4] = { (void*)&array_d, (void*)&arrayOut_d, (void*)&segmentSize, (void*)&arraySize };\n        dim3 gridDim(numBlocksUsed, 1, 1);\n        dim3 blockDim(numThreadsPerBlock, 1, 1);\n        // Block: (1000, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)&k_sortSegments, gridDim, blockDim, args, 0, stream));\n        // Copying result to host.\n        CUDA_CHECK(cudaMemcpyAsync(arrayOut_h, arrayOut_d, sizeof(float) * arraySize, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        int index = 0;\n        for(int i = 0; i < numSegments; i++) {\n            for(int j = 0; j < segmentSize - 1; j++) {\n                assert(arrayOut_h[index] < std::nextafter(arrayOut_h[index + 1], fabs(arrayOut_h[index + 1]) * 2.0f));\n                index++;\n            }\n            index++;\n        }\n    }\n    \n    // Test 2: segment size = 10, number of segments = 4\n    {\n        int segmentSize = 10;\n        int numSegments = 4;\n        int arraySize = numSegments * segmentSize;\n        int numWarpsRequired = numSegments;\n        int numBlocksRequired = (numWarpsRequired + numWarpsPerBlock - 1) / numWarpsPerBlock;\n        int numBlocksUsed = (maximumNumBlocks < numBlocksRequired ? maximumNumBlocks : numBlocksRequired);\n        auto initializerList = { 1230.0f, -123.0f, -200.0f, -300.0f, -4.1f, -5.2f, -6.7f, 7.8f, 8.1f, -19.0f, -190.0f, -1199.0f, -412.0f, -153.0f, -174.0f, -715.0f, -176.0f, -177.0f, -718.0f, -179.0f, -207.0f, -821.0f, -282.0f, -238.0f, -924.0f, -325.0f, -236.0f, -273.0f, -248.0f, -249.0f, -304.0f, -531.0f, -325.0f, -353.0f, -3400.0f, -3005.0f, -306.0f, -370.0f, -398.0f, -399.0f };\n        std::copy(initializerList.begin(), initializerList.end(), array_h);\n        // Copying array to device.\n        CUDA_CHECK(cudaMemcpyAsync(array_d, array_h, sizeof(float) * arraySize, hToD, stream));\n        void * args[4] = { (void*)&array_d, (void*)&arrayOut_d, (void*)&segmentSize, (void*)&arraySize };\n        dim3 gridDim(numBlocksUsed, 1, 1);\n        dim3 blockDim(numThreadsPerBlock, 1, 1);\n        // Block: (1000, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)&k_sortSegments, gridDim, blockDim, args, 0, stream));\n        // Copying result to host.\n        CUDA_CHECK(cudaMemcpyAsync(arrayOut_h, arrayOut_d, sizeof(float) * arraySize, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        int index = 0;\n        for(int i = 0; i < numSegments; i++) {\n            for(int j = 0; j < segmentSize - 1; j++) {\n                assert(arrayOut_h[index] < std::nextafter(arrayOut_h[index + 1], fabs(arrayOut_h[index + 1]) * 2.0f));\n                index++;\n            }\n            index++;\n        }\n    }\n    \n    // Test 3: segment size equals the maximum allowed size, number of segments equals the maximum allowed number, normalized random values.\n    {\n        int segmentSize = 11;\n        int numSegments = 1;\n        int arraySize = numSegments * segmentSize;\n        int numWarpsRequired = numSegments;\n        int numBlocksRequired = (numWarpsRequired + numWarpsPerBlock - 1) / numWarpsPerBlock;\n        int numBlocksUsed = (maximumNumBlocks < numBlocksRequired ? maximumNumBlocks : numBlocksRequired);\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_real_distribution<float> distribution(0.0f, 1.0f);\n        for(int i = 0; i < arraySize; i++) {\n            array_h[i] = distribution(generator);\n        }\n        // Copying array to device.\n        CUDA_CHECK(cudaMemcpyAsync(array_d, array_h, sizeof(float) * arraySize, hToD, stream));\n        void * args[4] = { (void*)&array_d, (void*)&arrayOut_d, (void*)&segmentSize, (void*)&arraySize };\n        dim3 gridDim(numBlocksUsed, 1, 1);\n        dim3 blockDim(numThreadsPerBlock, 1, 1);\n        // Block: (1000, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)&k_sortSegments, gridDim, blockDim, args, 0, stream));\n        // Copying result to host.\n        CUDA_CHECK(cudaMemcpyAsync(arrayOut_h, arrayOut_d, sizeof(float) * arraySize, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        int index = 0;\n        for(int i = 0; i < numSegments; i++) {\n            for(int j = 0; j < segmentSize - 1; j++) {\n                assert(arrayOut_h[index] < std::nextafter(arrayOut_h[index + 1], fabs(arrayOut_h[index + 1]) * 2.0f));\n                index++;\n            }\n            index++;\n        }\n    }\n    \n    // Test 4: segment size = 2, number of segments = 1000, large random values.\n    {\n        int segmentSize = 2;\n        int numSegments = 1000;\n        int arraySize = numSegments * segmentSize;\n        int numWarpsRequired = numSegments;\n        int numBlocksRequired = (numWarpsRequired + numWarpsPerBlock - 1) / numWarpsPerBlock;\n        int numBlocksUsed = (maximumNumBlocks < numBlocksRequired ? maximumNumBlocks : numBlocksRequired);\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_real_distribution<float> distribution(-1e30, 1e30);\n        for(int i = 0; i < arraySize; i++) {\n            array_h[i] = distribution(generator);\n        }\n        // Copying array to device.\n        CUDA_CHECK(cudaMemcpyAsync(array_d, array_h, sizeof(float) * arraySize, hToD, stream));\n        void * args[4] = { (void*)&array_d, (void*)&arrayOut_d, (void*)&segmentSize, (void*)&arraySize };\n        dim3 gridDim(numBlocksUsed, 1, 1);\n        dim3 blockDim(numThreadsPerBlock, 1, 1);\n        // Block: (1000, 1, 1)\n        // Grid: (32, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)&k_sortSegments, gridDim, blockDim, args, 0, stream));\n        // Copying result to host.\n        CUDA_CHECK(cudaMemcpyAsync(arrayOut_h, arrayOut_d, sizeof(float) * arraySize, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        int index = 0;\n        for(int i = 0; i < numSegments; i++) {\n            for(int j = 0; j < segmentSize - 1; j++) {\n                assert(arrayOut_h[index] < std::nextafter(arrayOut_h[index + 1], fabs(arrayOut_h[index + 1]) * 2.0f));\n                index++;\n            }\n            index++;\n        }\n    }\n    \n    // Test 5: segment size = 100, number of segments = 1500, inputs are randomly shuffled.\n    {\n        int segmentSize = 100;\n        int numSegments = 5000;\n        int arraySize = numSegments * segmentSize;\n        int numWarpsRequired = numSegments;\n        int numBlocksRequired = (numWarpsRequired + numWarpsPerBlock - 1) / numWarpsPerBlock;\n        int numBlocksUsed = (maximumNumBlocks < numBlocksRequired ? maximumNumBlocks : numBlocksRequired);\n        int index = 0;\n        for(int i = 0; i < numSegments; i++) {\n            for(int j = 0; j < segmentSize; j++) {\n                array_h[index++] = -j;\n            }\n            std::shuffle(array_h + index - segmentSize, array_h + index, std::default_random_engine(i));\n        }\n        // Copying array to device.\n        CUDA_CHECK(cudaMemcpyAsync(array_d, array_h, sizeof(float) * arraySize, hToD, stream));\n        void * args[4] = { (void*)&array_d, (void*)&arrayOut_d, (void*)&segmentSize, (void*)&arraySize };\n        dim3 gridDim(numBlocksUsed, 1, 1);\n        dim3 blockDim(numThreadsPerBlock, 1, 1);\n        // Block: (1000, 1, 1)\n        // Grid: (157, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)&k_sortSegments, gridDim, blockDim, args, 0, stream));\n        // Copying result to host.\n        CUDA_CHECK(cudaMemcpyAsync(arrayOut_h, arrayOut_d, sizeof(float) * arraySize, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        index = 0;\n        for(int i = 0; i < numSegments; i++) {\n            for(int j = 0; j < segmentSize - 1; j++) {\n                assert(arrayOut_h[index] < std::nextafter(arrayOut_h[index + 1], fabs(arrayOut_h[index + 1]) * 2.0f));\n                index++;\n            }\n            index++;\n        }\n    }\n    \n    // Test 6: segment size = 1, number of segments = 10, unique values equal to index + PI.\n    {\n        int segmentSize = 1;\n        int numSegments = 10;\n        int arraySize = numSegments * segmentSize;\n        int numWarpsRequired = numSegments;\n        int numBlocksRequired = (numWarpsRequired + numWarpsPerBlock - 1) / numWarpsPerBlock;\n        int numBlocksUsed = (maximumNumBlocks < numBlocksRequired ? maximumNumBlocks : numBlocksRequired);\n        constexpr float PI = 3.1415f;\n        for(int i = 0; i < numSegments; i++) {\n            array_h[i] = PI + cos(i);\n        }\n        // Copying array to device.\n        CUDA_CHECK(cudaMemcpyAsync(array_d, array_h, sizeof(float) * arraySize, hToD, stream));\n        void * args[4] = { (void*)&array_d, (void*)&arrayOut_d, (void*)&segmentSize, (void*)&arraySize };\n        dim3 gridDim(numBlocksUsed, 1, 1);\n        dim3 blockDim(numThreadsPerBlock, 1, 1);\n        // Block: (1000, 1, 1)\n        // Grid: (1, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)&k_sortSegments, gridDim, blockDim, args, 0, stream));\n        // Copying result to host.\n        CUDA_CHECK(cudaMemcpyAsync(arrayOut_h, arrayOut_d, sizeof(float) * arraySize, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        int index = 0;\n        for(int i = 0; i < numSegments; i++) {\n            for(int j = 0; j < segmentSize - 1; j++) {\n                assert(arrayOut_h[index] < std::nextafter(arrayOut_h[index + 1], fabs(arrayOut_h[index + 1]) * 2.0f));\n                index++;\n            }\n            index++;\n        }\n    }\n    \n    // Test 7: segment size equals the maximum allowed size, the number of segments equals the maximum allowed number, with one infinity value for each segment.\n    {\n        int segmentSize = MAXIMUM_SIZE_OF_SUB_ARRAY;\n        int numSegments = MAXIMUM_NUMBER_OF_SUB_ARRAYS;\n        int arraySize = numSegments * segmentSize;\n        int numWarpsRequired = numSegments;\n        int numBlocksRequired = (numWarpsRequired + numWarpsPerBlock - 1) / numWarpsPerBlock;\n        int numBlocksUsed = (maximumNumBlocks < numBlocksRequired ? maximumNumBlocks : numBlocksRequired);\n        for(int i = 0; i < arraySize; i++) {\n            array_h[i] = ((i % segmentSize == 0) ? std::numeric_limits<float>::infinity() : sin(i));\n        }\n        // Copying array to device.\n        CUDA_CHECK(cudaMemcpyAsync(array_d, array_h, sizeof(float) * arraySize, hToD, stream));\n        void * args[4] = { (void*)&array_d, (void*)&arrayOut_d, (void*)&segmentSize, (void*)&arraySize };\n        dim3 gridDim(numBlocksUsed, 1, 1);\n        dim3 blockDim(numThreadsPerBlock, 1, 1);\n        // Block: (1000, 1, 1)\n        // Grid: (313, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)&k_sortSegments, gridDim, blockDim, args, 0, stream));\n        // Copying result to host.\n        CUDA_CHECK(cudaMemcpyAsync(arrayOut_h, arrayOut_d, sizeof(float) * arraySize, dToH, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        int index = 0;\n        for(int i = 0; i < numSegments; i++) {\n            for(int j = 0; j < segmentSize - 1; j++) {\n                assert(arrayOut_h[index] < std::nextafter(arrayOut_h[index + 1], fabs(arrayOut_h[index + 1]) * 2.0f));\n                index++;\n            }\n            index++;\n        }\n    }\n    \n    CUDA_CHECK(cudaFreeAsync(array_d, stream));\n    CUDA_CHECK(cudaFreeAsync(arrayOut_d, stream));\n    delete [] array_h;\n    delete [] arrayOut_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_sortSegments(float *array_d, float *arrayOut_d, int segmentSize, int arraySize) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/218", "date": "2025-07-30", "prompt": "Write a CUDA kernel to implement insertion and reversal on an array leveraging the libcu++ library.\n\nThe signature of the function is __global__ void k_processArray(cuda::std::array<int,CAP>* arr_d, int* values, int numValues, bool doReverse), where CAP represents the maximum array size to be known at compile time for memory allocation, arr_d is your pre zeroed array<CAP> on device, values refers to the values being passed into the array, numValues represents the total number of elements to insert, doReverse represents the reverse operation of an array.\n\n>>> k_processArray(arr_d, {1, 2, 3}, 3, false) --> arr_d: ({1, 2, 3}) \n>>> k_processArray(arr_d, {1, 2, 3, 4}, 4, true) --> arr_d: ({4, 3, 2, 1}) \n", "cc_flags": "-std=c++17 -arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "\n#include <iostream>\n#include <vector>\n#include <cassert>\n#include <cuda_runtime.h>\n#include <cuda/std/array>\n#include <algorithm>\n\n// Error checking macro\n#define CUDA_CHECK(call) {                                                 \\\n    cudaError_t error = call;                                              \\\n    if (error != cudaSuccess) {                                            \\\n        fprintf(stderr, \"CUDA error at %s:%d - %s\\n\",                      \\\n                __FILE__, __LINE__, cudaGetErrorString(error));            \\\n        exit(EXIT_FAILURE);                                                \\\n    }                                                                      \\\n}\n#define TEMPLATE_CAP template<int CAP>\n\nTEMPLATE_CAP\n__global__ void k_processArray(cuda::std::array<int,CAP>* arr_d, int* values, int numValues, bool doReverse);\n\n// Host test harness\nstruct TestCase {\n    std::vector<int> insertVals;\n    bool doReverse;\n    int arrayLength;\n    std::vector<int> expectedArray;\n};\n\nvoid launch() {\n\n    const int testCaseCount = 8;\n    constexpr int insertValsSizes[testCaseCount] = {3, 0, 10, 4, 1, 5, 2, 12};\n    constexpr int MAX_VALUE = *std::max_element(insertValsSizes, insertValsSizes + testCaseCount);\n    constexpr int CAP = (MAX_VALUE == 0) ? 1 : MAX_VALUE;\n    \n    std::vector<TestCase> tests = {\n       // Basic insertion test\n        {\n            {1, 2, 3},      // Insert values Add 1, 2, 3 to array\n            false,          // No reverse operation\n            3,              // Array length 3 elements\n            {1, 2, 3}       //Expected Array        \n        },\n        \n        // Empty array test\n        {\n            {},             // No insertions --> array remains empty\n            false,          // No reverse operation\n            0,               // Array length 0 elements\n            {}\n        },\n        \n        // Large insertion test\n        {\n            {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, // Insert 10 values\n            false,          // No reverse operation\n            10,              // Array length 10 elements\n            {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}  // Expected array\n        },\n        \n        // Insertion with reverse test\n        {\n            {1, 2, 3, 4},   // Insert values Add 1, 2, 3, 4 to array & reverse\n            true,           // Reverse operation \n            4,               // Array length 4 elements\n            {4, 3, 2, 1}    // Expected array \n        },\n        \n        // Single element with reverse\n        {\n            {42},           // Insert single value\n            true,           // Reverse operation there will be no effect\n            1,               // Array length 1 element\n            {42}            // Expected array\n        },\n        \n        // Multiple elements with reverse\n        {\n            {10, 20, 30, 40, 50}, // Insert 5 values\n            true,           // Reverse operation \n            5,                 // Array length 5 elements\n            {50, 40, 30, 20, 10}  // Expected array\n        },\n\n        // Two element reverse test \n        {\n            {100, 200},     // Insert 2 values \n            true,           // Reverse operation\n            2,              // Array length 2 elements\n            {200, 100}      // Expected array\n        },\n        \n        // Large array reverse test\n        {\n            {12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1}, // Insert 12 values\n            true,           // Reverse operation\n            12,              // Array length 12 elements\n            {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}  // Expected array \n        }\n    };\n\n    // Create CUDA stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Device properties\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop,0));\n    \n    // Dynamic thread configuration based on CAP\n    int blockSize = prop.maxThreadsPerBlock;\n    int gridSize = (CAP + blockSize - 1) / blockSize;\n\n    // Execute each test case with proper memory management\n    for (size_t t = 0; t < tests.size(); ++t) {\n        auto &tc = tests[t];\n        int nInsert = tc.insertVals.size();\n\n        // Allocate device memory for libcu++ array and length counter\n        cuda::std::array<int,CAP>* arr_d;\n        CUDA_CHECK(cudaMallocAsync(&arr_d, sizeof(*arr_d), stream));\n        CUDA_CHECK(cudaMemsetAsync(arr_d->data(), 0, sizeof(*arr_d), stream));\n\n        // Allocate and copy input data arrays to device\n        int *insert_d = nullptr;\n\n        if(nInsert) { \n            CUDA_CHECK(cudaMallocAsync(&insert_d, nInsert*sizeof(int), stream));\n            CUDA_CHECK(cudaMemcpyAsync(insert_d, tc.insertVals.data(), nInsert*sizeof(int), cudaMemcpyHostToDevice, stream)); \n        }\n\n        // Prepare kernel arguments \n        void* args[] = { &arr_d, &insert_d, &nInsert, &tc.doReverse };\n        \n        // Launch kernel\n        CUDA_CHECK(cudaLaunchKernel((void*)k_processArray<CAP>, dim3(gridSize), dim3(blockSize), args,0,stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n\n        // Copy results back to host and validate against expected values\n        int finalLen = std::min(nInsert, CAP);\n\n        // Verify test results\n        assert(finalLen == tc.arrayLength);\n        \n        std::vector<int> resultArray(finalLen);\n        if(finalLen > 0) {\n            CUDA_CHECK(cudaMemcpy(resultArray.data(), arr_d->data(), finalLen*sizeof(int), cudaMemcpyDeviceToHost));\n            \n            // Assert that array contents match expected values\n            assert(resultArray == tc.expectedArray);\n        }else{\n            assert(tc.expectedArray.empty());\n        }\n\n        // Cleanup\n        CUDA_CHECK(cudaFreeAsync(arr_d,stream));\n        CUDA_CHECK(cudaFreeAsync(insert_d,stream));\n    }\n\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n// Kernel array operations with libcu++\nTEMPLATE_CAP\n__global__ void k_processArray(cuda::std::array<int,CAP>* arr_d, int* values, int numValues, bool doReverse) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/219", "date": "2025-07-30", "prompt": "Write a CUDA kernel to calculate the histogram for grayscale single-channel 8-bit color values of an image. Each 8-bit color value ranges from 0 to 255, and the histogram should have 256 bins. Utilize block privatization, aggregation of identical values, and coarsening (with a factor of 4) to optimize the histogram calculation.\n\nThe signature of the CUDA kernel is __global__ void k_calculateBlockPrivatizedColorHistogram(uint8_t* image_d, uint32_t* histogram_d, uint32_t numPixels), where image_d is a pointer to an array of 8-bit colors, histogram_d is a pointer to an array of 32-bit bins for the histogram, and numPixels is the number of pixels.\n\n>>> k_calculateBlockPrivatizedColorHistogram({ 0, 1, 0, 2, 0, 3, 0, 4, 5, 6 }, histogram_d, 10) -> histogram_d: { 4, 1, 1, 1, 1, 1, 1, and the remaining bins are 0 }\n>>> k_calculateBlockPrivatizedColorHistogram({ 7, 7, 7, 8, 8, 8, 9 }, histogram_d, 7) -> histogram_d: { 0, 0, 0, 0, 0, 0, 0, 3, 3, 1, and the remaining bins are 0 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if (error != cudaSuccess) {                                \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Number of bins for the 8Bit color histogram.\nconstexpr int NUM_HISTOGRAM_BINS = 256;\nconstexpr int NUM_BYTES_PER_INTEGER = sizeof(uint32_t);\nconstexpr int COARSENING_FACTOR = NUM_BYTES_PER_INTEGER;\n__global__ void k_calculateBlockPrivatizedColorHistogram(uint8_t* image_d, uint32_t* histogram_d, uint32_t numPixels);\n\nvoid launch() {\n    // Maximum number of image pixels.\n    constexpr size_t MAX_NUM_PIXELS = 10000;\n    // Amount of padding needed to read the last NUM_BYTES_PER_INTEGER elements without overflowing the array.\n    constexpr int NUM_PADDING = NUM_BYTES_PER_INTEGER - (MAX_NUM_PIXELS % NUM_BYTES_PER_INTEGER);\n    constexpr int NUM_TESTS = 7;\n    constexpr int DETERMINISTIC_RANDOM_SEED = 42;\n    constexpr int FIVE = 5;\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Allocating host memory.\n    uint8_t* image_h = new uint8_t[MAX_NUM_PIXELS];\n    uint32_t* histogram_h = new uint32_t[NUM_HISTOGRAM_BINS];\n    uint32_t* expectedHistogram_h = new uint32_t[NUM_HISTOGRAM_BINS];\n    uint8_t* testImage_h = new uint8_t[MAX_NUM_PIXELS * (size_t)NUM_TESTS];\n    uint32_t* testNumPixels_h = new uint32_t[NUM_TESTS];\n    // Allocating device memory.\n    uint8_t* image_d;\n    uint32_t* histogram_d;\n    CUDA_CHECK(cudaMallocAsync(&image_d, MAX_NUM_PIXELS + NUM_PADDING, stream));\n    CUDA_CHECK(cudaMallocAsync(&histogram_d, NUM_HISTOGRAM_BINS * sizeof(uint32_t), stream));\n    CUDA_CHECK(cudaMemsetAsync(image_d, 0, MAX_NUM_PIXELS + NUM_PADDING, stream));\n    // Preparing test data.\n    int testIndex = 0;\n    // Test 1: There are four inputs with a value of 0, and only a single existence for the values 1, 2, 3, 4, 5, and 6.\n    {\n        uint32_t numPixels = 10;\n        testNumPixels_h[testIndex] = numPixels;\n        std::initializer_list<uint8_t> input = { 0, 1, 0, 2, 0, 3, 0, 4, 5, 6 };\n        std::copy(input.begin(), input.end(), &testImage_h[testIndex * MAX_NUM_PIXELS]);\n        testIndex++;\n    }\n    // Test 2: There are three elements with a value of 7, three elements with a value of 8, and a single value of 9.\n    {\n        uint32_t numPixels = 7;\n        testNumPixels_h[testIndex] = numPixels;\n        std::initializer_list<uint8_t> input = { 7, 7, 7, 8, 8, 8, 9 };\n        std::copy(input.begin(), input.end(), &testImage_h[testIndex * MAX_NUM_PIXELS]);\n        testIndex++;\n    }\n    // Test 3: Single pixel.\n    {\n        uint32_t numPixels = 1;\n        testNumPixels_h[testIndex] = numPixels;\n        testImage_h[testIndex * MAX_NUM_PIXELS] = 5;\n        testIndex++;\n    }\n    // Test 4: All pixels with the same value.\n    {\n        uint32_t numPixels = 1000;\n        testNumPixels_h[testIndex] = numPixels;\n        for (uint32_t i = 0; i < numPixels; i++) {\n            testImage_h[testIndex * MAX_NUM_PIXELS + i] = 10;\n        }\n        testIndex++;\n    }\n    // Test 5: Nearly equal distribution across the histogram bins.\n    {\n        uint32_t numPixels = 2000;\n        testNumPixels_h[testIndex] = numPixels;\n        for (uint32_t i = 0; i < numPixels; i++) {\n            testImage_h[testIndex * MAX_NUM_PIXELS + i] = (uint8_t)i;\n        }\n        testIndex++;\n    }\n    // Test 6: Random colors.\n    {\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_int_distribution<int> distribution(0, NUM_HISTOGRAM_BINS - 1);\n        uint32_t numPixels = MAX_NUM_PIXELS;\n        testNumPixels_h[testIndex] = numPixels;\n        for (uint32_t i = 0; i < numPixels; i++) {\n            testImage_h[testIndex * MAX_NUM_PIXELS + i] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Test 7: There is a high count in the first bin of the histogram, while there is a random distribution across the other bins.\n    {\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_int_distribution<int> distribution(0, NUM_HISTOGRAM_BINS - 1);\n        uint32_t numPixels = MAX_NUM_PIXELS - 1;\n        testNumPixels_h[testIndex] = numPixels;\n        for (uint32_t i = 0; i < numPixels; i++) {\n            testImage_h[testIndex * MAX_NUM_PIXELS + i] = (((i % FIVE) == 0) ? 0 : distribution(generator));\n        }\n        testIndex++;\n    }\n    // Running the tests iteratively.\n    for (int test = 0; test < NUM_TESTS; test++)\n    {\n        uint32_t numPixels = testNumPixels_h[test];\n        for (int i = 0; i < NUM_HISTOGRAM_BINS; i++) {\n            expectedHistogram_h[i] = 0;\n        }\n        for (uint32_t i = 0; i < numPixels; i++) {\n            image_h[i] = testImage_h[test * MAX_NUM_PIXELS + i];\n            expectedHistogram_h[image_h[i]]++;\n        }\n        CUDA_CHECK(cudaMemcpyAsync(image_d, image_h, numPixels, cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemsetAsync(histogram_d, 0, sizeof(uint32_t) * NUM_HISTOGRAM_BINS, stream));\n        void* args[3] = { &image_d, &histogram_d, &numPixels };\n        size_t sharedMem = sizeof(uint32_t) * NUM_HISTOGRAM_BINS;\n        \n        cudaDeviceProp deviceProperties;\n        CUDA_CHECK(cudaGetDeviceProperties(&deviceProperties, 0));\n        int numBlocksOfGpu = deviceProperties.maxBlocksPerMultiProcessor * deviceProperties.multiProcessorCount;\n        int blockSize = 256;\n        int requiredBlocks = (numPixels + blockSize * COARSENING_FACTOR - 1) / (blockSize * COARSENING_FACTOR);\n        // Keep grid size to the minimum required to reduce contention for atomicAdd on global memory.\n        int usedBlocks = requiredBlocks < numBlocksOfGpu ? requiredBlocks : numBlocksOfGpu;\n        // Grid: (usedBlocks, 1, 1)\n        // Block: (blockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_calculateBlockPrivatizedColorHistogram, dim3(usedBlocks, 1, 1), dim3(blockSize, 1, 1), args, sharedMem, stream));\n        CUDA_CHECK(cudaMemcpyAsync(histogram_h, histogram_d, sizeof(uint32_t) * NUM_HISTOGRAM_BINS, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < NUM_HISTOGRAM_BINS; i++) {\n            assert(expectedHistogram_h[i] == histogram_h[i]);\n        }\n    }\n    \n    // Deallocating device memory.\n    CUDA_CHECK(cudaFreeAsync(image_d, stream));\n    CUDA_CHECK(cudaFreeAsync(histogram_d, stream));\n    // Deallocating host memory.\n    delete [] image_h;\n    delete [] histogram_h;\n    delete [] expectedHistogram_h;\n    delete [] testImage_h;\n    delete [] testNumPixels_h;\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_calculateBlockPrivatizedColorHistogram(uint8_t* image_d, uint32_t* histogram_d, uint32_t numPixels) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}
{"task_id": "CUDA/220", "date": "2025-07-30", "prompt": "Write a CUDA kernel with high occupancy to find all integers from array1 in array2 and count the occurrences of each number in array2. Enhances occupancy by utilizing an appropriate amount of shared memory for tiling and balancing the number of threads with the size of the shared memory allocation.\n\nThe signature of the CUDA kernel is __global__ void k_countOccurrences(int * array1_d, int * array2_d, int len1, int len2, int * count_d), where array1_d is a pointer to the array of integers to be counted within another array, array2_d is a pointer to the array of integers used for comparisons for each integer in the array pointed to by array1_d, len1 is the size of array1_d, len2 is the size of array2_d, and count_d is a pointer to array of counts.\n\n>>> k_countOccurrences({ 0, 1, 2, ..., 299998, 299999 }, { repeats 0, 1 for a total of 70 elements }, 300000, 70) -> count_d: { 35, 35, the rest are 0 }\n>>> k_countOccurrences({ index % 100 for all elements }, { 0, 1, 2, ..., 9 }, 200000, 10) -> count_d: { all elements are 1 }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80 --expt-relaxed-constexpr", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <algorithm>\n#include <random>\n#include <stdio.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if (error != cudaSuccess) {                                \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// Test settings.\nconstexpr int MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT = 1000000;\nconstexpr int MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST = 1000;\nconstexpr int ALLOCATION_SIZE_FOR_ELEMENTS_TO_COUNT = MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT * sizeof(int);\nconstexpr int ALLOCATION_SIZE_FOR_ELEMENTS_TO_COMPARE_AGAINST = MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST * sizeof(int);\nconstexpr int NUM_TESTS = 7;\n\n__global__ void k_countOccurrences(int * array1_d, \n                                   int * array2_d, \n                                   int len1, \n                                   int len2,\n                                   int * count_d);\n\nvoid launch() {\n    constexpr int DETERMINISTIC_RANDOM_SEED = 42;\n    int deviceId = 0;\n    CUDA_CHECK(cudaSetDevice(deviceId));\n    cudaDeviceProp deviceProperties;\n    CUDA_CHECK(cudaGetDeviceProperties_v2(&deviceProperties, deviceId));\n    \n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Allocating host buffers.\n    int * array1_h = new int[MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT];\n    int * array2_h = new int[MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST];\n    int * count_h = new int[MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT];\n    int * testNumIntegersToCount_h = new int[NUM_TESTS];\n    int * testNumIntegersToCompare_h = new int[NUM_TESTS];\n    int * testIntegersToCount_h = new int[NUM_TESTS * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT];\n    int * testIntegersToCompare_h = new int[NUM_TESTS * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST];\n    int * testResult_h = new int[NUM_TESTS * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT];\n    // Allocating device buffers.\n    int * array1_d;\n    int * array2_d;\n    int * count_d;\n    CUDA_CHECK(cudaMallocAsync(&array1_d, ALLOCATION_SIZE_FOR_ELEMENTS_TO_COUNT, stream));\n    CUDA_CHECK(cudaMallocAsync(&array2_d, ALLOCATION_SIZE_FOR_ELEMENTS_TO_COMPARE_AGAINST, stream));\n    CUDA_CHECK(cudaMallocAsync(&count_d, ALLOCATION_SIZE_FOR_ELEMENTS_TO_COUNT, stream));\n    // Preparing the tests.\n    int testIndex = 0;\n    // Test 1\n    {\n        int counts = 300000;\n        int comparisons = 70;\n        testNumIntegersToCount_h[testIndex] = counts;\n        testNumIntegersToCompare_h[testIndex] = comparisons;\n        for (int i = 0; i < counts; i++) {\n            testIntegersToCount_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = i;\n        }\n        for (int i = 0; i < comparisons; i++) {\n            testIntegersToCompare_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST] = i % 2;\n        }\n        for (int i = 0; i < counts; i++) {\n            testResult_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = (i < 2 ? comparisons / 2 : 0);\n        }\n        testIndex++;\n    }\n    // Test 2\n    {\n        int counts = 200000;\n        int comparisons = 10;\n        testNumIntegersToCount_h[testIndex] = counts;\n        testNumIntegersToCompare_h[testIndex] = comparisons;\n        for (int i = 0; i < counts; i++) {\n            testIntegersToCount_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = i % comparisons;\n        }\n        for (int i = 0; i < comparisons; i++) {\n            testIntegersToCompare_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST] = i;\n        }\n        for (int i = 0; i < counts; i++) {\n            testResult_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = 1;\n        }\n        testIndex++;\n    }\n    // Test 3\n    {\n        int counts = 400000;\n        int comparisons = 100;\n        testNumIntegersToCount_h[testIndex] = counts;\n        testNumIntegersToCompare_h[testIndex] = comparisons;\n        for (int i = 0; i < counts; i++) {\n            testIntegersToCount_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = i * 4;\n        }\n        for (int i = 0; i < comparisons; i++) {\n            testIntegersToCompare_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST] = i / 4;\n        }\n        for (int i = 0; i < counts; i++) {\n            testResult_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = i * 4 < comparisons / 4 ? 4 : 0;\n        }\n        testIndex++;\n    }\n    // Test 4\n    {\n        int counts = 500000;\n        int comparisons = 10;\n        testNumIntegersToCount_h[testIndex] = counts;\n        testNumIntegersToCompare_h[testIndex] = comparisons;\n        for (int i = 0; i < counts; i++) {\n            testIntegersToCount_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = 1;\n        }\n        for (int i = 0; i < comparisons; i++) {\n            testIntegersToCompare_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST] = i;\n        }\n        for (int i = 0; i < counts; i++) {\n            testResult_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = (testIntegersToCount_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] == 1 ? 1 : 0);\n        }\n        testIndex++;\n    }\n    // Test 5\n    {\n        int counts = 700000;\n        int comparisons = 70;\n        testNumIntegersToCount_h[testIndex] = counts;\n        testNumIntegersToCompare_h[testIndex] = comparisons;\n        for (int i = 0; i < counts; i++) {\n            testIntegersToCount_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = i;\n        }\n        for (int i = 0; i < comparisons; i++) {\n            testIntegersToCompare_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST] = i * 10000;\n        }\n        for (int i = 0; i < counts; i++) {\n            testResult_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = ((i / 10000) * 10000 == i) ? 1 : 0;\n        }\n        testIndex++;\n    }\n    // Test 6\n    {\n        int counts = MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT;\n        int comparisons = 1;\n        testNumIntegersToCount_h[testIndex] = counts;\n        testNumIntegersToCompare_h[testIndex] = comparisons;\n        for (int i = 0; i < counts; i++) {\n            testIntegersToCount_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = 0;\n        }\n        for (int i = 0; i < comparisons; i++) {\n            testIntegersToCompare_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST] = 0;\n        }\n        for (int i = 0; i < counts; i++) {\n            testResult_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = 1;\n        }\n        testIndex++;\n    }\n    // Test 7\n    {\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_int_distribution<int> distribution(-1000, 1000);\n        \n        int counts = MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT;\n        int comparisons = MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST;\n        testNumIntegersToCount_h[testIndex] = counts;\n        testNumIntegersToCompare_h[testIndex] = comparisons;\n        for (int i = 0; i < counts; i++) {\n            testResult_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = 0;\n        }\n        \n        for (int i = 0; i < comparisons; i++) {\n            testIntegersToCompare_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST] = distribution(generator);\n        }\n        for (int i = 0; i < counts; i++) {\n            testIntegersToCount_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] = distribution(generator);\n            for (int j = 0; j < comparisons; j++) {\n                int value1 = testIntegersToCount_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT];\n                int value2 = testIntegersToCompare_h[j + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST];\n                testResult_h[i + testIndex * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT] += (value1 == value2);\n            }\n        }\n        \n        testIndex++;\n    }\n    // Processing the test inputs.\n    for (int testId = 0; testId < NUM_TESTS; testId++)\n    {\n        int len1 = testNumIntegersToCount_h[testId];\n        int len2 = testNumIntegersToCompare_h[testId];\n        for (int i = 0; i < len1; i++) {\n            array1_h[i] = testIntegersToCount_h[i + testId * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT];\n        }\n        for (int i = 0; i < len2; i++) {\n            array2_h[i] = testIntegersToCompare_h[i + testId * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COMPARE_AGAINST];\n        }\n        CUDA_CHECK(cudaMemcpyAsync(array1_d, array1_h, len1 * sizeof(int), cudaMemcpyHostToDevice, stream));\n        CUDA_CHECK(cudaMemcpyAsync(array2_d, array2_h, len2 * sizeof(int), cudaMemcpyHostToDevice, stream));\n        void * args[5] = { (void*)&array1_d, (void*)&array2_d, (void*)&len1, (void*)&len2, (void*)&count_d };\n        \n        int minGridSize;\n        int blockSize;\n        \n        // Computing the minimum grid size and maximum potential block size that allows high occupancy.\n        CUDA_CHECK(cudaOccupancyMaxPotentialBlockSizeVariableSMem(&minGridSize, &blockSize, (void*)k_countOccurrences, [](int blockSize) { \n            return blockSize * sizeof(int);\n        }));\n        size_t sizeOfRequiredSharedMemory = blockSize * sizeof(int);\n        // Using the minimum grid size that can ensure maximum occupancy.\n        // Grid: (minGridSize, 1, 1)\n        // Block: (blockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_countOccurrences, dim3(minGridSize, 1, 1), dim3(blockSize, 1, 1), args, sizeOfRequiredSharedMemory, stream));\n        CUDA_CHECK(cudaMemcpyAsync(count_h, count_d, len1 * sizeof(int), cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < len1; i++) {\n            assert(count_h[i] == testResult_h[i + testId * MAXIMUM_NUMBER_OF_ELEMENTS_TO_COUNT]);\n        }\n    }\n    \n    CUDA_CHECK(cudaFreeAsync(array1_d, stream));\n    CUDA_CHECK(cudaFreeAsync(array2_d, stream));\n    CUDA_CHECK(cudaFreeAsync(count_d, stream));\n    delete [] array1_h;\n    delete [] array2_h;\n    delete [] count_h;\n    delete [] testNumIntegersToCount_h;\n    delete [] testNumIntegersToCompare_h;\n    delete [] testIntegersToCount_h;\n    delete [] testIntegersToCompare_h;\n    delete [] testResult_h;\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n\n__global__ void k_countOccurrences(int * array1_d, \n                                   int * array2_d, \n                                   int len1, \n                                   int len2, \n                                   int * count_d) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.2"}
{"task_id": "CUDA/221", "date": "2025-07-30", "prompt": "Write a CUDA kernel that uses shared memory to perform a 1D convolution with a 201-element-wide filter representing a symmetric Gaussian curve. Utilize shared memory for input tiling and coefficients. The halo region for each tile should include a 100-element extension on both the left and right sides of the center block, where the center has the same number of elements as the number of threads in the CUDA block. For simplicity, treat input array indices outside the valid range as zero.\n\nThe signature of the CUDA kernel is __global__ void k_runConvolution(float* coefficients_d, float sumOfCoefficients, float* inputArray_d, float* outputArray_d, int arraySize), where coefficients_d is a pointer to an array of float elements as coefficients, sumOfCoefficients is the sum of all coefficients, inputArray_d is a pointer to an array of float elements used as the input for convolution, outputArray_d is a pointer to an array of float elements used as the output for convolution, and arraySize is the number of elements in inputArray_d.\n\n>>> k_runConvolution({ value = expf(-((index - 100) * (index - 100)) / 2266.8892f) for index = [0 to 200] }, 84.1507f, { 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f }, outputArray_d, 10) -> outputArray_d: { 0.117358f, 0.11777f, 0.11808f, 0.118287f, 0.11839f, 0.11839f, 0.118287f, 0.11808f, 0.11777f, 0.117358f }\n>>> k_runConvolution({ value = expf(-((index - 100) * (index - 100)) / 2266.8892f) for index = [0 to 200] }, 84.1507f, { 3.0f, 4.0f, 5.0f, 6.0f, 7.0f }, outputArray_d, 5) -> outputArray_d: { 0.296093f, 0.296589f, 0.296824f, 0.296798f, 0.296511f }\n", "cc_flags": "-arch=sm_90a -arch=sm_89 -arch=sm_80", "ld_flags": "", "declaration": "#undef NDEBUG\n#include <assert.h>\n#include <stdio.h>\n#include <algorithm>\n#include <random>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <device_launch_parameters.h>\n#define CUDA_CHECK(call) {                                     \\\n    cudaError_t error = call;                                  \\\n    if (error != cudaSuccess) {                                \\\n        fprintf(stderr, \"CUDA error at %s: %d - %s \\n\",        \\\n                __FILE__, __LINE__, cudaGetErrorString(error));\\\n        exit(EXIT_FAILURE);                                    \\\n    }                                                          \\\n}\n\n// The filter is symmetrical, with 100 elements on the left and 100 elements on the right of a selected index.\nconstexpr int FILTER_WIDTH = 201;\nconstexpr int HALF_OF_FILTER_WIDTH = FILTER_WIDTH / 2;\nconstexpr int FILTER_CENTER_INDEX = 100;\nconstexpr float FILTER_STANDARD_DEVIATION = (FILTER_WIDTH - FILTER_CENTER_INDEX) / 3.0f;\nconstexpr float SQUARED_STD_MULT_2 = 2.0f * FILTER_STANDARD_DEVIATION * FILTER_STANDARD_DEVIATION;\n__global__ void k_runConvolution(float* coefficients_d, float sumOfCoefficients, float* inputArray_d, float* outputArray_d, int arraySize);\n\nvoid launch() {\n    constexpr int MAX_ARRAY_ELEMENTS = 100000;\n    constexpr int NUM_TESTS = 7;\n    constexpr int DETERMINISTIC_RANDOM_SEED = 42;\n    constexpr float ERROR_TOLERANCE = 1e-3f;\n    cudaDeviceProp deviceProperties;\n    int deviceIndex = 0;\n    CUDA_CHECK(cudaSetDevice(deviceIndex));\n    CUDA_CHECK(cudaGetDeviceProperties(&deviceProperties, deviceIndex));\n    \n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n    \n    // Allocating host memory.\n    float* coefficients_h = new float[FILTER_WIDTH];\n    float* array_h = new float[MAX_ARRAY_ELEMENTS];\n    float* testArray_h = new float[NUM_TESTS * MAX_ARRAY_ELEMENTS];\n    int* testArraySize_h = new int[NUM_TESTS];\n    \n    // Allocating device memory.\n    float* inputArray_d;\n    float* outputArray_d;\n    float* coefficients_d;\n    CUDA_CHECK(cudaMallocAsync(&inputArray_d, sizeof(float) * MAX_ARRAY_ELEMENTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&outputArray_d, sizeof(float) * MAX_ARRAY_ELEMENTS, stream));\n    CUDA_CHECK(cudaMallocAsync(&coefficients_d, sizeof(float) * FILTER_WIDTH, stream));\n    \n    // Initializing the coefficients based on the heights from a Gaussian curve.\n    float sumOfCoefficients = 0.0f;\n    for (int index = 0; index < FILTER_WIDTH; index++) {\n        float coefficient = expf(-((index - FILTER_CENTER_INDEX) * (index - FILTER_CENTER_INDEX)) / SQUARED_STD_MULT_2);\n        coefficients_h[index] = coefficient;\n        sumOfCoefficients += coefficient;\n    }\n    CUDA_CHECK(cudaMemcpyAsync(coefficients_d, coefficients_h, sizeof(float) * FILTER_WIDTH, cudaMemcpyHostToDevice, stream));\n    int testIndex = 0;\n    // Preparing the test data.\n    // Test 1:\n    {\n        int arraySize = 10;\n        testArraySize_h[testIndex] = arraySize;\n        std::initializer_list<float> testData = { 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0 };\n        std::copy(testData.begin(), testData.end(), &testArray_h[testIndex * MAX_ARRAY_ELEMENTS]);\n        testIndex++;\n    }\n    // Test 2:\n    {\n        int arraySize = 5;\n        testArraySize_h[testIndex] = arraySize;\n        std::initializer_list<float> testData = { 3.0, 4.0, 5.0, 6.0, 7.0 };\n        std::copy(testData.begin(), testData.end(), &testArray_h[testIndex * MAX_ARRAY_ELEMENTS]);\n        testIndex++;\n    }\n    // Test 3:\n    {\n        int arraySize = 1000;\n        testArraySize_h[testIndex] = arraySize;\n        for (int i = 0; i < arraySize; i++) {\n            testArray_h[testIndex * MAX_ARRAY_ELEMENTS + i] = sinf(i * 0.001f);\n        }\n        testIndex++;\n    }\n    // Test 4:\n    {\n        int arraySize = 1001;\n        std::mt19937 generator(DETERMINISTIC_RANDOM_SEED);\n        std::uniform_real_distribution<float> distribution(0.0f, 1.0f);\n        testArraySize_h[testIndex] = arraySize;\n        for (int i = 0; i < arraySize; i++) {\n            testArray_h[testIndex * MAX_ARRAY_ELEMENTS + i] = distribution(generator);\n        }\n        testIndex++;\n    }\n    // Test 5:\n    {\n        int arraySize = 1002;\n        testArraySize_h[testIndex] = arraySize;\n        for (int i = 0; i < arraySize; i++) {\n            testArray_h[testIndex * MAX_ARRAY_ELEMENTS + i] = 1.0f / (i + 1);\n        }\n        testIndex++;\n    }\n    // Test 6:\n    {\n        int arraySize = 1;\n        testArraySize_h[testIndex] = arraySize;\n        for (int i = 0; i < arraySize; i++) {\n            testArray_h[testIndex * MAX_ARRAY_ELEMENTS + i] = 41.0f;\n        }\n        testIndex++;\n    }\n    // Test 7:\n    {\n        int arraySize = MAX_ARRAY_ELEMENTS;\n        testArraySize_h[testIndex] = arraySize;\n        for (int i = 0; i < arraySize; i++) {\n            testArray_h[testIndex * MAX_ARRAY_ELEMENTS + i] = ((i % 2) ? 1.0f : -1.0f);\n        }\n        testIndex++;\n    }\n    \n    // Iterating the tests.\n    for (int test = 0; test < testIndex; test++) {\n        int arraySize = testArraySize_h[test];\n        for (int i = 0; i < arraySize; i++) {\n            array_h[i] = testArray_h[test * MAX_ARRAY_ELEMENTS + i];\n        }\n        CUDA_CHECK(cudaMemcpyAsync(inputArray_d, array_h, sizeof(float) * arraySize, cudaMemcpyHostToDevice, stream));\n        void* args[] = { &coefficients_d, &sumOfCoefficients, &inputArray_d, &outputArray_d, &arraySize };\n        \n        int maxResidentBlocks = deviceProperties.maxBlocksPerMultiProcessor * deviceProperties.multiProcessorCount;\n        // Using 1024 threads per block, for output tile size of 1024 elements and input tile size of 1024 + 200 elements.\n        int blockSize = 1024;\n        // Required shared memory size for convolution filter coefficients.\n        size_t sharedMemForFilter = FILTER_WIDTH * sizeof(float);\n        // Required shared memory size for the tiling, which includes the halo region of size (FILTER_WIDTH - 1) and the interior of size blockSize.\n        size_t sharedMemForTile = (blockSize + (FILTER_WIDTH - 1)) * sizeof(float);\n        size_t sharedMem = sharedMemForFilter + sharedMemForTile;\n        \n        int requiredNumberOfBlocks = (arraySize + blockSize - 1) / blockSize;\n        // Limiting the grid size accordingly with the computational resources of the GPU.\n        int gridSize = requiredNumberOfBlocks < maxResidentBlocks ? requiredNumberOfBlocks : maxResidentBlocks;\n        // Grid: (gridSize, 1, 1)\n        // Block: (blockSize, 1, 1)\n        CUDA_CHECK(cudaLaunchKernel((void*)k_runConvolution, dim3(gridSize, 1, 1), dim3(blockSize, 1, 1), args, sharedMem, stream));\n        CUDA_CHECK(cudaMemcpyAsync(array_h, outputArray_d, sizeof(float) * arraySize, cudaMemcpyDeviceToHost, stream));\n        CUDA_CHECK(cudaStreamSynchronize(stream));\n        for (int i = 0; i < arraySize; i++) {\n            float expectedResult = 0.0f;\n            for (int j = 0; j < FILTER_WIDTH; j++) {\n                float coefficient = coefficients_h[j];\n                int index = i - HALF_OF_FILTER_WIDTH + j;\n                float element = (index >= 0 && index < arraySize) ? testArray_h[test * MAX_ARRAY_ELEMENTS + index] : 0.0f;\n                expectedResult = fmaf(coefficient, element, expectedResult);\n            }\n            expectedResult /= sumOfCoefficients;\n            assert(fabsf(expectedResult - array_h[i]) / fabs(expectedResult) < ERROR_TOLERANCE);\n        }\n    }\n    \n    // Releasing device memory.\n    CUDA_CHECK(cudaFreeAsync(inputArray_d, stream));\n    CUDA_CHECK(cudaFreeAsync(outputArray_d, stream));\n    CUDA_CHECK(cudaFreeAsync(coefficients_d, stream));\n    // Releasing host memory.\n    delete [] coefficients_h;\n    delete [] array_h;\n    delete [] testArray_h;\n    delete [] testArraySize_h;\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n}\n\n__global__ void k_runConvolution(float* coefficients_d, float sumOfCoefficients, float* inputArray_d, float* outputArray_d, int arraySize) {\n", "test": "int main() {\n    launch();\n}\n", "example_test": "    @example_test\n", "cuda_toolkit": "12.3"}